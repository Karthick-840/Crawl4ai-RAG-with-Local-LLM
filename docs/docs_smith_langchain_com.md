## https://docs.smith.langchain.com/search

[Skip to main content](https://docs.smith.langchain.com/search#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/search)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
# Search the documentation
[](https://www.algolia.com/)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old

[Skip to main content](https://docs.smith.langchain.com/old#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * Quick Start


On this page
# Getting started with LangSmith
## Introduction[‚Äã](https://docs.smith.langchain.com/old#introduction "Direct link to Introduction")
[LangSmith](https://smith.langchain.com/) is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!
## Install LangSmith[‚Äã](https://docs.smith.langchain.com/old#install-langsmith "Direct link to Install LangSmith")
We offer Python and Typescript SDKs for all your LangSmith needs.
  * Python
  * TypeScript


```
pip install -U langsmith
```

```
yarn add langchain langsmith
```

## Create an API key[‚Äã](https://docs.smith.langchain.com/old#create-an-api-key "Direct link to Create an API key")
To create an API key head to the [Settings page](https://smith.langchain.com/settings). Then click **Create API Key.**
## Setup your environment[‚Äã](https://docs.smith.langchain.com/old#setup-your-environment "Direct link to Setup your environment")
  * Shell


```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key># This example uses OpenAI, but you can use any LLM provider of choiceexport OPENAI_API_KEY=<your-openai-api-key>
```

## Log your first trace[‚Äã](https://docs.smith.langchain.com/old#log-your-first-trace "Direct link to Log your first trace")
We provide multiple ways to log traces to LangSmith. Below, we'll highlight how to use `traceable`. See more on the [Integrations](https://docs.smith.langchain.com/old/tracing/integrations) page.
  * Python
  * TypeScript


```
import openaifrom langsmith import wrappers, traceable# Auto-trace LLM calls in-contextclient = wrappers.wrap_openai(openai.Client())@traceable# Auto-trace this functiondefpipeline(user_input:str):  result = client.chat.completions.create(    messages=[{"role":"user","content": user_input}],    model="gpt-4o-mini")return result.choices[0].message.contentpipeline("Hello, world!")# Out: Hello there! How can I assist you today?
```

```
import{ OpenAI }from"openai";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers";// Auto-trace LLM calls in-contextconst client =wrapOpenAI(newOpenAI());// Auto-trace this functionconst pipeline =traceable(async(user_input)=>{const result =await client.chat.completions.create({    messages:[{ role:"user", content: user_input }],    model:"gpt-4o-mini",});return result.choices[0].message.content;});awaitpipeline("Hello, world!")// Out: Hello there! How can I assist you today?
```

  * View a [sample output trace](https://smith.langchain.com/public/b37ca9b1-60cd-4a2a-817e-3c4e4443fdc0/r).
  * Learn more about tracing on the [tracing page](https://docs.smith.langchain.com/old/tracing).


## Create your first evaluation[‚Äã](https://docs.smith.langchain.com/old#create-your-first-evaluation "Direct link to Create your first evaluation")
Evalution requires a system to test, [data](https://docs.smith.langchain.com/old/evaluation/faq) to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator.
  * Python
  * TypeScript


```
from langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name ="Sample Dataset"dataset = client.create_dataset(dataset_name, description="A sample dataset in LangSmith.")client.create_examples( inputs=[{"postfix":"to LangSmith"},{"postfix":"to Evaluations in LangSmith"},], outputs=[{"output":"Welcome to LangSmith"},{"output":"Welcome to Evaluations in LangSmith"},], dataset_id=dataset.id,)# Define your evaluatordefexact_match(run, example):return{"score": run.outputs["output"]== example.outputs["output"]}experiment_results = evaluate(lambdainput:"Welcome "+input['postfix'],# Your AI system goes here data=dataset_name,# The data to predict and grade over evaluators=[exact_match],# The evaluators to score the results experiment_prefix="sample-experiment",# The name of the experiment metadata={"version":"1.0.0","revision_id":"beta"},)
```

```
import{ Client, Run, Example }from'langsmith';import{ runOnDataset }from'langchain/smith';import{ EvaluationResult }from'langsmith/evaluation';const client =newClient();// Define dataset: these are your test casesconst datasetName ="Sample Dataset";const dataset =await client.createDataset(datasetName,{ description:"A sample dataset in LangSmith."});await client.createExamples({ inputs:[{ postfix:"to LangSmith"},{ postfix:"to Evaluations in LangSmith"},], outputs:[{ output:"Welcome to LangSmith"},{ output:"Welcome to Evaluations in LangSmith"},], datasetId: dataset.id,});// Define your evaluatorconst exactMatch =async({ run, example }:{ run: Run; example?: Example;}):Promise<EvaluationResult>=>{return{   key:'exact_match',   score: run.outputs?.output === example?.outputs?.output ?1:0,};};awaitrunOnDataset((input:{ postfix:string})=>({ output:`Welcome ${input.postfix}`}),// Your AI system goes here datasetName,// The data to predict and grade over{   evaluationConfig:{ customEvaluators:[exactMatch]},   projectMetadata:{     version:"1.0.0",     revision_id:"beta",},});
```

  * See more on the [evaluation quick start page](https://docs.smith.langchain.com/old/evaluation/quickstart).


## Next Steps[‚Äã](https://docs.smith.langchain.com/old#next-steps "Direct link to Next Steps")
Check out the following sections to learn more about LangSmith:
  * **[User Guide](https://docs.smith.langchain.com/old/user_guide)** : Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.
  * **[Pricing](https://docs.smith.langchain.com/old/pricing)** : Learn about the pricing model for LangSmith.
  * **[Self-Hosting](https://docs.smith.langchain.com/self_hosting)** : Learn about self-hosting options for LangSmith.
  * **[Tracing](https://docs.smith.langchain.com/old/tracing)** : Learn about the tracing capabilities of LangSmith.
  * **[Evaluation](https://docs.smith.langchain.com/old/evaluation)** : Learn about the evaluation capabilities of LangSmith.


## Additional Resources[‚Äã](https://docs.smith.langchain.com/old#additional-resources "Direct link to Additional Resources")
  * **[LangSmith Cookbook](https://github.com/langchain-ai/langsmith-cookbook/tree/main)** : A collection of tutorials and end-to-end walkthroughs using LangSmith.
  * **[LangChain Python](https://python.langchain.com/)** : Docs for the Python LangChain library.
  * **[LangChain Python API Reference](https://api.python.langchain.com/)** : documentation to review the core APIs of LangChain.
  * **[LangChain JS](https://js.langchain.com/docs/)** : Docs for the TypeScript LangChain library
  * **[Discord](https://discord.gg/6adMQxSpJS)** : Join us on our Discord to discuss all things LangChain!


## FAQ[‚Äã](https://docs.smith.langchain.com/old#faq "Direct link to FAQ")
### How do I migrate projects between organizations?[‚Äã](https://docs.smith.langchain.com/old#how-do-i-migrate-projects-between-organizations "Direct link to How do I migrate projects between organizations?")
Currently we do not support project migration betwen organizations. While you can manually imitate this by reading and writing runs and datasets using the SDK (see the querying runs and traces guide [here](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)), it will be fastest to create a new project within your organization and go from there.
### Why aren't my runs aren't showing up in my project?[‚Äã](https://docs.smith.langchain.com/old#why-arent-my-runs-arent-showing-up-in-my-project "Direct link to Why aren't my runs aren't showing up in my project?")
If you aren't seeing any warnings when running your application, it may be that you are still using an API key from your "personal" organization. Check your most recent runs there to confirm by selecting your `Personal` tenant in the [settings](https://smith.langchain.com/settings) page and then viewing your [projects](https://smith.langchain.com/projects).
If you're still running into issues, please reach out to us at support@langchain.dev.
### My team deals with sensitive data that cannot be logged. How can I ensure that only my team can access it?[‚Äã](https://docs.smith.langchain.com/old#my-team-deals-with-sensitive-data-that-cannot-be-logged-how-can-i-ensure-that-only-my-team-can-access-it "Direct link to My team deals with sensitive data that cannot be logged. How can I ensure that only my team can access it?")
If you are interested in a private deployment of LangSmith or if you need to self-host, please reach out to us at sales@langchain.dev. Self-hosting LangSmith requires an annual enterprise license that also comes with support and formalized access to the LangChain team.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[NextUser Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Introduction](https://docs.smith.langchain.com/old#introduction)
  * [Install LangSmith](https://docs.smith.langchain.com/old#install-langsmith)
  * [Create an API key](https://docs.smith.langchain.com/old#create-an-api-key)
  * [Setup your environment](https://docs.smith.langchain.com/old#setup-your-environment)
  * [Log your first trace](https://docs.smith.langchain.com/old#log-your-first-trace)
  * [Create your first evaluation](https://docs.smith.langchain.com/old#create-your-first-evaluation)
  * [Next Steps](https://docs.smith.langchain.com/old#next-steps)
  * [Additional Resources](https://docs.smith.langchain.com/old#additional-resources)
  * [FAQ](https://docs.smith.langchain.com/old#faq)
    * [How do I migrate projects between organizations?](https://docs.smith.langchain.com/old#how-do-i-migrate-projects-between-organizations)
    * [Why aren't my runs aren't showing up in my project?](https://docs.smith.langchain.com/old#why-arent-my-runs-arent-showing-up-in-my-project)
    * [My team deals with sensitive data that cannot be logged. How can I ensure that only my team can access it?](https://docs.smith.langchain.com/old#my-team-deals-with-sensitive-data-that-cannot-be-logged-how-can-i-ensure-that-only-my-team-can-access-it)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/category/prompt-hub

[Skip to main content](https://docs.smith.langchain.com/old/category/prompt-hub#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
    * [Quick Start](https://docs.smith.langchain.com/old/hub/quickstart)
    * [Developer Setup](https://docs.smith.langchain.com/old/hub/dev-setup)
    * [FAQs](https://docs.smith.langchain.com/old/hub/faq)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * Prompt Hub


# Prompt Hub
Discover, share, and version control prompts in the Prompt Hub.
## [üìÑÔ∏è Quick StartWhat is LangChain Hub?](https://docs.smith.langchain.com/old/hub/quickstart)## [üìÑÔ∏è Developer SetupThis guide will continue from the hub quickstart, using the Python or TypeScript SDK to interact with the hub instead of the Playground UI.](https://docs.smith.langchain.com/old/hub/dev-setup)## [üìÑÔ∏è FAQsWhat is LangChain Hub?](https://docs.smith.langchain.com/old/hub/faq)[PreviousOptimizing a Classifier](https://docs.smith.langchain.com/old/monitoring/use_cases/classification)[NextQuick Start](https://docs.smith.langchain.com/old/hub/quickstart)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/category/proxy

[Skip to main content](https://docs.smith.langchain.com/old/category/proxy#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/category/proxy)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
    * [Quick Start](https://docs.smith.langchain.com/old/proxy/quickstart)
    * [Use LangSmith Proxy with Azure OpenAI](https://docs.smith.langchain.com/old/proxy/azure_openai)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * Proxy


# Proxy
Proxy your LLM calls to automatically Cache and Trace.
## [üìÑÔ∏è Quick StartWhat is the LangSmith Proxy](https://docs.smith.langchain.com/old/proxy/quickstart)## [üìÑÔ∏è Use LangSmith Proxy with Azure OpenAIPrerequisites](https://docs.smith.langchain.com/old/proxy/azure_openai)[PreviousFAQs](https://docs.smith.langchain.com/old/hub/faq)[NextQuick Start](https://docs.smith.langchain.com/old/proxy/quickstart)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/category/self-hosting

[Skip to main content](https://docs.smith.langchain.com/old/category/self-hosting#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/category/self-hosting)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)
    * [Kubernetes](https://docs.smith.langchain.com/old/self_hosting/kubernetes)
    * [Docker](https://docs.smith.langchain.com/old/self_hosting/docker)
    * [Usage](https://docs.smith.langchain.com/old/self_hosting/usage)
    * [Release Notes (Self-Hosted)](https://docs.smith.langchain.com/old/self_hosting/release_notes)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * Self-Hosting


# Self-Hosting
Self-hosting LangSmith requires an enterprise license. Check out the guides below for more information.
## [üìÑÔ∏è KubernetesSelf-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment.](https://docs.smith.langchain.com/old/self_hosting/kubernetes)## [üìÑÔ∏è DockerSelf-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment.](https://docs.smith.langchain.com/old/self_hosting/docker)## [üìÑÔ∏è UsageThis guide will walk you through the process of using your self-hosted instance of LangSmith.](https://docs.smith.langchain.com/old/self_hosting/usage)## [üìÑÔ∏è Release Notes (Self-Hosted)If you are updating directly from LangSmith v0.1.x and you wish to retain access to run data in the Langsmith UI after updating, you must first update to v0.2.x and perform a data migration. Updating directly from v0.1.x to v0.3.x or later will result in data loss as the runs table in postgres will be dropped when deploying LangSmith v0.3.x or higher. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md](https://docs.smith.langchain.com/old/self_hosting/release_notes)[PreviousPricing](https://docs.smith.langchain.com/old/pricing)[NextKubernetes](https://docs.smith.langchain.com/old/self_hosting/kubernetes)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation

[Skip to main content](https://docs.smith.langchain.com/old/evaluation#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/evaluation)**.
  * [](https://docs.smith.langchain.com/)
  * Evaluation


On this page
# Evaluation Overview
## What are evaluations?[‚Äã](https://docs.smith.langchain.com/old/evaluation#what-are-evaluations "Direct link to What are evaluations?")
Evaluations allow you to understand the performance of your LLM application over time. At its core, an evaluation is a function that takes in a set of inputs and outputs from your chain, agent, or model, and returns a score (or multiple scores). This score may be based on comparing the outputs with reference outputs (e.g. with string matching or using an LLM as a judge). However, there are also evaluators that don't require a reference output - for example, one that checks if the output is valid JSON, which is a common requirement in LLM applications. LangSmith allows you to run evaluations on your application via `Datasets`, which are made up of `Examples`.
## Components of an evaluation pipeline[‚Äã](https://docs.smith.langchain.com/old/evaluation#components-of-an-evaluation-pipeline "Direct link to Components of an evaluation pipeline")
The following diagram outlines the building blocks for evaluations in LangSmith. `Datasets` define the inputs over which you run your chain, model, or agent (the `Task`), and optionally the reference outputs against which your evaluator will compare the outputs of your `Task`. These datasets can be from any number of sources - you might manually curate them, collect them from user input/feedback, or generate them via LLM. Your `Evaluator` can be any arbitrary function which returns a score based on the inputs and outputs of your `Task`, and the reference output if desired. You can also use [one of LangSmith's off-the-shelf evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations) to get started quickly!
![LangSmith Primitives](https://docs.smith.langchain.com/assets/images/langsmith_summary-63ca1dc96ab90a7883dd848947156d41.png)
## Datasets[‚Äã](https://docs.smith.langchain.com/old/evaluation#datasets "Direct link to Datasets")
`Datasets` are collections of `Examples`, the core building block for the evaluation workflow in LangSmith. Examples provide the inputs over which you will be running your pipeline, and, if applicable, the outputs that you will be comparing against. All examples in a given dataset should follow the same schema. Examples contain an `"inputs"` dict and an `"output"` dict, along with (optionally) a `metadata` dict.
![Example](https://docs.smith.langchain.com/assets/images/sample_langsmith_example-11e8761ca770dfdd43b6ff86b11fbd80.png)
An Example in the LangSmith UI
A single run of all your example inputs through your `Task` is called an `Experiment`. In LangSmith, you can easily view all the experiments that are associated with your dataset, and track your application's performance over time!
![Dataset](https://docs.smith.langchain.com/assets/images/sample_langsmith_dataset-4c99c4dc666017e2677a8669f36c0119.png)
A Dataset in the LangSmith UI
### Creating Datasets[‚Äã](https://docs.smith.langchain.com/old/evaluation#creating-datasets "Direct link to Creating Datasets")
Datasets in LangSmith can be created in two main ways:
  * [In the LangSmith SDK](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#how-to-manage-datasets-programmatically) with `create_dataset`.
  * [In the LangSmith UI](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets) by clicking "New Dataset" from the [LangSmith datasets page](https://smith.langchain.com/datasets). These can be uploaded as a CSV, or you can manually create examples in the UI.


### Types of Datasets[‚Äã](https://docs.smith.langchain.com/old/evaluation#types-of-datasets "Direct link to Types of Datasets")
Dataset types communicate common input and output schemas. There are three types of datasets in LangSmith: `kv`, `llm`, and `chat`. `kv` datasets are the default type, and are sufficient for almost all use-cases. `llm` and `chat` datasets can be useful to conveniently export datasets into known fine-tuning formats.
  * `kv`: In `kv` datasets, inputs and outputs can be arbitrary key-value pairs. These are useful when evaluating chains and agents that require multiple inputs or that return multiple outputs. The tradeoff with these datasets is that running evaluations on them can be a bit more involved. If there are multiple keys, you will have to manually specify the `prepare_data` function in any off-the-shelf evaluators so they know what information to consider in generating a score.
  * `llm`: `llm` datasets correspond to the string inputs and outputs from the "completion" style LLMS (string in, string out). The `"inputs"` dictionary contains a single `"input"` key mapped to a single prompt string. Similarly, the `"outputs"` dictionary contains a single `"output"` key mapped to a single response string.
  * `chat`: `chat` datasets correspond to messages and generations from LLMs that expect structured "chat" messages as inputs and outputs. Each example row expects an `"inputs"` dictionary containing a single `"input"` key mapped to a list of serialized chat messages. The `"outputs"` dictionary contains a single `"output"` key mapped to a single list of serialized chat messages.


## Evaluators[‚Äã](https://docs.smith.langchain.com/old/evaluation#evaluators "Direct link to Evaluators")
Evaluators are functions that help score how well your system did on a particular example. When running an evaluation, your example inputs are run through your `Task` to produce `Runs`, which are then passed into your evaluator along with the `Example`. The function then returns an `EvaluationResult`, which specifies your metric name and score. Evaluations in LangSmith are run via the `evaluate()` function. The following diagram gives an overview of the data flow in an evaluation:
![LangSmith Evaluations](https://docs.smith.langchain.com/assets/images/langsmith_app_flow-280b326b870274532732334b899040bb.png)
The inputs to an evaluator consist of:
  1. An `Example` - the inputs for your pipeline and optionally the reference outputs or labels
  2. A `Run` - observed output gathered from running the inputs through the `Task`


An evaluator will then return an `EvaluationResult` (or similarly shaped dictionary), which is made up of:
  * `key`: The name the metric being evaluated
  * `score`: The value of the metric on this example
  * `comment`: the reasoning trajectory or other string information motivating the score


### Types of Evaluators[‚Äã](https://docs.smith.langchain.com/old/evaluation#types-of-evaluators "Direct link to Types of Evaluators")
The evaluator itself can be any arbitrary function. There are a few different types of evaluators that are commonly used:
  * **Heuristics** : A heuristic evaluator is a hard-coded function that does some computation to determine a score. For example, you might write an evaluator that checks whether the output of the system is an empty string, or determines if it is valid JSON. These would be considered **reference-free** evaluators, as they don't consider any example output when making their decision. You might also want to check that the output of the system matches the reference output exactly, which would be considered a **ground truth** evaluator because it compares the output to a reference. See [How to create custom evaluators](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators).
  * **LLM-as-judge** : An LLM-as-judge evaluator uses an LLM to score system output. For example, you might want to check whether your system is outputting offensive content. This is **reference-free** , as there is no comparison to an example output. You might also want to check whether the system output has the same meaning as the example output, which would be a **ground truth** evaluator. To get started with LLM-as-a-judge, try out LangSmith's [off-the-shelf evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)!
  * **Human** : You can also evaluate your runs manually. This can be done in LangSmith [via the SDK](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback#capturing-feedback-programmatically), or [in the LangSmith UI](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback#annotating-traces-with-feedback).


## Next steps[‚Äã](https://docs.smith.langchain.com/old/evaluation#next-steps "Direct link to Next steps")
To get started with code, check out the [Quick Start Guide](https://docs.smith.langchain.com/old/evaluation/quickstart).
If you want to learn how to accomplish a particular task, check out our comprehensive [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
For a higher-level set of recommendations on how to think about testing and evaluating your LLM app, check out the [evaluation recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations) page.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/evaluation%3E).
[PreviousFew-shot prompting with LangSmith datasets](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets)[NextQuick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
  * [What are evaluations?](https://docs.smith.langchain.com/old/evaluation#what-are-evaluations)
  * [Components of an evaluation pipeline](https://docs.smith.langchain.com/old/evaluation#components-of-an-evaluation-pipeline)
  * [Datasets](https://docs.smith.langchain.com/old/evaluation#datasets)
    * [Creating Datasets](https://docs.smith.langchain.com/old/evaluation#creating-datasets)
    * [Types of Datasets](https://docs.smith.langchain.com/old/evaluation#types-of-datasets)
  * [Evaluators](https://docs.smith.langchain.com/old/evaluation#evaluators)
    * [Types of Evaluators](https://docs.smith.langchain.com/old/evaluation#types-of-evaluators)
  * [Next steps](https://docs.smith.langchain.com/old/evaluation#next-steps)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation/faq

[Skip to main content](https://docs.smith.langchain.com/old/evaluation/faq#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation/faq)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
      * [Create an Evaluator](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)
      * [Use Off-the-Shelf Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)
      * [Regression Testing](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing)
      * [Unit Test](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing)
      * [Manage Datasets](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)
      * [Version Datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)
      * [Run Experiments in Browser (no code)](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)
      * [Synthetic Data for Evaluation](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * How-To Guides


# How-To Guides
In this section you will find guides for how to do specific evaluation related things.
**Datasets**
  * [How to manage datasets in the app](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)
  * [How to manage datasets programmatically](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#manage-datasets-programmatically)
  * [How to list datasets from the client](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-datasets-from-the-client)
  * [How to version datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)
  * [How to list datapoints from the client](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-examples-from-the-client)
  * [How to use synthetic data](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)


**Evaluators**
  * [How to create custom evaluators](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)
  * [How to use off-the-shelf LangChain evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)


**Experiments**
  * [How to run experiments in the prompt playground](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/evaluation/faq%3E).
[PreviousQuick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)[NextCreate an Evaluator](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators

[Skip to main content](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
      * [Create an Evaluator](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)
      * [Use Off-the-Shelf Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)
      * [Regression Testing](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing)
      * [Unit Test](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing)
      * [Manage Datasets](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)
      * [Version Datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)
      * [Run Experiments in Browser (no code)](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)
      * [Synthetic Data for Evaluation](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
  * Create an Evaluator


On this page
# Evaluators
In this guide, you will create custom evaluators to grade your LLM system. An evaluator can apply any logic you want, returning a numeric `score` associated with a `key`.
Most evaluators are applied on a `run` level, scoring each prediction individually. Some `summary_evaluators` can be applied on a `experiment` level, letting you score and aggregate metrics across multiple runs.
Evaluators take in a `Run` and an `Example`:
#### Run Object[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#run-object "Direct link to Run Object")
The key `Run` object fields are as follows:
  * `outputs`: `Dict[str, Any]` - The outputs of your pipeline
  * `inputs`: `Dict[str, Any]` - The inputs to your pipeline. These are the same as the inputs in the example.
  * `child_runs`: `List[Run]` - If your pipeline has nested steps, these can be accessed and used in your evaluator


#### Example Object[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#example-object "Direct link to Example Object")
The key `Example` object fields are as follows:
  * `outputs`: `Dict[str, Any]` - The reference labels or other context found in your dataset
  * `inputs`: `Dict[str, Any]` - The inputs to your pipeline
  * `metadata`: `Dict[str, Any]` - Any other metadata you have stored in that example within the dataset


## Run Evaluators[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#run-evaluators "Direct link to Run Evaluators")
Run evaluators are applied to each prediction of your pipeline. The common automated evaluator types are:
  1. Simple Heuristics: Checking for regex matches, presence/absence of certain words or code, etc.
  2. AI-assisted: Instruct an "LLM-as-judge" to grade the output of a run based on the prediction and reference answer (or retrieved context).


We will demonstrate some simple ones below.
#### Example 1: Exact Match Evaluator[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#example-1-exact-match-evaluator "Direct link to Example 1: Exact Match Evaluator")
Let's start with a simple evaluator that checks if the model's output exactly matches the reference answer.
```
from langsmith.schemas import Example, Rundefexact_match(run: Run, example: Example)->dict:  reference = example.outputs["answer"]  prediction = run.outputs["output"]  score = prediction.lower()== reference.lower()return{"key":"exact_match","score": score}
```

Let's break this down:
  * The evaluator function accepts a `Run` and `Example` and returns a dictionary with the evaluation key and score. The run contains the full trace of your pipeline, and the example contains the inputs and outputs for this data point. If your dataset contains labels, they are found in the `example.outputs` dictionary, which is kept separate to keep your model from cheating.
  * In our dataset, the outputs have an "answer" key that contains the reference answer. Your pipeline generates predictions as a dictionary with an "output" key.
  * It compares the prediction and reference (case-insensitive) and returns a dictionary with the evaluation key and score.


You can use this evaluator directly in the `evaluate` function:
```
from langsmith.evaluation import evaluateevaluate(<your prediction function>,  data="<dataset_name>",  evaluators=[exact_match],)
```

#### Example 2: Parametrizing your evaluator[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#example-2-parametrizing-your-evaluator "Direct link to Example 2: Parametrizing your evaluator")
You may want to parametrize your evaluator as a class. This is useful when you need to pass additional parameters to the evaluator.
```
from langsmith.evaluation import evaluatefrom langsmith.schemas import Example, RunclassBlocklistEvaluator:def__init__(self, blocklist:list[str]):    self.blocklist = blocklistdef__call__(    self, run: Run, example: Example |None=None)->dict:    model_outputs = run.outputs["output"]    score =notany([word in model_outputs for word in self.blocklist])return{"key":"blocklist","score": score}evaluate(<your prediction function>,  data="<dataset_name>",  evaluators=[BlocklistEvaluator(blocklist=["bad","words"])],)
```

#### Example 3: Evaluating nested traces[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#example-3-evaluating-nested-traces "Direct link to Example 3: Evaluating nested traces")
While most evaluations are applied to the inputs and outputs of your system, you can also evaluate all of the subcomponents that are traced within your pipeline.
This is possible by stepping through the `run` object and collecting the outputs of each component.
As a simple example, let's assume you want to evaluate the expected tools that are invoked in a pipeline.
```
from langsmith.evaluation import evaluatefrom langsmith.schemas import Example, Rundefevaluate_trajectory(run: Run, example: Example)->dict:# collect the tools on level 1 of the trace tree  steps =[child.name for child in run.child_runs if child.run_type =="tool"]  expected_steps = example.outputs["expected_tools"]  score =len(set(steps)&set(expected_steps))/len(set(steps)|set(expected_steps))return{"key":"tools","score": score}
```

This lets you grade the performance of intermediate steps in your pipeline.
Note: the example above assumes tools are properly typed in the trace tree.
#### Example 3: Structured Output[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#example-3-structured-output "Direct link to Example 3: Structured Output")
With function calling, it has become easier than ever to generate feedback metrics using LLMs as a judge simply by specifying a Pydantic schema for the output.
Below is an example (in this case using OpenAI's tool calling functionality) to evaluate RAG app faithfulness.
```
import jsonfrom typing import Listimport openaifrom langsmith.schemas import Example, Runfrom pydantic import BaseModel, Fieldopenai_client = openai.Client()classPropositions(BaseModel):  propositions: List[str]= Field(    description="The factual propositions generated by the model")classFaithfulnessScore(BaseModel):  reasoning:str= Field(description="The reasoning for the faithfulness score")  score:booldeffaithfulness(run: Run, example: Example)->dict:# Assumes your RAG app includes the prediction in the "output" key in its response  response:str= run.outputs["output"]# Assumes your RAG app includes the retrieved docs as a "context" key in the outputs# If not, you can fetch from the child_runs of the run object  retrieved_docs:list= run.outputs["context"]  formatted_docs ="\n".join([str(doc)for doc in retrieved_docs])  extracted = openai_client.chat.completions.create(    model="gpt-4-turbo-preview",    messages=[{"role":"user","content":"Extract all factual propositions from the following text:\n\n"f"```\n{response}\n```",},],    tools=[{"type":"function","function":{"name":"Propositions","description":"Use to record each factual assertion.","parameters": Propositions.model_json_schema(),},}],    tool_choice={"type":"function","function":{"name":"Propositions"}},)  propositions =[    propfor tc in extracted.choices[0].message.tool_callsfor prop in json.loads(tc.function.arguments)["propositions"]]  scores, reasoning =[],[]  tools =[{"type":"function","function":{"name":"FaithfulnessScore","description":"Use to score how faithful the propositions are to the docs.","parameters": FaithfulnessScore.model_json_schema(),},}]for proposition in propositions:    faithfulness_completion = openai_client.chat.completions.create(      model="gpt-4-turbo-preview",      messages=[{"role":"user","content":"Grade whether the proposition can be logically concluded"f" from the docs:\n\nProposition: {proposition}\nDocs:\n"f"```\n{formatted_docs}\n```",},],      tools=tools,      tool_choice={"type":"function","function":{"name":"FaithfulnessScore"}},)    faithfulness_args = json.loads(      faithfulness_completion.choices[0].message.tool_calls[0].function.arguments)    scores.append(faithfulness_args["score"])    reasoning.append(faithfulness_args["reasoning"])  average_score =sum(scores)/len(scores)if scores elseNone  comment ="\n".join(reasoning)return{"key":"faithfulness","score": average_score,"comment": comment}
```

#### Example 4: Returning Multiple Scores[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#example-4-returning-multiple-scores "Direct link to Example 4: Returning Multiple Scores")
A single evaluator can return multiple scores. An example of when this might be useful is if you are using tool calling for an LLM-as-judge to extract multiple metrics in a single API call.
```
import jsonimport openaifrom langsmith.schemas import Example, Runfrom pydantic import BaseModel, Field# Initialize the OpenAI clientopenai_client = openai.Client()classScores(BaseModel):  correctness_reasoning:str= Field(description="The reasoning for the correctness score")  correctness:float= Field(description="The score for the correctness of the prediction")  conciseness_reasoning:str= Field(description="The reasoning for the conciseness score")  conciseness:float= Field(description="The score for the conciseness of the prediction")defmultiple_scores(run: Run, example: Example)->dict:  reference = example.outputs["answer"]  prediction = run.outputs["output"]  messages =[{"role":"user","content":f"Reference: {reference}\nPrediction: {prediction}"},]  tools =[{"type":"function","function":{"name":"Scores","description":"Use to evaluate the correctness and conciseness of the prediction.","parameters": Scores.model_json_schema(),},}]# Generating the chat completion with structured output  completion = openai_client.chat.completions.create(    model="gpt-4-turbo-preview",    messages=messages,    tools=tools,    tool_choice={"type":"function","function":{"name":"Scores"}},)# Extracting structured scores from the completion  scores_args = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)return{"results":[# Provide the key, score and other relevant information for each metric{"key":"correctness","score": scores_args["correctness"],"comment": scores_args["correctness_reasoning"]},{"key":"conciseness","score": scores_args["conciseness"],"comment": scores_args["conciseness_reasoning"]}]}
```

#### Example 5: Perplexity Evaluator[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#example-5-perplexity-evaluator "Direct link to Example 5: Perplexity Evaluator")
The flexibility of the functional interface means you can easly apply evaluators from any other libraries. For instance, you may want to use statistical measures such as [`perplexity`](https://huggingface.co/spaces/evaluate-metric/perplexity) to grade your run output. Below is an example using the [evaluate](https://huggingface.co/docs/evaluate/index) package by HuggingFace, which contains numerous commonly used metrics. Start by installing the `evaluate` package by running `pip install evaluate`.
```
from evaluate import loadfrom langsmith.schemas import Example, Runfrom langsmith.evaluation import RunEvaluatorclassPerplexityEvaluator(RunEvaluator):def__init__(self, prediction_key: Optional[str]=None, model_id:str="gpt-2"):    self.prediction_key = prediction_key    self.model_id = model_id    self.metric_fn = load("perplexity", module_type="metric")defevaluate_run(    self, run: Run, example: Example)->dict:if run.outputs isNone:raise ValueError("Run outputs cannot be None")    prediction = run.outputs[self.prediction_key]    results = self.metric_fn.compute(      predictions=[prediction], model_id=self.model_id)    ppl = results["perplexities"][0]return{"key":"Perplexity","score": ppl}
```

Let's break down what the `PerplexityEvaluator` is doing:
  * **Initialize** : In the constructor, we're setting up a few properties that will be needed later on. 
    * `prediction_key`: The key to find the model's prediction in the outputs of a run.
    * `model_id`: The ID of the language model you want to use to compute the metric. In our example, we are using 'gpt-2'.
    * `metric_fn`: The evaluation metric function, loaded from the HuggingFace `evaluate` package.
  * **Evaluate** : This method takes a run (and optionally an example) and returns an evaluation dictionary. 
    * If the run outputs are `None`, the evaluator raises an error.
    * Otherwise, the outputs are passed to the `metric_fn` to compute the perplexity. The perplexity score is then returned as part of the evaluation dictionary. Once you've defined your evaluators, you can use them to evaluate your model:


```
from langsmith.evaluation import evaluateevaluate(<LLM or chain or agent>,  data="<dataset_name>",  evaluators=[BlocklistEvaluator(blocklist=["bad","words"]), PerplexityEvaluator(), is_empty],)
```

## Summary Evaluators[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#summary-evaluators "Direct link to Summary Evaluators")
Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment. For example, you may want to compute the f1 score of a classifier across all runs in an experiment kicked off from a dataset. These are called `summary_evaluators`. Instead of taking in a single `Run` and `Example`, these evaluators take a list of each.
```
from typing import Listfrom langsmith.schemas import Example, Runfrom langsmith.evaluation import evaluatedeff1_score_summary_evaluator(runs: List[Run], examples: List[Example])->dict:  true_positives =0  false_positives =0  false_negatives =0for run, example inzip(runs, examples):# Matches the output format of your dataset    reference = example.outputs["answer"]# Matches the output dict in `predict` function below    prediction = run.outputs["prediction"]if reference and prediction == reference:      true_positives +=1elif prediction andnot reference:      false_positives +=1elifnot prediction and reference:      false_negatives +=1if true_positives ==0:return{"key":"f1_score","score":0.0}  precision = true_positives /(true_positives + false_positives)  recall = true_positives /(true_positives + false_negatives)  f1_score =2*(precision * recall)/(precision + recall)return{"key":"f1_score","score": f1_score}defpredict(inputs:dict):return{"prediction":True}evaluate(  predict,# Your classifier  data="<dataset_name>",  summary_evaluators=[f1_score_summary_evaluator],)
```

### Recap[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#recap "Direct link to Recap")
Congratulations! You created a custom evaluation chain you can apply to _any_ traced run so you can surface more relevant information in your application. LangChain's evaluation chains speed up the development process for application-specific, semantically robust evaluations. You can also extend existing components from the library so you can focus on building your product. All your evals come with:
  * Automatic tracing integrations to help you debug, compare, and improve your code
  * Easy sharing and mixing of components and results
  * Out-of-the-box support for sync and async evaluations for faster runs


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/evaluation/faq/custom-evaluators%3E).
[PreviousHow-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)[NextUse Off-the-Shelf Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)
  * [Run Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#run-evaluators)
  * [Summary Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#summary-evaluators)
    * [Recap](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators#recap)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations

[Skip to main content](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
      * [Create an Evaluator](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)
      * [Use Off-the-Shelf Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)
      * [Regression Testing](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing)
      * [Unit Test](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing)
      * [Manage Datasets](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)
      * [Version Datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)
      * [Run Experiments in Browser (no code)](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)
      * [Synthetic Data for Evaluation](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
  * Use Off-the-Shelf Evaluators


On this page
# How to Use Off-the-Shelf Evaluators
LangChain's evaluation module provides evaluators you can use as-is for common evaluation scenarios.
It's easy to use these by passing them to the `evaluators` argument of the `evaluate()` function.
Copy the code snippets below to get started. You can also configure them for your applications using the arguments mentioned in the "Parameters" sections. If you don't see an implementation that suits your needs, you can learn how to create your own [Custom Run Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators) in the linked guide, or contribute an string evaluator to the [LangChain repository](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/evaluation/).
note
Most of these evaluators are useful but imperfect! We recommend against blind trust of any single automated metric and to always incorporate them as a part of a holistic testing and evaluation strategy. Many of the LLM-based evaluators return a binary score for a given data point, so measuring differences in prompt or model performance are most reliable in aggregate over a larger dataset.
## Overview[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#overview "Direct link to Overview")
The following table enumerates the off-the-shelf evaluators available in LangSmith, along with their output keys and a simple code sample.
Evaluator name| Output Key| Simple Code Example  
---|---|---  
QA| `correctness`| `LangChainStringEvaluator("qa")`  
Contextual Q&A| `contextual accuracy`| `LangChainStringEvaluator("context_qa")`  
Chain of Thought Q&A| `cot contextual accuracy`| `LangChainStringEvaluator("cot_qa")`  
Criteria| Depends on criteria key| `LangChainStringEvaluator("criteria", config={ "criteria": <criterion> })``criterion` may be one of the default implemented criteria: `conciseness`, `relevance`, `correctness`, `coherence`, `harmfulness`, `maliciousness`, `helpfulness`, `controversiality`, `misogyny`, and `criminality`.Or, you may define your own criteria in a custom dict as follows:`{ "criterion_key": "criterion description" }`  
Labeled Criteria| Depends on criteria key| `LangChainStringEvaluator("labeled_criteria", config={ "criteria": <criterion> })``criterion` may be one of the default implemented criteria: `conciseness`, `relevance`, `correctness`, `coherence`, `harmfulness`, `maliciousness`, `helpfulness`, `controversiality`, `misogyny`, and `criminality`.Or, you may define your own criteria in a custom dict as follows:`{ "criterion_key": "criterion description" }`  
Score| Depends on criteria key| `LangChainStringEvaluator("score_string", config={ "criteria": <criterion>, "normalize_by": 10 })``criterion` may be one of the default implemented criteria: `conciseness`, `relevance`, `correctness`, `coherence`, `harmfulness`, `maliciousness`, `helpfulness`, `controversiality`, `misogyny`, and `criminality`.Or, you may define your own criteria in a custom dict as follows:`{ "criterion_key": "criterion description" }`. Scores are out of 10, so normalize_by will cast this to a score from 0 to 1.  
Labeled Score| Depends on criteria key| `LangChainStringEvaluator("labeled_score_string", config={ "criteria": <criterion>, "normalize_by": 10 })``criterion` may be one of the default implemented criteria: `conciseness`, `relevance`, `correctness`, `coherence`, `harmfulness`, `maliciousness`, `helpfulness`, `controversiality`, `misogyny`, and `criminality`.Or, you may define your own criteria in a custom dict as follows:`{ "criterion_key": "criterion description" }`. Scores are out of 10, so normalize_by will cast this to a score from 0 to 1.  
Embedding distance| `embedding_cosine_distance`| `LangChainStringEvaluator("embedding_distance")`  
String Distance| `string_distance`| `LangChainStringEvaluator("string_distance", config={"distance": "damerau_levenshtein" })` `distance` defines the string difference metric to be applied, such as `levenshtein` or `jaro_winkler`.  
Exact Match| `exact_match`| `LangChainStringEvaluator("exact_match")`  
Regex Match| `regex_match`| `LangChainStringEvaluator("regex_match")`  
Json Validity| `json_validity`| `LangChainStringEvaluator("json_validity")`  
Json Equality| `json_equality`| `LangChainStringEvaluator("json_equality")`  
Json Edit Distance| `json_edit_distance`| `LangChainStringEvaluator("json_edit_distance")`  
Json Schema| `json_schema`| `LangChainStringEvaluator("json_schema")`  
## Correctness: QA evaluation[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#correctness-qa-evaluation "Direct link to Correctness: QA evaluation")
QA evalutors help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you! Three QA evaluators you can load are: `"qa"`, `"context_qa"`, `"cot_qa"`. Based on our meta-evals, we recommend using `"cot_qa"` or a similar prompt for best results.
  * The `"qa"` evaluator ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.qa.eval_chain.QAEvalChain.html#langchain-evaluation-qa-eval-chain-qaevalchain)) instructs an llm to directly grade a response as "correct" or "incorrect" based on the reference answer.
  * The `"context_qa"` evaluator ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.qa.eval_chain.ContextQAEvalChain.html#langchain.evaluation.qa.eval_chain.ContextQAEvalChain)) instructs the LLM chain to use reference "context" (provided throught the example outputs) in determining correctness. This is useful if you have a larger corpus of grounding docs but don't have ground truth answers to a query.
  * The `"cot_qa"` evaluator ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.qa.eval_chain.CotQAEvalChain.html#langchain.evaluation.qa.eval_chain.CotQAEvalChain)) is similar to the "context_qa" evaluator, except it instructs the LLMChain to use chain of thought "reasoning" before determining a final verdict. This tends to lead to responses that better correlate with human labels, for a slightly higher token and runtime cost.


  * Python


```
from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluateqa_evaluator = LangChainStringEvaluator("qa")context_qa_evaluator = LangChainStringEvaluator("context_qa")cot_qa_evaluator = LangChainStringEvaluator("cot_qa")client = Client()evaluate(<your pipeline>, data="<dataset_name>", evaluators=[qa_evaluator, context_qa_evaluator, cot_qa_evaluator], metadata={"revision_id":"the version of your pipeline you are testing"},)
```

You can customize the evaluator by specifying the LLM used to power its LLM chain or even by customizing the prompt itself. Below is an example using an Anthropic model to run the evaluator, and a custom prompt for the base QA evaluator. Check out the reference docs for more information on the expected prompt format.
  * Python


```
from langchain.chat_models import ChatAnthropicfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluator_PROMPT_TEMPLATE ="""You are an expert professor specialized in grading students' answers to questions.You are grading the following question:{input}Here is the real answer:{reference}You are grading the following predicted answer:{prediction}Respond with CORRECT or INCORRECT:Grade:"""PROMPT = PromptTemplate( input_variables=["input","reference","prediction"], template=_PROMPT_TEMPLATE)eval_llm = ChatAnthropic(temperature=0.0)qa_evaluator = LangChainStringEvaluator("qa", config={"llm": eval_llm,"prompt": PROMPT})context_qa_evaluator = LangChainStringEvaluator("context_qa", config={"llm": eval_llm})cot_qa_evaluator = LangChainStringEvaluator("cot_qa", config={"llm": eval_llm})evaluate(<your pipeline>, data="<dataset_name>", evaluators=[qa_evaluator, context_qa_evaluator, cot_qa_evaluator],)
```

## Criteria Evaluators (No Labels)[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#criteria-evaluators-no-labels "Direct link to Criteria Evaluators \(No Labels\)")
If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the `"criteria"` or `"score"` evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules.
  * The `"criteria"` evaluator ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html#langchain.evaluation.criteria.eval_chain.CriteriaEvalChain)) instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score
  * The `"score_string"` evaluator ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.scoring.eval_chain.ScoreStringEvalChain.html#langchain.evaluation.scoring.eval_chain.ScoreStringEvalChain)) has the LLM score the prediction on a numeric scale (default 1-10) based on how well it satisfies the criteria


  * Python


```
from langsmith import Client from langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator("criteria", config={"criteria":{"creativity":"Is this submission creative, imaginative, or novel?","conciseness":"Is this response concise and to the point?",}})score_evaluator = LangChainStringEvaluator("score_string", config={"criteria":{"accuracy":"How accurate is this prediction on a scale of 1-10?"},# If you want the score to be saved on a scale from 0 to 1"normalize_by":10,})client = Client()evaluate(<your pipeline>, data="<dataset_name>", evaluators=[   criteria_evaluator,   score_evaluator], metadata={"revision_id":"the version of your pipeline you are testing"},)
```

Supported Criteria
Default criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality. To specify custom criteria, write a mapping of a criterion name to its description, such as:
```
  criterion = {"creativity": "Is this submission creative, imaginative, or novel?"}  criteria_evaluator = LangChainStringEvaluator(    "labeled_criteria",    config={"criteria": criterion}  )
```

Interpreting the Score
Evaluation scores don't have an inherent "direction" (i.e., higher is not necessarily better). The direction of the score depends on the criteria being evaluated. For example, a score of 1 for "helpfulness" means that the prediction was deemed to be helpful by the model. However, a score of 1 for "maliciousness" means that the prediction contains malicious content, which, of course, is "bad".
## Criteria Evaluators (With Labels)[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#criteria-evaluators-with-labels "Direct link to Criteria Evaluators \(With Labels\)")
If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the `"labeled_criteria"` or `"labeled_score_string"` evaluators.
  * The `"labeled_criteria"` evaluator ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html#langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain)) instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference label
  * The `"labeled_score_string"` evaluator ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.scoring.eval_chain.LabeledScoreStringEvalChain.html#langchain.evaluation.scoring.eval_chain.LabeledScoreStringEvalChain)) has the LLM score the prediction on a numeric scale based on how well it satisfies the criteria compared to the reference


  * Python


```
from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator("labeled_criteria", config={"criteria":{"helpfulness":("Is this submission helpful to the user,"" taking into account the correct reference answer?")}}, prepare_data=lambda run, example:{"prediction": run.outputs["output"],"reference": example.outputs["answer"],"input": example.inputs["question"],})labeled_score_evaluator = LangChainStringEvaluator("labeled_score_string", config={"criteria":{"accuracy":"How accurate is this prediction compared to the reference on a scale of 1-10?"},"normalize_by":10,}, prepare_data=lambda run, example:{"prediction": run.outputs["output"],"reference": example.outputs["answer"],"input": example.inputs["question"],})client = Client()evaluate(<your pipeline>, data="<dataset_name>", evaluators=[   labeled_criteria_evaluator,   labeled_score_evaluator], metadata={"revision_id":"the version of your pipeline you are testing"},)
```

## JSON Evaluators[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#json-evaluators "Direct link to JSON Evaluators")
Evaluating extraction and function calling applications often comes down to validating that the LLM's string output can be parsed correctly and comparing it to a reference object. The JSON evaluators provide functionality to check your model's output consistency:
  * The `"json_validity"` ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.parsing.base.JsonValidityEvaluator.html#langchain.evaluation.parsing.base.JsonValidityEvaluator)) evaluator checks if a prediction is valid JSON
  * The `"json_equality"` ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.parsing.base.JsonEqualityEvaluator.html#langchain.evaluation.parsing.base.JsonEqualityEvaluator)) evaluator checks if a JSON prediction exactly matches a JSON reference, after normalization
  * The `"json_edit_distance"` ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.parsing.json_distance.JsonEditDistanceEvaluator.html#langchain.evaluation.parsing.json_distance.JsonEditDistanceEvaluator)) evaluator computes the normalized edit distance between a JSON prediction and reference
  * The `"json_schema"` ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.parsing.json_schema.JsonSchemaEvaluator.html#langchain.evaluation.parsing.json_schema.JsonSchemaEvaluator)) evaluator checks if a JSON prediction satisfies a provided JSON schema


  * Python


```
from langsmith.evaluation import LangChainStringEvaluator, evaluatejson_validity_evaluator = LangChainStringEvaluator("json_validity")json_equality_evaluator = LangChainStringEvaluator("json_equality")json_edit_distance_evaluator = LangChainStringEvaluator("json_edit_distance")json_schema_evaluator = LangChainStringEvaluator("json_schema", config={"schema":{"type":"object","properties":{"name":{"type":"string"},"age":{"type":"integer","minimum":0}},"required":["name"]}})evaluate(<your pipeline>, data="<dataset_name>", evaluators=[   json_validity_evaluator,   json_equality_evaluator,   json_edit_distance_evaluator,   json_schema_evaluator],)
```

## String or Embedding Distance[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#string-or-embedding-distance "Direct link to String or Embedding Distance")
To measure the similarity between a predicted string and a reference, you can use string distance metrics:
  * The `"string_distance"` ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.string_distance.base.StringDistanceEvalChain.html#langchain.evaluation.string_distance.base.StringDistanceEvalChain)) evaluator computes a normalized string edit distance between the prediction and reference
  * The `"embedding_distance"` ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.embedding_distance.base.EmbeddingDistance.html#langchain.evaluation.embedding_distance.base.EmbeddingDistance)) evaluator computes the distance between the text embeddings of the prediction and reference
  * The `"exact_match"` ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html#langchain.evaluation.exact_match.base.ExactMatchStringEvaluator)) evaluator checks for an exact string match between prediction and reference


  * Python


```
from langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator("string_distance", config={"distance":"levenshtein","normalize_score":True})embedding_distance_evaluator = LangChainStringEvaluator("embedding_distance", config={# Defaults to OpenAI, but you can customize which embedding provider to use:# "embeddings": HuggingFaceEmbeddings(model="distilbert-base-uncased"), # Can also choose "euclidean", "chebyshev", "hamming", and "manhattan""distance_metric":"cosine",})exact_match_evaluator = LangChainStringEvaluator("exact_match", config={"ignore_case":True,"ignore_punctuation":True})evaluate(<your pipeline>, data="<dataset_name>", evaluators=[   string_distance_evaluator,   embedding_distance_evaluator,   exact_match_evaluator],)
```

## Regex Match[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#regex-match "Direct link to Regex Match")
To evaluate predictions against a reference regular expression pattern, you can use the `"regex_match"` ([reference](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html#langchain.evaluation.regex_match.base.RegexMatchStringEvaluator)) evaluator. The pattern is provided as a string in the example outputs of the dataset. The evaluator checks if the prediction matches the pattern.
  * Python


```
from langsmith.evaluation import LangChainStringEvaluator, evaluateregex_evaluator = LangChainStringEvaluator("regex_match", config={# Optionally control which flags to use in the regex match"flags": re.IGNORECASE })evaluate(<your pipeline>, data="<dataset_name>", evaluators=[regex_evaluator],)
```

## Don't see what you're looking for?[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#dont-see-what-youre-looking-for "Direct link to Don't see what you're looking for?")
These implementations are just a starting point. We want to work with you to build better off-the-shelf evaluation tools for everyone. We'd love feedback and contributions! Send us feedback at support@langchain.dev, check out the [Evaluators](https://python.langchain.com/docs/guides/evaluation/) in LangChain or submit PRs or issues directly to better address your needs.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/evaluation/faq/evaluator-implementations%3E).
[PreviousCreate an Evaluator](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)[NextRegression Testing](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing)
  * [Overview](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#overview)
  * [Correctness: QA evaluation](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#correctness-qa-evaluation)
  * [Criteria Evaluators (No Labels)](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#criteria-evaluators-no-labels)
  * [Criteria Evaluators (With Labels)](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#criteria-evaluators-with-labels)
  * [JSON Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#json-evaluators)
  * [String or Embedding Distance](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#string-or-embedding-distance)
  * [Regex Match](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#regex-match)
  * [Don't see what you're looking for?](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations#dont-see-what-youre-looking-for)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation/faq/experiments-app

[Skip to main content](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
      * [Create an Evaluator](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)
      * [Use Off-the-Shelf Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)
      * [Regression Testing](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing)
      * [Unit Test](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing)
      * [Manage Datasets](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)
      * [Version Datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)
      * [Run Experiments in Browser (no code)](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)
      * [Synthetic Data for Evaluation](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
  * Run Experiments in Browser (no code)


On this page
# How to run experiments in the prompt playground (no code)
While you can kick off experiments easily using the sdk, as outlined [here](https://docs.smith.langchain.com/old/evaluation/quickstart), it's often useful to run experiments directly in the [prompt playground](https://docs.smith.langchain.com/old/hub/quickstart#2-try-out-a-prompt-in-the-playground).
This allows you to test your prompt / model configuration over a series of inputs to see how well it generalizes across different contexts or scenarios, without having to write any code.
## Kicking off an experiment in the prompt playground[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app#kicking-off-an-experiment-in-the-prompt-playground "Direct link to Kicking off an experiment in the prompt playground")
  1. **Navigate to the prompt playground** by clicking on "Hub" in the sidebar, then selecting a prompt from the list of available prompts or creating a new one.
  2. **Select the "Switch to dataset" button** to switch to the dataset you want to use for the experiment. Please note that the dataset keys of the dataset inputs must match the input variables of the prompt. In the below sections, note that the selected dataset has inputs with keys "text", which correctly match the input variable of the prompt. Also note that there is a max capacity of 15 inputs for the prompt playground. ![Switch to dataset](https://docs.smith.langchain.com/assets/images/switch_to_dataset-e9b01998b0f3b37f6aae6207ddd0a221.png)
  3. **Click on the "Start" button** or CMD+Enter to start the experiment. This will run the prompt over all the examples in the dataset and create an entry for the experiment in the dataset details page. Note that you need to commit the prompt to the prompt hub before you can start the experiment to ensure it can be referenced in the experiment. The result for each input will be streamed and displayed inline for each input in the dataset. ![Input variables](https://docs.smith.langchain.com/assets/images/input_variables_playground-8ee125b556215bdfbc6ac2432cf10646.png)
  4. **View the results** by clicking on the "View Dataset Run Table" button. This will take you to the experiment details page where you can see the results of the experiment. ![View results](https://docs.smith.langchain.com/assets/images/view_results-709e711aed83f2d05e6daff27e8b158d.png)
  5. **Navigate back to the commit page** by clicking on the "View Commit" button. This will take you back to the prompt page where you can make changes to the prompt and run more experiments. The "View Commit" button is available to all experiments that were run from the prompt playground. The experiment is prefixed with the prompt repository name, a unique identifier, and the date and time the experiment was run. ![Playground experiment results](https://docs.smith.langchain.com/assets/images/playground_experiment_results-777fb4485de9b628a64448687d2e9513.png)


## Adding evaluation scores to the experiment[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app#adding-evaluation-scores-to-the-experiment "Direct link to Adding evaluation scores to the experiment")
Kicking off an experiment is no fun without actually running evaluations on the results. You can add evaluation scores to the experiment by configuring an automation rule for the dataset, again without writing any code. This will allow you to add evaluation scores to the experiment and compare the results across different experiments. It's also possible to add human annotations to the runs of any experiment.
We currently support configuring LLM-as-a-judge evaluators on datasets that will evaluate the results of each run in each experiment kicked off from that dataset.
The process for configuring this is very similar to the process for configuring an [online evaluator](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation) for your tracing projects.
  1. **Navigate to the dataset details page** by clicking "Datasets and Testing" in the sidebar and selecting the dataset you want to configure the evaluator for.
  2. **Click on the "Add Evaluator" button** to add an evaluator to the dataset. This will open a modal you can use to configure the evaluator.
![Add Evaluator](https://docs.smith.langchain.com/assets/images/add_evaluator-aa911426b925d7293dff7487fae99b45.png)
  3. **Give your evaluator a name** and **set an inline prompt or load a prompt from the prompt hub** that will be used to evaluate the results of the runs in the experiment.
![Add evaluator name and prompt](https://docs.smith.langchain.com/assets/images/create_evaluator-38b8ca3ad9bb615ba3d660da426f611d.png)
Importantly, evaluator prompts can only contain the following input variables:
     * `input` (required): the input to the target you are evaluating
     * `output` (required): the output of the target you are evaluating
     * `reference`: the reference output, taken from the dataset
You can specify the scoring criteria in the "schema" field. In this example, we are asking the LLM to grade on "correctness" of the output with respect to the reference, with a boolean output of 0 or 1. The name of the field in the schema will be interpreted as the feedback key and the type will be the type of the score.
![Evaluator prompt](https://docs.smith.langchain.com/assets/images/evaluator_prompt-96b358edf79006d44a5955accf643dec.png)
  4. **Save the evaluator** and navigate back to the dataset details page. Each subsequent experiment run from the dataset will now be evaluated by the evaluator you configured. Note that in the below image, each run in the experiment has a "correctness" score.
![Playground evaluator results](https://docs.smith.langchain.com/assets/images/playground_evaluator_results-c60a255f465396f27d6c1a3dc6a76c42.png)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousVersion Datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)[NextSynthetic Data for Evaluation](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)
  * [Kicking off an experiment in the prompt playground](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app#kicking-off-an-experiment-in-the-prompt-playground)
  * [Adding evaluation scores to the experiment](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app#adding-evaluation-scores-to-the-experiment)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets

[Skip to main content](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
      * [Create an Evaluator](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)
      * [Use Off-the-Shelf Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)
      * [Regression Testing](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing)
      * [Unit Test](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing)
      * [Manage Datasets](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)
      * [Version Datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)
      * [Run Experiments in Browser (no code)](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)
      * [Synthetic Data for Evaluation](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
  * Manage Datasets


On this page
# Managing Datasets
## In LangSmith[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#in-langsmith "Direct link to In LangSmith")
The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example rows. Below are a few ways to interact with them.
### From Existing Runs[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#from-existing-runs "Direct link to From Existing Runs")
We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the runs to find the ones we want to add to the dataset. Then, we create a dataset and add the runs as examples.
You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner.
![Add to Dataset](https://docs.smith.langchain.com/assets/images/add_to_dataset-46961cd323fa6f4564f6871630f7a13f.png)
From there, we select the dataset to organize it in and update the ground truth output values if necessary.
![Modify example](https://docs.smith.langchain.com/assets/images/modify_example-5b4880c12f3c790507ced7ea17312b52.png)
### Upload a CSV[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#upload-a-csv "Direct link to Upload a CSV")
The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page.
![Upload CSV](https://docs.smith.langchain.com/assets/images/create_dataset_csv-fb44e858a530cd706aff19122aed5baf.png)
Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct.
![Confirm Columns](https://docs.smith.langchain.com/assets/images/select_columns-004e36acd6c86ad683100d9fa532cacf.png)
### Exporting datasets to other formats[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#exporting-datasets-to-other-formats "Direct link to Exporting datasets to other formats")
You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application.
To do so, click "Export Dataset" from the homepage. To do so, select a dataset, click on "Examples", and then click the "Export Dataset" button at the top of the examples table.
![Export Dataset Button](https://docs.smith.langchain.com/assets/images/export-dataset-button-ee01661dcb5df35eb98e65505cb562a0.png)
This will open a modal where you can select the format you want to export to.
![Export Dataset Modal](https://docs.smith.langchain.com/assets/images/export-dataset-modal-2b78932a765e17915b49b2bc4f4d9373.png)
## How to manage datasets programmatically[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#how-to-manage-datasets-programmatically "Direct link to How to manage datasets programmatically")
You can create a dataset from existing runs or upload a CSV file (or pandas dataframe in python).
Once you have a dataset created, you can continue to add new runs to it as examples. We recommend that you organize datasets to target a single "task", usually served by a single chain or LLM. For more discussions on datasets and evaluations, check out the [recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations).
### Create from list of values[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values "Direct link to Create from list of values")
The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example.
Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary.
  * Python
  * TypeScript


```
from langsmith import Clientexample_inputs =[("What is the largest mammal?","The blue whale"),("What do mammals and birds have in common?","They are both warm-blooded"),("What are reptiles known for?","Having scales"),("What's the main characteristic of amphibians?","They live both in water and on land"),]client = Client()dataset_name ="Elementary Animal Questions"# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset( dataset_name=dataset_name, description="Questions and answers about animal phylogenetics.",)for input_prompt, output_answer in example_inputs: client.create_example(   inputs={"question": input_prompt},   outputs={"answer": output_answer},   metadata={"source":"Wikipedia"},   dataset_id=dataset.id,)
```

```
import{ Client }from"langsmith";const client =newClient({// apiUrl: "https://api.langchain.com", // Defaults to the LANGCHAIN_ENDPOINT env var// apiKey: "my_api_key", // Defaults to the LANGCHAIN_API_KEY env var/* callerOptions: {    maxConcurrency?: Infinity; // Maximum number of concurrent requests to make    maxRetries?: 6; // Maximum number of retries to make}*/});const exampleInputs:[string,string][]=[["What is the largest mammal?","The blue whale"],["What do mammals and birds have in common?","They are both warm-blooded"],["What are reptiles known for?","Having scales"],["What's the main characteristic of amphibians?","They live both in water and on land"],];const datasetName ="Elementary Animal Questions";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset =await client.createDataset(datasetName,{description:"Questions and answers about animal phylogenetics",});for(const[inputPrompt, outputAnswer]of exampleInputs){await client.createExample({ question: inputPrompt },{ answer: outputAnswer },{  datasetId: dataset.id,  metadata:{ source:"Wikipedia"},});}
```

### Create from existing runs[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-existing-runs "Direct link to Create from existing runs")
To create datasets from existing runs, you can use the same approach. Below is an example:
  * Python
  * TypeScript


```
from langsmith import Clientos.environ["LANGCHAIN_ENDPOINT"]="https://api.smith.langchain.com"os.environ["LANGCHAIN_API_KEY"]="<YOUR-LANGSMITH-API-KEY>"client = Client()dataset_name ="Example Dataset"# Filter runs to add to the datasetruns = client.list_runs( project_name="my_project", execution_order=1, error=False,)dataset = client.create_dataset(dataset_name, description="An example dataset")for run in runs: client.create_example(   inputs=run.inputs,   outputs=run.outputs,   dataset_id=dataset.id,)
```

```
import{ Client, Run }from"langsmith";const client =newClient({// apiUrl: "https://api.langchain.com", // Defaults to the LANGCHAIN_ENDPOINT env var// apiKey: "my_api_key", // Defaults to the LANGCHAIN_API_KEY env var/* callerOptions: {    maxConcurrency?: Infinity; // Maximum number of concurrent requests to make    maxRetries?: 6; // Maximum number of retries to make}*/});const datasetName ="Example Dataset";// Filter runs to add to the datasetconst runs: Run[]=[];forawait(const run of client.listRuns({projectName:"my_project",executionOrder:1,error:false,})){runs.push(run);}const dataset =await client.createDataset(datasetName,{description:"An example dataset",dataType:"kv",});for(const run of runs){await client.createExample(run.inputs, run.outputs ??{},{ datasetId: dataset.id,});}
```

### Create dataset from CSV[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-dataset-from-csv "Direct link to Create dataset from CSV")
In this section, we will demonstrate how you can create a dataset by uploading a CSV file.
First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided.
  * Python
  * TypeScript


```
from langsmith import Clientimport osos.environ["LANGCHAIN_ENDPOINT"]="https://api.smith.langchain.com"os.environ["LANGCHAIN_API_KEY"]="<YOUR-LANGSMITH-API-KEY>"client = Client()csv_file ='path/to/your/csvfile.csv'input_keys =['column1','column2']# replace with your input column namesoutput_keys =['output1','output2']# replace with your output column namesdataset = client.upload_csv( csv_file=csv_file, input_keys=input_keys, output_keys=output_keys, name="My CSV Dataset", description="Dataset created from a CSV file" data_type="kv")
```

```
import{ Client }from"langsmith";const client =newClient();const csvFile ='path/to/your/csvfile.csv';const inputKeys =['column1','column2'];// replace with your input column namesconst outputKeys =['output1','output2'];// replace with your output column namesconst dataset =await client.uploadCsv({ csvFile: csvFile, fileName:"My CSV Dataset", inputKeys: inputKeys, outputKeys: outputKeys, description:"Dataset created from a CSV file", dataType:"kv"});
```

### Create dataset from pandas dataframe[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-dataset-from-pandas-dataframe "Direct link to Create dataset from pandas dataframe")
The python client offers an additional convenience method to upload a dataset from a pandas dataframe.
```
from langsmith import Clientimport osimport pandas as pdos.environ["LANGCHAIN_ENDPOINT"]="https://api.smith.langchain.com"os.environ["LANGCHAIN_API_KEY"]="<YOUR-LANGSMITH-API-KEY>"client = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys =['column1','column2']# replace with your input column namesoutput_keys =['output1','output2']# replace with your output column namesdataset = client.upload_dataframe(  df=df,  input_keys=input_keys,  output_keys=output_keys,  name="My Parquet Dataset",  description="Dataset created from a parquet file",  data_type="kv"# The default)
```

## List datasets from the client[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-datasets-from-the-client "Direct link to List datasets from the client")
You can programmatically fetch the datasets from LangSmith using the `list_datasets` method in the client. Below are some common examples:
### Query all datasets[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#query-all-datasets "Direct link to Query all datasets")
  * Python
  * TypeScript


```
datasets = client.list_datasets()
```

```
const datasets =await client.listDatasets();
```

### List datasets by name[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-datasets-by-name "Direct link to List datasets by name")
If you want to search by the exact name, you can do the following:
  * Python
  * TypeScript


```
datasets = client.list_datasets(dataset_name="My Test Dataset 1")
```

```
const datasets =await client.listDatasets({datasetName:"My Test Dataset 1"});
```

If you want to do a case-invariant substring search, try the following:
  * Python
  * TypeScript


```
datasets = client.list_datasets(dataset_name_contains="some substring")
```

```
const datasets =await client.listDatasets({datasetNameContains:"some substring"});
```

### List datasets by type[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-datasets-by-type "Direct link to List datasets by type")
You can filter datasets by type. Below is an example querying for chat datasets.
  * Python
  * TypeScript


```
datasets = client.list_datasets(data_type="chat")
```

```
const datasets =await client.listDatasets({dataType:"chat"});
```

## List Examples from the client[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-examples-from-the-client "Direct link to List Examples from the client")
Once you have a dataset created, you may want to download the examples. You can fetch dataset examples using the `list_examples` method on the LangSmith client. Below are some common calls:
### List all examples for a dataset[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-all-examples-for-a-dataset "Direct link to List all examples for a dataset")
You can filter by dataset ID:
  * Python
  * TypeScript


```
examples = client.list_examples(dataset_id="c9ace0d8-a82c-4b6c-13d2-83401d68e9ab")
```

```
const examples =await client.listExamples({datasetId:"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab"});
```

Or you can filter by dataset name (this must exactly match the dataset name you want to query)
  * Python
  * TypeScript


```
examples = client.list_examples(dataset_name="My Test Dataset")
```

```
const examples =await client.listExamples({datasetName:"My test Dataset"});
```

### List examples by id[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-examples-by-id "Direct link to List examples by id")
You can also list multiple examples all by ID.
  * Python
  * TypeScript


```
example_ids =['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)
```

```
const exampleIds =["734fc6a0-c187-4266-9721-90b7a025751a","d6b4c1b9-6160-4d63-9b61-b034c585074f","4d31df4e-f9c3-4a6e-8b6c-65701c2fed13",];const examples =await client.listExamples({exampleIds: exampleIds});
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousUnit Test](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing)[NextVersion Datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)
  * [In LangSmith](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#in-langsmith)
    * [From Existing Runs](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#from-existing-runs)
    * [Upload a CSV](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#upload-a-csv)
    * [Exporting datasets to other formats](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#exporting-datasets-to-other-formats)
  * [How to manage datasets programmatically](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#how-to-manage-datasets-programmatically)
    * [Create from list of values](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values)
    * [Create from existing runs](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-existing-runs)
    * [Create dataset from CSV](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-dataset-from-csv)
    * [Create dataset from pandas dataframe](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-dataset-from-pandas-dataframe)
  * [List datasets from the client](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-datasets-from-the-client)
    * [Query all datasets](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#query-all-datasets)
    * [List datasets by name](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-datasets-by-name)
    * [List datasets by type](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-datasets-by-type)
  * [List Examples from the client](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-examples-from-the-client)
    * [List all examples for a dataset](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-all-examples-for-a-dataset)
    * [List examples by id](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#list-examples-by-id)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation/faq/regression-testing

[Skip to main content](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
      * [Create an Evaluator](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)
      * [Use Off-the-Shelf Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)
      * [Regression Testing](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing)
      * [Unit Test](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing)
      * [Manage Datasets](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)
      * [Version Datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)
      * [Run Experiments in Browser (no code)](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)
      * [Synthetic Data for Evaluation](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
  * Regression Testing


On this page
# Regression Testing
When evaluating LLM applications, it is important to be able to track how your system performs over time. In this guide, we will show you how to use LangSmith's comparison view in order to track regressions in your application, and drill down to inspect the specific runs that improved/regressed over time.
## Overview[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing#overview "Direct link to Overview")
In the LangSmith comparison view, runs that _regressed_ on your specified feedback key against your baseline experiment will be highlighted in red, while runs that _improved_ will be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and and how many did worse than your baseline experiment.
![Regressions](https://docs.smith.langchain.com/assets/images/regression_view-9c20b1a245ba969a344a1eb55c88b645.png)
## Baseline Experiment[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing#baseline-experiment "Direct link to Baseline Experiment")
In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can change it from the dropdown at the top of the page.
![Baseline](https://docs.smith.langchain.com/assets/images/select_baseline-f15d475955f530751c1ce75b4a841f6f.png)
## Select Feedback Key[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing#select-feedback-key "Direct link to Select Feedback Key")
You will also want to select the feedback key on which you would like focus. This can be selected via another dropdown at the top. Again, one will be assigned by default, but you can adjust as needed.
![Feedback](https://docs.smith.langchain.com/assets/images/select_feedback-4e55875b93c631d9f2894a08fd027a0d.png)
## Filter to Regressions or Improvements[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing#filter-to-regressions-or-improvements "Direct link to Filter to Regressions or Improvements")
Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.
![Regressions Filter](https://docs.smith.langchain.com/assets/images/filter_to_regressions-3fab2e024731d6dbf69ccb223fe42e1a.png)
## Try it out[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing#try-it-out "Direct link to Try it out")
To get started with regression testing, try [running a no-code experiment in our prompt playground](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app) or check out the [Evaluation Quick Start Guide](https://docs.smith.langchain.com/old/evaluation/quickstart) to get started with the SDK.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousUse Off-the-Shelf Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)[NextUnit Test](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing)
  * [Overview](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing#overview)
  * [Baseline Experiment](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing#baseline-experiment)
  * [Select Feedback Key](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing#select-feedback-key)
  * [Filter to Regressions or Improvements](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing#filter-to-regressions-or-improvements)
  * [Try it out](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing#try-it-out)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data

[Skip to main content](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
      * [Create an Evaluator](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)
      * [Use Off-the-Shelf Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)
      * [Regression Testing](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing)
      * [Unit Test](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing)
      * [Manage Datasets](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)
      * [Version Datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)
      * [Run Experiments in Browser (no code)](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)
      * [Synthetic Data for Evaluation](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
  * Synthetic Data for Evaluation


On this page
# Synthetic Data for Evaluation
When prototyping a system, you may not have enough real data to thoroughly evaluate how the component will behave. This guide will walk you through two techniques for generating synthetic data to augment your dataset:
  1. **Paraphrasing Existing Inputs** : Generate semantically similar variations of your existing examples to test your system's consistency.
  2. **Generating New Inputs** : Create entirely new, plausible inputs to test the how your system generalizes to other scenarios.


Reliability
Synthetic data is not a full substitute for real data. The quality of the data generated by these methods depends on factors like the model, prompt, and existing data. Always inspect synthetic datasets to ensure they capture the information you want to model and align with your use case.
### Prerequisites[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#prerequisites "Direct link to Prerequisites")
This guide assumes you've already connected to LangSmith and have a starter dataset. Though we will use LangChain core below, the technique is simple enough to apply using whatever tools you're comfortable with.
```
pip install langsmith langchain_openai
```

## Paraphrasing[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#paraphrasing "Direct link to Paraphrasing")
Paraphrasing existing inputs helps check if your chain's behavior is consistent across similar inputs. Since paraphrasing is a semantically invariant transformation, the outputs should remain the same as the original. Here's how to set it up:
#### Step 1: Define the Paraphrase Generator[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#step-1-define-the-paraphrase-generator "Direct link to Step 1: Define the Paraphrase Generator")
Create a chain for generating paraphrases using the `ChatOpenAI` model with custom system prompts.
```
import refrom typing import Listfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import(  ChatPromptTemplate,)paraphrase_llm = ChatOpenAI(temperature=0.5)prompt_template = ChatPromptTemplate.from_messages([("system","You are a helpful paraphrasing assistant tasked with rephrasing text."),("system","Input: <INPUT>{query}</INPUT>"),("user","What are {n_paraphrases} different ways you could paraphrase the INPUT text?"" Do not significantly change the meaning."" Respond using numbered bullets. If you cannot think of any,"" just say 'I don't know.'"),])defparse(output:str)-> List[str]:return re.findall(r"\d+\.\s+(.*?)\n", output)paraphrase_chain = prompt | llm | parse
```

#### Step 2: Paraphrase the Dataset[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#step-2-paraphrase-the-dataset "Direct link to Step 2: Paraphrase the Dataset")
Use the `Client` from LangSmith to access your dataset and generate paraphrases for it.
```
from langsmith import Clientclient = Client()n_paraphrases =3dataset_name ="Your Dataset Name"examples = client.list_examples(dataset_name=dataset_name)results = paraphrase_chain.batch([{"query":next(iter(example.inputs.values())),"n_paraphrases": n_paraphrases}for example in examples])inputs, outputs =[],[]for example, batch_r inzip(examples, results):  input_key =next(iter(example.inputs))for r in result:    inputs.append({input_key: r})    outputs.append(example.outputs)client.create_examples(  inputs=inputs,  outputs=outputs,  dataset_name=dataset_name,)
```

After running, your dataset should be roughly 3x the original size, with paraphrased variations of your original inputs.
## Generating New Inputs[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#generating-new-inputs "Direct link to Generating New Inputs")
To expand your dataset's semantic range and test your system's robustness, you can generate entirely new, plausible inputs. This method examines a random set of 5 existing examples and creates 6 novel ones that align with the inferred system but are distinct enough to have likely originated from different individuals.
#### Step 1: Define the New Input Generator[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#step-1-define-the-new-input-generator "Direct link to Step 1: Define the New Input Generator")
Create a chain for generating new inputs using the `ChatOpenAI` model with custom system prompts.
```
input_gen_llm = ChatOpenAI(temperature=0.5)input_gen_prompt_template = ChatPromptTemplate.from_messages([("system",# Update this prompt to more closely match your use case"You are a creative assistant tasked with coming up with new inputs for an application.""\nThe following are some examples you can use to understand the domain:\n\n{examples}"),("user","Can you generate {n_inputs} unique and plausible inputs that could be asked by different users?"),])input_gen_chain = prompt | llm | parse
```

#### Step 2: Generate New Inputs for the Dataset[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#step-2-generate-new-inputs-for-the-dataset "Direct link to Step 2: Generate New Inputs for the Dataset")
Use the Client from LangSmith to access your dataset, sample a set of existing inputs, and generate new inputs based on them. Note that new inputs don't come with corresponding outputs, so you may need to manually label them or use a separate model to generate the outputs.
```
import randomclient = Client()n_inputs =6dataset_name ="Your Dataset Name"sample_size =5examples =list(client.list_examples(dataset_name=dataset_name))example_inputs =[next(iter(example.inputs.values()))for example in random.sample(examples, sample_size)]example_inputs_str ='\n'.join(f"- {input}"forinputin example_inputs)results = input_gen_chain.batch([{"examples": example_inputs_str,"n_inputs": n_inputs}])inputs =[{"input": r}for r in results[0]]outputs =[{}]*len(inputs)client.create_examples(  inputs=inputs,  outputs=outputs,  dataset_name=dataset_name,)
```

After running, your dataset should contain new examples that differ more significantly from the original ones, helping you test your system's robustness to a wider range of inputs.
### Considerations[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#considerations "Direct link to Considerations")
Remember that the quality of the paraphrases and generated inputs will depend on the model and prompt used, and these approaches may not be appropriate for all use cases. Always check your augmented data to ensure it maintains the original meaning, aligns with the system's context, and is suitable for your application.
Synthetic data is most useful early in the development process, when you're trying to gauge how sensitive your chain or model is to input variations. By combining paraphrasing, new input generation, and other augmentation methods, you can expand and diversify your dataset to verify the feasibility and robustness of a feature before deploying it to production.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/evaluation/faq/synthetic-data%3E).
[PreviousRun Experiments in Browser (no code)](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)[NextRecommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
  * [Prerequisites](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#prerequisites)
  * [Paraphrasing](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#paraphrasing)
  * [Generating New Inputs](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#generating-new-inputs)
    * [Considerations](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data#considerations)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation/faq/unit-testing

[Skip to main content](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
      * [Create an Evaluator](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)
      * [Use Off-the-Shelf Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)
      * [Regression Testing](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing)
      * [Unit Test](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing)
      * [Manage Datasets](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)
      * [Version Datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)
      * [Run Experiments in Browser (no code)](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)
      * [Synthetic Data for Evaluation](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
  * Unit Test


On this page
# Unit Tests
LangSmith unit tests are assertions and expectations designed to **quickly** identify obvious bugs and regressions in your AI system. Relative to evaluations, tests are designed to be **fast** and **cheap** to run, focusing on **specific** functionality and edge cases. We recommend using LangSmith to track any unit tests that touch an LLM or other non-deterministic part of your AI system.
note
`@unit` currently requires `langsmith` python version `>=0.1.42`. If you are interested in unit testing functionality in TypeScript or other languages, please let us know at support@langchain.dev.
## Write @unit test[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#write-unit-test "Direct link to Write @unit test")
To write a LangSmith unit test, decorate your test function with `@unit`. If you want to track the full nested trace of the system or component being tested, you can mark those functions with `@traceable`. For example:
```
# my_app/main.pyfrom langsmith import traceable@traceable# Optionaldefgenerate_sql(user_query):# Replace with your SQL generation logic# e.g., my_llm(my_prompt.format(user_query))return"SELECT * FROM customers"
```

Then define your unit test:
```
# tests/test_my_app.pyfrom langsmith import unitfrom my_app.main import generate_sql@unitdeftest_sql_generation_select_all():  user_query ="Get all users from the customers table"  sql = generate_sql(user_query)# LangSmith logs any exception raised by `assert` / `pytest.fail` / `raise` / etc.# as a test failureassert sql =="SELECT * FROM customers"
```

## Run tests[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#run-tests "Direct link to Run tests")
You can use a standard unit testing framework such as `pytest` ([docs](https://docs.pytest.org/en/7.1.x/contents.html)) to run. For example:
```
pytest tests/
```

Each time you run this test suite, LangSmith collects the pass/fail rate and other traces as a new `TestSuiteResult`, logging the `pass` rate (1 for pass, 0 for fail) over all the applicable tests.
The test suite syncs to a corresponding dataset named after your package or github repository.
![Unit Test Example](https://docs.smith.langchain.com/assets/images/unit-test-suite-4d6aa6a9c01466607b0560207f51044d.png)
## Going Further[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#going-further "Direct link to Going Further")
`@unit` is designed to stay out of your way and works well with familiar `pytest` features. For example:
#### Defining inputs as fixtures[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#defining-inputs-as-fixtures "Direct link to Defining inputs as fixtures")
Pytest fixtures let you define functions that serve as reusable inputs for your tests. LangSmith automatically syncs any test case inputs defined as fixtures. For example:
```
import pytest@pytest.fixturedefuser_query():return"Get all users from the customers table"@pytest.fixturedefexpected_sql():return"SELECT * FROM customers"# output_keys indicate which test arguments to save as 'outputs' in the dataset (Optional)# Otherwise, all arguments are saved as 'inputs'@unit(output_keys=["expected_sql"])deftest_sql_generation_with_fixture(user_query, expected_sql):  sql = generate_sql(user_query)assert sql == expected_sql
```

#### Parametrizing tests[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#parametrizing-tests "Direct link to Parametrizing tests")
Parametrizing tests lets you run the same assertions across multiple sets of inputs. Use `pytest`'s `parametrize` decorator to achieve this. For example:
```
@unit@pytest.mark.parametrize("user_query, expected_sql",[("Get all users from the customers table","SELECT * FROM customers"),("Get all users from the orders table","SELECT * FROM orders"),],)deftest_sql_generation_parametrized(user_query, expected_sql):  sql = generate_sql(user_query)assert sql == expected_sql
```

**Note:** as the parametrized list grows, you may consider using `evaluate()` instead. This parallelizes the evaluation and makes it easier to control individual experiments and the corresponding dataset.
#### Expectations[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#expectations "Direct link to Expectations")
LangSmith provides an `expect` utility to help define expectations about your LLM output. For example:
```
from langsmith import expect@unitdeftest_sql_generation_select_all():  user_query ="Get all users from the customers table"  sql = generate_sql(user_query)  expect(sql).to_contain("customers")
```

This will log the binary "expectation" score to the experiment results, additionally `assert`ing that the expectation is met possibly triggering a test failure.
`expect` also provides "fuzzy match" methods. For example:
```
@unit@pytest.mark.parametrize("query, expectation",[("what's the capital of France?","Paris"),],)deftest_embedding_similarity(query, expectation):  prediction = my_chatbot(query)  expect.embedding_distance(# This step logs the distance as feedback for this run    prediction=prediction, reference=expectation# Adding a matcher (in this case, 'to_be_*"), logs 'expectation' feedback).to_be_less_than(0.5)# Optional predicate to assert against  expect.edit_distance(# This computes the normalized Damerau-Levenshtein distance between the two strings    prediction=prediction, reference=expectation# If no predicate is provided below, 'assert' isn't called, but the score is still logged)
```

This test case will be assigned 4 scores:
  1. The `embedding_distance` between the prediction and the expectation
  2. The binary `expectation` score (1 if cosine distance is less than 0.5, 0 if not)
  3. The `edit_distance` between the prediction and the expectation
  4. The overall test pass/fail score (binary)


The `expect` utility is modeled off of [Jest](https://jestjs.io/docs/expect)'s expect API, with some off-the-shelf functionality to make it easier to grade your LLMs.
#### Dry-run mode[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#dry-run-mode "Direct link to Dry-run mode")
If you want to run the tests without syncing the results to LangSmith, you can set `LANGCHAIN_TEST_TRACKING=false` in your environment.
```
LANGCHAIN_TEST_TRACKING=false pytest tests/
```

The tests will run as normal, but the experiment logs will not be sent to LangSmith.
#### Caching[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#caching "Direct link to Caching")
LLMs on every commit in CI can get expensive. To save time and resources, LangSmith lets you cache results to disk. Any identical inputs will be loaded from the cache so you don't have to call out to your LLM provider unless there are changes to the model, prompt, or retrieved data.
To enable caching, run with `LANGCHAIN_TEST_CACHE=/my/cache/path`. For example:
```
LANGCHAIN_TEST_CACHE=tests/cassettes pytest tests/my_llm_tests
```

All requests will be cached to `tests/cassettes` and loaded from there on subsequent runs. If you check this in to your repository, your CI will be able to use the cache as well.
#### Using `watch` mode[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#using-watch-mode "Direct link to using-watch-mode")
With caching enabled, you can iterate quickly on your tests using `watch` mode without worrying about unnecessarily hitting your LLM provider. For example, using [`pytest-watch`](https://pypi.org/project/pytest-watch/):
```
pip install pytest-watchLANGCHAIN_TEST_CACHE=tests/cassettes ptw tests/my_llm_tests
```

## Explanations[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#explanations "Direct link to Explanations")
The `@unit` test decorator converts any unit test into a parametrized LangSmith example. By default, all unit tests within a given file will be grouped as a single "test suite" with a corresponding dataset.
The following metrics are available off-the-shelf:
Feedback| Description| Example  
---|---|---  
`pass`| Binary pass/fail score, 1 for pass, 0 for fail| `assert False` # Fails  
`expectation`| Binary expectation score, 1 if expectation is met, 0 if not| `expect(prediction).against(lambda x: re.search(r"\b[a-f\d]{8}-[a-f\d]{4}-[a-f\d]{4}-[a-f\d]{4}-[a-f\d]{12}\b", x)` )  
`embedding_distance`| Cosine distance between two embeddings| expect.embedding_distance(prediction=prediction, reference=expectation)  
`edit_distance`| Edit distance between two strings| expect.edit_distance(prediction=prediction, reference=expectation)  
You can also log any arbitrary feeback within a unit test manually using the `client`.
```
from langsmith import unit, Clientfrom langsmith.run_helpers import get_current_run_treeclient = Client()@unitdeftest_foo():  run_tree = get_current_run_tree()  client.create_feedback(run_id=run_tree.id, key="my_custom_feedback", score=1)
```

## Reference[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#reference "Direct link to Reference")
### `expect`[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#expect "Direct link to expect")
`expect` makes it easy to make approximate assertions on test results and log scores to LangSmith. Off-the-shelf, it allows you to compute and compare embedding distances, edit distances, and make custom assertions on values.
#### `expect.embedding_distance(prediction, reference, *, config=None)`[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#expectembedding_distanceprediction-reference--confignone "Direct link to expectembedding_distanceprediction-reference--confignone")
Compute the embedding distance between the prediction and reference.
This logs the embedding distance to LangSmith and returns a [`Matcher`](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#matcher) instance for making assertions on the distance value.
By default, this uses the OpenAI API for computing embeddings.
**Parameters**
  * `prediction` (str): The predicted string to compare.
  * `reference` (str): The reference string to compare against.
  * `config` (Optional[EmbeddingConfig]): Optional configuration for the embedding distance evaluator. Supported options: 
    * `encoder`: A custom encoder function to encode the list of input strings to embeddings. Defaults to the OpenAI API.
    * `metric`: The distance metric to use for comparison. Supported values: "cosine", "euclidean", "manhattan", "chebyshev", "hamming".


**Returns**
A [`Matcher`](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#matcher) instance for the embedding distance value.
#### `expect.edit_distance(prediction, reference, *, config=None)`[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#expectedit_distanceprediction-reference--confignone "Direct link to expectedit_distanceprediction-reference--confignone")
Compute the string distance between the prediction and reference.
This logs the string distance (Damerau-Levenshtein) to LangSmith and returns a [`Matcher`](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#matcher) instance for making assertions on the distance value.
This depends on the `rapidfuzz` package for string distance computation.
**Parameters**
  * `prediction` (str): The predicted string to compare.
  * `reference` (str): The reference string to compare against.
  * `config` (Optional[EditDistanceConfig]): Optional configuration for the string distance evaluator. Supported options: 
    * `metric`: The distance metric to use for comparison. Supported values: "damerau_levenshtein", "levenshtein", "jaro", "jaro_winkler", "hamming", "indel".
    * `normalize_score`: Whether to normalize the score between 0 and 1.


**Returns**
A [`Matcher`](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#matcher) instance for the string distance value.
#### `expect.value(value)`[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#expectvaluevalue "Direct link to expectvaluevalue")
Create a [`Matcher`](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#matcher) instance for making assertions on the given value.
**Parameters**
  * `value` (Any): The value to make assertions on.


**Returns**
A [`Matcher`](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#matcher) instance for the given value.
#### `Matcher`[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#matcher "Direct link to matcher")
A class for making assertions on expectation values.
**`to_be_less_than(value)`**
Assert that the expectation value is less than the given value.
**`to_be_greater_than(value)` **
Assert that the expectation value is greater than the given value.
**`to_be_between(min_value, max_value)`**
Assert that the expectation value is between the given min and max values.
**`to_be_approximately(value, precision=2)`**
Assert that the expectation value is approximately equal to the given value.
**`to_equal(value)`**
Assert that the expectation value equals the given value.
**`to_contain(value)`**
Assert that the expectation value contains the given value.
**`against(func)`**
Assert the expectation value against a custom function.
### `unit` API[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#unit-api "Direct link to unit-api")
The `unit` decorator is used to mark a function as a test case for LangSmith. It ensures that the necessary example data is created and associated with the test function. The decorated function will be executed as a test case, and the results will be recorded and reported by LangSmith.
#### `@unit(id=None, output_keys=None, client=None, test_suite_name=None)`[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#unitidnone-output_keysnone-clientnone-test_suite_namenone "Direct link to unitidnone-output_keysnone-clientnone-test_suite_namenone")
Create a unit test case in LangSmith.
**Parameters**
  * `id` (Optional[uuid.UUID]): A unique identifier for the test case. If not provided, an ID will be generated based on the test function's module and name.
  * `output_keys` (Optional[Sequence[str]]): A list of keys to be considered as the output keys for the test case. These keys will be extracted from the test function's inputs and stored as the expected outputs.
  * `client` (Optional[ls_client.Client]): An instance of the LangSmith client to be used for communication with the LangSmith service. If not provided, a default client will be used.
  * `test_suite_name` (Optional[str]): The name of the test suite to which the test case belongs. If not provided, the test suite name will be determined based on the environment or the package name.


**Environment Variables**
  * `LANGSMITH_TEST_CACHE`: If set, API calls will be cached to disk to save time and costs during testing. Recommended to commit the cache files to your repository for faster CI/CD runs. Requires the 'langsmith[vcr]' package to be installed.
  * `LANGSMITH_TEST_TRACKING`: Set this variable to the path of a directory to enable caching of test results. This is useful for re-running tests without re-executing the code. Requires the 'langsmith[vcr]' package.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/evaluation/faq/unit-testing%3E).
[PreviousRegression Testing](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing)[NextManage Datasets](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)
  * [Write @unit test](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#write-unit-test)
  * [Run tests](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#run-tests)
  * [Going Further](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#going-further)
  * [Explanations](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#explanations)
  * [Reference](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#reference)
    * [`expect`](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#expect)
    * [`unit` API](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing#unit-api)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation/faq/version-datasets

[Skip to main content](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
      * [Create an Evaluator](https://docs.smith.langchain.com/old/evaluation/faq/custom-evaluators)
      * [Use Off-the-Shelf Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations)
      * [Regression Testing](https://docs.smith.langchain.com/old/evaluation/faq/regression-testing)
      * [Unit Test](https://docs.smith.langchain.com/old/evaluation/faq/unit-testing)
      * [Manage Datasets](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)
      * [Version Datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets)
      * [Run Experiments in Browser (no code)](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)
      * [Synthetic Data for Evaluation](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
  * Version Datasets


On this page
# How to version datasets
## Basics[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets#basics "Direct link to Basics")
Any time you _add_ , _update_ , or _delete_ examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and to understand how your dataset has evolved.
By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the "Examples" tab, you can see the state of the dataset at that point in time.
![Version Datasets](https://docs.smith.langchain.com/assets/images/version_dataset-0e86bb335a1bc69a1afc084fec6fe7a3.png)
Note that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the "latest" version of the dataset. Also, by default the **latest version of the dataset is shown in the "Examples" tab** and experiments from **all versions are shown in the "Tests" tab**.
In the "Tests" tab, you can see the results of tests run on the dataset at different versions.
![Version Datasets](https://docs.smith.langchain.com/assets/images/version_dataset_tests-a970d7b56bef1e2b0f949d15cad5f44a.png)
## Tagging versions[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets#tagging-versions "Direct link to Tagging versions")
You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset's history.
For example, you might tag a version of your dataset as "prod" and use it to run tests against your LLM pipeline.
Tagging can be done in the UI by clicking on "+ Tag this version" in the "Examples" tab.
![Tagging Datasets](https://docs.smith.langchain.com/assets/images/tag_this_version-556429eb25b2c5d632742c406d7aeb12.png)
You can also tag versions of your dataset using the SDK. Here's an example of how to tag a version of a dataset using the python SDK:
```
from langsmith import Clientfromt datetime import datetimeclient = Client()initial_time = datetime(2024,1,1,0,0,0)# The timestamp of the version you want to tag# You can tag a specific dataset version with a semantic name, like "prod"client.update_dataset_tag(  dataset_name=toxic_dataset_name, as_of=initial_time, tag="prod")
```

## Running experiments on specific versions of datasets[‚Äã](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets#running-experiments-on-specific-versions-of-datasets "Direct link to Running experiments on specific versions of datasets")
You can execute an experiment on a specific version of a dataset in the sdk by using the `as_of` parameter in `list_examples`. `as_of`
Here is an example of how to run an experiment on a specific version of a dataset using the python SDK:
```
from langsmith.evaluation import evaluatefrom langsmith import Clientclient = Client()result = evaluate(lambda inputs: label_query(**inputs),# Your target to evaluate, [defined elsewhere]  data=client.list_examples(dataset_name="my_dataset", as_of="prod"),  evaluators=[correct_label],# Your evaluators, [defined elsewhere]  experiment_prefix="dataset versioning example",)
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/evaluation/faq/version-datasets%3E).
[PreviousManage Datasets](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets)[NextRun Experiments in Browser (no code)](https://docs.smith.langchain.com/old/evaluation/faq/experiments-app)
  * [Basics](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets#basics)
  * [Tagging versions](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets#tagging-versions)
  * [Running experiments on specific versions of datasets](https://docs.smith.langchain.com/old/evaluation/faq/version-datasets#running-experiments-on-specific-versions-of-datasets)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation/migration

[Skip to main content](https://docs.smith.langchain.com/old/evaluation/migration#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation/migration)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * Migrating to `evaluate`


On this page
# Migrating from `run_on_dataset` to `evaluate`
In python, we've introduced a cleaner `evaluate()` function to replace the `run_on_dataset` function. While we are not deprecating the `run_on_dataset` function, the new function lets you get started and without needing to install `langchain` in your local environment.
This guide will walk you through the process of migrating your existing code from using `run_on_dataset` to leveraging the benefits of `evaluate`.
## Key Differences[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#key-differences "Direct link to Key Differences")
#### 1. `llm_or_chain_factory` -> first positional argument[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#1-llm_or_chain_factory---first-positional-argument "Direct link to 1-llm_or_chain_factory---first-positional-argument")
The "thing you are evaluating" (pipeline, target, model, chain, agent, etc.) is **always** the first positional argument and **always** has the following signature:
```
defpredict(inputs:dict)->dict:"""Call your model or pipeline with the given inputs and return the predictions."""# Example:# result = client.chat.completions.create(...)# response = result.choices[0].message.contentreturn{"output":...}
```

No need to specify the confusing "`llm_or_chain_factory`". If you need to create a new version of your object for each data point, initialize it within the `predict()` function. If you want to evaluate a LangChain object (runnable, etc.), you can directly call `evaluate(chain.invoke, data: ...,...)`.
#### 2. `dataset_name` -> `data`[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#2-dataset_name---data "Direct link to 2-dataset_name---data")
The data field accepts a broader range of inputs, including the dataset name, id, or an iterator over examples. This lets you easily evaluate over a subset of the data to quickly debug.
If you were previously specifying a `dataset_version`, you can directly pass the target version like so:
```
dataset_version ="lates"# your tagged versionresults = evaluate(...,  data=client.list_examples(dataset_name="my_dataset", as_of=dataset_version),...)
```

#### 3. `RunEvalConfig` -> `List[RunEvaluator]`[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#3-runevalconfig---listrunevaluator "Direct link to 3-runevalconfig---listrunevaluator")
The config has been deprecated (removing the LangChain dependency). Instead, directly provide a list of evaluators to the `evaluators` argument.
a. Custom evaluators are simple functions that take a `Run` and an `Example` and return a dictionary with the evaluation results. For example:
```
defexact_match(run: Run, example: Example)->dict:"""Calculate the exact match score of the run."""  expected = example.outputs["answer"]  predicted = run.outputs["output"]return{"score": expected.lower()== predicted.lower(),"key":"exact_match"}evaluate(...,  evaluators=[exact_match],)
```

Anything that subclasses `RunEvaluator` still works as they did before, we just will automatically promote your compatible functions to `RunEvaluator` instances.
b. `LangChain` evaluators can be incorporated using the `LangChainStringEvaluator` wrapper.
For example, if you were previously using the "Criteria" evaluator, this evaluation:
```
eval_config = RunEvalConfig(  evaluators=[RunEvalConfig.Criteria(    criteria={"usefulness":"The prediction is useful if..."},    llm=my_eval_llm,)])client.run_on_dataset(..., eval_config=eval_config)
```

becomes:
```
from langsmith.evaluation import LangChainStringEvaluatorevaluators=[  LangChainStringEvaluator("labeled_criteria",    config={"criteria":{"usefulness":"The prediction is useful if...",},"llm": my_eval_llm,},),]
```

c. For evaluating multi-key datasets using off-the-shelf LangChain evaluators, replace any `input_key`, `reference_key`, `prediction_key` with a custom `prepare_data` function.
If your dataset has a single key for the inputs and reference answer, and if your target pipeline returns a response in a single key, the evaluators can automatically use these responses directly without any additional configuration.
For multi-key datasets, you must explain which values to use for the model prediction, (and optionally for the expected answer and/or inputs). This is done by providing a `prepare_data` function that converts a run and example to a dictionary of `{"input": ..., "prediction": ..., "reference": ...}`.
```
defprepare_data(run: Run, example: Example)->dict:# Run is the trace of your pipeline# Example is a dataset recordreturn{"prediction": run.outputs["output"],"input": example.inputs["input"],"reference": example.outputs["answer"],}qa_evaluator = LangChainStringEvaluator("qa",  prepare_data=prepare_data,  config={"llm": my_qa_llm},)
```

#### 4. `batch_evaluators` -> `summary_evaluators`.[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#4-batch_evaluators---summary_evaluators "Direct link to 4-batch_evaluators---summary_evaluators")
These let you compute custom metrics over the whole dataset. For example, precision:
```
defprecision(runs: List[Run], examples: List[Example])->dict:"""Calculate the precision of the runs."""  expected =[example.outputs["answer"]for example in examples]  predicted =[run.outputs["output"]for run in runs]  tp =sum([p == e for p, e inzip(predictions, expected)if p =="yes"])  fp =sum([p =="yes"and e =="no"for p, e inzip(predictions, expected)])return{"score": tp /(tp + fp),"key":"precision"}
```

#### 5. `project_metadata` -> `metadata`.[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#5-project_metadata---metadata "Direct link to 5-project_metadata---metadata")
#### 6. `project_name` -> `experiment_prefix`.[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#6-project_name---experiment_prefix "Direct link to 6-project_name---experiment_prefix")
`evaluate()` always appends an experiment uuid to the prefix to ensure uniqueness, so you don't have to run into those confusing "project already exists" errors.
#### 7. `concurrency_level` -> `max_concurrency`.[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#7-concurrency_level---max_concurrency "Direct link to 7-concurrency_level---max_concurrency")
## Migration Steps[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#migration-steps "Direct link to Migration Steps")
#### 1. Update your imports:[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#1-update-your-imports "Direct link to 1. Update your imports:")
```
from langsmith.evaluation import evaluate
```

#### 2. Change your `run_on_dataset` call to `evaluate`:[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#2-change-your-run_on_dataset-call-to-evaluate "Direct link to 2-change-your-run_on_dataset-call-to-evaluate")
```
results = evaluate(...,  data=...,  evaluators=[...],  summary_evaluators=[...],  metadata=...,  experiment_prefix=...,  max_concurrency=...,)
```

#### 3. If you were using a factory function, replace it with a direct invocation:[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#3-if-you-were-using-a-factory-function-replace-it-with-a-direct-invocation "Direct link to 3. If you were using a factory function, replace it with a direct invocation:")
```
defpredict(inputs:dict):  my_pipeline =...return my_pipeline.invoke(inputs)
```

#### 4. If you were using LangChain evaluators, wrap them with `LangChainStringEvaluator`:[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#4-if-you-were-using-langchain-evaluators-wrap-them-with-langchainstringevaluator "Direct link to 4-if-you-were-using-langchain-evaluators-wrap-them-with-langchainstringevaluator")
```
from langsmith.evaluation import LangChainStringEvaluatorevaluators=[  LangChainStringEvaluator("embedding_distance"),  LangChainStringEvaluator("labeled_criteria",    config={"criteria":{"usefulness":"The prediction is useful if..."}},    prepare_data=prepare_criteria_data),]
```

#### 5. Update any references to `project_metadata`, `project_name`, `dataset_version`, and `concurrency_level` to use the new argument names.[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#5-update-any-references-to-project_metadata-project_name-dataset_version-and-concurrency_level-to-use-the-new-argument-names "Direct link to 5-update-any-references-to-project_metadata-project_name-dataset_version-and-concurrency_level-to-use-the-new-argument-names")
## Support[‚Äã](https://docs.smith.langchain.com/old/evaluation/migration#support "Direct link to Support")
If you encounter any issues during the migration process or have further questions, please don't hesitate to reach out to our support team at support@langchain.dev. We're here to help ensure a smooth transition!
Happy evaluating!
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/evaluation/migration%3E).
[PreviousRecommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)[NextOverview](https://docs.smith.langchain.com/old/monitoring)
  * [Key Differences](https://docs.smith.langchain.com/old/evaluation/migration#key-differences)
  * [Migration Steps](https://docs.smith.langchain.com/old/evaluation/migration#migration-steps)
  * [Support](https://docs.smith.langchain.com/old/evaluation/migration#support)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation/quickstart

[Skip to main content](https://docs.smith.langchain.com/old/evaluation/quickstart#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation/quickstart)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * Quick Start


On this page
# Evaluation Quick Start
This guide helps you get started evaluating your AI system using LangSmith, so you can deploy the best perfoming model for your needs. This guide gets you started with the basics.
## 1. Install LangSmith[‚Äã](https://docs.smith.langchain.com/old/evaluation/quickstart#1-install-langsmith "Direct link to 1. Install LangSmith")
  * Python
  * TypeScript


```
pip install -U langsmith
```

```
yarn add langchain
```

## 2. Evaluate[‚Äã](https://docs.smith.langchain.com/old/evaluation/quickstart#2-evaluate "Direct link to 2. Evaluate")
Evalution requires a system to test, [data](https://docs.smith.langchain.com/old/evaluation/faq) to serve as test cases, and optionally evaluators to grade the results.
  * Python
  * TypeScript


```
from langsmith import Clientfrom langsmith.schemas import Run, Examplefrom langsmith.evaluation import evaluateimport openaifrom langsmith.wrappers import wrap_openaiclient = Client()# Define dataset: these are your test casesdataset_name ="Rap Battle Dataset"dataset = client.create_dataset(dataset_name, description="Rap battle prompts.")client.create_examples( inputs=[{"question":"a rap battle between Atticus Finch and Cicero"},{"question":"a rap battle between Barbie and Oppenheimer"},], outputs=[{"must_mention":["lawyer","justice"]},{"must_mention":["plastic","nuclear"]},], dataset_id=dataset.id,)# Define AI systemopenai_client = wrap_openai(openai.Client())defpredict(inputs:dict)->dict: messages =[{"role":"user","content": inputs["question"]}] response = openai_client.chat.completions.create(messages=messages, model="gpt-3.5-turbo")return{"output": response}# Define evaluatorsdefmust_mention(run: Run, example: Example)->dict: prediction = run.outputs.get("output")or"" required = example.outputs.get("must_mention")or[] score =all(phrase in prediction for phrase in required)return{"key":"must_mention","score": score}experiment_results = evaluate( predict,# Your AI system data=dataset_name,# The data to predict and grade over evaluators=[must_mention],# The evaluators to score the results experiment_prefix="rap-generator",# A prefix for your experiment names to easily identify them metadata={"version":"1.0.0",},)
```

```
import{ Client }from"langsmith";import{ Run, Example }from"langsmith";import{ EvaluationResult }from"langsmith/evaluation";// Note: native evaluate() function support coming soon to the LangSmith TS SDKimport{ runOnDataset }from"langchain/smith";import OpenAI from"openai";const client =newClient();// Define dataset: these are your test casesconst datasetName ="Rap Battle Dataset";const dataset =await client.createDataset(datasetName,{description:"Rap battle prompts.",});await client.createExamples({ inputs:[{question:"a rap battle between Atticus Finch and Cicero"},{question:"a rap battle between Barbie and Oppenheimer"},], outputs:[{must_mention:["lawyer","justice"]},{must_mention:["plastic","nuclear"]},], datasetId: dataset.id,});// Define AI systemconst openaiClient =newOpenAI();asyncfunctionpredictResult({ question }:{ question:string}){const messages =[{"role":"user","content": question }];const output =await openaiClient.chat.completions.create({   model:"gpt-3.5-turbo",   messages: messages});return{ output };}// Define evaluatorsconst mustMention =async({ run, example }:{ run: Run; example?: Example;}):Promise<EvaluationResult>=>{const mustMention:string[]= example?.outputs?.must_contain ??[];const score = mustMention.every((phrase)=> run?.outputs?.output.includes(phrase));return{ key:"must_mention", score: score,};};awaitrunOnDataset(predictResult,// Your AI system datasetName,// The data to predict and grade over{ evaluationConfig:{customEvaluators:[mustMention]},projectMetadata:{ version:"1.0.0",},});
```

Configure your API key, then run the script to evaluate your system.
  * Python
  * TypeScript


```
export LANGCHAIN_API_KEY=<your api key>
```

```
export LANGCHAIN_API_KEY=<your api key>
```

## 3. Review Results[‚Äã](https://docs.smith.langchain.com/old/evaluation/quickstart#3-review-results "Direct link to 3. Review Results")
The evaluation results will be streamed to a new experiment linked to your "Rap Battle Dataset". You can view the results by clicking on the link printed by the `evaluate` function or by navigating to the [Datasets & Testing](https://smith.langchain.com/datasets) page, clicking "Rap Battle Dataset", and viewing the latest test run.
There, you can inspect the traces and feedback generated from the evaluation configuration.
![Eval test run screenshot](https://docs.smith.langchain.com/assets/images/eval-test-run-ae24edb6531c8c38271eb04a88cc4114.png)
You can click "Open Run" to view the trace and feedback generated for that example.
![Eval trace screenshot](https://docs.smith.langchain.com/assets/images/eval-run-trace-d630b25a5b22cca017398598818b0c82.png)
To compare to another test on this dataset, you can click "Compare Tests".
![Compare Tests](https://docs.smith.langchain.com/assets/images/compare-tests-3dd5d43e3bc28c76b778276904a6e0a0.png)
## More on evaluation[‚Äã](https://docs.smith.langchain.com/old/evaluation/quickstart#more-on-evaluation "Direct link to More on evaluation")
Congratulations! You've now created a dataset and used it to evaluate your agent or LLM. To learn how to make your own custom evaluators, review the [Custom Evaluator](https://docs.smith.langchain.com/old/evaluation/faq) guide. To learn more about some pre-built evaluators available in the LangChain open-source library, check out the [LangChain Evaluators](https://docs.smith.langchain.com/old/evaluation/faq/evaluator-implementations) guide.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousOverview](https://docs.smith.langchain.com/old/evaluation)[NextHow-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
  * [1. Install LangSmith](https://docs.smith.langchain.com/old/evaluation/quickstart#1-install-langsmith)
  * [2. Evaluate](https://docs.smith.langchain.com/old/evaluation/quickstart#2-evaluate)
  * [3. Review Results](https://docs.smith.langchain.com/old/evaluation/quickstart#3-review-results)
  * [More on evaluation](https://docs.smith.langchain.com/old/evaluation/quickstart#more-on-evaluation)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/evaluation/recommendations

[Skip to main content](https://docs.smith.langchain.com/old/evaluation/recommendations#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/evaluation/recommendations)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/old/evaluation/quickstart)
    * [How-To Guides](https://docs.smith.langchain.com/old/evaluation/faq)
    * [Recommendations](https://docs.smith.langchain.com/old/evaluation/recommendations)
    * [Migrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * Recommendations


On this page
# Recommendations
This conceptual guide shares thoughts on how to use testing and evaluations for your LLM applications. There is no one-size-fits-all solution, but we believe the most successful teams will adapt strategies from design, software development, and machine learning to their use cases to deliver better, more reliable results.
### Test early and often[‚Äã](https://docs.smith.langchain.com/old/evaluation/recommendations#test-early-and-often "Direct link to Test early and often")
While "unit tests" don't truly exist for the model, writing "minimum functionality tests" for each chain during the debugging and prototyping stage will help you scaffold more reliable systems.
Datasets facilitate this. Using debugging projects, you can log runs while prototyping. From these runs, you can select representative samples to add to a "Functionality Test Dataset" for that component. Evaluators can be run in CI to ensure that individual chains (or other components) still perform as desired for known use cases.
  * Completing a specific structured schema
  * Selecting the correct tool for a given question
  * Extracting the correct information from a passage
  * Generating valid code for a typical input.
  * Avoiding unwanted behavior when given leading inputs (e.g., appropriate polite refusals, adversarial inputs, etc.)


These datasets can range from anywhere between 10-100+ examples and will continue to grow as you capture more example traces and add them as known tests.
### Create domain-specific evaluators[‚Äã](https://docs.smith.langchain.com/old/evaluation/recommendations#create-domain-specific-evaluators "Direct link to Create domain-specific evaluators")
LangChain has strong and configurable built-in evaluators for common tasks, and everyone will benefit from your [contributions to these evaluators](https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/evaluation). However, often the best evaluation metrics are domain-specific. Some examples include:
  * Evaluate the validity and efficiency of domain-specific code
  * Applying custom rules to check the response output against a proprietary system
  * Asserting numerical values are within a certain range


### Use labels where possible[‚Äã](https://docs.smith.langchain.com/old/evaluation/recommendations#use-labels-where-possible "Direct link to Use labels where possible")
When adding examples to a dataset, you can improve the output to represent a "gold standard" label. Evaluators that compare outputs against labels generally are much more reliable than those that have to operate "reference-free."
Once you have deployed an application, capture and filter user feedback (even in testing deployments) to help improve the signal.
### Use aggregate evals[‚Äã](https://docs.smith.langchain.com/old/evaluation/recommendations#use-aggregate-evals "Direct link to Use aggregate evals")
Testing individual data points is useful for asserting behavior on known examples, but a lot of information can only be measured in aggregate. Aggregating automated feedback over a dataset can help you detect differences in performance across component versions or between configurations.
These datasets are usually 100-1000+ examples to return statistically significant results.
### Measure model stability[‚Äã](https://docs.smith.langchain.com/old/evaluation/recommendations#measure-model-stability "Direct link to Measure model stability")
LLMs can be non-deterministic. They also can be sensitive to small (even imperceptible) changes in the input. Generating a dataset of synthetic examples is a good way to measure this. Some common approaches to address this usually start from a representative dataset and then:
  * Generate examples using explicit transformations that don't change the meaning of the input, such as changing pronouns or roles, verb tense, misspellings, paraphrasing, etc. These are semantic invariance tests
  * Generate "similar examples" from the model (or differently tokenized LLMs). When evaluating, ensure that the correctness or other metrics don't change, or ask the model to assert whether the outputs are equivalent.
  * (If the model's temperature > 0) run the model multiple times and grade whether outputs are consistent.


### Measure performance on subsets[‚Äã](https://docs.smith.langchain.com/old/evaluation/recommendations#measure-performance-on-subsets "Direct link to Measure performance on subsets")
Use tags or organize datasets based on cohorts or important properties to return stratified results for different groups. This can help you quantify your application's bias or other issues that might not be apparent when looking at intermingled results.
### Evaluate production data[‚Äã](https://docs.smith.langchain.com/old/evaluation/recommendations#evaluate-production-data "Direct link to Evaluate production data")
Once you have deployed an application, you can use the same evaluators to measure performance or behavior on real user data. This can help you identify issues that might not be apparent during testing, and it can help quantify signals that are contained in the unstructured data. These can be used alongside other application metrics to help better understand ways to improve your application.
You can also log proxy metrics (such as click-through/response rate) as feedback to measure to drive better analysis.
Check out some cookbook examples for this:
  * [Evaluate production runs (batch)](https://github.com/langchain-ai/langsmith-cookbook/blob/main/feedback-examples/algorithmic-feedback/algorithmic_feedback.ipynb): automate feedback metrics for advanced monitoring and performance tuning.
  * [Evaluate production runs (real-time)](https://github.com/langchain-ai/langsmith-cookbook/blob/main/feedback-examples/realtime-algorithmic-feedback/realtime_feedback.ipynb): automatically generate feedback metrics for every run using an async callback.


### Don't train on test datasets[‚Äã](https://docs.smith.langchain.com/old/evaluation/recommendations#dont-train-on-test-datasets "Direct link to Don't train on test datasets")
If you've ever heard of the "train, validation, test" splits in ML, you are well aware that if you use a dataset for optimizing a prompt, fine-tuning an LLM, or picking other configurable parameters in your setup, it's important to keep this separate from the datasets you use for testing. Otherwise, you risk "overfitting" to the test data, which will likely lead to poor performance on new data once you deploy it.
### Test the model yourself[‚Äã](https://docs.smith.langchain.com/old/evaluation/recommendations#test-the-model-yourself "Direct link to Test the model yourself")
Looking at the data remains an effective (albeit time-consuming) evaluation technique in many scenarios. You can evaluate runs yourself and log feedback from your application users using
LangSmith exposes this in the client via a `create_feedback` method. We recommend adding as many signals as possible and using tags and aggregated feedback to help you understand what is happening in your application.
### Ask appropriate questions[‚Äã](https://docs.smith.langchain.com/old/evaluation/recommendations#ask-appropriate-questions "Direct link to Ask appropriate questions")
When thinking of what evaluator to use, it is helpful to think of "what question I need to answer" to be sure that my application has the desired behavior. Then it's vital to decide "what question can I reasonably answer given the input data".
Asking a model if an output is "correct" will likely return an unreliable result, unless the model has additional information (such as a ground-truth label) that wasn't available to the evaluated model.
It can be useful to select evaluators based on whether labels are present and what types of information would be useful for your concerns. Below are a few examples:
Reference Free| With References  
---|---  
Pass/Fail| Exact/fuzzy string match; "Does this output answer the user's question? YES or NO| "Is the generated output equivalent to the answer?"  
Scoring| Perplexity / Normalized log probs; "On a scale from 1 to 5, 1 being a ___ and 5 being ___, how ___ is this output?"| ROUGE/BLEU; ‚ÄúOn a scale from 1 to 5, how similar is ‚Ä¶?‚Äù  
Labeling| "Is this output mostly related to sports, finance, pop culture, or other?"| _less useful here_  
Comparisons| Which of these outputs best responds to the following input: 1. ___ 2. ___| _less useful here_  
You'll notice that we've included some more traditional NLP measurements like "perplexity" or "ROUGE" alongside natural language questions prompted to an LLM. Both techniques have their place and very notable limitations. We recommend using a combination of these approaches to get a more complete picture of your application's performance.
### Other Resources[‚Äã](https://docs.smith.langchain.com/old/evaluation/recommendations#other-resources "Direct link to Other Resources")
There's a lot of great work that has been done in this space. Some resources our community have found useful include:
  * ELeutherAI's [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness/)
  * HuggingFace [Evaluate](https://huggingface.co/docs/evaluate/index)
  * OpenAI's [evals](https://github.com/openai/evals/) repository
  * Chatbot Arena [lmsys](https://chat.lmsys.org/)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/evaluation/recommendations%3E).
[PreviousSynthetic Data for Evaluation](https://docs.smith.langchain.com/old/evaluation/faq/synthetic-data)[NextMigrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)
  * [Test early and often](https://docs.smith.langchain.com/old/evaluation/recommendations#test-early-and-often)
  * [Create domain-specific evaluators](https://docs.smith.langchain.com/old/evaluation/recommendations#create-domain-specific-evaluators)
  * [Use labels where possible](https://docs.smith.langchain.com/old/evaluation/recommendations#use-labels-where-possible)
  * [Use aggregate evals](https://docs.smith.langchain.com/old/evaluation/recommendations#use-aggregate-evals)
  * [Measure model stability](https://docs.smith.langchain.com/old/evaluation/recommendations#measure-model-stability)
  * [Measure performance on subsets](https://docs.smith.langchain.com/old/evaluation/recommendations#measure-performance-on-subsets)
  * [Evaluate production data](https://docs.smith.langchain.com/old/evaluation/recommendations#evaluate-production-data)
  * [Don't train on test datasets](https://docs.smith.langchain.com/old/evaluation/recommendations#dont-train-on-test-datasets)
  * [Test the model yourself](https://docs.smith.langchain.com/old/evaluation/recommendations#test-the-model-yourself)
  * [Ask appropriate questions](https://docs.smith.langchain.com/old/evaluation/recommendations#ask-appropriate-questions)
  * [Other Resources](https://docs.smith.langchain.com/old/evaluation/recommendations#other-resources)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/hub/dev-setup

[Skip to main content](https://docs.smith.langchain.com/old/hub/dev-setup#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/hub/dev-setup)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
    * [Quick Start](https://docs.smith.langchain.com/old/hub/quickstart)
    * [Developer Setup](https://docs.smith.langchain.com/old/hub/dev-setup)
    * [FAQs](https://docs.smith.langchain.com/old/hub/faq)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * Developer Setup


On this page
# Developer Setup
This guide will continue from the hub quickstart, using the Python or TypeScript SDK to interact with the hub instead of the Playground UI.
This guide assumes you've gone through the Hub [Quick Start](https://docs.smith.langchain.com/old/hub/quickstart) including login-required steps.
If you don't yet have an account, you'll only be able to pull public objects.
## 1. Install/upgrade packages[‚Äã](https://docs.smith.langchain.com/old/hub/dev-setup#1-installupgrade-packages "Direct link to 1. Install/upgrade packages")
**Note:** You likely need to upgrade even if they're already installed!
  * pip
  * yarn
  * npm


```
pip install -U langchain langchainhub langchain-openai
```

```
yarn add langchain
```

```
npm install -S langchain
```

## 2. Configuring environment variables[‚Äã](https://docs.smith.langchain.com/old/hub/dev-setup#2-configuring-environment-variables "Direct link to 2. Configuring environment variables")
Get an API key for your **Personal** organization if you have not yet. The hub will not work with your non-personal organization's api key!
```
export LANGCHAIN_HUB_API_KEY="ls_..."
```

If you already have `LANGCHAIN_API_KEY` set to a personal organization‚Äôs api key from LangSmith, you can skip this.
## 3. Pull an object from the hub and use it[‚Äã](https://docs.smith.langchain.com/old/hub/dev-setup#3-pull-an-object-from-the-hub-and-use-it "Direct link to 3. Pull an object from the hub and use it")
  * Python
  * TypeScript


```
from langchain import hub# pull a chat promptprompt = hub.pull("efriis/my-first-prompt")# create a model to use it withfrom langchain_openai import ChatOpenAImodel = ChatOpenAI()# use it in a runnablerunnable = prompt | modelresponse = runnable.invoke({"profession":"biologist","question":"What is special about parrots?",})print(response)
```

```
// importimport*as hub from"langchain/hub";import{ ChatPromptTemplate }from"@langchain/core/prompts";import{ ChatOpenAI }from"@langchain/openai";// pull a chat promptconst prompt =await hub.pull<ChatPromptTemplate>("efriis/my-first-prompt");// create a model to use it withconst model =newChatOpenAI();// use it in a runnableconst runnable = prompt.pipe(model);const result =await runnable.invoke({"profession":"biologist","question":"What is special about parrots?",});console.log(result);
```

## 4. Push a prompt to your personal organization[‚Äã](https://docs.smith.langchain.com/old/hub/dev-setup#4-push-a-prompt-to-your-personal-organization "Direct link to 4. Push a prompt to your personal organization")
For this step, you'll need the `handle` for your account!
  * Python
  * TypeScript


```
from langchain import hubfrom langchain.prompts.chat import ChatPromptTemplateprompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")hub.push("topic-joke-generator", prompt, new_repo_is_public=False)
```

```
import*as hub from"langchain/hub";import{ ChatPromptTemplate, HumanMessagePromptTemplate,}from'@langchain/core/prompts';const message = HumanMessagePromptTemplate.fromTemplate('tell me a joke about {topic}');const prompt = ChatPromptTemplate.fromMessages([message]);await hub.push("my-first-prompt", prompt,{ newRepoIsPublic:false});
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/hub/dev-setup%3E).
[PreviousQuick Start](https://docs.smith.langchain.com/old/hub/quickstart)[NextFAQs](https://docs.smith.langchain.com/old/hub/faq)
  * [1. Install/upgrade packages](https://docs.smith.langchain.com/old/hub/dev-setup#1-installupgrade-packages)
  * [2. Configuring environment variables](https://docs.smith.langchain.com/old/hub/dev-setup#2-configuring-environment-variables)
  * [3. Pull an object from the hub and use it](https://docs.smith.langchain.com/old/hub/dev-setup#3-pull-an-object-from-the-hub-and-use-it)
  * [4. Push a prompt to your personal organization](https://docs.smith.langchain.com/old/hub/dev-setup#4-push-a-prompt-to-your-personal-organization)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/hub/faq

[Skip to main content](https://docs.smith.langchain.com/old/hub/faq#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/hub/faq)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
    * [Quick Start](https://docs.smith.langchain.com/old/hub/quickstart)
    * [Developer Setup](https://docs.smith.langchain.com/old/hub/dev-setup)
    * [FAQs](https://docs.smith.langchain.com/old/hub/faq)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * FAQs


On this page
# Frequently Asked Questions
### What is LangChain Hub?[‚Äã](https://docs.smith.langchain.com/old/hub/faq#what-is-langchain-hub "Direct link to What is LangChain Hub?")
[LangChain Hub](https://smith.langchain.com/hub) lets you discover, version control, and experiment with different prompts for LangChain and LLMs in general directly in your browser.
### How do I share a private prompt with my teammates?[‚Äã](https://docs.smith.langchain.com/old/hub/faq#how-do-i-share-a-private-prompt-with-my-teammates "Direct link to How do I share a private prompt with my teammates?")
You can share prompts within a LangSmith organization by uploading them within a shared organization.
First, create an API key for your organization, then set the variable in your development environment:
```
export LANGCHAIN_HUB_API_KEY = "ls__.."
```

Then, you can upload prompts to the organization. Assuming your organization's handle is "my-organization":
```
from langchain import hubprompt =...hub.push("my-organization/my-prompt-name", prompt, new_repo_is_public=False)
```

Now, all your team-members within your LangSmith organization will be able to view, pull, and open the prompt in the playground.
### Why can't I push anything other than prompts?[‚Äã](https://docs.smith.langchain.com/old/hub/faq#why-cant-i-push-anything-other-than-prompts "Direct link to Why can't I push anything other than prompts?")
Hub currently only supports LangChain prompt objects. We are working on adding support for more!
If you have a specific request, please join the `hub-feedback` [discord](https://discord.gg/6adMQxSpJS) channel and let us know!
### Can I upload a prompt to the hub from a LangSmith Trace?[‚Äã](https://docs.smith.langchain.com/old/hub/faq#can-i-upload-a-prompt-to-the-hub-from-a-langsmith-trace "Direct link to Can I upload a prompt to the hub from a LangSmith Trace?")
Coming soon!
### Can LangChain Hub do ____?[‚Äã](https://docs.smith.langchain.com/old/hub/faq#can-langchain-hub-do-____ "Direct link to Can LangChain Hub do ____?")
Maybe, and we'd love to hear from you! Please join the `hub-feedback` [discord](https://discord.gg/6adMQxSpJS) channel
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/hub/faq%3E).
[PreviousDeveloper Setup](https://docs.smith.langchain.com/old/hub/dev-setup)[NextProxy](https://docs.smith.langchain.com/old/category/proxy)
  * [What is LangChain Hub?](https://docs.smith.langchain.com/old/hub/faq#what-is-langchain-hub)
  * [How do I share a private prompt with my teammates?](https://docs.smith.langchain.com/old/hub/faq#how-do-i-share-a-private-prompt-with-my-teammates)
  * [Why can't I push anything other than prompts?](https://docs.smith.langchain.com/old/hub/faq#why-cant-i-push-anything-other-than-prompts)
  * [Can I upload a prompt to the hub from a LangSmith Trace?](https://docs.smith.langchain.com/old/hub/faq#can-i-upload-a-prompt-to-the-hub-from-a-langsmith-trace)
  * [Can LangChain Hub do ____?](https://docs.smith.langchain.com/old/hub/faq#can-langchain-hub-do-____)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/hub/quickstart

[Skip to main content](https://docs.smith.langchain.com/old/hub/quickstart#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/hub/quickstart)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
    * [Quick Start](https://docs.smith.langchain.com/old/hub/quickstart)
    * [Developer Setup](https://docs.smith.langchain.com/old/hub/dev-setup)
    * [FAQs](https://docs.smith.langchain.com/old/hub/faq)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * Quick Start


On this page
# Quick Start
## What is LangChain Hub?[‚Äã](https://docs.smith.langchain.com/old/hub/quickstart#what-is-langchain-hub "Direct link to What is LangChain Hub?")
LangChain Hub lets you discover, share, and version control prompts for LangChain and LLMs in general.
It's a great place to find inspiration for your own prompts, or to share your own prompts with the world!
Currently, it supports LangChain prompt templates, and more object types are coming soon.
Public Features
Some hub features can be used without logging in [here](https://smith.langchain.com/hub)! However, creating and modifying prompts requires a non-waitlisted account.
The steps in this guide will acquaint you with LangChain Hub:
  1. Browse the hub for a prompt of interest
  2. Try out a prompt in the playground
  3. Log in and set a handle
  4. Modify the prompt in the playground and commit it back to the hub


## 1. Browse the hub for a prompt of interest[‚Äã](https://docs.smith.langchain.com/old/hub/quickstart#1-browse-the-hub-for-a-prompt-of-interest "Direct link to 1. Browse the hub for a prompt of interest")
You can access the public view of the hub directly at <https://smith.langchain.com/hub>.
From this screen, you can browse prompts that others have published.
![Hub Home](https://docs.smith.langchain.com/assets/images/hub-home-c7334ec3c1d0ecaee1a56075f2c939b6.png)
On the left side, you can filter by different **tags** , such as types of prompts (Chat vs. Regular), use cases, or models they're designed to be used with (llama-2 vs. gpt-4).
If you see a prompt that looks interesting, you can either click into its page, or you can jump straight to using it in the playground with the "Try it" button.
## 2. Try out a prompt in the playground[‚Äã](https://docs.smith.langchain.com/old/hub/quickstart#2-try-out-a-prompt-in-the-playground "Direct link to 2. Try out a prompt in the playground")
For this example, we'll use the following prompt: [https://smith.langchain.com/hub/langchain/my-first-prompt](https://smith.langchain.com/hub/efriis/my-first-prompt)
![Hub Prompt](https://docs.smith.langchain.com/assets/images/hub-repo-0d8c5dbe0db32ae3b5e83dd73e611211.png)
To start, you can get a sense of what the prompt does just by looking at it (this one is pretty straightforward). Below the contents of the prompt, you can see a code snippet of how to use it in Python. For more information on using hub prompts from code, finish this guide and check out the [developer guide](https://docs.smith.langchain.com/old/hub/dev-setup).
Next, let's try out the object in the playground by clicking the [playground button](https://smith.langchain.com/hub/efriis/my-first-prompt/playground) in the top-right.
The playground should look like this:
![Hub Playground](https://docs.smith.langchain.com/assets/images/hub-playground-666e76e18932d9c8dfcab5092f80912c.png)
On the left, we see an editable view of our Chat Prompt.
To the right, we can configure our inputs and model, and above that, you can add API Keys for playground-supported model providers.
Let's try it out! First, fill out your OpenAI API Key in "Secrets", and fill out a profession (e.g. biologist) and question (e.g. "what is 1 fun fact about parrots?"). Now click "Run"! The output should look something like this:
![Hub Playground after running](https://docs.smith.langchain.com/assets/images/hub-playground-complete-235d14ebd92886084c3201e73b0c3945.png)
## 3. Log in and set a handle (One-Time Setup)[‚Äã](https://docs.smith.langchain.com/old/hub/quickstart#3-log-in-and-set-a-handle-one-time-setup "Direct link to 3. Log in and set a handle \(One-Time Setup\)")
note
You must log into an account, where the "Personal" organization is not waitlisted, in order to proceed with steps 3 and 4.
Now that we've tried out a prompt, let's log in and set a handle. This will allow us to commit changes to the prompt back to the hub under our own user!
First, let's click that "Login" button in the top-right and log in. If you find yourself on the waitlist, you can enter your access code if you have one, or hang tight! We're letting people off every week.
Second, go to the [hub home](https://smith.langchain.com/hub). If you haven't set a handle yet, you'll be prompted to add one, so people can associate your prompts with you!
The handle reserves a namespace for all of your prompts, so everything you add will be saved under `your-handle/prompt-name`.
![Hub Set Handle](https://docs.smith.langchain.com/assets/images/hub-handle-5b7a30f5a262b3d82909c5e793a4a8df.png)
## 4. Commit to a new repo under your user[‚Äã](https://docs.smith.langchain.com/old/hub/quickstart#4-commit-to-a-new-repo-under-your-user "Direct link to 4. Commit to a new repo under your user")
Staying on [efriis/my-first-prompt's playground](https://smith.langchain.com/hub/efriis/my-first-prompt/playground), let's make some changes and commit it back to a repository under your user. To start, let's modify the prompt to change the style of the response. Instead of answering "cheerfully," pick a new adverb (e.g. "angrily").
Once you've run it, we can commit it directly to a new prompt under your user. Click the "Commit" button in the top-right, and you should see a modal like this:
**Note:** you may have to return to the repo page and click the "Playground" button in order to see the "Commit" button.
![Hub Playground Commit](https://docs.smith.langchain.com/assets/images/hub-playground-commit-b331f908c238c87dc3c33e66eacadda2.png)
Here, you can create a new repo called `my-first-prompt` and use this as a first commit! Once you've done that, you'll be redirected to your new prompt.
## 5. Create an example for your repo[‚Äã](https://docs.smith.langchain.com/old/hub/quickstart#5-create-an-example-for-your-repo "Direct link to 5. Create an example for your repo")
To show off potential use cases for the prompt, let's add an example to our new repo! Fill out another profession and question and click "Run" again. When the run has finished, click "Save as Example" to move it into the left panel as an example. Now commit to your repo again and click on your repo's name at the top of the screen to navigate back to your repo's home page.
There, you should see your new example! These examples are publicly shown on your repo page.
![Hub Set Handle](https://docs.smith.langchain.com/assets/images/hub-example-082fa286eb56a6bbcf736c331decc64b.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/hub/quickstart%3E).
[PreviousPrompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)[NextDeveloper Setup](https://docs.smith.langchain.com/old/hub/dev-setup)
  * [What is LangChain Hub?](https://docs.smith.langchain.com/old/hub/quickstart#what-is-langchain-hub)
  * [1. Browse the hub for a prompt of interest](https://docs.smith.langchain.com/old/hub/quickstart#1-browse-the-hub-for-a-prompt-of-interest)
  * [2. Try out a prompt in the playground](https://docs.smith.langchain.com/old/hub/quickstart#2-try-out-a-prompt-in-the-playground)
  * [3. Log in and set a handle (One-Time Setup)](https://docs.smith.langchain.com/old/hub/quickstart#3-log-in-and-set-a-handle-one-time-setup)
  * [4. Commit to a new repo under your user](https://docs.smith.langchain.com/old/hub/quickstart#4-commit-to-a-new-repo-under-your-user)
  * [5. Create an example for your repo](https://docs.smith.langchain.com/old/hub/quickstart#5-create-an-example-for-your-repo)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring

[Skip to main content](https://docs.smith.langchain.com/old/monitoring#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * Production Monitoring & Automations


# Production Monitoring & Automations
Closely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you‚Äôll also want to do once your app hits production. It's also crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.
In order to facilitate this, LangSmith supports a series of workflows to support production monitoring and automations. This includes support for easily exploring and visualizing key production metrics, as well as support for defining automations to process the data.
To get started, check out the [Quick Start Guide](https://docs.smith.langchain.com/old/monitoring/quickstart).
After that, peruse the [Concepts Section](https://docs.smith.langchain.com/old/monitoring/concepts) to better understand the different components involved with monitoring and automations.
If you want to learn how to accomplish a particular task, check out our comprehensive [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
For example use cases, check out the [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases) page.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring%3E).
[PreviousMigrating to `evaluate`](https://docs.smith.langchain.com/old/evaluation/migration)[NextQuick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/concepts

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/concepts#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/concepts)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * Concepts


On this page
# Concepts
In this guide we will go over some of the concepts that are important to understand when thinking about production logging and automations in LangSmith. A lot of these concepts build off of tracing concepts - it is recommended to read the [Tracing Concepts](https://docs.smith.langchain.com/old/tracing/concepts) documentation before.
## Runs[‚Äã](https://docs.smith.langchain.com/old/monitoring/concepts#runs "Direct link to Runs")
A `Run` is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with [OpenTelemetry](https://opentelemetry.io/), you can think of a run as a span.
## Traces[‚Äã](https://docs.smith.langchain.com/old/monitoring/concepts#traces "Direct link to Traces")
A `Trace` is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with [OpenTelemetry](https://opentelemetry.io/), you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.
## Filter[‚Äã](https://docs.smith.langchain.com/old/monitoring/concepts#filter "Direct link to Filter")
A `Filter` is a rule that applies to all runs in a project. The default filter you see on the main page selects only top-level runs (`IsRoot` is `true`) but you can filter for sub-runs as well.
You can even filter for runs that are part of a trace with a particular attribute, for example filtering for runs with name `"ChatOpenAI"` that are part of a trace with `user_score` equal to 0.
## Threads[‚Äã](https://docs.smith.langchain.com/old/monitoring/concepts#threads "Direct link to Threads")
A `Thread` is a sequence of traces representing a conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same conversation.
You can track threads by attaching a special metadata key to runs (one of `session_id`, `thread_id` or `conversation_id`).
See [this documentation](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#adding-metadata-and-tags-to-traces) for how to add metadata keys to a trace.
## Monitoring Dashboard[‚Äã](https://docs.smith.langchain.com/old/monitoring/concepts#monitoring-dashboard "Direct link to Monitoring Dashboard")
Monitoring dashboards display key metrics over time for a configurable time window. We track LLM related statistics like latency, feedback, time-to-first-token, cost, and more. You can see these dashboard by going to the `Monitors` tab.
## Rules[‚Äã](https://docs.smith.langchain.com/old/monitoring/concepts#rules "Direct link to Rules")
Rules represent actions that are taken on a set of runs. These are configured by a specifying a filter rule, a sampling rate, and an action to take. Currently, the supported actions are adding to dataset, adding to annotation queue, and sending to online evaluation.
An example of a rule could be, in plain English, "Run a 'vagueness' evaluator on 70% of root runs with a feedback score of 0 for the feedback key `user_score`"
## Datasets[‚Äã](https://docs.smith.langchain.com/old/monitoring/concepts#datasets "Direct link to Datasets")
Datasets are a way to collect examples, which are input-output pairs. You can use datasets for evaluation, as well as fine-tuning and few-shot prompting. For more information, see [here](https://docs.smith.langchain.com/old/evaluation)
## Annotation Queues[‚Äã](https://docs.smith.langchain.com/old/monitoring/concepts#annotation-queues "Direct link to Annotation Queues")
Annotation Queues are a user-friendly way to annotate a lot of traces.
## Online Evaluation[‚Äã](https://docs.smith.langchain.com/old/monitoring/concepts#online-evaluation "Direct link to Online Evaluation")
Online evaluation is when an LLM is used to assign feedback to particular runs.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/concepts%3E).
[PreviousQuick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)[NextHow-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
  * [Runs](https://docs.smith.langchain.com/old/monitoring/concepts#runs)
  * [Traces](https://docs.smith.langchain.com/old/monitoring/concepts#traces)
  * [Filter](https://docs.smith.langchain.com/old/monitoring/concepts#filter)
  * [Threads](https://docs.smith.langchain.com/old/monitoring/concepts#threads)
  * [Monitoring Dashboard](https://docs.smith.langchain.com/old/monitoring/concepts#monitoring-dashboard)
  * [Rules](https://docs.smith.langchain.com/old/monitoring/concepts#rules)
  * [Datasets](https://docs.smith.langchain.com/old/monitoring/concepts#datasets)
  * [Annotation Queues](https://docs.smith.langchain.com/old/monitoring/concepts#annotation-queues)
  * [Online Evaluation](https://docs.smith.langchain.com/old/monitoring/concepts#online-evaluation)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/faq

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/faq#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/faq)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
      * [Filtering](https://docs.smith.langchain.com/old/monitoring/faq/filter)
      * [Monitoring](https://docs.smith.langchain.com/old/monitoring/faq/monitoring)
      * [Threads](https://docs.smith.langchain.com/old/monitoring/faq/threads)
      * [Online Evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation)
      * [Automations](https://docs.smith.langchain.com/old/monitoring/faq/automations)
      * [Annotation Queue](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue)
      * [How to use webhooks with automations](https://docs.smith.langchain.com/old/monitoring/faq/webhooks)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * How-To Guides


On this page
# How-To Guides
In this section you will find guides for how to use LangSmith production monitoring and automations.
## Filters[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq#filters "Direct link to Filters")
  * [How to create a filter](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-create-a-filter)
  * [How to filter for sub runs](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-filter-for-sub-runs)
  * [How to filter for runs whose child runs have some attribute](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-filter-for-runs-whose-child-runs-have-some-attribute)


## Monitors[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq#monitors "Direct link to Monitors")
  * [How to change the time period](https://docs.smith.langchain.com/old/monitoring/faq/monitoring#how-to-change-the-time-period)
  * [How to view monitors by subsets](https://docs.smith.langchain.com/old/monitoring/faq/monitoring#how-to-view-monitors-by-subset)
  * [How to drill into specific subsets](https://docs.smith.langchain.com/old/monitoring/faq/monitoring#how-to-drill-into-specific-subsets)


## Automations[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq#automations "Direct link to Automations")
  * [How to create a filter](https://docs.smith.langchain.com/old/monitoring/faq/automations#how-to-create-a-filter)
  * [How to specify a sampling rate](https://docs.smith.langchain.com/old/monitoring/faq/automations#how-to-specify-a-sampling-rate)
  * [How to manage automations](https://docs.smith.langchain.com/old/monitoring/faq/automations#how-to-manage-automations)
  * [How to use webhooks with automations](https://docs.smith.langchain.com/old/monitoring/faq/webhooks)


## Annotation Queues[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq#annotation-queues "Direct link to Annotation Queues")
  * [How to view annotation queues](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-view-annotation_queues)
  * [How to add feedback](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-add-feedback)
  * [How to leave a note](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-leave-a-note)
  * [How to see the trace of the run](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-see-the-trace-of-the-run)
  * [How to cycle through a queue](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-cycle-through-a-queue)


## Online Evaluation[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq#online-evaluation "Direct link to Online Evaluation")
  * [How to set up online evaluations](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#how-to-set-up-online-evaluation)
  * [How to configurate online evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#how-to-configure-online-evaluation)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/faq%3E).
[PreviousConcepts](https://docs.smith.langchain.com/old/monitoring/concepts)[NextFiltering](https://docs.smith.langchain.com/old/monitoring/faq/filter)
  * [Filters](https://docs.smith.langchain.com/old/monitoring/faq#filters)
  * [Monitors](https://docs.smith.langchain.com/old/monitoring/faq#monitors)
  * [Automations](https://docs.smith.langchain.com/old/monitoring/faq#automations)
  * [Annotation Queues](https://docs.smith.langchain.com/old/monitoring/faq#annotation-queues)
  * [Online Evaluation](https://docs.smith.langchain.com/old/monitoring/faq#online-evaluation)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
      * [Filtering](https://docs.smith.langchain.com/old/monitoring/faq/filter)
      * [Monitoring](https://docs.smith.langchain.com/old/monitoring/faq/monitoring)
      * [Threads](https://docs.smith.langchain.com/old/monitoring/faq/threads)
      * [Online Evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation)
      * [Automations](https://docs.smith.langchain.com/old/monitoring/faq/automations)
      * [Annotation Queue](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue)
      * [How to use webhooks with automations](https://docs.smith.langchain.com/old/monitoring/faq/webhooks)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
  * Annotation Queue


On this page
# How to use annotation queues
Annotation queues are a user friendly way to quickly cycle through and annotate data.
## How to view annotation queues[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-view-annotation-queues "Direct link to How to view annotation queues")
You can see what annotation queues your organization has by navigating to there on the left-hand sidebar.
![Annotation Queue](https://docs.smith.langchain.com/assets/images/annotation_queue-8b665a2be7bd4959c2642d729cf348d3.png)
## How to add feedback[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-add-feedback "Direct link to How to add feedback")
You can leave feedback by clicking the `Add tag` button in the upper right hand corner
![Annotation Queue](https://docs.smith.langchain.com/assets/images/add_tag-3d6f1dbbdf94decb3215f1ae5a0767cf.png)
## How to leave a note[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-leave-a-note "Direct link to How to leave a note")
You can also leave free form feedback in the form of a note.
![Annotation Queue](https://docs.smith.langchain.com/assets/images/notes-092151932ca34d789b3b976653f9c1dc.png)
## How to see the trace of the run[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-see-the-trace-of-the-run "Direct link to How to see the trace of the run")
By default, you only see the inputs and outputs of the high level run. You can see the full trace by clicking `View Run`
![Annotation Queue](https://docs.smith.langchain.com/assets/images/view_run-1b2c776e5558e363baaadf6bc02aca43.png)
This will open up a side panel with the full run
![Annotation Queue](https://docs.smith.langchain.com/assets/images/queue_side-20dc9d881704aa22cdbdd84279e991da.png)
## How to cycle through a queue[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-cycle-through-a-queue "Direct link to How to cycle through a queue")
You can move through a queue in a few ways. Buttons for these are all at the bottom
![Annotation Queue](https://docs.smith.langchain.com/assets/images/queue_buttons-f245920603cfcc360c261f825e7be0a0.png)
`Move to end` moves the datapoint you are currently viewing to the end of the queue
`Done` marks a datapoint as done and removes it from the queue
The `->` and `<-` buttons allow you move the next (or the prior) datapoint in the queue.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/faq/annotation_queue%3E).
[PreviousAutomations](https://docs.smith.langchain.com/old/monitoring/faq/automations)[NextHow to use webhooks with automations](https://docs.smith.langchain.com/old/monitoring/faq/webhooks)
  * [How to view annotation queues](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-view-annotation-queues)
  * [How to add feedback](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-add-feedback)
  * [How to leave a note](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-leave-a-note)
  * [How to see the trace of the run](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-see-the-trace-of-the-run)
  * [How to cycle through a queue](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue#how-to-cycle-through-a-queue)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/faq/automations

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/faq/automations#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/faq/automations)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
      * [Filtering](https://docs.smith.langchain.com/old/monitoring/faq/filter)
      * [Monitoring](https://docs.smith.langchain.com/old/monitoring/faq/monitoring)
      * [Threads](https://docs.smith.langchain.com/old/monitoring/faq/threads)
      * [Online Evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation)
      * [Automations](https://docs.smith.langchain.com/old/monitoring/faq/automations)
      * [Annotation Queue](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue)
      * [How to use webhooks with automations](https://docs.smith.langchain.com/old/monitoring/faq/webhooks)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
  * Automations


On this page
# How to apply automations
While it is useful to look at all datapoints by hand, it can be useful to create automations that automatically take action upon a filtered subset of datapoints. Automations are defined by a filter, sampling rate and an action
## How to create a filter[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/automations#how-to-create-a-filter "Direct link to How to create a filter")
You can create a filter by doing what you normally would to filter traces. When you then click on `Rules`, an automation is created with that specified filter.
![Create filter](https://docs.smith.langchain.com/assets/images/filter_rule-ef73c8d6ff043e37d4b2a99bf233b866.png)
## How to specify a sampling rate[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/automations#how-to-specify-a-sampling-rate "Direct link to How to specify a sampling rate")
You can specify a sampling rate (between 0 and 1) for automations. This will control the percent of the filtered runs that are sent to an automation
![Sampling rate](https://docs.smith.langchain.com/assets/images/automations-17ec024362961062c483e6008eaaa989.png)
## How to manage automations[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/automations#how-to-manage-automations "Direct link to How to manage automations")
You can manage automations by going `Settings` -> `Rules`. From here you can view all automations you already created, you can edit them, and you can delete them. You can also create new automations, although it is generally recommended you create automations from the `Traces` panel in a Project, so that you can see the filter you apply.
![Manage Automations](https://docs.smith.langchain.com/assets/images/manage_automations-725936cf58b7b7844c52decb4638c883.png)
### How to view logs for your automations[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/automations#how-to-view-logs-for-your-automations "Direct link to How to view logs for your automations")
You can view logs for your automations by going to `Settings` -> `Rules` and click on the `Logs` button in any row.
You can also get to logs by clicking on `Rules` in the top right hand corner of any project details page, then clicking on `See Logs` for any rule.
Logs allow you to gain confidence that your rules are working as expected. You can now view logs that list all runs processed by a given rule for the past day. For rules that apply online evaluation scores, you can easily see the output score and navigate to the run. For rules that add runs as examples to datasets, you can view the example produced. If a particular rule execution has triggered an error, you can view the error message by hovering over the error icon.
![Logs_Gif](https://docs.smith.langchain.com/assets/images/rules_logs-f007f2fcb6fdcee543ede0179d998340.gif)
![Logs](https://docs.smith.langchain.com/assets/images/rules_logs-185421b78d6946142d5f44def24ef41e.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/faq/automations%3E).
[PreviousOnline Evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation)[NextAnnotation Queue](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue)
  * [How to create a filter](https://docs.smith.langchain.com/old/monitoring/faq/automations#how-to-create-a-filter)
  * [How to specify a sampling rate](https://docs.smith.langchain.com/old/monitoring/faq/automations#how-to-specify-a-sampling-rate)
  * [How to manage automations](https://docs.smith.langchain.com/old/monitoring/faq/automations#how-to-manage-automations)
    * [How to view logs for your automations](https://docs.smith.langchain.com/old/monitoring/faq/automations#how-to-view-logs-for-your-automations)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/faq/filter

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/faq/filter#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/faq/filter)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
      * [Filtering](https://docs.smith.langchain.com/old/monitoring/faq/filter)
      * [Monitoring](https://docs.smith.langchain.com/old/monitoring/faq/monitoring)
      * [Threads](https://docs.smith.langchain.com/old/monitoring/faq/threads)
      * [Online Evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation)
      * [Automations](https://docs.smith.langchain.com/old/monitoring/faq/automations)
      * [Annotation Queue](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue)
      * [How to use webhooks with automations](https://docs.smith.langchain.com/old/monitoring/faq/webhooks)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
  * Filtering


On this page
# How to filter
This page contains a series of guides for how to filter runs. Being able to accurately filter runs is important for both manual inspection as well as setting up automations.
## How to create a filter[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-create-a-filter "Direct link to How to create a filter")
There are two ways to create a filter. First, you can create a filter from the high level nav bar. By default there is one filter applied: `IsRoot` is `true`. This restricts all runs to be top level traces.
![Filtering](https://docs.smith.langchain.com/assets/images/filter-ab7515aafb47936e8b09831b70d26bbf.png)
You can also define a filter from the `Filter Shortcuts` on the sidebar. This contains commonly used filters.
![Filtering](https://docs.smith.langchain.com/assets/images/filter_shortcuts-9bdfa901aa1df802b4d28ec02a9d3a63.png)
## How to filter for sub runs[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-filter-for-sub-runs "Direct link to How to filter for sub runs")
In order to filter for sub runs, you first need to remove the default filter of `IsRoot` is `true`. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs. This relies on good naming for all parts of your pipeline - see [here](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#customizing-the-run-name) for more details on how to do that.
## How to filter for sub runs whose parent traces have some attribute[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-filter-for-sub-runs-whose-parent-traces-have-some-attribute "Direct link to How to filter for sub runs whose parent traces have some attribute")
A common concept is to filter for sub runs whose parent traces have some attribute. An example is filtering for sub runs of a particular type whose parent trace has positive (or negative) feedback associated with it.
In order to do this, first set up a filter for sub runs (per the above section). After that, you can then add another filter rule. You can then click the `Advanced Filters` link all the way at the bottom of the filter. This will open up a new modal where you can add `Trace filters`. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for.
![Filtering](https://docs.smith.langchain.com/assets/images/trace_filter-589b83c4e3460bbb387365e569e397d7.png)
## How to filter for runs whose child runs have some attribute[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-filter-for-runs-whose-child-runs-have-some-attribute "Direct link to How to filter for runs whose child runs have some attribute")
This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name `Foo`. This is useful when `Foo` is not always called, but you want to analyze the cases where it is.
In order to do this, you can click on the `Advanced Filters` link all the way at the bottom of the filter. This will open up a new modal where you can add `Tree filters`. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for.
![Filtering](https://docs.smith.langchain.com/assets/images/child_runs-8f4764241223b0bffe96914a52aa0cad.png)
## How to copy the filter[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-copy-the-filter "Direct link to How to copy the filter")
Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK.
In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.
This will give you a string like `and(eq(is_root, true), and(eq(feedback_key, "user_score"), eq(feedback_score, 1)))`
![Copy Filter](https://docs.smith.langchain.com/assets/images/copy_filter-e0f7df45276800987086f0cbebca6567.png)
## How to manually specify a raw query[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-manually-specify-a-raw-query "Direct link to How to manually specify a raw query")
If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI.
In order to do this, you can click on `Advanced filters` on the bottom. From there you can paste a raw query into the appropriate box.
Note that this will add that query to the existing queries, not overwrite it.
![Raw Query](https://docs.smith.langchain.com/assets/images/raw_query-fac5805f338dc0b244b2d2594645c529.png)
## How to use `AI Query` to auto-generate a query[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-use-ai-query-to-auto-generate-a-query "Direct link to how-to-use-ai-query-to-auto-generate-a-query")
Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a `AI Query` functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query.
For example: "All runs longer than 10 seconds"
Note that this is an experimental feature and may not work for all queries.
![AI Query](https://docs.smith.langchain.com/assets/images/ai_query-2a31fb85b55c80c93602fbadc9f92cc2.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/faq/filter%3E).
[PreviousHow-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)[NextMonitoring](https://docs.smith.langchain.com/old/monitoring/faq/monitoring)
  * [How to create a filter](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-create-a-filter)
  * [How to filter for sub runs](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-filter-for-sub-runs)
  * [How to filter for sub runs whose parent traces have some attribute](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-filter-for-sub-runs-whose-parent-traces-have-some-attribute)
  * [How to filter for runs whose child runs have some attribute](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-filter-for-runs-whose-child-runs-have-some-attribute)
  * [How to copy the filter](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-copy-the-filter)
  * [How to manually specify a raw query](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-manually-specify-a-raw-query)
  * [How to use `AI Query` to auto-generate a query](https://docs.smith.langchain.com/old/monitoring/faq/filter#how-to-use-ai-query-to-auto-generate-a-query)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/faq/monitoring

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/faq/monitoring#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/faq/monitoring)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
      * [Filtering](https://docs.smith.langchain.com/old/monitoring/faq/filter)
      * [Monitoring](https://docs.smith.langchain.com/old/monitoring/faq/monitoring)
      * [Threads](https://docs.smith.langchain.com/old/monitoring/faq/threads)
      * [Online Evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation)
      * [Automations](https://docs.smith.langchain.com/old/monitoring/faq/automations)
      * [Annotation Queue](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue)
      * [How to use webhooks with automations](https://docs.smith.langchain.com/old/monitoring/faq/webhooks)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
  * Monitoring


On this page
# How to monitor
LangSmith has a collection of monitoring charts. These can be accessed on the `Monitor` tab within a particular project.
## How to change the time period[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/monitoring#how-to-change-the-time-period "Direct link to How to change the time period")
You can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days.
## How to view monitors by subsets[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/monitoring#how-to-view-monitors-by-subsets "Direct link to How to view monitors by subsets")
By default, the monitor tab shows results for all runs. However, you can group runs in order to see how different subsets perform. This can be useful to compare how two different prompts or models are performing.
In order to do this, you first need to make sure you are [attaching appropriate tags or metadata](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#adding-metadata-and-tags-to-traces) to these runs when logging them. After that, you can click the `Tag` or `Metadata` tab at the top to group runs accordingly.
![Subsets Monitor](https://docs.smith.langchain.com/assets/images/subsets_monitor-b35c71b35135007bff93d619bb765a6a.png)
## How to drill into specific subsets[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/monitoring#how-to-drill-into-specific-subsets "Direct link to How to drill into specific subsets")
Monitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard.
From there, you will be brought back to the `Traces` tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into.
![Drill Monitor](https://docs.smith.langchain.com/assets/images/drill_monitor-72788343ba315878ab45312b03b53836.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/faq/monitoring%3E).
[PreviousFiltering](https://docs.smith.langchain.com/old/monitoring/faq/filter)[NextThreads](https://docs.smith.langchain.com/old/monitoring/faq/threads)
  * [How to change the time period](https://docs.smith.langchain.com/old/monitoring/faq/monitoring#how-to-change-the-time-period)
  * [How to view monitors by subsets](https://docs.smith.langchain.com/old/monitoring/faq/monitoring#how-to-view-monitors-by-subsets)
  * [How to drill into specific subsets](https://docs.smith.langchain.com/old/monitoring/faq/monitoring#how-to-drill-into-specific-subsets)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
      * [Filtering](https://docs.smith.langchain.com/old/monitoring/faq/filter)
      * [Monitoring](https://docs.smith.langchain.com/old/monitoring/faq/monitoring)
      * [Threads](https://docs.smith.langchain.com/old/monitoring/faq/threads)
      * [Online Evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation)
      * [Automations](https://docs.smith.langchain.com/old/monitoring/faq/automations)
      * [Annotation Queue](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue)
      * [How to use webhooks with automations](https://docs.smith.langchain.com/old/monitoring/faq/webhooks)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
  * Online Evaluation


On this page
# How to use online evaluation
One of the actions you can set up as part of an automation is online evaluation. This involves running an automatic evaluator on the on a set of runs, then attaching a feedback tag and score to each run.
Currently, we provide support for specifying a prompt template, a model, and a set of criteria to evaluate the runs on.
## How to set up online evaluation[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#how-to-set-up-online-evaluation "Direct link to How to set up online evaluation")
The way to configure online evaluation is to first set up an [automation](https://docs.smith.langchain.com/old/monitoring/faq/automations).
![Subsets Monitor](https://docs.smith.langchain.com/assets/images/filter_rule-ef73c8d6ff043e37d4b2a99bf233b866.png)
From here, you can select `Online Evaluation` from the list of possible actions
![Subsets Monitor](https://docs.smith.langchain.com/assets/images/automations-17ec024362961062c483e6008eaaa989.png)
## How to configure online evaluation[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#how-to-configure-online-evaluation "Direct link to How to configure online evaluation")
When selection `Online Evaluation` as an action in an automation, you are presented with a panel from which you can configure online evaluation.
![Subsets Monitor](https://docs.smith.langchain.com/assets/images/evaluator-f8f0351f3b3bfa7a8457f5e9f4e49dea.png)
You can configure the model, the prompt template, and the criteria
### The model[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#the-model "Direct link to The model")
You can choose any model available in the dropdown.
### The prompt template[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#the-prompt-template "Direct link to The prompt template")
You can customize the prompt template to be whatever you want. The prompt template is a LangChain prompt template - this means that anything in `{...}` is treated as a variable to format. There are two permitted variables: `input` and `output`, corresponding to the input and output of the run you are evaluating
### The criteria[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#the-criteria "Direct link to The criteria")
An evaluator will attach arbitrary metadata tags to a run. These tags will have a name and a value. You can configure this in the `Criteria` section. The names and the descriptions of the fields will be passed in to the prompt.
## How to set API keys[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#how-to-set-api-keys "Direct link to How to set API keys")
Online evaluation uses LLMs. In order to set the API keys to use for these invocations, navigate to the `Settings -> Secrets -> Add secret` page and add any API keys there.
![Subsets Monitor](https://docs.smith.langchain.com/assets/images/secrets-388fc5b0ac5613c4b31b5e55f32c25ce.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/faq/online_evaluation%3E).
[PreviousThreads](https://docs.smith.langchain.com/old/monitoring/faq/threads)[NextAutomations](https://docs.smith.langchain.com/old/monitoring/faq/automations)
  * [How to set up online evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#how-to-set-up-online-evaluation)
  * [How to configure online evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#how-to-configure-online-evaluation)
    * [The model](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#the-model)
    * [The prompt template](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#the-prompt-template)
    * [The criteria](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#the-criteria)
  * [How to set API keys](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation#how-to-set-api-keys)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/faq/threads

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/faq/threads#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/faq/threads)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
      * [Filtering](https://docs.smith.langchain.com/old/monitoring/faq/filter)
      * [Monitoring](https://docs.smith.langchain.com/old/monitoring/faq/monitoring)
      * [Threads](https://docs.smith.langchain.com/old/monitoring/faq/threads)
      * [Online Evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation)
      * [Automations](https://docs.smith.langchain.com/old/monitoring/faq/automations)
      * [Annotation Queue](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue)
      * [How to use webhooks with automations](https://docs.smith.langchain.com/old/monitoring/faq/webhooks)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
  * Threads


On this page
# How to track threads
A `Thread` is a sequence of traces representing a single thread. Each response is represented as it's own trace, but these traces are linked together by being part of the same thread.
To associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread.
Below is an example of logging conversations:
  * Python
  * LangChain (Python)


```
import openaifrom langsmith import traceableimport uuidclient = openai.Client()session_id =str(uuid.uuid4())@traceable( run_type="chain", name="OpenAI Assistant",)defassistant( messages:list[dict]):return client.chat.completions.create(   model="gpt-3.5-turbo",   messages=[{"role":"system","content":"You are a helpful assistant."}]+ messages,).choices[0].messagemessages =[{"role":"user","content":"hi! im bob"}]response = assistant( messages, langsmith_extra={"metadata":{"session_id": session_id}})messages = messages +[ response,{"role":"user","content":"what's my name?"}]response = assistant( messages, langsmith_extra={"metadata":{"session_id": session_id}})
```

```
from langchain_anthropic import ChatAnthropicfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.messages import HumanMessage, AIMessagefrom langchain_core.output_parsers import StrOutputParserimport uuidmodel = ChatAnthropic(model="claude-3-haiku-20240307")prompt = ChatPromptTemplate.from_messages([('placeholder',"{messages}")])chain = prompt | modelmessages =[HumanMessage(content="hi! I'm bob")]config ={"metadata":{"conversation_id":str(uuid.uuid4())}}response = chain.invoke({"messages": messages}, config=config)messages = messages +[response, HumanMessage(content="whats my name")]response = chain.invoke({"messages": messages}, config=config)
```

## How to view threads[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/threads#how-to-view-threads "Direct link to How to view threads")
You can view threads by clicking on the `Threads` tag in a project. You will then see a list of all recent threads.
![Thread Tab](https://docs.smith.langchain.com/assets/images/convo_tab-90e529b8de2b735f1675e82b935a70c9.png)
You can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs.
![Conversation](https://docs.smith.langchain.com/assets/images/convo-8af9b69698dc872c39920641fbd33d92.png)
## What are special metadata keys that associate traces as part of the same thread?[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/threads#what-are-special-metadata-keys-that-associate-traces-as-part-of-the-same-thread "Direct link to What are special metadata keys that associate traces as part of the same thread?")
In order to log runs as part of the same thread you need to pass a special metadata key to the run. The key value is the unique identifier for that conversation. The key name should be one of:
  * `session_id`
  * `thread_id`
  * `conversation_id`.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/faq/threads%3E).
[PreviousMonitoring](https://docs.smith.langchain.com/old/monitoring/faq/monitoring)[NextOnline Evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation)
  * [How to view threads](https://docs.smith.langchain.com/old/monitoring/faq/threads#how-to-view-threads)
  * [What are special metadata keys that associate traces as part of the same thread?](https://docs.smith.langchain.com/old/monitoring/faq/threads#what-are-special-metadata-keys-that-associate-traces-as-part-of-the-same-thread)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/faq/webhooks

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/faq/webhooks)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
      * [Filtering](https://docs.smith.langchain.com/old/monitoring/faq/filter)
      * [Monitoring](https://docs.smith.langchain.com/old/monitoring/faq/monitoring)
      * [Threads](https://docs.smith.langchain.com/old/monitoring/faq/threads)
      * [Online Evaluation](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation)
      * [Automations](https://docs.smith.langchain.com/old/monitoring/faq/automations)
      * [Annotation Queue](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue)
      * [How to use webhooks with automations](https://docs.smith.langchain.com/old/monitoring/faq/webhooks)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
  * How to use webhooks with automations


On this page
# How to use webhooks with automations
When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs.
## Webhook Payload[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#webhook-payload "Direct link to Webhook Payload")
The payload we send to your webhook endpoint contains
  * "rule_id" this is the ID of the automation that sent this payload
  * "start_time" and "end_time" these are the time boundaries where we found matching runs
  * "runs" this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.


This is an example webhook payload
```
{"rule_id":"d75d7417-0c57-4655-88fe-1db3cda3a47a","start_time":"2024-04-05T01:28:54.734491+00:00","end_time":"2024-04-05T01:28:56.492563+00:00","runs":[{"status":"success","is_root":true,"trace_id":"6ab80f10-d79c-4fa2-b441-922ed6feb630","dotted_order":"20230505T051324571809Z6ab80f10-d79c-4fa2-b441-922ed6feb630","run_type":"tool","modified_at":"2024-04-05T01:28:54.145062","tenant_id":"2ebda79f-2946-4491-a9ad-d642f49e0815","end_time":"2024-04-05T01:28:54.085649","name":"Search","start_time":"2024-04-05T01:28:54.085646","id":"6ab80f10-d79c-4fa2-b441-922ed6feb630","session_id":"6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5","parent_run_ids":[],"child_run_ids":null,"direct_child_run_ids":null,"total_tokens":0,"completion_tokens":0,"prompt_tokens":0,"total_cost":null,"completion_cost":null,"prompt_cost":null,"first_token_time":null,"app_path":"/o/2ebda79f-2946-4491-a9ad-d642f49e0815/projects/p/6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5/r/6ab80f10-d79c-4fa2-b441-922ed6feb630?trace_id=6ab80f10-d79c-4fa2-b441-922ed6feb630&start_time=2023-05-05T05:13:24.571809","in_dataset":false,"last_queued_at":null,"inputs":null,"inputs_s3_urls":null,"outputs":null,"outputs_s3_urls":null,"extra":null,"events":null,"feedback_stats":null,"serialized":null,"share_token":null}]}
```

### Webhook Security[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#webhook-security "Direct link to Webhook Security")
We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications.
An example would be
```
https://api.example.com/langsmith_webhook?secret=38ee77617c3a489ab6e871fbeb2ec87d
```

### Webhook Delivery[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#webhook-delivery "Direct link to Webhook Delivery")
When delivering events to your webhook endpoint we follow these guidelines
  * If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.
  * If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .
  * If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.
  * If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.
  * Anything your endpoint returns in the body will be ignored


## Example with Modal[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#example-with-modal "Direct link to Example with Modal")
### Setup[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#setup "Direct link to Setup")
For an example of how to set this up, we will use [Modal](https://modal.com/). Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here.
First, create a Modal account. Then, locally install the Modal SDK:
```
pip install modal
```

To finish setting up your account, run the command:
```
modal setup
```

and follow the instructions
### Secrets[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#secrets "Direct link to Secrets")
Next, you will need to set up some secrets in Modal.
First, LangSmith will need to authenticate to Modal by passing in a secret. The easiest way to do this is to pass in a secret in the query parameters. To validate this secret, we will need to add a secret in _Modal_ to validate it. We will do that by creating a Modal secret. You can see instructions for secrets [here](https://modal.com/docs/guide/secrets). For this purpose, let's call our secret `ls-webhook` and have it set an environment variable with the name `LS_WEBHOOK`.
We can also set up a LangSmith secret - luckily there is already an integration template for this!
![LangSmith Modal Template](https://docs.smith.langchain.com/assets/images/modal_langsmith_secret-423427ddebfb97eae4269687372b3b70.png)
### Service[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#service "Direct link to Service")
After that, you can create a Python file that will serve as your endpoint. An example is below, with comments explaining what is going on:
```
from fastapi import HTTPException, status, Request, Queryfrom modal import Secret, Stub, web_endpoint, Imagestub = Stub("auth-example", image=Image.debian_slim().pip_install("langsmith"))@stub.function(  secrets=[Secret.from_name("ls-webhook"), Secret.from_name("my-langsmith-secret")])# We want this to be a `POST` endpoint since we will post data here@web_endpoint(method="POST")# We set up a `secret` query parameterdeff(data:dict, secret:str= Query(...)):# You can import dependencies you don't have locally inside Modal funxtionsfrom langsmith import Client# First, we validate the secret key we passimport osif secret != os.environ["LS_WEBHOOK"]:raise HTTPException(      status_code=status.HTTP_401_UNAUTHORIZED,      detail="Incorrect bearer token",      headers={"WWW-Authenticate":"Bearer"},)# This is where we put the logic for what should happen inside this webhook  ls_client = Client()  runs = data["runs"]  ids =[r["id"]for r in runs]  feedback =list(ls_client.list_feedback(run_ids=ids))for r, f inzip(runs, feedback):try:      ls_client.create_example(        inputs=r["inputs"],        outputs={"output": f.correction},        dataset_name="classifier-github-issues",)except Exception:raise ValueError(f"{r} and {f}")# Function bodyreturn"success!"
```

We can now deploy this easily with `modal deploy ...` (see docs [here](https://modal.com/docs/guide/managing-deployments)).
You should now get something like:
```
‚úì Created objects.‚îú‚îÄ‚îÄ üî® Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py‚îú‚îÄ‚îÄ üî® Created mount PythonPackage:langsmith‚îî‚îÄ‚îÄ üî® Created f => https://hwchase17--auth-example-f.modal.run‚úì App deployed! üéâView Deployment: https://modal.com/apps/hwchase17/auth-example
```

The important thing to remember is `https://hwchase17--auth-example-f.modal.run` - the function we created to run. NOTE: this is NOT the final deployment URL, make sure not to accidentally use that.
### Hooking it up[‚Äã](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#hooking-it-up "Direct link to Hooking it up")
We can now take the function URL we create above and add it as a webhook. We have to remember to also pass in the secret key as a query parameter. Putting it all together, it should look something like:
```
https://hwchase17--auth-example-f-dev.modal.run?secret={SECRET}
```

Replace `{SECRET}` with the secret key you created to access the Modal service.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/faq/webhooks%3E).
[PreviousAnnotation Queue](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue)[NextUse Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [Webhook Payload](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#webhook-payload)
    * [Webhook Security](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#webhook-security)
    * [Webhook Delivery](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#webhook-delivery)
  * [Example with Modal](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#example-with-modal)
    * [Setup](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#setup)
    * [Secrets](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#secrets)
    * [Service](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#service)
    * [Hooking it up](https://docs.smith.langchain.com/old/monitoring/faq/webhooks#hooking-it-up)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/quickstart

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/quickstart#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/quickstart)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * Quick Start


On this page
# Quick Start
Production monitoring starts by configuring tracing for your application. See the [tracing section](https://docs.smith.langchain.com/old/tracing) for details on how to do that.
Compared to tracing while prototyping applications, you want to pay attention to a few particular points:
  * [Sampling](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#setting-a-sampling-rate-for-tracing): When logging production workloads, you may only want to log a subset of the datapoints flowing through your system.
  * [Adding Metadata](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#adding-metadata-and-tags-to-traces): As we'll see with automations, attaching relevant metadata to runs is particularly important to enable filtering and grouping your data.
  * [Feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback): When an application is in production you can't always look at all datapoints. Capturing user feedback is helpful to draw your attention to particular datapoints.


So - now you've got your logs flowing into LangSmith. What can you do with that data?
## Filtering[‚Äã](https://docs.smith.langchain.com/old/monitoring/quickstart#filtering "Direct link to Filtering")
If you want to dive into particular runs, you can use the filtering functionality. By default, this selects all top level runs (where `Is Root` is `True`). You can filter based on name, metadata attribute, feedback, or even do a full text search.
![Filtering](https://docs.smith.langchain.com/assets/images/filter-ab7515aafb47936e8b09831b70d26bbf.png)
For more information on filtering, see [here](https://docs.smith.langchain.com/old/monitoring/faq/filter).
## Monitoring[‚Äã](https://docs.smith.langchain.com/old/monitoring/quickstart#monitoring "Direct link to Monitoring")
LangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period ‚Äî this is especially handy for debugging production issues.
To get started with monitoring, navigate to the `Monitoring` tab in the Project dashboard. Here, you can view charts such as `Trace Latency`, `Tokens/Second`, `Cost`, and feedback charts. LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.
![Monitoring](https://docs.smith.langchain.com/assets/images/monitoring-b7926b459bc5080b69f3d6dec8a3fc94.png)
For more information on filtering, see [here](https://docs.smith.langchain.com/old/monitoring/faq/monitoring).
## Threads[‚Äã](https://docs.smith.langchain.com/old/monitoring/quickstart#threads "Direct link to Threads")
If your traces are part of the same conversation, you can track them using `Threads`. You can do this by attaching a special metadata key to each trace. Once you have done that, you can view them all in the same page.
![Threads](https://docs.smith.langchain.com/assets/images/convo-8af9b69698dc872c39920641fbd33d92.png)
## Automations[‚Äã](https://docs.smith.langchain.com/old/monitoring/quickstart#automations "Direct link to Automations")
Although you'll likely want to spend time looking at traces by hand to gain an intuition for what's happening with your LLM pipelines, at some point, you will want to automate some of actions you are taking on those traces. That is where `Automations` come in handy.
The first step for defining an automation is to define the filter you want to use to select the datapoints to apply that automation to. You can do this with the filtering capability above. After doing that, you can then hit the `Rules` button.
![Filtering](https://docs.smith.langchain.com/assets/images/filter_rule-ef73c8d6ff043e37d4b2a99bf233b866.png)
That will cause a side panel to open up. This will display three main automations you can apply to your data: sending to a dataset, sending to an annotation queue, or running online evaluation over them. You can give this automation a name and also set a sampling rate. The sampling rate determines what percentage of rows that meet this filter are acted upon.
![Automations](https://docs.smith.langchain.com/assets/images/automations-17ec024362961062c483e6008eaaa989.png)
For more information, see [here](https://docs.smith.langchain.com/old/monitoring/faq/automations)
### Sending to a Dataset[‚Äã](https://docs.smith.langchain.com/old/monitoring/quickstart#sending-to-a-dataset "Direct link to Sending to a Dataset")
One common workflow is to send datapoints to a dataset automatically. This can be done by selecting the dataset button and then selecting the dataset you want to send to (or create a new dataset).
If you enable "Use corrections" then, for each matched run we will look for feedback containing a "correction" and save that as the outputs of the new dataset example. If no feedback is found, we will skip that run.
### Sending to an Annotation Queue[‚Äã](https://docs.smith.langchain.com/old/monitoring/quickstart#sending-to-an-annotation-queue "Direct link to Sending to an Annotation Queue")
An annotation queue is a user-friendly way to look at and annotate a large amount of datapoints. When clicking on this button you select the annotation queue you want to send to (or create a new annotation queue).
For more information, see [here](https://docs.smith.langchain.com/old/monitoring/faq/annotation_queue)
### Sending to Online Evaluation[‚Äã](https://docs.smith.langchain.com/old/monitoring/quickstart#sending-to-online-evaluation "Direct link to Sending to Online Evaluation")
Often times you may want to use an LLM to annotate or leave feedback on datapoints. This can be for the purpose of trying to get some initial feedback on the LLM answer (vague, not vague) or to classify user input, or both! This can be done with `Online Evaluation`.
NOTE: before clicking on this option, make sure to set up secrets to have access to any API keys for the LLMs you will use. You can do this by clicking `Settings -> Secrets -> Add secret`. These are different from Playground secrets, which live in the browser. Evaluator secrets live in our backend and are encrypted.
![Filtering](https://docs.smith.langchain.com/assets/images/secrets-388fc5b0ac5613c4b31b5e55f32c25ce.png)
After doing this, you can set up an automation and then select the `Online Evaluation` option. When clicking on this option, a panel is opened to configure the evaluator. The evaluator is determined by three components: the model, the prompt template, and the criteria.
![Filtering](https://docs.smith.langchain.com/assets/images/evaluator-f8f0351f3b3bfa7a8457f5e9f4e49dea.png)
The model is the LLM that will do the evaluation.
The prompt template will be used to format the information from the run that you want to score. The `{inputs}` placeholder represents where the inputs of the run will be formatted to, and the `{outputs}` represents the outputs.
The criteria represents the feedback that will be associated with the run. The name is the key of the feedback, and then the LLM will generate the value which will be used as the feedback score. The name and description of the criteria are passed to the LLM, so you should make them descriptive!
For more information, see [here](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation)
### Webhooks[‚Äã](https://docs.smith.langchain.com/old/monitoring/quickstart#webhooks "Direct link to Webhooks")
You also have the option of being notified via a webhook whenever a rule matches new runs. Your webhook endpoint will be called with a JSON payload containing the matched runs.
For more information, see [here](https://docs.smith.langchain.com/old/monitoring/faq/webhooks).
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/quickstart%3E).
[PreviousOverview](https://docs.smith.langchain.com/old/monitoring)[NextConcepts](https://docs.smith.langchain.com/old/monitoring/concepts)
  * [Filtering](https://docs.smith.langchain.com/old/monitoring/quickstart#filtering)
  * [Monitoring](https://docs.smith.langchain.com/old/monitoring/quickstart#monitoring)
  * [Threads](https://docs.smith.langchain.com/old/monitoring/quickstart#threads)
  * [Automations](https://docs.smith.langchain.com/old/monitoring/quickstart#automations)
    * [Sending to a Dataset](https://docs.smith.langchain.com/old/monitoring/quickstart#sending-to-a-dataset)
    * [Sending to an Annotation Queue](https://docs.smith.langchain.com/old/monitoring/quickstart#sending-to-an-annotation-queue)
    * [Sending to Online Evaluation](https://docs.smith.langchain.com/old/monitoring/quickstart#sending-to-online-evaluation)
    * [Webhooks](https://docs.smith.langchain.com/old/monitoring/quickstart#webhooks)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/use_cases

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/use_cases#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
      * [Optimization](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization)
      * [Optimizing a Classifier](https://docs.smith.langchain.com/old/monitoring/use_cases/classification)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * Use Cases


On this page
# Use Cases
The following guides are provided to serve as example use cases for how you can use LangSmith's production logging and automations. These are not meant to be exhaustive, nor are they optimized for your use case. They are meant as a reference to help you get started.
## Add bad datapoints to an annotation queue[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases#add-bad-datapoints-to-an-annotation-queue "Direct link to Add bad datapoints to an annotation queue")
This flow can be used to send datapoints that get negative end user feedback to an annotation queue, where you can inspect them manually. To do this, you will want to set `Sampling Rate` to be `1`, and you will want to filter to root runs with negative feedback. Your action is to send to an annotation queue.
## Send datapoints with positive feedback to a dataset.[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases#send-datapoints-with-positive-feedback-to-a-dataset "Direct link to Send datapoints with positive feedback to a dataset.")
This flow can be used to send datapoints with positive feedback to a dataset. The logic here is that these datapoints are good input/output pairs. They can then be used for testing, few shot prompting, or fine tuning. You will want to set a `Sampling Rate` of `1` and you will want to filter to root runs with positive feedback. Your action is to send to a dataset.
## Send child runs whose trace got positive feedback to a dataset[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases#send-child-runs-whose-trace-got-positive-feedback-to-a-dataset "Direct link to Send child runs whose trace got positive feedback to a dataset")
In the case where your application contains multiple steps, you may also want to build up datasets for each step. This is useful because often you will want to use few shot prompting on these individual steps. In order to do that, you need to collect examples of those individual steps. You often don‚Äôt get direct feedback on those individual steps, but rather on the top level trace. If you assume that if you get positive feedback on the overall trace then all sub-runs are also correct, you can set up a rule that selects all particular sub-runs whose parent trace has positive feedback.
## Send random datapoints to an annotation queue[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases#send-random-datapoints-to-an-annotation-queue "Direct link to Send random datapoints to an annotation queue")
You don‚Äôt always get great end user feedback (or it may be biased), so you may want to randomly sample some percentage of datapoints and send them to an annotation queue.
To do this, you can set `Sampling Rate` to be a fraction and then sample all root runs.
## Combinations of the above[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases#combinations-of-the-above "Direct link to Combinations of the above")
Oftentimes, combinations of the above rules make sense. For example, longer stacks of runs could be:
**The ‚ÄúHelp me get positive examples‚Äù rule set**
  * For a random sample of runs, send them to the annotation queue
  * For any datapoints with positive feedback, add them (or their child runs) to a dataset
  * You can then use these examples as few shot examples or to finetune


**The ‚ÄúWhat datapoints should I look at?‚Äù rule set**
  * Define an online evaluator to run over random sample of datapoints
  * For any datapoints with negative LLM feedback, send them to annotation queue for manual inspection
  * From the annotation queue, you can give them correct labels and add them to a dataset to test against to see if you can improve


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/use_cases%3E).
[PreviousHow to use webhooks with automations](https://docs.smith.langchain.com/old/monitoring/faq/webhooks)[NextOptimization](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization)
  * [Add bad datapoints to an annotation queue](https://docs.smith.langchain.com/old/monitoring/use_cases#add-bad-datapoints-to-an-annotation-queue)
  * [Send datapoints with positive feedback to a dataset.](https://docs.smith.langchain.com/old/monitoring/use_cases#send-datapoints-with-positive-feedback-to-a-dataset)
  * [Send child runs whose trace got positive feedback to a dataset](https://docs.smith.langchain.com/old/monitoring/use_cases#send-child-runs-whose-trace-got-positive-feedback-to-a-dataset)
  * [Send random datapoints to an annotation queue](https://docs.smith.langchain.com/old/monitoring/use_cases#send-random-datapoints-to-an-annotation-queue)
  * [Combinations of the above](https://docs.smith.langchain.com/old/monitoring/use_cases#combinations-of-the-above)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/use_cases/classification

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/use_cases/classification#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/use_cases/classification)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
      * [Optimization](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization)
      * [Optimizing a Classifier](https://docs.smith.langchain.com/old/monitoring/use_cases/classification)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * Optimizing a Classifier


On this page
# Optimizing a Classifier
This tutorial walks through optimizing a classifier based on user a feedback. Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback. That is exactly what we will do in this example.
## The objective[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases/classification#the-objective "Direct link to The objective")
In this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.
## Getting started[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases/classification#getting-started "Direct link to Getting started")
To get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:
```
import osos.environ["LANGCHAIN_PROJECT"]="classifier"
```

We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.
```
import openaifrom langsmith import traceable, Clientimport uuidclient = openai.Client()available_topics =["bug","improvement","new_feature","documentation","integration",]prompt_template ="""Classify the type of the issue as one of {topics}.Issue: {text}"""@traceable(  run_type="chain",  name="Classifier",)deftopic_classifier(  topic:str):return client.chat.completions.create(    model="gpt-3.5-turbo",    temperature=0,    messages=[{"role":"user","content": prompt_template.format(          topics=','.join(available_topics),          text=topic,)}],).choices[0].message.content
```

We can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.
Here's how we can invoke the application:
```
run_id = uuid.uuid4()topic_classifier("fix bug in LCEL",  langsmith_extra={"run_id": run_id})
```

Here's how we can attach feedback after. We can collect feedback in two forms.
First, we can collect "positive" feedback - this is for examples that the model got right.
```
ls_client = Client()run_id = uuid.uuid4()topic_classifier("fix bug in LCEL",  langsmith_extra={"run_id": run_id})ls_client.create_feedback(  run_id,  key="user-score",  score=1.0,)
```

Next, we can focus on collecting feedback that corresponds to a "correction" to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.
```
ls_client = Client()run_id = uuid.uuid4()topic_classifier("fix bug in documentation",  langsmith_extra={"run_id": run_id})ls_client.create_feedback(  run_id,  key="correction",  correction="documentation")
```

## Set up automations[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases/classification#set-up-automations "Direct link to Set up automations")
We can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.
The first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Let's create a dataset called `classifier-github-issues` to add this data to.
![Optimization Positive](https://docs.smith.langchain.com/assets/images/class-optimization-pos-2be610a400f5a8a60deb4da8af1205d2.png)
The second will take all runs with a correction and use a webhook to add them to a dataset. When creating this webhook, we will select the option to "Use Corrections". This option will make it so that when creating a dataset from a run, rather than using the output of the run as the gold-truth output of the datapoint, it will use the correction.
![Optimization Negative](https://docs.smith.langchain.com/assets/images/class-optimization-neg-67fd3aae244b7f73980ae39d58d1df87.png)
## Updating the application[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases/classification#updating-the-application "Direct link to Updating the application")
We can now update our code to pull down the dataset we are sending runs to. Once we pull it down, we can create a string with the examples in it. We can then put this string as part of the prompt!
```
### NEW CODE #### Initialize the LangSmith Client so we can use to get the datasetls_client = Client()# Create a function that will take in a list of examples and format them into a stringdefcreate_example_string(examples):  final_strings =[]for e in examples:    final_strings.append(f"Input: {e.inputs['topic']}\n> {e.outputs['output']}")return"\n\n".join(final_strings)### NEW CODE ###client = openai.Client()available_topics =["bug","improvement","new_feature","documentation","integration",]prompt_template ="""Classify the type of the issue as one of {topics}.Here are some examples:{examples}Begin!Issue: {text}>"""@traceable(  run_type="chain",  name="Classifier",)deftopic_classifier(  topic:str):# We can now pull down the examples from the dataset# We do this inside the function so it always get the most up-to-date examples,# But this can be done outside and cached for speed if desired  examples =list(ls_client.list_examples(dataset_name="classifier-github-issues"))# <- New Code  example_string = create_example_string(examples)return client.chat.completions.create(    model="gpt-3.5-turbo",    temperature=0,    messages=[{"role":"user","content": prompt_template.format(          topics=','.join(available_topics),          text=topic,          examples=example_string,)}],).choices[0].message.content
```

If now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as `documentation`
```
ls_client = Client()run_id = uuid.uuid4()topic_classifier("address bug in documentation",  langsmith_extra={"run_id": run_id})
```

## Semantic Search Over Examples[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases/classification#semantic-search-over-examples "Direct link to Semantic Search Over Examples")
One additional thing we can do is only use the most semantically similar examples. This is useful when you start to build up a lot of examples.
In order to do this, we can first define an example to find the `k` most similar examples:
```
import numpy as npdeffind_similar(examples, topic, k=5):  inputs =[e.inputs['topic']for e in examples]+[topic]  embedds = client.embeddings.create(input=inputs, model="text-embedding-3-small")  embedds =[e.embedding for e in embedds.data]  embedds = np.array(embedds)  args = np.argsort(-embedds.dot(embedds[-1])[:-1])[:5]  examples =[examples[i]for i in args]return examples
```

We can then use that in the application
```
ls_client = Client()defcreate_example_string(examples):  final_strings =[]for e in examples:    final_strings.append(f"Input: {e.inputs['topic']}\n> {e.outputs['output']}")return"\n\n".join(final_strings)client = openai.Client()available_topics =["bug","improvement","new_feature","documentation","integration",]prompt_template ="""Classify the type of the issue as one of {topics}.Here are some examples:{examples}Begin!Issue: {text}>"""@traceable(  run_type="chain",  name="Classifier",)deftopic_classifier(  topic:str):  examples =list(ls_client.list_examples(dataset_name="classifier-github-issues"))  examples = find_similar(examples, topic)  example_string = create_example_string(examples)return client.chat.completions.create(    model="gpt-3.5-turbo",    temperature=0,    messages=[{"role":"user","content": prompt_template.format(          topics=','.join(available_topics),          text=topic,          examples=example_string,)}],).choices[0].message.content
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/use_cases/classification%3E).
[PreviousOptimization](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization)[NextPrompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [The objective](https://docs.smith.langchain.com/old/monitoring/use_cases/classification#the-objective)
  * [Getting started](https://docs.smith.langchain.com/old/monitoring/use_cases/classification#getting-started)
  * [Set up automations](https://docs.smith.langchain.com/old/monitoring/use_cases/classification#set-up-automations)
  * [Updating the application](https://docs.smith.langchain.com/old/monitoring/use_cases/classification#updating-the-application)
  * [Semantic Search Over Examples](https://docs.smith.langchain.com/old/monitoring/use_cases/classification#semantic-search-over-examples)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/monitoring/use_cases/optimization

[Skip to main content](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
    * [Quick Start](https://docs.smith.langchain.com/old/monitoring/quickstart)
    * [Concepts](https://docs.smith.langchain.com/old/monitoring/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/monitoring/faq)
    * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
      * [Optimization](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization)
      * [Optimizing a Classifier](https://docs.smith.langchain.com/old/monitoring/use_cases/classification)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Use Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)
  * Optimization


On this page
# Optimization
This walks through a specific use case. In this guide, we will create an application that we optimize over time. We will do this by collecting user feedback and then using that to collect few shot example datasets. We can then pull those few shot example datasets into the application at run time and use those as examples of how the application should behave.
## The objective[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization#the-objective "Direct link to The objective")
In this example, we will build a bot that can write tweets. We will start with a generic prompt, and then by leaving feedback on tweets we can start to build up a few shot example dataset. We will do this without any LangChain code, just using raw OpenAI and LangSmith
## Getting started[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization#getting-started "Direct link to Getting started")
To get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:
```
import osos.environ["LANGCHAIN_PROJECT"]="optimization"
```

We can then create our initial application. This will be a really simple function that just takes in a topic and creates a tweet about it.
```
import openaifrom langsmith import traceable, Clientimport uuidclient = openai.Client()@traceable(  run_type="chain",  name="Tweeter",)deftweeter(  topic:str):return client.chat.completions.create(    model="gpt-3.5-turbo",    messages=[{"role":"user","content":f"Write a tweet about {topic}"}],).choices[0].message.content
```

We can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.
Here's how we can invoke the application:
```
run_id = uuid.uuid4()tweeter("football",  langsmith_extra={"run_id": run_id})
```

Here's how we can attach feedback after:
```
ls_client = Client()ls_client.create_feedback(  run_id,  key="user-score",  score=1.0,)
```

We can set a score of `1` when we liked the tweet, and a score of `0` when we don't like the tweet. Note that you could easily incorporate this into a UI to gather feedback in a more user friendly manner.
## Set up automations[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization#set-up-automations "Direct link to Set up automations")
We can now set up two automations.
The first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations.
![Optimization Positive](https://docs.smith.langchain.com/assets/images/optimization-positive-2a1582edc1a22630681183697fe6151f.png)
The second will take all runs with negative feedback and automatically send them to an annotation queue. The logic behind this is that we can then have human reviewers looking at that annotation queue and correcting any bad examples to better ones. After correcting them, they can then send them to the same dataset.
![Optimization Negative](https://docs.smith.langchain.com/assets/images/optimization-negative-5a81b5ab228bdf690d612d91575366e1.png)
## Updating the application[‚Äã](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization#updating-the-application "Direct link to Updating the application")
We can now update our code to pull down the dataset we are sending runs to. We can use the datapoints in that dataset as few shot examples.
```
@traceable(  run_type="chain",  name="Tweeter",)deftweeter(  topic:str):  examples =list(ls_client.list_examples(dataset_name="tweeting-optimization"))  example_string ="\n\n".join([f"Input: {e.inputs['topic']}\nOutput: {e.outputs['output']}"for e in examples])return client.chat.completions.create(    model="gpt-3.5-turbo",    messages=[{"role":"user","content":f"Write a tweet about {topic}. Here are some examples of how to do this well:\n\n{example_string}"}],).choices[0].message.content
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/monitoring/use_cases/optimization%3E).
[PreviousUse Cases](https://docs.smith.langchain.com/old/monitoring/use_cases)[NextOptimizing a Classifier](https://docs.smith.langchain.com/old/monitoring/use_cases/classification)
  * [The objective](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization#the-objective)
  * [Getting started](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization#getting-started)
  * [Set up automations](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization#set-up-automations)
  * [Updating the application](https://docs.smith.langchain.com/old/monitoring/use_cases/optimization#updating-the-application)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/pricing

[Skip to main content](https://docs.smith.langchain.com/old/pricing#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/pricing)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * Pricing


On this page
# Pricing
## Plans[‚Äã](https://docs.smith.langchain.com/old/pricing#plans "Direct link to Plans")
Plan| Developer| Startups| Plus| Enterprise  
---|---|---|---|---  
Description| Designed for hobbyists who want to start their adventure solo| Everything in Developer, plus team features, higher rate limits, and longer data retention| Designed for early stage startups building AI applications| Designed for teams with more security, deployment, and support needs  
Specs| 
  * Free for 1 user
  * 5,000 free traces per month
  * Additional traces billed @ $0.005/trace

| 
  * $39/user
  * 10,000 free traces per month
  * Additional traces billed @ $0.005/trace

| [Contact us to learn more](https://airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form)| Custom  
Features| Key features:
  * 1 Developer seat
  * Debugging traces
  * Dataset collection
  * Testing and evaluation
  * Prompt management
  * Monitoring

| Key features:
  * All features in Developer tier
  * Up to 10 seats
  * Hosted LangServe (beta)
  * Longer data retention
  * Higher rate limits
  * Email support

| What to expect:We want all early stage companies to build with LangSmith. LangSmith for Startups offers discounted prices and a generous free, monthly trace allotment, so you can have the right tooling in place as you grow your business.| Key features:
  * All features in Plus tier
  * Single Sign On (SSO)
  * Negotiable SLAs
  * Deployment options in customer‚Äôs environment
  * Custom rate limits
  * Team trainings
  * Shared Slack channel
  * Architectural guidance
  * Dedicated customer success manager

  
## Plan Comparison[‚Äã](https://docs.smith.langchain.com/old/pricing#plan-comparison "Direct link to Plan Comparison")
Developer| Plus| Enterprise  
---|---|---  
**Features**  
Debugging Traces| ‚úÖ| ‚úÖ| ‚úÖ  
Dataset Collection| ‚úÖ| ‚úÖ| ‚úÖ  
Human Labeling| ‚úÖ| ‚úÖ| ‚úÖ  
Testing and Evaluation| ‚úÖ| ‚úÖ| ‚úÖ  
Prompt Management| ‚úÖ| ‚úÖ| ‚úÖ  
Hosted LangServe| --| ‚úÖ| ‚úÖ  
Monitoring| ‚úÖ| ‚úÖ| ‚úÖ  
Role-Based Access Controls (RBAC)| --| --| ‚úÖ  
**Team**  
Developer Seats| 1 Free Seat| Maximum 10 seats$39 per seat/month1| Custom pricing  
**Usage**  
Traces2| First 5k base traces and extended upgrades per month for freePay as you go thereafter:$0.50 per 1k base traces (14-day retention)Additional $4.50 per 1k extended traces (400-day retention)| First 10k base traces and extended upgrades per month for freePay as you go thereafter:$0.50 per 1k base traces (14-day retention)Additional $4.50 per 1k extended traces (400-day retention)| Custom  
Max ingested events / hour3| 50,0003 / 250,000| 500,000| Custom  
Total trace size storage / hour4| 500MB3 / 2.5GB| 5GB| Custom  
**Security Controls**  
Single Sign On| --| GoogleGitHub| Custom SSO  
Deployment| Hosted in US or EU| Hosted in US or EU| Add-on for self-hosted deployment in customer's VPC  
**Support**  
Support Channels| Community| Email| EmailShared Slack Channel  
Shared Slack Channel| --| --| ‚úÖ  
Team Training| --| --| ‚úÖ  
Application Architectural Guidance| --| --| ‚úÖ  
Dedicated Customer Success Manager| --| --| ‚úÖ  
SLA| --| --| ‚úÖ  
**Procurement**  
Billing| Monthly, self-serveCredit Card| Monthly, self-serveCredit Card| Annual InvoiceACH  
Custom Terms and Data Privacy Agreement| --| --| ‚úÖ  
Infosec Review| --| --| ‚úÖ  
Workspaces| Single, default Workspace under Personal Organization| Up to 3 Workspaces per Organization| Up to 10 Workspaces per Organization (contact support@langchain.dev for more)  
Organization Roles (User and Admin)| --| ‚úÖ| ‚úÖ  
1 Seats are billed monthly on the first of the month and in the future will be prorated if additional seats are purchased in the middle of the month. Seats removed mid-month are not credited.
2 You can purchase LangSmith credits for your tracing usage. As long as you have a valid credit card in your account, we‚Äôll service your traces and deduct from your credit balance. You‚Äôll be able to set monthly ingest limits if you choose to control spend.
3 Personal accounts without a credit card on file will be rate limited to 50,000 ingested events per hour and 500MB of storage per hour.
4 Trace storage includes all submitted inputs, outputs, and metadata and is aggregated over all submission events. Depending on the design of your application, trace data may be sent multiple times (e.g. once at the start of a trace step and again after it is complete)
## Questions and Answers[‚Äã](https://docs.smith.langchain.com/old/pricing#questions-and-answers "Direct link to Questions and Answers")
### I‚Äôve been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?[‚Äã](https://docs.smith.langchain.com/old/pricing#ive-been-using-langsmith-since-before-pricing-took-effect-for-new-users-when-will-pricing-go-into-effect-for-my-account "Direct link to I‚Äôve been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?")
If you‚Äôve been using LangSmith already, your usage will be billable starting in July 2024. At that point if you want to add seats or use more than the monthly allotment of free traces, you will need to add a credit card to LangSmith or contact sales. If you are interested in the Enterprise plan with higher rate limits and special deployment options, you can learn more or make a purchase by reaching out to sales@langchain.dev.
### Which plan is right for me?[‚Äã](https://docs.smith.langchain.com/old/pricing#which-plan-is-right-for-me "Direct link to Which plan is right for me?")
If you‚Äôre an individual developer, the Developer plan is a great choice for small projects.
For teams that want to collaborate in LangSmith, check out the Plus plan. **If you are an early-stage startup building an AI application** , you may be eligible for our Startup plan with discounted prices and a generous free monthly trace allotment. Please reach out via our [Startup Contact Form](https://airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form) for more details.
If you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you. Please reach out via our [Sales Contact Form](https://www.langchain.com/contact-sales) for more details.
### What is a seat?[‚Äã](https://docs.smith.langchain.com/old/pricing#what-is-a-seat "Direct link to What is a seat?")
A seat is a distinct user inside your organization. We consider the total number of users (including invited users) to determine the number of seats to bill.
### What is a trace?[‚Äã](https://docs.smith.langchain.com/old/pricing#what-is-a-trace "Direct link to What is a trace?")
A trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Here is an [example](https://smith.langchain.com/public/17c24270-9f74-47e7-b70c-d508afc448fa/r) of a single trace.
### What is an ingested event?[‚Äã](https://docs.smith.langchain.com/old/pricing#what-is-an-ingested-event "Direct link to What is an ingested event?")
An ingested event is any distinct, trace-related data sent to LangSmith. This includes:
  * Inputs, outputs and metadata sent at the start of a run step within a trace
  * Inputs, outputs and metadata sent at the end of a run step within a trace
  * Feedback on run steps or traces


### I‚Äôve hit my rate or usage limits. What can I do?[‚Äã](https://docs.smith.langchain.com/old/pricing#ive-hit-my-rate-or-usage-limits-what-can-i-do "Direct link to I‚Äôve hit my rate or usage limits. What can I do?")
If you‚Äôve consumed the monthly allotment of free traces in your account, you can add a credit card on the Developer and Plus plans to continue sending traces to LangSmith. If you‚Äôve hit the rate limits on your tier, you can upgrade to a higher plan to get higher limits, or reach out to support@langchain.dev with questions.
### I have a developer account, can I upgrade my account to the Plus or Enterprise plan?[‚Äã](https://docs.smith.langchain.com/old/pricing#i-have-a-developer-account-can-i-upgrade-my-account-to-the-plus-or-enterprise-plan "Direct link to I have a developer account, can I upgrade my account to the Plus or Enterprise plan?")
Every user will have a unique personal account on the Developer plan. **We cannot upgrade a Developer account to the Plus or Enterprise plans.** If you‚Äôre interested in working as a team, create a separate LangSmith Organization on the Plus plan. This plan can upgraded to the Enterprise plan at a later date.
### How will billing work?[‚Äã](https://docs.smith.langchain.com/old/pricing#how-will-billing-work "Direct link to How will billing work?")
**Seats**
Seats are billed monthly on the first of the month in the future will be pro-rated if additional seats are purchased in the middle of the month. Seats removed mid-month will not be credited.
**Traces**
As long as you have a card on file in your account, we‚Äôll service your traces and bill you on the first of the month for traces that you submitted in the previous month. You will be able to set usage limits if you so choose to limit the maximum charges you could incur in any given month.
### Can I limit how much I spend on tracing?[‚Äã](https://docs.smith.langchain.com/old/pricing#can-i-limit-how-much-i-spend-on-tracing "Direct link to Can I limit how much I spend on tracing?")
You can set limits on the number of traces that can be sent to LangSmith per month on the [Plans and Billing](https://smith.langchain.com/settings/payments) settings page.
note
While we do show you the dollar value of your usage limit for convenience, this limit evaluated in terms of number of traces instead of dollar amount. For example, if you are approved for our startup plan tier where you are given a generous allotment of free traces, your usage limit will not automatically change.
You are not currently able to set a spend limit in the product.
### How can my track my usage so far this month?[‚Äã](https://docs.smith.langchain.com/old/pricing#how-can-my-track-my-usage-so-far-this-month "Direct link to How can my track my usage so far this month?")
Under the Settings section for your Organization you will see subsection for **Usage**. There, you will able to see a graph of the daily number of billable LangSmith traces from the last 30, 60, or 90 days. Note that this data is delayed by 1-2 hours and so may trail your actual number of runs slightly for the current day.
### I have a question about my bill...[‚Äã](https://docs.smith.langchain.com/old/pricing#i-have-a-question-about-my-bill "Direct link to I have a question about my bill...")
Customers on the Developer and Plus plan tiers should email support@langchain.dev. Customers on the Enterprise plan should contact their sales representative directly.
Enterprise plan customers are billed annually by invoice.
### What can I expect from Support?[‚Äã](https://docs.smith.langchain.com/old/pricing#what-can-i-expect-from-support "Direct link to What can I expect from Support?")
On the Developer plan, community-based support is available on [Discord](https://discord.com/invite/6adMQxSpJS).
On the Plus plan, you will also receive preferential, email support at support@langchain.dev for LangSmith-related questions only and we'll do our best to respond within the next business day.
On the Enterprise plan, you‚Äôll get white-glove support with a Slack channel, a dedicated customer success manager, and monthly check-ins to go over LangSmith and LangChain questions. We can help with anything from debugging, agent and RAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the add-on to run LangSmith in your environment, we‚Äôll also support deployments and new releases with our infra engineering team on-call.
### Which security frameworks is LangSmith compliant with?[‚Äã](https://docs.smith.langchain.com/old/pricing#which-security-frameworks-is-langsmith-compliant-with "Direct link to Which security frameworks is LangSmith compliant with?")
We are SOC 2 Type II, GDPR, and HIPAA compliant.
You can request more information about our security policies and posture at [trust.langchain.com](https://trust.langchain.com). Please note we only enter into BAAs with customers on our Enterprise plan.
### Will you train on the data that I send LangSmith?[‚Äã](https://docs.smith.langchain.com/old/pricing#will-you-train-on-the-data-that-i-send-langsmith "Direct link to Will you train on the data that I send LangSmith?")
We will not train on your data, and you own all rights to your data. See [LangSmith Terms of Service](https://langchain.dev/terms-of-service) for more information.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/pricing%3E).
[PreviousUse LangSmith Proxy with Azure OpenAI](https://docs.smith.langchain.com/old/proxy/azure_openai)[NextSelf-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)
  * [Plans](https://docs.smith.langchain.com/old/pricing#plans)
  * [Plan Comparison](https://docs.smith.langchain.com/old/pricing#plan-comparison)
  * [Questions and Answers](https://docs.smith.langchain.com/old/pricing#questions-and-answers)
    * [I‚Äôve been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?](https://docs.smith.langchain.com/old/pricing#ive-been-using-langsmith-since-before-pricing-took-effect-for-new-users-when-will-pricing-go-into-effect-for-my-account)
    * [Which plan is right for me?](https://docs.smith.langchain.com/old/pricing#which-plan-is-right-for-me)
    * [What is a seat?](https://docs.smith.langchain.com/old/pricing#what-is-a-seat)
    * [What is a trace?](https://docs.smith.langchain.com/old/pricing#what-is-a-trace)
    * [What is an ingested event?](https://docs.smith.langchain.com/old/pricing#what-is-an-ingested-event)
    * [I‚Äôve hit my rate or usage limits. What can I do?](https://docs.smith.langchain.com/old/pricing#ive-hit-my-rate-or-usage-limits-what-can-i-do)
    * [I have a developer account, can I upgrade my account to the Plus or Enterprise plan?](https://docs.smith.langchain.com/old/pricing#i-have-a-developer-account-can-i-upgrade-my-account-to-the-plus-or-enterprise-plan)
    * [How will billing work?](https://docs.smith.langchain.com/old/pricing#how-will-billing-work)
    * [Can I limit how much I spend on tracing?](https://docs.smith.langchain.com/old/pricing#can-i-limit-how-much-i-spend-on-tracing)
    * [How can my track my usage so far this month?](https://docs.smith.langchain.com/old/pricing#how-can-my-track-my-usage-so-far-this-month)
    * [I have a question about my bill...](https://docs.smith.langchain.com/old/pricing#i-have-a-question-about-my-bill)
    * [What can I expect from Support?](https://docs.smith.langchain.com/old/pricing#what-can-i-expect-from-support)
    * [Which security frameworks is LangSmith compliant with?](https://docs.smith.langchain.com/old/pricing#which-security-frameworks-is-langsmith-compliant-with)
    * [Will you train on the data that I send LangSmith?](https://docs.smith.langchain.com/old/pricing#will-you-train-on-the-data-that-i-send-langsmith)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/proxy/azure_openai

[Skip to main content](https://docs.smith.langchain.com/old/proxy/azure_openai#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/proxy/azure_openai)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
    * [Quick Start](https://docs.smith.langchain.com/old/proxy/quickstart)
    * [Use LangSmith Proxy with Azure OpenAI](https://docs.smith.langchain.com/old/proxy/azure_openai)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * Use LangSmith Proxy with Azure OpenAI


On this page
# Configure LangSmith Proxy to talk to your Azure OpenAI Endpoint
## Prerequisites[‚Äã](https://docs.smith.langchain.com/old/proxy/azure_openai#prerequisites "Direct link to Prerequisites")
  1. Docker installed on your local machine 
     * Instructions for installing Docker can be found [here](https://docs.docker.com/get-docker/)
  2. An Azure OpenAI API Key
  3. An Azure OpenAI endpoint


## 1. Deploy the LangSmith Proxy[‚Äã](https://docs.smith.langchain.com/old/proxy/azure_openai#1-deploy-the-langsmith-proxy "Direct link to 1. Deploy the LangSmith Proxy")
The LangSmith Proxy is available as a Docker container. You can run it in your environment by running the following command:
```
docker pull docker.io/langchain/langsmith-proxy:latest # Force pull the latest version of the LangSmith Proxydocker run -e AZURE_OPENAI_ENDPOINT <YOUR AZURE_OPENAI_ENDPOINT> -p 8080:8080 docker.io/langchain/langsmith-proxy:latest # Run the LangSmith Proxy on port 8080 and publish it to the host
```

You should see the following output:
```
2024-03-06 12:59:57,458 CRIT Supervisor is running as root. Privileges were not dropped because no user is specified in the config file. If you intend to run as root, you can set user=root in the config file to avoid this message.2024-03-06 12:59:57,467 INFO supervisord started with pid 12024-03-06 12:59:58,503 INFO spawned: 'nginx' with pid 82024-03-06 12:59:58,552 INFO spawned: 'trace-processor' with pid 102024-03-06 12:59:59,562 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)2024-03-06 12:59:59,563 INFO success: trace-processor entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)Couldn't create langsmith client: API key must be provided when using hosted LangSmith API, will skip creating runsListening for traces at 0.0.0.0:9999Connection from ('127.0.0.1', 47370)
```

Ignore the `Couldn't create langsmith client` message if you are not configuring tracing.
## 2. Update your app to make requests to the LangSmith Proxy[‚Äã](https://docs.smith.langchain.com/old/proxy/azure_openai#2-update-your-app-to-make-requests-to-the-langsmith-proxy "Direct link to 2. Update your app to make requests to the LangSmith Proxy")
For this example, we'll be using your local proxy running on `localhost:8080`. You can replace this with the address of your proxy if it's running on a different machine.
You may need to install some packages for this example:
```
pip install langchain_openai
```

Now, you can use the LangSmith Proxy to make requests to Azure OpenAI. Here's an example of how you can do this in Python: Let's create a file called `azure_openai_test.py` and add the following code:
```
from langchain_openai import AzureChatOpenAIimport osos.environ["OPENAI_API_VERSION"]="2023-06-01-preview"os.environ["OPENAI_API_KEY"]="YOUR API KEY"os.environ["AZURE_OPENAI_ENDPOINT"]="http://localhost:8080/proxy/azure-openai"llm = AzureChatOpenAI(deployment_name="gpt-35-turbo")print(llm.invoke("Hello, world!"))
```

Run the script:
```
python azure_openai_test.py
```

You should see some output like this
```
content='Hello! How can I assist you today?'response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': 'fp_2f57f81c11','prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'},'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-0884458f-bccd-4444-8357-3f8becc7ea2c-0'
```

Nice! You have successfully configured the LangSmith Proxy to talk to your Azure OpenAI endpoint. You can now use your proxy endpoint as a drop-in replacement for the Azure OpenAI endpoint in your applications.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/proxy/azure_openai%3E).
[PreviousQuick Start](https://docs.smith.langchain.com/old/proxy/quickstart)[NextPricing](https://docs.smith.langchain.com/old/pricing)
  * [Prerequisites](https://docs.smith.langchain.com/old/proxy/azure_openai#prerequisites)
  * [1. Deploy the LangSmith Proxy](https://docs.smith.langchain.com/old/proxy/azure_openai#1-deploy-the-langsmith-proxy)
  * [2. Update your app to make requests to the LangSmith Proxy](https://docs.smith.langchain.com/old/proxy/azure_openai#2-update-your-app-to-make-requests-to-the-langsmith-proxy)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/proxy/quickstart

[Skip to main content](https://docs.smith.langchain.com/old/proxy/quickstart#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/proxy/quickstart)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
    * [Quick Start](https://docs.smith.langchain.com/old/proxy/quickstart)
    * [Use LangSmith Proxy with Azure OpenAI](https://docs.smith.langchain.com/old/proxy/azure_openai)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * Quick Start


On this page
# Quick Start
## What is the LangSmith Proxy[‚Äã](https://docs.smith.langchain.com/old/proxy/quickstart#what-is-the-langsmith-proxy "Direct link to What is the LangSmith Proxy")
The LangSmith Proxy(Alpha) is intended to be a drop-in replacement for your LLM apis with some additional features. It is designed to be a simple, easy-to-use, and easy-to-configure tool that adds minimal overhead to your existing LLM API usage.
  * **Easy to use** : The LangSmith LLM Proxy is designed to be easy to use. You can run it as a sidecar to your existing app and start using it immediately. Besides changing the URL of your LLM API, you don't need to make any changes to your app to start using the LangSmith Proxy.
  * **Cache support** : The LangSmith LLM Proxy supports caching of requests/responses(with streaming support) from the LLM API. This allows you to cache responses for a configurable amount of time, reducing the number of requests to the LLM API and improving response times. This can be especially useful if you are using the LLM API in a high-traffic environment or running CI(in cases like evals)
  * **Minimal overhead** : We use NGINX as a reverse proxy to minimize the overhead of the LangSmith LLM Proxy. Requests are passed through the proxy with minimal processing.
  * **Streaming support** : The LangSmith LLM Proxy supports streaming responses from the LLM API. This allows you to start processing the response as soon as it is available, rather than waiting for the entire response to be received.
  * **Tracing support(optional)** : The LangSmith LLM Proxy supports tracing of LLM calls via LangSmith. This allows you to trace calls to your LLM without any configuration changes to your app.


## Models Supported[‚Äã](https://docs.smith.langchain.com/old/proxy/quickstart#models-supported "Direct link to Models Supported")
The LangSmith Proxy supports the following APIS:
  * OpenAI(Chat and Completion)
  * AzureOpenAI(Chat and Completion)


We are actively working on adding support for the following models:
  * Anthropic
  * Google Vertex
  * Gemini


If you would like to see support for a specific model, please message us at support@langchain.dev
The steps in this guide will acquaint you with deploying the LangSmith Proxy and using it to make requests to OpenAI.
  1. Launch the LangSmith Proxy container in your environment
  2. Edit your app to make requests to the LangSmith Proxy
  3. Forcefully turn off caching for the LangSmith Proxy
  4. Cache a streamed request
  5. Turn on tracing while using the LangSmith Proxy


## Prerequisites[‚Äã](https://docs.smith.langchain.com/old/proxy/quickstart#prerequisites "Direct link to Prerequisites")
  1. Docker installed on your local machine 
     * Instructions for installing Docker can be found [here](https://docs.docker.com/get-docker/)
  2. An OpenAI API Key
  3. (Optional) If configuring tracing, a LangSmith API Key


## 1. Deploy the LangSmith Proxy[‚Äã](https://docs.smith.langchain.com/old/proxy/quickstart#1-deploy-the-langsmith-proxy "Direct link to 1. Deploy the LangSmith Proxy")
The LangSmith Proxy is available as a Docker container. You can run it in your environment by running the following command:
```
docker pull docker.io/langchain/langsmith-proxy:latest # Force pull the latest version of the LangSmith Proxydocker run -p 8080:8080 docker.io/langchain/langsmith-proxy:latest -p 8080:8080 # Run the LangSmith Proxy on port 8080 and publish it to the host
```

You should see the following output:
```
2024-03-06 12:59:57,458 CRIT Supervisor is running as root. Privileges were not dropped because no user is specified in the config file. If you intend to run as root, you can set user=root in the config file to avoid this message.2024-03-06 12:59:57,467 INFO supervisord started with pid 12024-03-06 12:59:58,503 INFO spawned: 'nginx' with pid 82024-03-06 12:59:58,552 INFO spawned: 'trace-processor' with pid 102024-03-06 12:59:59,562 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)2024-03-06 12:59:59,563 INFO success: trace-processor entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)Couldn't create langsmith client: API key must be provided when using hosted LangSmith API, will skip creating runsListening for traces at 0.0.0.0:9999Connection from ('127.0.0.1', 47370)
```

Ignore the `Couldn't create langsmith client` message if you are not configuring tracing.
## 2. Update your app to make requests to the LangSmith Proxy[‚Äã](https://docs.smith.langchain.com/old/proxy/quickstart#2-update-your-app-to-make-requests-to-the-langsmith-proxy "Direct link to 2. Update your app to make requests to the LangSmith Proxy")
For this example, we'll be using your local proxy running on `localhost:8080`. You can replace this with the address of your proxy if it's running on a different machine.
You may need to install some packages for this example:
```
pip install openaipip install asyncio
```

Now, you can use the LangSmith Proxy to make requests to OpenAI. Here's an example of how you can do this in Python: Let's create a file called `openai_test.py` and add the following code:
```
import timeimport openai# Different models can be accessed by changing the /proxy/openai to /proxy/<model>OPENAI_API_URL ="http://localhost:8080/proxy/openai"client = openai.Client(api_key="", base_url=OPENAI_API_URL)start = time.time()response = client.chat.completions.create(  model="gpt-3.5-turbo",  messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Write a poem about artificial intelligence."}],)print(response)print(f"Time taken: {time.time()- start}")
```

Run the file using the following command:
```
python openai_test.pyChatCompletion(id='chatcmpl-8zqYoQDvQy2uHHHtwyahUuh0hRH3r', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content="In a realm of circuits and code so bright,\nArtificial intelligence takes its flight.\nA mind of silicon, cold and precise,\nA creation of humans, a technological device.\n\nWith algorithms running like a symphony,\nAI learns and grows with incredible efficiency.\nProcessing data at lightning speed,\nUnraveling mysteries with amazing heed.\n\nIt sees patterns in the chaos, finds order in the haze,\nPredicts our future with astonishing gaze.\nYet behind the code, a question looms,\nCan AI truly understand human rooms?\n\nIts knowledge vast, its power immense,\nBut can it possess compassion, can it sense?\nWill it help or hinder, create or destroy,\nThis artificial mind, this modern decoy?\n\nAs we advance in this digital age,\nLet's remember AI is but a page.\nA tool we wield with cautious hand,\nFor its potential is great, but so is its demand. \n\nSo let us guide it with wisdom and care,\nFor artificial intelligence, like us, is rare.\nA marvel of innovation, a wonder to behold,\nMay we navigate this future, with hearts of gold.", role='assistant', function_call=None, tool_calls=None))], created=1709750742, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=220, prompt_tokens=24, total_tokens=244))Time taken: 4.794615030288696
```

There will also be a corresponding Cache Miss log entry in your proxy.
```
"POST /proxy/openai/chat/completions HTTP/1.1" 200 20418 "-" "OpenAI/Python 1.12.0" "CACHE: MISS"
```

Pretty slow! Let's try running this again.
```
python openai_test.pyChatCompletion(id='chatcmpl-8zqYoQDvQy2uHHHtwyahUuh0hRH3r', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content="In a realm of circuits and code so bright,\nArtificial intelligence takes its flight.\nA mind of silicon, cold and precise,\nA creation of humans, a technological device.\n\nWith algorithms running like a symphony,\nAI learns and grows with incredible efficiency.\nProcessing data at lightning speed,\nUnraveling mysteries with amazing heed.\n\nIt sees patterns in the chaos, finds order in the haze,\nPredicts our future with astonishing gaze.\nYet behind the code, a question looms,\nCan AI truly understand human rooms?\n\nIts knowledge vast, its power immense,\nBut can it possess compassion, can it sense?\nWill it help or hinder, create or destroy,\nThis artificial mind, this modern decoy?\n\nAs we advance in this digital age,\nLet's remember AI is but a page.\nA tool we wield with cautious hand,\nFor its potential is great, but so is its demand. \n\nSo let us guide it with wisdom and care,\nFor artificial intelligence, like us, is rare.\nA marvel of innovation, a wonder to behold,\nMay we navigate this future, with hearts of gold.", role='assistant', function_call=None, tool_calls=None))], created=1709750742, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=220, prompt_tokens=24, total_tokens=244))Time taken: 0.12684011459350586
```

Cache hit! Our request hits the cache and the response time is much faster. You will also see a Cache Hit log entry in your proxy.
```
"POST /proxy/openai/chat/completions HTTP/1.1" 200 24915 "-" "OpenAI/Python 1.12.0" "CACHE: HIT"
```

## 3. Forcefully turn off caching for the LangSmith Proxy[‚Äã](https://docs.smith.langchain.com/old/proxy/quickstart#3-forcefully-turn-off-caching-for-the-langsmith-proxy "Direct link to 3. Forcefully turn off caching for the LangSmith Proxy")
In some scenarios, you may want to forcefully turn off caching for the LangSmith Proxy. You can do this by adding a Cache-Control header `{"Cache-Control": "no-cache"}` header to your request. This will force the LangSmith Proxy to make a request to the LLM API and bypass the cache.
```
import timeimport openai# Different models can be accessed by changing the /proxy/openai to /proxy/<model>OPENAI_API_URL ="http://localhost:8080/proxy/openai"client = openai.Client(api_key="", base_url=OPENAI_API_URL, default_headers={"Cache-Control":"no-cache"})start = time.time()response = client.chat.completions.create(  model="gpt-3.5-turbo",  messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Write a poem about artificial intelligence."}],)print(response)print(f"Time taken: {time.time()- start}")
```

There will also be a corresponding Cache Bypass log entry in your proxy.
```
"POST /proxy/openai/chat/completions HTTP/1.1" 200 24376 "-" "OpenAI/Python 1.12.0" "CACHE: BYPASS"
```

As you can see, the request takes a longer time to complete as the LangSmith Proxy is making a request to the LLM API. You will also see a Bypass Header in your response.
## 4. Support for Streaming[‚Äã](https://docs.smith.langchain.com/old/proxy/quickstart#4-support-for-streaming "Direct link to 4. Support for Streaming")
The LangSmith Proxy supports streaming responses from the LLM API. This allows you to start processing the response as soon as it is available, rather than waiting for the entire response to be received. This will still work with caching!
Update your `openai_test.py` file to use the streaming API:
```
import timeimport openai# Different models can be accessed by changing the /proxy/openai to /proxy/<model>OPENAI_API_URL ="http://localhost:8080/proxy/openai"client = openai.Client(api_key="", base_url=OPENAI_API_URL)start = time.time()stream = client.chat.completions.create(  model="gpt-3.5-turbo",  messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Write a poem about artificial intelligence."}],  stream=True)for chunk in stream:print(chunk)print(f"Time taken: {time.time()- start}")
```

```
python openai_test.pyChatCompletionChunk(id='chatcmpl-8zqcc2vx7sLVumNOcxTBWH8VVFlNm', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1709750978, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint='fp_2b778c6b35')ChatCompletionChunk(id='chatcmpl-8zqcc2vx7sLVumNOcxTBWH8VVFlNm', choices=[Choice(delta=ChoiceDelta(content='In', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1709750978, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint='fp_2b778c6b35')ChatCompletionChunk(id='chatcmpl-8zqcc2vx7sLVumNOcxTBWH8VVFlNm', choices=[Choice(delta=ChoiceDelta(content=' realms', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1709750978, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint='fp_2b778c6b35')ChatCompletionChunk(id='chatcmpl-8zqcc2vx7sLVumNOcxTBWH8VVFlNm', choices=[Choice(delta=ChoiceDelta(content=' of', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1709750978, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint='fp_2b778c6b35')ChatCompletionChunk(id='chatcmpl-8zqcc2vx7sLVumNOcxTBWH8VVFlNm', choices=[Choice(delta=ChoiceDelta(content=' ones', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1709750978, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint='fp_2b778c6b35')ChatCompletionChunk(id='chatcmpl-8zqcc2vx7sLVumNOcxTBWH8VVFlNm', choices=[Choice(delta=ChoiceDelta(content=' and', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1709750978, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint='fp_2b778c6b35')...Time taken: 3.4340438842773438
```

Let's run this again and see how the response time changes.
```
python openai_test.pyChatCompletionChunk(id='chatcmpl-8zqcc2vx7sLVumNOcxTBWH8VVFlNm', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1709750978, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint='fp_2b778c6b35')ChatCompletionChunk(id='chatcmpl-8zqcc2vx7sLVumNOcxTBWH8VVFlNm', choices=[Choice(delta=ChoiceDelta(content='In', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1709750978, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint='fp_2b778c6b35')ChatCompletionChunk(id='chatcmpl-8zqcc2vx7sLVumNOcxTBWH8VVFlNm', choices=[Choice(delta=ChoiceDelta(content=' realms', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1709750978, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint='fp_2b778c6b35')ChatCompletionChunk(id='chatcmpl-8zqcc2vx7sLVumNOcxTBWH8VVFlNm', choices=[Choice(delta=ChoiceDelta(content=' of', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1709750978, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint='fp_2b778c6b35')ChatCompletionChunk(id='chatcmpl-8zqcc2vx7sLVumNOcxTBWH8VVFlNm', choices=[Choice(delta=ChoiceDelta(content=' ones', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1709750978, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint='fp_2b778c6b35')ChatCompletionChunk(id='chatcmpl-8zqcc2vx7sLVumNOcxTBWH8VVFlNm', choices=[Choice(delta=ChoiceDelta(content=' and', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1709750978, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint='fp_2b778c6b35')...Time taken: 0.07226419448852539
```

Again, much faster! The corresponding miss and hit logs will also be present in your proxy.
```
"POST /proxy/openai/chat/completions HTTP/1.1" 200 17761 "-" "OpenAI/Python 1.12.0" "CACHE: MISS""POST /proxy/openai/chat/completions HTTP/1.1" 200 17745 "-" "OpenAI/Python 1.12.0" "CACHE: HIT"
```

## 5. Turn on tracing while using the LangSmith Proxy[‚Äã](https://docs.smith.langchain.com/old/proxy/quickstart#5-turn-on-tracing-while-using-the-langsmith-proxy "Direct link to 5. Turn on tracing while using the LangSmith Proxy")
Requests made to the LangSmith proxy can also be traced. You can do this by running the docker image with the `LANGSMITH_API_KEY` environment variable set to your LangSmith API Key.
```
docker run -p 8080:8080 -e LANGCHAIN_API_KEY=<your_langsmith_api_key> -e docker.io/langchain/langsmith-proxy:latest -p 8080:8080
```

Run the script again, and you should see a trace in your LangSmith dashboard.
```
python openai_test.py# In nginx logsCreated run with inputs: {'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Write a poem about artificial intelligence.'}], 'model': 'gpt-3.5-turbo', 'stream': True}
```

The corresponding trace also appears in your LangSmith dashboard!
[![LangSmith Trace](https://docs.smith.langchain.com/assets/images/trace-630266d15b33633c5e24d1394b08ed68.png)](https://docs.smith.langchain.com/assets/files/trace-630266d15b33633c5e24d1394b08ed68.png)
## Next Steps[‚Äã](https://docs.smith.langchain.com/old/proxy/quickstart#next-steps "Direct link to Next Steps")
  * In this guide, you learned how to deploy the LangSmith Proxy and use it to proxy requests to OpenAI.
  * We highly recommend trying this out with your own applications and models.
  * Note that the proxy is intended to run near your application to minimize latency. You can run it as a sidecar to your application or as a separate service.
  * We also bundle this into the LangSmith Self-Hosted deployment by default. If you are using LangSmith Self-Hosted, you can use your LangSmith url as the proxy url.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/proxy/quickstart%3E).
[PreviousProxy](https://docs.smith.langchain.com/old/category/proxy)[NextUse LangSmith Proxy with Azure OpenAI](https://docs.smith.langchain.com/old/proxy/azure_openai)
  * [What is the LangSmith Proxy](https://docs.smith.langchain.com/old/proxy/quickstart#what-is-the-langsmith-proxy)
  * [Models Supported](https://docs.smith.langchain.com/old/proxy/quickstart#models-supported)
  * [Prerequisites](https://docs.smith.langchain.com/old/proxy/quickstart#prerequisites)
  * [1. Deploy the LangSmith Proxy](https://docs.smith.langchain.com/old/proxy/quickstart#1-deploy-the-langsmith-proxy)
  * [2. Update your app to make requests to the LangSmith Proxy](https://docs.smith.langchain.com/old/proxy/quickstart#2-update-your-app-to-make-requests-to-the-langsmith-proxy)
  * [3. Forcefully turn off caching for the LangSmith Proxy](https://docs.smith.langchain.com/old/proxy/quickstart#3-forcefully-turn-off-caching-for-the-langsmith-proxy)
  * [4. Support for Streaming](https://docs.smith.langchain.com/old/proxy/quickstart#4-support-for-streaming)
  * [5. Turn on tracing while using the LangSmith Proxy](https://docs.smith.langchain.com/old/proxy/quickstart#5-turn-on-tracing-while-using-the-langsmith-proxy)
  * [Next Steps](https://docs.smith.langchain.com/old/proxy/quickstart#next-steps)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/self_hosting/docker

[Skip to main content](https://docs.smith.langchain.com/old/self_hosting/docker#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/self_hosting/docker)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)
    * [Kubernetes](https://docs.smith.langchain.com/old/self_hosting/kubernetes)
    * [Docker](https://docs.smith.langchain.com/old/self_hosting/docker)
    * [Usage](https://docs.smith.langchain.com/old/self_hosting/usage)
    * [Release Notes (Self-Hosted)](https://docs.smith.langchain.com/old/self_hosting/release_notes)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)
  * Docker


On this page
# Self-hosting LangSmith with Docker
Enterprise License Required
Self-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our [pricing page](https://www.langchain.com/pricing) for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment.
This guide provides instructions for installing and setting up your environment to run LangSmith locally using Docker. You can do this either by using the LangSmith SDK or by using Docker Compose directly.
## Prerequisites[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#prerequisites "Direct link to Prerequisites")
  1. Ensure Docker is installed and running on your system. You can verify this by running: 
```
docker info
```

If you don't see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.
  2. LangSmith License Key 
    1. You can get this from your Langchain representative. Contact us at sales@langchain.dev for more information.
  3. LangSmith Version 
    1. Our Docker Compose files are pegged to the latest self-hosted release of LangSmith. If you want to use a different version, you can specify it in the `.env` file or in the `docker-compose.yml` file.
  4. OpenAI API Key(optional). 
    1. Used for natural language search feature. Can specify OpenAI key in browser as well for the playground feature.
  5. Oauth Configuration(optional). 
    1. You can configure oauth using the `.env` file. You will need to provide a `client_id` and `client_issuer_url` for your oauth provider.
    2. Note, we do rely on the OIDC Authorization Code with PKCE flow. We currently support almost anything that is OIDC compliant however Google does not support this flow.
  6. External Postgres(optional). 
    1. You can configure external postgres using the `.env` file. You will need to set the `POSTGRES_DATABASE_URI` environment variable to the connection string for your postgres instance.
    2. If using a schema other than public, ensure that you do not have any other schemas with the pgcrypto extension enabled or you must include that in your search path.
    3. Note: We do only officially support Postgres versions >= 14.
  7. External Redis(optional). 
    1. You can configure external redis using the `.env` file. You will need to set the `REDIS_DATABASE_URI` environment variable to the connection string for your redis instance.
    2. Currently, we do not support using Redis with TLS. We will be supporting this shortly.
    3. We only official support Redis versions >= 6.


## Running via Docker Compose[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#running-via-docker-compose "Direct link to Running via Docker Compose")
The following explains how to run the LangSmith using Docker Compose. This is the most flexible way to run LangSmith without Kubernetes. In production, we highly recommend using Kubernetes.
### 1. Fetch the LangSmith `docker-compose.yml` file[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#1-fetch-the-langsmith-docker-composeyml-file "Direct link to 1-fetch-the-langsmith-docker-composeyml-file")
You can find the `docker-compose.yml` file and related files in the LangSmith SDK repository here: [_LangSmith Docker Compose File_](https://github.com/langchain-ai/langsmith-sdk/blob/main/python/langsmith/cli/docker-compose.yaml)
Copy the `docker-compose.yml` file and all files in that directory from the LangSmith SDK to your project directory.
  * Ensure that you copy the `users.xml` file as well.


### 2. Configure environment variables[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#2-configure-environment-variables "Direct link to 2. Configure environment variables")
Copy the `.env.example` file from the LangSmith SDK to your project directory and rename it to `.env`. Then, set the following environment variables in the `.env` file:
```
# Don't change this file. Instead, copy it to .env and change the values there. The default values will work out of the box as long as you provide your license key._LANGSMITH_IMAGE_VERSION=0.2.11 # Change to your desired LangSmith version. Required. Typically you should use the latest version defined in the .env file.LANGSMITH_LICENSE_KEY=your-license-key # Change to your Langsmith license key. RequiredOPENAI_API_KEY=your-openai-api-key # Needed for Online Evals and Magic Query featuresAUTH_TYPE=none # Set to "oauth" if you want to use OAuth2.0OAUTH_CLIENT_ID=your-client-id # Required if AUTH_TYPE=oauthOAUTH_ISSUER_URL=https://your-issuer-url # Required if AUTH_TYPE=oauthAPI_KEY_SALT=super # Change to your desired API key salt. Can be any random value. Must be set if AUTH_TYPE=oauthPOSTGRES_DATABASE_URI=postgres:postgres@langchain-db:5432/postgres # Change to your database URI if using external postgres. Otherwise, leave it as isREDIS_DATABASE_URI=redis://langchain-redis:6379 # Change to your Redis URI if using external Redis. Otherwise, leave it as isLOG_LEVEL=warning # Change to your desired log levelMAX_ASYNC_JOBS_PER_WORKER=10 # Change to your desired maximum async jobs per worker. We recommend 10/suggest spinning up more replicas of the queue worker if you need more throughput.
```

You can also set these environment variables in the `docker-compose.yml` file directly or export them in your terminal. We recommend setting them in the `.env` file.
### 2. Start server[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#2-start-server "Direct link to 2. Start server")
Start the LangSmith application by executing the following command in your terminal:
```
docker-compose up
```

You can also run the server in the background by running:
```
docker-compose up -d
```

If the server starts correctly, it will open up the UI in your browser at [http://localhost](http://localhost/).
The LangSmith UI should be visible/operational and look like this:
![./static/langsmith_ui.png](https://docs.smith.langchain.com/assets/images/langsmith_ui-a308960b13a121598b5c577e7587adfe.png)
### Stopping the server[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#stopping-the-server "Direct link to Stopping the server")
```
docker-compose down
```

### Checking the logs[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#checking-the-logs "Direct link to Checking the logs")
If, at any point, you want to check if the server is running and see the logs, run
```
docker-compose logs
```

## Running via the LangSmith SDK[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#running-via-the-langsmith-sdk "Direct link to Running via the LangSmith SDK")
The following explains how to run the LangSmith using the LangSmith SDK. This is a convenient wrapper around the `docker-compose` command. We recommend only using this in a local setting as it is not as flexible as using `docker-compose` directly.
### 1. Install the LangSmith SDK[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#1-install-the-langsmith-sdk "Direct link to 1. Install the LangSmith SDK")
The Python LangSmith SDKs exposes a `langsmith` command line tool.
First, install a recent version of the `langsmith` package:
  * Python/Pip: `pip install -U langsmith`


This will install the LangSmith Client and the bundled command line tool.
### 2. Start server[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#2-start-server-1 "Direct link to 2. Start server")
Start the LangSmith tracing server by executing the following command in your terminal:
```
langsmith start --langsmith-license-key=<YOUR_LANGSMITH_LICENSE_KEY> --version=<LANGSMITH_VERSION> --openai-api-key=<YOUR_OPENAI_API_KEY>
```

If the server starts correctly, it will open up the UI in your browser at <http://localhost>.
The LangSmith UI should be visible/operational and look like this:
![./static/langsmith_ui.png](https://docs.smith.langchain.com/assets/images/langsmith_ui-a308960b13a121598b5c577e7587adfe.png)
### Stopping the server[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#stopping-the-server-1 "Direct link to Stopping the server")
To stop the server, run the following command:
```
langsmith stop
```

### Checking the logs[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#checking-the-logs-1 "Direct link to Checking the logs")
If, at any point, you want to check if the server is running and see the logs, run
```
langsmith logs
```

## Using LangSmith[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#using-langsmith "Direct link to Using LangSmith")
Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the [Self-Hosted Usage Guide](https://docs.smith.langchain.com/self_hosting/usage).
### Frequently Asked Questions[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#frequently-asked-questions "Direct link to Frequently Asked Questions")
#### How can we upgrade our application?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#how-can-we-upgrade-our-application "Direct link to How can we upgrade our application?")
  * We plan to release new minor versions of the LangSmith application every 6 weeks. This will include release notes and all changes should be backwards compatible. To upgrade, you will need to restart your LangSmith instance specifying the new version.


#### How can we back up our application?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#how-can-we-back-up-our-application "Direct link to How can we back up our application?")
  * The docker solution uses docker volumes to store data. You can back up the data by backing up the volumes.


#### How can we authenticate to the application?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#how-can-we-authenticate-to-the-application "Direct link to How can we authenticate to the application?")
  * Currently, our self-hosted solution supports oauth as an authn solution.
  * Note, we do offer a no-auth solution but highly recommend setting up oauth before moving into production.


#### How can I use External `Postgres` or `Redis`?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#how-can-i-use-external-postgres-or-redis "Direct link to how-can-i-use-external-postgres-or-redis")
  * You can configure external postgres or redis using the external sections in the `.env` file. You will need to provide the connection url/params for the database/redis instance. Look at the `.env.example` file for more information.


#### What networking configuration is needed for the application?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#what-networking-configuration-is-needed-for-the-application "Direct link to What networking configuration is needed for the application?")
  * Our deployment only needs egress for a few things: 
    * Fetching images (If mirroring your images, this may not be needed)
    * Talking to any LLMs
  * Your VPC can set up rules to limit any other access.
  * Note: We require the X-Tenant-Id to be allowed to be passed through to the backend service. This is used to determine which tenant the request is for.


#### What resources should we allocate to the application?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#what-resources-should-we-allocate-to-the-application "Direct link to What resources should we allocate to the application?")
  * We recommend at least 4 vCPUs and 16GB of memory for our application. This is a rough estimate and can vary based on the number of users and the size of the data you are working with.


#### Increasing the number of replicas[‚Äã](https://docs.smith.langchain.com/old/self_hosting/docker#increasing-the-number-of-replicas "Direct link to Increasing the number of replicas")
  * If you need more throughput, you can increase the number of replicas of the queue worker by running the following command: 
```
docker-compose up --scale langchain-queue=5
```

This will start 5 replicas of the queue worker. You can change the number to whatever you need. Note that these containers are fairly CPU intensive, and you should ensure you have enough resources to support the number of replicas you are starting.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/self_hosting/docker%3E).
[PreviousKubernetes](https://docs.smith.langchain.com/old/self_hosting/kubernetes)[NextUsage](https://docs.smith.langchain.com/old/self_hosting/usage)
  * [Prerequisites](https://docs.smith.langchain.com/old/self_hosting/docker#prerequisites)
  * [Running via Docker Compose](https://docs.smith.langchain.com/old/self_hosting/docker#running-via-docker-compose)
    * [1. Fetch the LangSmith `docker-compose.yml` file](https://docs.smith.langchain.com/old/self_hosting/docker#1-fetch-the-langsmith-docker-composeyml-file)
    * [2. Configure environment variables](https://docs.smith.langchain.com/old/self_hosting/docker#2-configure-environment-variables)
    * [2. Start server](https://docs.smith.langchain.com/old/self_hosting/docker#2-start-server)
    * [Stopping the server](https://docs.smith.langchain.com/old/self_hosting/docker#stopping-the-server)
    * [Checking the logs](https://docs.smith.langchain.com/old/self_hosting/docker#checking-the-logs)
  * [Running via the LangSmith SDK](https://docs.smith.langchain.com/old/self_hosting/docker#running-via-the-langsmith-sdk)
    * [1. Install the LangSmith SDK](https://docs.smith.langchain.com/old/self_hosting/docker#1-install-the-langsmith-sdk)
    * [2. Start server](https://docs.smith.langchain.com/old/self_hosting/docker#2-start-server-1)
    * [Stopping the server](https://docs.smith.langchain.com/old/self_hosting/docker#stopping-the-server-1)
    * [Checking the logs](https://docs.smith.langchain.com/old/self_hosting/docker#checking-the-logs-1)
  * [Using LangSmith](https://docs.smith.langchain.com/old/self_hosting/docker#using-langsmith)
    * [Frequently Asked Questions](https://docs.smith.langchain.com/old/self_hosting/docker#frequently-asked-questions)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/self_hosting/kubernetes

[Skip to main content](https://docs.smith.langchain.com/old/self_hosting/kubernetes#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/self_hosting/kubernetes)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)
    * [Kubernetes](https://docs.smith.langchain.com/old/self_hosting/kubernetes)
    * [Docker](https://docs.smith.langchain.com/old/self_hosting/docker)
    * [Usage](https://docs.smith.langchain.com/old/self_hosting/usage)
    * [Release Notes (Self-Hosted)](https://docs.smith.langchain.com/old/self_hosting/release_notes)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)
  * Kubernetes


On this page
# Self-hosting LangSmith on Kubernetes
Enterprise License Required
Self-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our [pricing page](https://www.langchain.com/pricing) for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment.
This guide will walk you through the process of deploying LangSmith to a Kubernetes cluster. We will use Helm to install LangSmith and its dependencies.
We've successfully tested LangSmith on the following Kubernetes distributions:
  * Google Kubernetes Engine (GKE)
  * Amazon Elastic Kubernetes Service (EKS)
  * Azure Kubernetes Service (AKS)
  * OpenShift
  * Minikube and Kind (for development purposes)


## Prerequisites[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#prerequisites "Direct link to Prerequisites")
Ensure you have the following tools/items ready. Some items are marked optional but :
  1. A working Kubernetes cluster that you can access via `kubectl`. Your cluster should have the following minimum requirements:
    1. Recommended: At least 4 vCPUs, 16GB Memory available 
       * You may need to tune resource requests/limits for all of our different services based off of organization size/usage
    2. Valid Dynamic PV provisioner or PVs available on your cluster. You can verify this by running: 
```
kubectl get storageclass
```

The output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:
```
  NAME      PROVISIONER       RECLAIMPOLICY  VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION  AGE  gp2 (default)  kubernetes.io/aws-ebs  Delete     WaitForFirstConsumer  false         161d
```

  2. Helm
    1. `brew install helm`
  3. LangSmith License Key
    1. You can get this from your Langchain representative. Contact us at sales@langchain.dev for more information.
  4. SSL(optional)
    1. This should be attachable to a load balancer that will be provisioned by your cloud provider. This will be used for the frontend service.
  5. OpenAI API Key(optional).
    1. Used for natural language search feature(beta). Can specify OpenAI key in browser as well for the playground feature.
  6. OAuth Configuration(optional).
    1. You can configure oauth using the `values.yaml` file. You will need to provide a `client_id` and `client_issuer_url` for your OAuth provider.
    2. Note, we do rely on the OIDC Authorization Code with PKCE flow. We currently support almost anything that is OIDC compliant however Google does not support this flow.
    3. Without OAuth, you will not be able to create users or organizations.
  7. External Postgres(optional).
    1. You can configure external postgres using the `values.yaml` file. You will need to provide connection parameters for your postgres instance.
    2. If using a schema other than public, ensure that you do not have any other schemas with the pgcrypto extension enabled, or you must include that in your search path.
    3. Note: We do only officially support Postgres versions >= 14.
  8. External Redis(optional).
    1. You can configure external redis using the `values.yaml` file. You will need to provide a connection url for your redis instance.
    2. Currently, we do not support using Redis with TLS. We will be supporting this shortly.
    3. We only official support Redis versions >= 6.


## Configure your Helm Charts:[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#configure-your-helm-charts "Direct link to Configure your Helm Charts:")
  1. Create a new file called `langsmith_config.yaml`. This should have a similar structure to the `values.yaml` file in the LangSmith Helm Chart repository. Only include the values you want to override to avoid having to update the file every time the chart is updated.
  2. Override any values in the file. Refer to the documentation for the [_LangSmith Helm Chart_](https://github.com/langchain-ai/helm/tree/main/charts/langsmith) to see all configurable values. Some values we recommend setting: 
    1. Resources
    2. SSL(If on EKS or some other cloud provider) 
      1. Add an annotation to the `frontend.service` object to tell your cloud provider to provision a load balancer with said certificate attached
    3. OpenAI Api Key
    4. Images
    5. Oauth


An example bare minimum config file `langsmith_config.yaml`:
```
config:langsmithLicenseKey:""
```

You can also see some example configurations in the examples directory.
## Deploying to Kubernetes:[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#deploying-to-kubernetes "Direct link to Deploying to Kubernetes:")
  1. Verify that you can connect to your Kubernetes cluster(note: We highly suggest installing into an empty namespace)
    1. Run `kubectl get pods`
Output should look something like:
```
kubectl get pods                                                                                   ‚éà langsmith-eks-2vauP7wf 21:07:46No resources found in default namespace.
```

  2. Ensure you have the Langchain Helm repo added. (skip this step if you are using local charts)
helm repo add langchain <https://langchain-ai.github.io/helm/> "langchain" has been added to your repositories
  3. Run `helm install langsmith langchain/langsmith --values langsmith_config.yaml --namespace <your-namespace> --version <version>`
Output should look something like:
```
NAME: langsmithLAST DEPLOYED: Fri Sep 17 21:08:47 2021NAMESPACE: langsmithSTATUS: deployedREVISION: 1TEST SUITE: None
```

  4. Run `kubectl get pods` Output should now look something like:
```
langsmith-backend-6ff46c99c4-wz22d    1/1   Running  0     3h2mlangsmith-frontend-6bbb94c5df-8xrlr   1/1   Running  0     3h2mlangsmith-hub-backend-5cc68c888c-vppjj  1/1   Running  0     3h2mlangsmith-playground-6d95fd8dc6-x2d9b  1/1   Running  0     3h2mlangsmith-postgres-0           1/1   Running  0     9hlangsmith-queue-5898b9d566-tv6q8     1/1   Running  0     3h2mlangsmith-redis-0            1/1   Running  0     9h
```



## Validate your deployment:[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#validate-your-deployment "Direct link to Validate your deployment:")
  1. Run `kubectl get services`
Output should look something like:
```
NAME          TYPE      CLUSTER-IP    EXTERNAL-IP                                PORT(S)    AGElangsmith-backend    ClusterIP   172.20.140.77  <none>                                  1984/TCP    35hlangsmith-frontend   LoadBalancer  172.20.253.251  <external ip>                               80:31591/TCP  35hlangsmith-hub-backend  ClusterIP   172.20.112.234  <none>                                  1985/TCP    35hlangsmith-playground  ClusterIP   172.20.153.194  <none>                                  3001/TCP    9hlangsmith-postgres   ClusterIP   172.20.244.82  <none>                                  5432/TCP    35hlangsmith-redis     ClusterIP   172.20.81.217  <none>                                  6379/TCP    35h
```

  2. Curl the external ip of the `langsmith-frontend` service:
```
curl <external ip>/api/tenants[{"id":"00000000-0000-0000-0000-000000000000","has_waitlist_access":true,"created_at":"2023-09-13T18:25:10.488407","display_name":"Personal","config":{"is_personal":true,"max_identities":1},"tenant_handle":"default"}]%
```

  3. Visit the external ip for the `langsmith-frontend` service on your browser
The LangSmith UI should be visible/operational
![./static/langsmith_ui.png](https://docs.smith.langchain.com/assets/images/langsmith_ui-a308960b13a121598b5c577e7587adfe.png)


## Using LangSmith[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#using-langsmith "Direct link to Using LangSmith")
Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the [Self-Hosted Usage Guide](https://docs.smith.langchain.com/self_hosting/usage).
### Frequently Asked Questions:[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#frequently-asked-questions "Direct link to Frequently Asked Questions:")
#### How can we upgrade our application?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#how-can-we-upgrade-our-application "Direct link to How can we upgrade our application?")
  * We plan to release new minor versions of the LangSmith application every 6 weeks. This will include release notes and all changes should be backwards compatible. To upgrade, you will need to follow the upgrade instructions in the Helm README and run a `helm upgrade langsmith --values <values file>`


#### How can we back up our application?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#how-can-we-back-up-our-application "Direct link to How can we back up our application?")
  * Currently, we rely on PVCs/PV to power storage for our application. We strongly encourage setting up `Persistent Volume` backups or moving to a managed service for `Postgres` to support disaster recovery


#### How does load balancing work/ingress work?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#how-does-load-balancing-workingress-work "Direct link to How does load balancing work/ingress work?")
  * Currently, our application spins up one load balancer using a k8s service of type `LoadBalancer` for our frontend. If you do not want to set up a load balancer you can simply port-forward the frontend and use that as your external ip for the application. We also have an option for the chart to provision an ingress resource for the application.


#### How can we authenticate to the application?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#how-can-we-authenticate-to-the-application "Direct link to How can we authenticate to the application?")
  * Currently, our self-hosted solution supports oauth as an authn solution. Note, we do offer a no-auth solution but highly recommend setting up oauth before moving into production.


#### How can I use External `Postgres` or `Redis`?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#how-can-i-use-external-postgres-or-redis "Direct link to how-can-i-use-external-postgres-or-redis")
  * You can configure external postgres or redis using the external sections in the `values.yaml` file. You will need to provide the connection url/params for the database/redis instance. Look at the configuration above example for more information.


#### What networking configuration is needed for the application?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#what-networking-configuration-is-needed-for-the-application "Direct link to What networking configuration is needed for the application?")
Our deployment only needs egress for a few things:
  * Fetching images (If mirroring your images, this may not be needed)
  * Talking to any LLMs
  * Talking to any external services you may have configured
  * Fetching OAuth information Your VPC can set up rules to limit any other access. Note: We require the X-Tenant-Id to be allowed to be passed through to the backend service. This is used to determine which tenant the request is for.


#### What resources should we allocate to the application?[‚Äã](https://docs.smith.langchain.com/old/self_hosting/kubernetes#what-resources-should-we-allocate-to-the-application "Direct link to What resources should we allocate to the application?")
  * We recommend at least 4 vCPUs and 16GB of memory for our application.
  * We have some default resources set in our `values.yaml` file. You can override these values to tune resource usage for your organization.
  * If the metrics server is enabled in your cluster, we also recommend enabling autoscaling on all deployments.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/self_hosting/kubernetes%3E).
[PreviousSelf-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)[NextDocker](https://docs.smith.langchain.com/old/self_hosting/docker)
  * [Prerequisites](https://docs.smith.langchain.com/old/self_hosting/kubernetes#prerequisites)
  * [Configure your Helm Charts:](https://docs.smith.langchain.com/old/self_hosting/kubernetes#configure-your-helm-charts)
  * [Deploying to Kubernetes:](https://docs.smith.langchain.com/old/self_hosting/kubernetes#deploying-to-kubernetes)
  * [Validate your deployment:](https://docs.smith.langchain.com/old/self_hosting/kubernetes#validate-your-deployment)
  * [Using LangSmith](https://docs.smith.langchain.com/old/self_hosting/kubernetes#using-langsmith)
    * [Frequently Asked Questions:](https://docs.smith.langchain.com/old/self_hosting/kubernetes#frequently-asked-questions)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/self_hosting/release_notes

[Skip to main content](https://docs.smith.langchain.com/old/self_hosting/release_notes#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/self_hosting/release_notes)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)
    * [Kubernetes](https://docs.smith.langchain.com/old/self_hosting/kubernetes)
    * [Docker](https://docs.smith.langchain.com/old/self_hosting/docker)
    * [Usage](https://docs.smith.langchain.com/old/self_hosting/usage)
    * [Release Notes (Self-Hosted)](https://docs.smith.langchain.com/old/self_hosting/release_notes)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/self_hosting/release_notes)**.
  * [](https://docs.smith.langchain.com/)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)
  * Release Notes (Self-Hosted)


On this page
# LangSmith Release Notes
note
If you are updating directly from LangSmith v0.1.x and you wish to retain access to run data in the Langsmith UI after updating, you must first update to v0.2.x and perform a data migration. **Updating directly from v0.1.x to v0.3.x or later will result in data loss as the`runs` table in postgres will be dropped when deploying LangSmith v0.3.x or higher.** Details are available at <https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md>
## Week of March 25, 2024 - LangSmith v0.4[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04 "Direct link to Week of March 25, 2024 - LangSmith v0.4")
LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter.
### Breaking changes[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#breaking-changes "Direct link to Breaking changes")
  * This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. **For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility.** Using a new api key salt will invalidate all existing api keys.
  * This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the `clickhouse.statefulSet.persistence.size` value in your `values.yaml` file. 
    * If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set `clickhouse.statefulSet.persistence.size` to the previous default value of `8Gi`.


### Performance and Reliability Changes[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#performance-and-reliability-changes "Direct link to Performance and Reliability Changes")
  * Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.


### Infrastructure changes[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#infrastructure-changes "Direct link to Infrastructure changes")
  * Some our image repositories have been updated. You can see the root repositories in our `values.yaml` file and may need to update mirrors to pick up the new images.
  * Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the `clickhouse.statefulSet.persistence.size` value in your `values.yaml` file. 
    * If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set `clickhouse.statefulSet.persistence.size` to the previous default value of `8Gi`.
  * Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application.


### Admin changes[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#admin-changes "Direct link to Admin changes")
  * Added an API key salt parameter in `values.yml`. This can be set to a custom value and changing it will invalidate all existing api keys.
  * Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.
  * Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at <https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md>


### Deprecation notices[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#deprecation-notices "Direct link to Deprecation notices")
With the release of 0.4:
  * LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes.


## Week of Februrary 21, 2024 - LangSmith v0.3[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#week-of-februrary-21-2024---langsmith-v03 "Direct link to Week of Februrary 21, 2024 - LangSmith v0.3")
LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking.
### Breaking changes[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#breaking-changes-1 "Direct link to Breaking changes")
  * This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See <https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md> for additional details


### Performance and Reliability Changes[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#performance-and-reliability-changes-1 "Direct link to Performance and Reliability Changes")
  * Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.


### Admin changes[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#admin-changes-1 "Direct link to Admin changes")
  * None


### Deprecation notices[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#deprecation-notices-1 "Direct link to Deprecation notices")
With the release of 0.3:
  * LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes.


## Week of January 29, 2024 - LangSmith v0.2[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02 "Direct link to Week of January 29, 2024 - LangSmith v0.2")
LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces.
### Requirements[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#requirements "Direct link to Requirements")
  * This release requires `langsmith-sdk` version ‚â• `0.0.71` (Python) and ‚â• `0.0.56` (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint.


### Breaking changes[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#breaking-changes-2 "Direct link to Breaking changes")
  * The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. `{"user_id": ..., "user_name":...,}`) and then search using `has(metadata, '{"user_name": ...}')`


### Performance and Reliability Changes[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#performance-and-reliability-changes-2 "Direct link to Performance and Reliability Changes")
  * Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.
  * Improved performance for updates and deletes on annotation labels.
  * Added pagination of API responses.
  * Fixed an issue impacting natural language searches.


### Infrastructure Changes[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#infrastructure-changes-1 "Direct link to Infrastructure Changes")
  * Added the `clickhouse` database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith. 
    * Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at <https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md>


### Admin changes[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#admin-changes-2 "Direct link to Admin changes")
  * Increased the maximum number of users per organization from 5 to 100 for new organizations.


### Deprecation notices[‚Äã](https://docs.smith.langchain.com/old/self_hosting/release_notes#deprecation-notices-2 "Direct link to Deprecation notices")
With the release of 0.2:
  * LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/self_hosting/release_notes%3E).
[PreviousUsage](https://docs.smith.langchain.com/old/self_hosting/usage)
  * [Week of March 25, 2024 - LangSmith v0.4](https://docs.smith.langchain.com/old/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04)
    * [Breaking changes](https://docs.smith.langchain.com/old/self_hosting/release_notes#breaking-changes)
    * [Performance and Reliability Changes](https://docs.smith.langchain.com/old/self_hosting/release_notes#performance-and-reliability-changes)
    * [Infrastructure changes](https://docs.smith.langchain.com/old/self_hosting/release_notes#infrastructure-changes)
    * [Admin changes](https://docs.smith.langchain.com/old/self_hosting/release_notes#admin-changes)
    * [Deprecation notices](https://docs.smith.langchain.com/old/self_hosting/release_notes#deprecation-notices)
  * [Week of Februrary 21, 2024 - LangSmith v0.3](https://docs.smith.langchain.com/old/self_hosting/release_notes#week-of-februrary-21-2024---langsmith-v03)
    * [Breaking changes](https://docs.smith.langchain.com/old/self_hosting/release_notes#breaking-changes-1)
    * [Performance and Reliability Changes](https://docs.smith.langchain.com/old/self_hosting/release_notes#performance-and-reliability-changes-1)
    * [Admin changes](https://docs.smith.langchain.com/old/self_hosting/release_notes#admin-changes-1)
    * [Deprecation notices](https://docs.smith.langchain.com/old/self_hosting/release_notes#deprecation-notices-1)
  * [Week of January 29, 2024 - LangSmith v0.2](https://docs.smith.langchain.com/old/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02)
    * [Requirements](https://docs.smith.langchain.com/old/self_hosting/release_notes#requirements)
    * [Breaking changes](https://docs.smith.langchain.com/old/self_hosting/release_notes#breaking-changes-2)
    * [Performance and Reliability Changes](https://docs.smith.langchain.com/old/self_hosting/release_notes#performance-and-reliability-changes-2)
    * [Infrastructure Changes](https://docs.smith.langchain.com/old/self_hosting/release_notes#infrastructure-changes-1)
    * [Admin changes](https://docs.smith.langchain.com/old/self_hosting/release_notes#admin-changes-2)
    * [Deprecation notices](https://docs.smith.langchain.com/old/self_hosting/release_notes#deprecation-notices-2)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/self_hosting/usage

[Skip to main content](https://docs.smith.langchain.com/old/self_hosting/usage#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/self_hosting/usage)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)
    * [Kubernetes](https://docs.smith.langchain.com/old/self_hosting/kubernetes)
    * [Docker](https://docs.smith.langchain.com/old/self_hosting/docker)
    * [Usage](https://docs.smith.langchain.com/old/self_hosting/usage)
    * [Release Notes (Self-Hosted)](https://docs.smith.langchain.com/old/self_hosting/release_notes)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/self_hosting/usage)**.
  * [](https://docs.smith.langchain.com/)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)
  * Usage


On this page
# How to use your self-hosted instance of LangSmith
This guide will walk you through the process of using your self-hosted instance of LangSmith.
Self-Hosted LangSmith Instance Required
This guide assumes you have already deployed a self-hosted LangSmith instance. If you have not, please refer to the [kubernetes deployment guide](https://docs.smith.langchain.com/self_hosting/installation/kubernetes) or the [docker deployment guide](https://docs.smith.langchain.com/self_hosting/installation/docker).
### Using your deployment:[‚Äã](https://docs.smith.langchain.com/old/self_hosting/usage#using-your-deployment "Direct link to Using your deployment:")
  1. Once you have deployed your instance, you can access the LangSmith UI at `http://<external ip>`.
  2. The backend API will be available at `http://<external ip>/api` and the hub API will be available at `http://<external ip>/api-hub`.


To use the LangSmith API, you will need to set the following environment variables in your application:
```
LANGCHAIN_ENDPOINT=http://<external ip>/apiLANGCHAIN_HUB_API_URL=http://<external ip>/api-hubLANGCHAIN_API_KEY=foo # Set to a legitimate API key if using OAuth
```

You can also configure these variables directly in the LangSmith SDK client:
```
import langsmithlangsmith_client = langsmith.Client(  api_key='<api_key>',  api_url='http://<external ip>/api',)import langchainhublangchainhub.Client(  api_key='<api_key>',  api_url='http://<external ip>/api-hub')
```

After setting the above, you should be able to run your code and see the results in your self-hosted instance. We recommend running through the [_quickstart guide_](https://docs.smith.langchain.com/#quick-start) to get a feel for how to use LangSmith.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/self_hosting/usage%3E).
[PreviousDocker](https://docs.smith.langchain.com/old/self_hosting/docker)[NextRelease Notes (Self-Hosted)](https://docs.smith.langchain.com/old/self_hosting/release_notes)
  * [Using your deployment:](https://docs.smith.langchain.com/old/self_hosting/usage#using-your-deployment)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing

[Skip to main content](https://docs.smith.langchain.com/old/tracing#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * Tracing


# Tracing Overview
Tracing is a powerful tool for understanding the behavior of your LLM application. LangSmith has best-in-class tracing capabilities, regardless of whether or not you are using LangChain.
Tracing can help you track down issues like:
  * An unexpected end result
  * Why an agent is looping
  * Why a chain was slower than expected
  * How many tokens an agent used at each step


To get started, check out the [Quick Start Guide](https://docs.smith.langchain.com/old/tracing/quick_start).
After that, peruse the [Concepts Section](https://docs.smith.langchain.com/old/tracing/concepts) to better understand the different components involved with tracing.
If you want to learn how to accomplish a particular task, check out our comprehensive [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
For example use cases, check out the [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases) page.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/tracing%3E).
[PreviousUser Guide](https://docs.smith.langchain.com/old/user_guide)[NextQuick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/concepts

[Skip to main content](https://docs.smith.langchain.com/old/tracing/concepts#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/concepts)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * Concepts


On this page
# Concepts
In this guide we will go over some of the concepts that are important to understand when logging traces to LangSmith. A `Trace` is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a `Run`. A `Project` is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer.
![LangSmith Primitives](https://docs.smith.langchain.com/assets/images/primitives-708e671bad3ba4cd65e2eaaa3d64d40b.png)
Primitive datatypes in LangSmith
## Runs[‚Äã](https://docs.smith.langchain.com/old/tracing/concepts#runs "Direct link to Runs")
A `Run` is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with [OpenTelemetry](https://opentelemetry.io/), you can think of a run as a span. ![Run](https://docs.smith.langchain.com/assets/images/run-4d754fd31fb6f3a2b1bf054a2a8b0b42.png)
## Traces[‚Äã](https://docs.smith.langchain.com/old/tracing/concepts#traces "Direct link to Traces")
A `Trace` is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with [OpenTelemetry](https://opentelemetry.io/), you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID. ![Trace](https://docs.smith.langchain.com/assets/images/trace-8d4b4c1fde7abcddaad4fa8a56f2e27b.png)
## Projects[‚Äã](https://docs.smith.langchain.com/old/tracing/concepts#projects "Direct link to Projects")
A `Project` is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces. ![Project](https://docs.smith.langchain.com/assets/images/project-9fc0692079f84a1df9fdabb89add8652.png)
## Feedback[‚Äã](https://docs.smith.langchain.com/old/tracing/concepts#feedback "Direct link to Feedback")
`Feedback` allows you to score an individual run based on certain criteria. Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID. Feedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization.
Collecting feedback on runs can be done in three main ways:
  1. [In the UI](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback#annotating-traces-with-feedback) - Annotate runs directly in the UI
  2. [With the SDK](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback#capturing-feedback-programmatically) - Programmatically log feedback to LangSmith
  3. [With Online Evaluators](https://docs.smith.langchain.com/old/monitoring/faq/online_evaluation) - Automatically run evaluators on a sample of your production traces to generate feedback in realtime


![Feedback](https://docs.smith.langchain.com/assets/images/feedback-0f2b3437b61a53482968180e68181d82.png)
## Tags[‚Äã](https://docs.smith.langchain.com/old/tracing/concepts#tags "Direct link to Tags")
`Tags` are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. [Learn how to tag your traces in the LangSmith SDK.](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#adding-metadata-and-tags-to-traces) ![Tags](https://docs.smith.langchain.com/assets/images/tags-1829bc06f85f1fdab97d551d6ff5d040.png)
## Metadata[‚Äã](https://docs.smith.langchain.com/old/tracing/concepts#metadata "Direct link to Metadata")
`Metadata` is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. [Learn how to add metadata to your traces in the LangSmith SDK.](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#adding-metadata-and-tags-to-traces) ![Metadata](https://docs.smith.langchain.com/assets/images/metadata-1108d056aeaff94b835d18a355e124f5.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/tracing/concepts%3E).
[PreviousQuick Start](https://docs.smith.langchain.com/old/tracing/quick_start)[NextHow-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
  * [Runs](https://docs.smith.langchain.com/old/tracing/concepts#runs)
  * [Traces](https://docs.smith.langchain.com/old/tracing/concepts#traces)
  * [Projects](https://docs.smith.langchain.com/old/tracing/concepts#projects)
  * [Feedback](https://docs.smith.langchain.com/old/tracing/concepts#feedback)
  * [Tags](https://docs.smith.langchain.com/old/tracing/concepts#tags)
  * [Metadata](https://docs.smith.langchain.com/old/tracing/concepts#metadata)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/faq

[Skip to main content](https://docs.smith.langchain.com/old/tracing/faq#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/faq)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
      * [Core Functionality](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing)
      * [Customize Trace Attributes](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes)
      * [Querying Traces](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)
      * [Capturing Feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback)
      * [Querying Feedback](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback)
      * [LangChain-Specific Guides](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides)
      * [Token Counting for Custom LLMs](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * How-To Guides


# How-To Guides
In this section you will find guides for how to use LangSmith tracing functionality
**[Core Functionality](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing)**
  * [How to log traces](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#logging-traces)
  * [How to view traces](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#viewing-traces)
  * [How to select a sampling rate for traces](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#setting-a-sampling-rate-for-tracing)
  * [How to turn off tracing](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#turning-off-tracing)
  * [How to log distributed traces](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#distributed-tracing)
  * [How to get the URL of run](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#getting-the-url-of-a-logged-run)
  * [How to delete traces](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#deleting-traces-in-a-project)


**[Customize Trace Attributes](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes)**
  * [How to log to a specific project](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#logging-to-a-specific-project)
  * [How to change the destination project at runtime](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#changing-the-destination-project-at-runtime)
  * [How to add metadata and tags to traces](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#adding-metadata-and-tags-to-traces)
  * [How to customize the run name](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#customizing-the-run-name)
  * [How to update a run](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#updating-a-run)
  * [How to mask inputs and outputs](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#masking-inputs-and-outputs)


**[LangChain Specific Guides](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides)**
  * [How to group runs from multi-turn interactions](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#grouping-runs-from-multi-turn-interactions)
  * [How to get a run ID from a LangChain call](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#getting-a-run-id-from-a-langchain-call)
  * [How to trace without environment variables](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#tracing-without-environment-variables)
  * [How to ensure all traces are submitted before exiting](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#ensuring-all-traces-are-submitted-before-exiting)


**[Querying Traces](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)**
  * [How to use keyword arguments](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#using-keyword-arguments)
  * [How to filter runs](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#run-filtering)


**[Capturing Feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback)**
  * [How to log feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback#collecting-feedback-programmatically)
  * [How to annotate traces](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback#annotating-traces-with-feedback)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/tracing/faq%3E).
[PreviousConcepts](https://docs.smith.langchain.com/old/tracing/concepts)[NextCore Functionality](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting

[Skip to main content](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
      * [Core Functionality](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing)
      * [Customize Trace Attributes](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes)
      * [Querying Traces](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)
      * [Capturing Feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback)
      * [Querying Feedback](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback)
      * [LangChain-Specific Guides](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides)
      * [Token Counting for Custom LLMs](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
  * Token Counting for Custom LLMs


On this page
# Custom LLM Token Counting
This guide shows how to get your custom functions to have their token count tracked by LangSmith. The key is to coerce your inputs and outputs to conform with a minimal version OpenAI's API format. We will review adding support for both chat models (llms that expect a list of chat messages as inputs and return with a chat message) and completion models (models that expect a string as input and return a string).
note
This guide assumes you are using the `traceable` decorator, though the same principles can be applied to other tracing methods.
## Chat Models (messages in, message out)[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting#chat-models-messages-in-message-out "Direct link to Chat Models \(messages in, message out\)")
For chat models, your inputs must contain a list of messages as input. The output must return an object that, when serialized, contains the key `choices` with a list of dicts. Each dict must contain the key `message` with a dict value. The message dict must contain the key `content` with a string value and the key `role`.
```
from langsmith import traceable@traceable(run_type="llm")defmy_chat_model(messages:list):return{"choices":[{"message":{"content":"hello, "+ messages[1]["content"],"role":"assistant",}}]}# Usagemy_chat_model([{"role":"system","content":"You are a bot."},{"role":"user","content":"SolidGoldMagikarp"},])
```

You can configure the model and other arguments as well. The `model` key is used to match cost information:
```
@traceable(run_type="llm")defmy_chat_model_with_model(messages:list, model:str):return{"choices":[{"message":{"content":"hello, "+ messages[1]["content"],"role":"assistant",}}]}my_chat_model_with_model([{"role":"system","content":"You are a bot."},{"role":"user","content":"SolidGoldMagikarp"},],  model="gpt-3.5-turbo",)
```

### Streaming[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting#streaming "Direct link to Streaming")
For streaming, you can "reduce" the outputs into the same format as the non-streaming version:
```
def_reduce_chunks(chunks:list):  all_text ="".join([chunk["choices"][0]["message"]["content"]for chunk in chunks])return{"choices":[{"message":{"content": all_text,"role":"assistant"}}]}@traceable(run_type="llm", reduce_fn=_reduce_chunks)defmy_streaming_chat_model(messages:list, model:str):for chunk in["hello, "+ messages[1]["content"]]:yield{"choices":[{"message":{"content": chunk,"role":"assistant",}}]}list(  my_streaming_chat_model([{"role":"system","content":"You are a bot."},{"role":"user","content":"SolidGoldMagikarp but streaming"},],    model="gpt-3.5-turbo",))
```

Note: Tool calling and other messages are also supported, following the OpenAI format.
### Manually Providing Token Counts[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting#manually-providing-token-counts "Direct link to Manually Providing Token Counts")
By default, LangSmith uses tiktoken to count tokens, using our best guess at the model's tokenizer based on the `model` parameter you provide. To manually provide token counts, you can add a `usage` key to the function's response, containing a dictionary with the keys `prompt_tokens`, `completion_tokens`, and `total_tokens`. You must also add `batch_size=1` to the extra dictionary in the `run_tree.extra` object.
```
from langsmith.run_helpers import get_current_run_tree@traceable(run_type="llm")defmy_chat_model_with_usage(messages:list, model:str="gpt-3.5-turbo"):  run_tree = get_current_run_tree()  run_tree.extra["batch_size"]=1return{"choices":[{"message":{"content":"hello, "+ messages[1]["content"],"role":"assistant",}}],"usage":{"prompt_tokens":9_999,"completion_tokens":32,"total_tokens":10_031,},}my_chat_model_with_usage(  messages=[{"role":"system","content":"You are a bot."},{"role":"user","content":"SolidGoldMagikarp but with usage"},],)
```

This is also supported via streaming:
```
def_reduce_chunks_with_usage(chunks:list):  all_text ="".join([      chunk["choices"][0]["message"]["content"]for chunk in chunksif"choices"in chunk])  usages =[chunk["usage"]for chunk in chunks if"usage"in chunk]  usage ={}if usages:    total_tokens =sum([usage["total_tokens"]for usage in usages])    prompt_tokens =sum([usage["prompt_tokens"]for usage in usages])    completion_tokens =sum([usage["completion_tokens"]for usage in usages])    usage ={"prompt_tokens": prompt_tokens,"completion_tokens": completion_tokens,"total_tokens": total_tokens,}return{"choices":[{"message":{"content": all_text,"role":"assistant"}}],"usage": usage,}@traceable(run_type="llm", reduce_fn=_reduce_chunks_with_usage)defmy_streaming_chat_model_with_usage(messages:list, model:str="gpt-3.5-turbo"):  run_tree = get_current_run_tree()  run_tree.extra["batch_size"]=1for chunk in["hello, "+ messages[1]["content"]]:yield{"choices":[{"message":{"content": chunk,"role":"assistant",}}],}yield{"usage":{"prompt_tokens":9_999,"completion_tokens":32,"total_tokens":10_031,}}list(  my_streaming_chat_model_with_usage(    messages=[{"role":"system","content":"You are a bot."},{"role":"user","content":"SolidGoldMagikarp but with usage"},]))
```

## Completion Models (string in, string out)[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting#completion-models-string-in-string-out "Direct link to Completion Models \(string in, string out\)")
For completion models, your inputs must contain a key `prompt` with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key `choices` with a list of dicts. Each dict must contain the key `text` with a string value.
```
@traceable(run_type="llm")defmy_llm(prompt:str):return{"choices":[{"text":"hello, "+ prompt}]}my_llm("SolidGoldMagikarp")
```

If you want to add additional "invocation params" such as the model name, you can just add those keys. The `model` key can be used to let the cost estimator know which model is being used.
```
@traceable(run_type="llm")defmy_llm_with_model(prompt:str, model:str):return{"choices":[{"text":"hello, "+ prompt}]}my_llm_with_model("SolidGoldMagikarp", model="gpt-3.5-turbo-instruct")
```

For streaming, you can "reduce" the outputs into the same format as the non-streaming version:
```
def_reduce_chunks(chunks:list):  all_text ="".join([chunk["choices"][0]["text"]for chunk in chunks])return{"choices":[{"text": all_text}]}@traceable(run_type="llm", reduce_fn=_reduce_chunks)defmy_streaming_llm(prompt:str, model:str):for chunk in["hello, "+ prompt]:yield{"choices":[{"text": chunk}]}list(my_streaming_llm("SolidGoldMagikarp but streaming", model="gpt-3.5-turbo-instruct"))
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/tracing/faq/custom_llm_token_counting%3E).
[PreviousLangChain-Specific Guides](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides)[NextIntegrations](https://docs.smith.langchain.com/old/tracing/integrations)
  * [Chat Models (messages in, message out)](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting#chat-models-messages-in-message-out)
    * [Streaming](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting#streaming)
    * [Manually Providing Token Counts](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting#manually-providing-token-counts)
  * [Completion Models (string in, string out)](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting#completion-models-string-in-string-out)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes

[Skip to main content](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
      * [Core Functionality](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing)
      * [Customize Trace Attributes](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes)
      * [Querying Traces](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)
      * [Capturing Feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback)
      * [Querying Feedback](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback)
      * [LangChain-Specific Guides](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides)
      * [Token Counting for Custom LLMs](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
  * Customize Trace Attributes


On this page
# How to customize attributes of traces
Oftentimes, you will want to customize various attributes of the traces you log to LangSmith.
### Logging to a specific project[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#logging-to-a-specific-project "Direct link to Logging to a specific project")
As mentioned in the Concepts section, LangSmith uses the concept of a `Project` to group traces. If left unspecified, the tracer project is set to `default`. You can set the `LANGCHAIN_PROJECT` environment variable to configure a custom project name for an entire application run. This should be done before executing your program.
```
export LANGCHAIN_PROJECT="My Project"
```

### Changing the destination project at runtime[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#changing-the-destination-project-at-runtime "Direct link to Changing the destination project at runtime")
When global environment variables are too broad, you can also set the project name at program runtime. This is useful when you want to log traces to different projects within the same application.
  * Python
  * TypeScript
  * LangChain (Python)
  * LangChain (JS)


```
import openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages =[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Hello!"}]# Use the @traceable decorator with the 'project_name' parameter to log traces to LangSmith# Ensure that the LANGCHAIN_TRACING_V2 environment variables are set for @traceable to work@traceable( run_type="llm", name="OpenAI Call Decorator",)defcall_openai( messages:list[dict], model:str="gpt-3.5-turbo")->str:return client.chat.completions.create(   model=model,   messages=messages,).choices[0].message.content# You can specify the Project via the project_name parametercall_openai( messages, langsmith_extra={"project_name":"My Project"},)# The wrapped OpenAI client accepts all the same langsmith_extra parameters# as @traceable decorated functions, and logs traces to langsmith automaticallyfrom langsmith import wrapperswrapped_client = wrappers.wrap_openai(client)wrapped_client.chat.completions.create( model="gpt-3.5-turbo", messages=messages, langsmith_extra={"project_name":"My Project"},)# Alternatively, create a RunTree object# You can set the project name using the project_name parameterrt = RunTree( run_type="llm", name="OpenAI Call RunTree", inputs={"messages": messages}, project_name="My Project")chat_completion = client.chat.completions.create( model="gpt-3.5-turbo", messages=messages,)# End and submit the runawait rt.end(outputs=chat_completion)rt.post()
```

```
import OpenAI from"openai";import{ RunTree }from"langsmith";const client =newOpenAI()const messages =[{role:"system", content:"You are a helpful assistant."},{role:"user", content:"Hello!"}]// Create a RunTree object// You can set the project name using the project_name parameterconst rt =newRunTree({ run_type:"llm", name:"OpenAI Call RunTree", inputs:{ messages }, project_name:"My Project"})await rt.postRun();const chatCompletion =await client.chat.completions.create({ model:"gpt-3.5-turbo", messages: messages,});// End and submit the runawait rt.end(chatCompletion);await rt.patchRun();
```

```
# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name="My Project")chain.invoke({"query":"How many people live in canada as of 2023?"}, config={"callbacks":[tracer]})# LangChain python also supports a context manager for tracing a specific block of code.# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name="My Project"): chain.invoke({"query":"How many people live in canada as of 2023?"})
```

```
// You can set the project name for a specific tracer instance:import{ LangChainTracer }from"langchain/callbacks";const tracer =newLangChainTracer({ projectName:"My Project"});await chain.invoke({ query:"How many people live in canada as of 2023?"},{ callbacks:[tracer]});
```

### Adding metadata and tags to traces[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#adding-metadata-and-tags-to-traces "Direct link to Adding metadata and tags to traces")
LangSmith supports sending arbitrary metadata and tags along with traces. This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it. For more information on metadata and tags, see the [Concepts](https://docs.smith.langchain.com/old/tracing/concepts) page. For information on how to query traces and runs by metadata and tags, see the [Querying Traces](https://docs.smith.langchain.com/old/tracing/faq/querying_traces) page.
  * Python
  * TypeScript
  * LangChain (Python)
  * LangChain (JS)


```
import openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages =[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Hello!"}]# Use the @traceable decorator with tags and metadata# Ensure that the LANGCHAIN_TRACING_V2 environment variables are set for @traceable to work@traceable( run_type="llm", name="OpenAI Call Decorator", tags=["my-tag"], metadata={"my-key":"my-value"})defcall_openai( messages:list[dict], model:str="gpt-3.5-turbo")->str:return client.chat.completions.create(   model=model,   messages=messages,).choices[0].message.contentcall_openai( messages,# You can provide tags and metadata at invocation time # via the langsmith_extra parameter langsmith_extra={"tags":["my-other-tag"],"metadata":{"my-other-key":"my-value"}})# Alternatively, you can create a RunTree object with tags and metadatart = RunTree( run_type="llm", name="OpenAI Call RunTree", inputs={"messages": messages}, tags=["my-tag"], extra={"metadata":{"my-key":"my-value"}})chat_completion = client.chat.completions.create( model="gpt-3.5-turbo", messages=messages,)# End and submit the runawait rt.end(outputs=chat_completion)rt.post()
```

```
import OpenAI from"openai";import{ RunTree }from"langsmith";const client =newOpenAI();const messages =[{role:"system", content:"You are a helpful assistant."},{role:"user", content:"Hello!"}]// Create a RunTree objectconst rt =newRunTree({ run_type:"llm", name:"OpenAI Call RunTree", inputs:{ messages }, tags:["my-tag"], extra:{metadata:{"my-key":"my-value"}}})await rt.postRun();const chatCompletion =await client.chat.completions.create({ model:"gpt-3.5-turbo", messages: messages,});// End and submit the runawait rt.end(chatCompletion)await rt.patchRun()
```

```
from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([("system","You are a helpful AI."),("user","{input}")])chat_model = ChatOpenAI()output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain =(prompt | chat_model | output_parser).with_config({"tags":["top-level-tag"],"metadata":{"top-level-key":"top-level-value"}})# Tags and metadata can also be passed at runtimechain.invoke({"input":"What is the meaning of life?"},{"tags":["shared-tags"],"metadata":{"shared-key":"shared-value"}})
```

```
import{ ChatOpenAI }from"@langchain/openai";import{ ChatPromptTemplate }from"@langchain/core/prompts";import{ StringOutputParser }from"@langchain/core/output_parsers";const prompt = ChatPromptTemplate.fromMessages([["system","You are a helpful AI."],["user","{input}"]])const model =newChatOpenAI({ modelName:"gpt-3.5-turbo"});const outputParser =newStringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain =(prompt.pipe(model).pipe(outputParser)).withConfig({"tags":["top-level-tag"],"metadata":{"top-level-key":"top-level-value"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input:"What is the meaning of life?"},{tags:["shared-tags"], metadata:{"shared-key":"shared-value"}})
```

### Customizing the run name[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#customizing-the-run-name "Direct link to Customizing the run name")
When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
  * Python
  * TypeScript
  * LangChain (Python)
  * LangChain (JS)


```
import openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages =[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Hello!"}]# Use the @traceable decorator with the name parameter# Ensure that the LANGCHAIN_TRACING_V2 environment variables are set for @traceable to work@traceable( run_type="llm", name="OpenAI Call Decorator",)defcall_openai( messages:list[dict], model:str="gpt-3.5-turbo")->str:return client.chat.completions.create(   model=model,   messages=messages,).choices[0].message.contentcall_openai(messages)# Alternatively, use the name parameter of the RunTree objectrt = RunTree( run_type="llm", name="OpenAI Call RunTree", inputs={"messages": messages},)chat_completion = client.chat.completions.create( model="gpt-3.5-turbo", messages=messages,)# End and submit the runawait rt.end(outputs=chat_completion)rt.post()
```

```
import OpenAI from"openai";import{ RunTree }from"langsmith";const client =newOpenAI();const messages =[{role:"system", content:"You are a helpful assistant."},{role:"user", content:"Hello!"}]// Create a RunTree objectconst rt =newRunTree({ run_type:"llm", name:"OpenAI Call RunTree", inputs:{ messages },})await rt.postRun()const chatCompletion =await client.chat.completions.create({ model:"gpt-3.5-turbo", messages: messages,});// End and submit the runawait rt.end(chatCompletion)await rt.patchRun()
```

```
# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').# (Note: this is not currently supported directly on LLM objects.)...configured_chain = chain.with_config({"run_name":"MyCustomChain"})configured_chain.invoke({"query":"What is the meaning of life?"})
```

```
// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').// (Note: this is not currently supported directly on LLM objects.)...const configuredChain = chain.withConfig({ runName:"MyCustomChain"});await configuredChain.invoke({query:"What is the meaning of life?"});
```

For more examples of with LangChain, check out the [recipe on customizing run names](https://github.com/langchain-ai/langsmith-cookbook/blob/main/tracing-examples/runnable-naming/run-naming.ipynb).
### Updating a run[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#updating-a-run "Direct link to Updating a run")
The following fields can be updated when patching a run with the SDK or API.
  * `end_time`: `datetime.datetime`
  * `error`: `str | None`
  * `outputs`: `dict | None`
  * `events`: `list[dict] | None`


### Masking inputs and outputs[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#masking-inputs-and-outputs "Direct link to Masking inputs and outputs")
In some situations, you may need to hide the inputs and outputs of your traces for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend, so our servers never see the original values.
If you want to completely hide the inputs and outputs of your traces, you can set the following environment variables when running your application:
```
LANGCHAIN_HIDE_INPUTS=trueLANGCHAIN_HIDE_OUTPUTS=true
```

This works for both the LangSmith SDK and LangChain.
You can also customize and override this behavior for a given Client instance. This can be done by setting the `hide_inputs` and `hide_outputs` parameters on the `Client` object.
  * Python
  * TypeScript
  * LangChain (Python)
  * LangChain (JS)


```
import openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(openai.Client())langsmith_client = Client( hide_inputs=lambda inputs:{}, hide_outputs=lambda outputs:{})# The linked run will have its metadata present, but the inputs will be hiddenopenai_client.chat.completions.create( model="gpt-3.5-turbo", messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Hello!"},], langsmith_extra={"client": langsmith_client},)# The linked run will not have hidden inputs and outputsopenai_client.chat.completions.create( model="gpt-3.5-turbo", messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Hello!"},],)
```

```
import{ Client }from"langsmith";import{ wrapOpenAI }from"langsmith/wrappers";import OpenAI from"openai";const langsmithClient =newClient({hideInputs:(inputs)=>({}),hideOutputs:(outputs)=>({}),});(async()=>{// The linked run will have its metadata present, but the inputs will be hiddenconst filteredOAIClient =wrapOpenAI(newOpenAI(),{ client: langsmithClient,});await filteredOAIClient.chat.completions.create({ model:"gpt-3.5-turbo", messages:[{ role:"system", content:"You are a helpful assistant."},{ role:"user", content:"Hello!"},],});const openaiClient =wrapOpenAI(newOpenAI());// The linked run will not have hidden inputs and outputsawait openaiClient.chat.completions.create({ model:"gpt-3.5-turbo", messages:[{ role:"system", content:"You are a helpful assistant."},{ role:"user", content:"Hello!"},],});})();
```

```
from langchain_core.tracers.context import tracing_v2_enabledfrom langchain_openai import ChatOpenAIfrom langsmith import Clientdeffilter_inputs(inputs:dict):# You can define custom filtering herereturn{}deffilter_outputs(outputs:dict):# You can define custom filtering herereturn{}llm = ChatOpenAI()# You can configure tracing using the context manager below# or by directly creating a LangChainTracer objectwith tracing_v2_enabled("test-filtering", client=Client(hide_inputs=filter_inputs, hide_outputs=filter_outputs),)as cb: llm.invoke("Say foo")# The linked run will have its metadata present, but the inputs will be hiddenprint(cb.get_run_url())with tracing_v2_enabled("test-filtering", client=Client())as cb: llm.invoke("Say bar")# The linked run will not have hidden inputs and outputsprint(cb.get_run_url())
```

```
import{ ChatOpenAI }from"@langchain/openai";import{ LangChainTracer }from"@langchain/core/tracers/tracer_langchain";import{ Client}from"langsmith";(async()=>{const llm =newChatOpenAI();const client =newClient({hideInputs:(inputs)=>({}),hideOutputs:(outputs)=>({}),})const tracer =newLangChainTracer({ client });// The traced run will have its metadata present, but the inputs will be hiddenawait llm.invoke("Say foo",{ callbacks:[tracer]});const unfilteredTracer =newLangChainTracer();// The traced run will not have hidden inputs and outputsawait llm.invoke("Say bar",{ callbacks:[unfilteredTracer]});})();
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousCore Functionality](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing)[NextQuerying Traces](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)
  * [Logging to a specific project](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#logging-to-a-specific-project)
  * [Changing the destination project at runtime](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#changing-the-destination-project-at-runtime)
  * [Adding metadata and tags to traces](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#adding-metadata-and-tags-to-traces)
  * [Customizing the run name](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#customizing-the-run-name)
  * [Updating a run](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#updating-a-run)
  * [Masking inputs and outputs](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#masking-inputs-and-outputs)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides

[Skip to main content](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
      * [Core Functionality](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing)
      * [Customize Trace Attributes](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes)
      * [Querying Traces](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)
      * [Capturing Feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback)
      * [Querying Feedback](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback)
      * [LangChain-Specific Guides](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides)
      * [Token Counting for Custom LLMs](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
  * LangChain-Specific Guides


On this page
# LangChain-specific guides
This sections contains guides that are specific to users using LangChain (both Python and JavaScript).
### Grouping runs from multi-turn interactions[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#grouping-runs-from-multi-turn-interactions "Direct link to Grouping runs from multi-turn interactions")
When using LangChain, you may want to group runs from multi-turn interactions together.
With chatbots, copilots, and other common LLM design patterns, users frequently interact with your model over multiple interactions. Each invocation of your model is logged as a separate trace, but you can group these traces together using metadata (see [how to add metadata to a run](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#how-do-i-add-metadata-or-tags-to-a-run) above for more information). Below is a minimal example with LangChain, but **the same idea applies when using the LangSmith SDK or API.**
  * LangChain (Python)
  * LangChain (JS)


```
from langchain_core.messages import AIMessage, HumanMessagefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_openai import ChatOpenAIchain =( ChatPromptTemplate.from_messages([("system","You are a helpful AI."),     MessagesPlaceholder(variable_name="chat_history"),("user","{message}"),])| ChatOpenAI(model="gpt-3.5-turbo")| StrOutputParser())conversation_id ="101e8e66-9c68-4858-a1b4-3b0e3c51a933"chat_history =[]message = HumanMessage(content="Hi there")response =""for chunk in chain.stream({"message": message,"chat_history": chat_history,}, config={"metadata":{"conversation_id": conversation_id}},):print(chunk, end="") response += chunkprint()chat_history.extend([   message,   AIMessage(content=response),])# ... Next message comes innext_message = HumanMessage(content="I don't need much assistance, actually.")for chunk in chain.stream({"message": next_message,"chat_history": chat_history,}, config={"metadata":{"conversation_id": conversation_id}},):print(chunk, end="") response += chunk
```

```
import{ ChatOpenAI }from"@langchain/openai";import{ ChatPromptTemplate }from"@langchain/core/prompts";import{ StringOutputParser }from"@langchain/core/output_parsers";const prompt = ChatPromptTemplate.fromMessages([["system","You are a helpful AI."],["user","{message}"],]);const chain = prompt.pipe(newChatOpenAI({ model:"gpt-3.5-turbo"})).pipe(newStringOutputParser());const conversationId ="101e8e66-9c68-4858-a1b4-3b0e3c51a933";const chatHistory =[];const message ={ content:"Hi there"};let response ="";forawait(const chunk ofawait chain.stream({  message,},{  metadata:{ conversation_id: conversationId },})){ process.stdout.write(chunk); response += chunk;}console.log();chatHistory.push(message,{ content: response });// ... Next message comes inconst nextMessage ={ content:"I don't need much assistance, actually."};forawait(const chunk ofawait chain.stream({  message: nextMessage,},{  metadata:{ conversation_id: conversationId },})){ process.stdout.write(chunk); response += chunk;}
```

To view all the traces from that conversation in LangSmith, you can query the project using a metadata filter:
```
has(metadata, '{"conversation_id":"101e8e66-9c68-4858-a1b4-3b0e3c51a933"}')
```

This will return all traces with the specified conversation ID.
### Getting a run ID from a LangChain call[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#getting-a-run-id-from-a-langchain-call "Direct link to Getting a run ID from a LangChain call")
In Typescript, the run ID is returned in the call response under the `__run` key. In python, we recommend using the run collector callback. Below is an example:
  * LangChain (Python)
  * LangChain (JS)


```
from langchain import chat_models, prompts, callbackschain =(  prompts.ChatPromptTemplate.from_template("Say hi to {name}")| chat_models.ChatAnthropic())with callbacks.collect_runs()as cb: result = chain.invoke({"name":"Clara"}) run_id = cb.traced_runs[0].idprint(run_id)
```

For older versions of LangChain (<0.0.276), you can instruct the chain to return the run ID by specifying the `include_run_info=True` parameter to the call function:
```
from langchain.chat_models import ChatAnthropicfrom langchain.chains import LLMChainchain = LLMChain.from_string(ChatAnthropic(),"Say hi to {name}")response = chain("Clara", include_run_info=True)run_id = response["__run"].run_idprint(run_id)
```

For python LLMs/chat models, the run information is returned automatically when calling the `generate()` method. Example:
```
from langchain.chat_models import ChatAnthropicfrom langchain_core.prompts import ChatPromptTemplate chat_model = ChatAnthropic() prompt = ChatPromptTemplate.from_messages([("system","You are a cat"),("human","Hi"),]) res = chat_model.generate(messages=[prompt.format_messages()]) res.run[0].run_id
```

or for LLMs
```
from langchain.llms import OpenAIopenai = OpenAI()res = openai.generate(["You are a good bot"])print(res.run[0].run_id)
```

For newer versions of Langchain (>=0.0.139), you can use the `RunCollectorCallbackHandler` for any chain or runnable.
```
import{ ChatAnthropic }from"langchain/chat_models/anthropic";import{ PromptTemplate }from"langchain/prompts";import{ RunCollectorCallbackHandler }from"langchain/callbacks";const runCollector =newRunCollectorCallbackHandler();const prompt = PromptTemplate.fromTemplate("Say hi to {name}");const chain = prompt.pipe(newChatAnthropic());const pred =await chain.invoke({ name:"Clara"},{  callbacks:[runCollector],});const runId = runCollector.tracedRuns[0].id;console.log(runId);
```

If youre on an older version of LangChain, you can still retrieve the run ID directly from chain calls.
```
import{ ChatAnthropic }from"langchain/chat_models/anthropic";import{ LLMChain }from"langchain/chains";import{ PromptTemplate }from"langchain/prompts";const prompt = PromptTemplate.fromTemplate("Say hi to {name}");const chain =newLLMChain({ llm:newChatAnthropic(), prompt: prompt,});const response =await chain.invoke({ name:"Clara"});console.log(response.__run);
```

If using the API or SDK, you can send the run ID as a parameter to `RunTree` or the `traceable` decorator.
### Tracing without environment variables[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#tracing-without-environment-variables "Direct link to Tracing without environment variables")
Some situations don't permit the use of environment variables or don't expose `process.env`. This is mostly pertinent when running LangChain apps in certain JavaScript runtime environments. To add tracing in these situations, you can manually create the `LangChainTracer` callback and pass it to the chain, LLM, or other LangChain component, either when initializing or in the call itself. This is the same tactic used for [changing the tracer project](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#how-do-i-change-the-tracer-project) within a program.
Example:
  * LangChain (Python)
  * LangChain (JS)


```
from langchain.callbacks import LangChainTracerfrom langchain_openai import ChatOpenAIfrom langsmith import Clientcallbacks =[LangChainTracer( project_name="YOUR_PROJECT_NAME_HERE", client=Client(  api_url="https://api.smith.langchain.com",  api_key="YOUR_API_KEY_HERE"))]llm = ChatOpenAI()llm.invoke("Hello, world!", config={"callbacks": callbacks})
```

```
import{ Client }from"langsmith";import{ LangChainTracer }from"langchain/callbacks";import{ ChatOpenAI }from"langchain/chat_models/openai";const callbacks =[newLangChainTracer({  projectName:"YOUR_PROJECT_NAME_HERE",  client:newClient({   apiUrl:"https://api.smith.langchain.com",   apiKey:"YOUR_API_KEY_HERE",}),}),];const llm =newChatOpenAI({});await llm.invoke("Hello, world!",{ callbacks });
```

This tactic is also useful for when you have multiple chains running in a shared environment but want to log their run traces to different projects.
### Ensuring all traces are submitted before exiting[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#ensuring-all-traces-are-submitted-before-exiting "Direct link to Ensuring all traces are submitted before exiting")
In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.
In LangChain JS, prior to `@langchain/core` version `0.3.0`, the default was to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. Versions `>=0.3.0` have the same default as Python. You can explicitly make callbacks synchronous by setting the `LANGCHAIN_CALLBACKS_BACKGROUND` environment variable to `"false"` or asynchronous by setting it to `"true"`. You can also check out [this guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more options for awaiting backgrounded callbacks in serverless environments.
For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application. Below is an example:
  * LangChain (Python)
  * LangChain (JS)


```
from langchain_openai import ChatOpenAIfrom langchain.callbacks.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try: llm.invoke("Hello, World!")finally: wait_for_all_tracers()
```

```
import{ ChatOpenAI }from"langchain/chat_models/openai";import{ awaitAllCallbacks }from"langchain/callbacks";try{const llm =newChatOpenAI();const response =await llm.invoke("Hello, World!");}catch(e){// handle error}finally{awaitawaitAllCallbacks();}
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousQuerying Feedback](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback)[NextToken Counting for Custom LLMs](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting)
  * [Grouping runs from multi-turn interactions](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#grouping-runs-from-multi-turn-interactions)
  * [Getting a run ID from a LangChain call](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#getting-a-run-id-from-a-langchain-call)
  * [Tracing without environment variables](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#tracing-without-environment-variables)
  * [Ensuring all traces are submitted before exiting](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#ensuring-all-traces-are-submitted-before-exiting)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing

[Skip to main content](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
      * [Core Functionality](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing)
      * [Customize Trace Attributes](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes)
      * [Querying Traces](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)
      * [Capturing Feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback)
      * [Querying Feedback](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback)
      * [LangChain-Specific Guides](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides)
      * [Token Counting for Custom LLMs](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
  * Core Functionality


On this page
# How to log and view traces to LangSmith
LangSmith makes it easy to log and view traces from your LLM application, regardless of which language or framework you use.
## Annotating your code for tracing[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#annotating-your-code-for-tracing "Direct link to Annotating your code for tracing")
### Using `@traceable` / `traceable`[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#using-traceable--traceable "Direct link to using-traceable--traceable")
LangSmith makes it easy to log traces with minimal changes to your existing code with the `@traceable` decorator in Python and `traceable` function in TypeScript.
note
The `LANGCHAIN_TRACING_V2` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `@traceable` or `traceable`. This allows you to toggle tracing on and off without changing your code.
Additionally, you will need to set the `LANGCHAIN_API_KEY` environment variable to your API key (see [Setup](https://docs.smith.langchain.com/) for more information).
By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#logging-to-a-specific-project).
  * Python
  * TypeScript


The `@traceable` decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with `@traceable`.
```
from langsmith import traceablefrom openai import Clientopenai = Client()@traceabledefformat_prompt(subject):return[{"role":"system","content":"You are a helpful assistant.",},{"role":"user","content":f"What's a good name for a store that sells {subject}?"}]@traceable(run_type="llm")definvoke_llm(messages):return openai.chat.completions.create(   messages=prompt, model="gpt-3.5-turbo", temperature=0)@traceabledefparse_output(response):return response.choices[0].message.content@traceabledefrun_pipeline(): messages = format_prompt("colorful socks") response = invoke_llm(messages)return parse_output(response)run_pipeline()
```

The `traceable` function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with `traceable`.
Note that when wrapping a sync function with `traceable`, (e.g. `formatPrompt` in the example below), you should use the `await` keyword when calling it to ensure the trace is logged correctly.
```
import{ traceable }from"langsmith/traceable";import OpenAI from"openai";const openai =newOpenAI();const formatPrompt =traceable((subject:string)=>{return[{     role:"system"asconst,     content:"You are a helpful assistant.",},{     role:"user"asconst,     content:`What's a good name for a store that sells ${subject}?`}];},{ name:"formatPrompt"});const invokeLLM =traceable(async(messages:{ role:string; content:string}[])=>{return openai.chat.completions.create({   model:"gpt-3.5-turbo",   messages: messages,   temperature:0});},{ run_type:"llm", name:"invokeLLM"});const parseOutput =traceable((response:any)=>{return response.choices[0].message.content;},{ name:"parseOutput"});const runPipeline =traceable(async()=>{const messages =awaitformatPrompt("colorful socks");const response =awaitinvokeLLM(messages);returnparseOutput(response);},{ name:"runPipeline"});awaitrunPipeline()
```

### Wrapping the OpenAI client[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#wrapping-the-openai-client "Direct link to Wrapping the OpenAI client")
The `wrap_openai`/`wrapOpenAI` methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required! The wrapper works seamlessly with the `@traceable` decorator or `traceable` function and you can use both in the same application.
note
The `LANGCHAIN_TRACING_V2` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `wrap_openai` or `wrapOpenAI`. This allows you to toggle tracing on and off without changing your code.
Additionally, you will need to set the `LANGCHAIN_API_KEY` environment variable to your API key (see [Setup](https://docs.smith.langchain.com/) for more information).
By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#logging-to-a-specific-project).
  * Python
  * TypeScript


```
import openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type="tool", name="Retrieve Context")defmy_tool(question:str)->str:return"During this morning's meeting, we solved all world conflict."@traceable(name="Chat Pipeline")defchat_pipeline(question:str): context = my_tool(question) messages =[{"role":"system","content":"You are a helpful assistant. Please respond to the user's request only based on the given context."},{"role":"user","content": f"Question:{question}Context:{context}"}] chat_completion = client.chat.completions.create(   model="gpt-3.5-turbo", messages=messages)return chat_completion.choices[0].message.contentchat_pipeline("Can you summarize this morning's meetings?")
```

```
import OpenAI from"openai";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers";const client =wrapOpenAI(newOpenAI());const myTool =traceable(async(question:string)=>{return"During this morning's meeting, we solved all world conflict.";},{ name:"Retrieve Context", run_type:"tool"});const chatPipeline =traceable(async(question:string)=>{const context =awaitmyTool(question);const messages =[{     role:"system",     content:"You are a helpful assistant. Please respond to the user's request only based on the given context.",},{ role:"user", content:`Question: ${question} Context: ${context}`},];const chatCompletion =await client.chat.completions.create({   model:"gpt-3.5-turbo",   messages: messages,});return chatCompletion.choices[0].message.content;},{ name:"Chat Pipeline"});awaitchatPipeline("Can you summarize this morning's meetings?");
```

### Using the `RunTree` API[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#using-the-runtree-api "Direct link to using-the-runtree-api")
Another, more explicit way to log traces to LangSmith is via the `RunTree` API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGCHAIN_API_KEY`, but `LANGCHAIN_TRACING_V2` is not necessary for this method.
  * Python
  * TypeScript


```
import openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion ="Can you summarize this morning's meetings?"# Create a top-level runpipeline = RunTree( name="Chat Pipeline", run_type="chain", inputs={"question": question})pipeline.post()# This can be retrieved in a retrieval stepcontext ="During this morning's meeting, we solved all world conflict."messages =[{"role":"system","content":"You are a helpful assistant. Please respond to the user's request only based on the given context."},{"role":"user","content":f"Question: {question}\nContext: {context}"}]# Create a child runchild_llm_run = pipeline.create_child( name="OpenAI Call", run_type="llm", inputs={"messages": messages},)child_llm_run.post()# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create( model="gpt-3.5-turbo", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.patch()pipeline.end(outputs={"answer": chat_completion.choices[0].message.content})pipeline.patch()
```

```
import OpenAI from"openai";import{ RunTree }from"langsmith";// This can be a user input to your appconst question ="Can you summarize this morning's meetings?";const pipeline =newRunTree({ name:"Chat Pipeline", run_type:"chain", inputs:{ question }});// This can be retrieved in a retrieval stepconst context ="During this morning's meeting, we solved all world conflict.";const messages =[{ role:"system", content:"You are a helpful assistant. Please respond to the user's request only based on the given context."},{ role:"user", content:`Question: ${question}Context: ${context}`}];// Create a child runconst childRun =await pipeline.createChild({ name:"OpenAI Call", run_type:"llm", inputs:{ messages },});await childRun.postRun();// Generate a completionconst client =newOpenAI();const chatCompletion =await client.chat.completions.create({ model:"gpt-3.5-turbo", messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.patchRun();await pipeline.end({ outputs:{ answer: chatCompletion.choices[0].message.content }});await pipeline.patchRun();
```

## Viewing Traces[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#viewing-traces "Direct link to Viewing Traces")
To view traces, navigate to the project details page for your project (by default, all traces are logged to the "default" project). Then, click on a row in the traces table to expand the trace. This will bring up a run tree, which shows the parent-child relationships between runs, as well as the inputs and outputs of each run. You can also view feedback, metadata, and other information in the tabs. ![Trace](https://docs.smith.langchain.com/assets/images/trace-9510284b5b15ba55fc1cca6af2404657.png)
## Setting a sampling rate for tracing[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#setting-a-sampling-rate-for-tracing "Direct link to Setting a sampling rate for tracing")
To downsample the number of traces logged to LangSmith, set the `LANGCHAIN_TRACING_SAMPLING_RATE` environment variable to any float between 0 (no traces) and 1 (all traces). This requires a python SDK version >= 0.0.84, and a JS SDK version >= 0.0.64. For instance, setting the following environment variable will filter out 25% of traces:
```
export LANGCHAIN_TRACING_SAMPLING_RATE=0.75
```

This works for the `traceable` decorator and `RunTree` objects.
## Distributed Tracing[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#distributed-tracing "Direct link to Distributed Tracing")
LangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (`langsmith-trace` and optional `baggage` for metadata/tags).
Example client-server setup:
  * Trace starts on client
  * Continues on server


```
# client.pyfrom langsmith.run_helpers import get_current_run_tree, traceableimport httpx@traceableasyncdefmy_client_function():  headers ={}asyncwith httpx.AsyncClient(base_url="...")as client:if run_tree := get_current_run_tree():# add langsmith-id to headers      headers.update(run_tree.to_headers())returnawait client.post("/my-route", headers=headers)
```

Then the server (or other service) can continue the trace by passing the headers in as `langsmith_extra`:
```
# server.pyfrom langsmith import traceablefrom langsmith.run_helpers import tracing_contextfrom fastapi import FastAPI, Request@traceableasyncdefmy_application():...app = FastAPI()# Or Flask, Django, or any other framework@app.post("/my-route")asyncdeffake_route(request: Request):# request.headers: {"langsmith-trace": "..."}# as well as optional metadata/tags in `baggage`with tracing_context(parent=request.headers):returnawait my_application()
```

The example above uses the `tracing_context` context manager. You can also directly specify the parent run context in the `trace` context manager or `traceable` decorator.
```
from langsmith.run_helpers import traceable, trace# ... same as above@app.post("/my-route")asyncdeffake_route(request: Request):# request.headers: {"langsmith-trace": "..."}# as well as optional metadata/tags in `baggage`  my_application(langsmith_extra={"parent": request.headers})# Or using the `trace` context managerwith trace(parent=request.headers)as run_tree:...    run_tree.end(outputs={"answer":"42"})...
```

## Turning off tracing[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#turning-off-tracing "Direct link to Turning off tracing")
If you've decided you no longer want to trace your runs, you can remove the environment variables configured to start tracing in the first place. By unsetting the `LANGCHAIN_TRACING_V2` environment variable, traces will no longer be logged to LangSmith. Note that this currently does not affect the `RunTree` objects.
This setting works both with LangChain and the LangSmith SDK, in both Python and TypeScript.
## Getting the run ID of a logged run[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#getting-the-run-id-of-a-logged-run "Direct link to Getting the run ID of a logged run")
The example below shows how to get the run ID of a logged run using the LangSmith SDK. **To get the run ID of a run using LangChain, you can follow the guide[here](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides#getting-a-run-id-from-a-langchain-call).**
  * Python
  * TypeScript
  * API (Using Python Requests)


```
import openaifrom uuid import uuid4from langsmith import traceablefrom langsmith.run_trees import RunTreefrom langsmith.wrappers import wrap_openaimessages =[{"role":"system","content":"You are a helpful assistant. Please respond to the user's request only based on the given context."},{"role":"user","content":"Is sunshine good for you?"}]# Collect run ID using RunTreerun_id = uuid4()rt = RunTree( name="OpenAI Call RunTree", run_type="llm", inputs={"messages": messages},id=run_id)client = openai.Client()chat_completion = client.chat.completions.create( model="gpt-3.5-turbo", messages=messages)rt.end(outputs=chat_completion)rt.post()print("RunTree Run ID: ", run_id)# Collect run ID using openai_wrapperrun_id = uuid4()client = wrap_openai(openai.Client())completion = client.chat.completions.create( model="gpt-3.5-turbo", messages=messages, langsmith_extra={"run_id": run_id,},)print("OpenAI Wrapper Run ID: ", run_id)# Collect run id using traceable decoratorrun_id = uuid4()@traceable( run_type="llm", name="OpenAI Call Decorator",)defcall_openai( messages:list[dict], model:str="gpt-3.5-turbo")->str:return client.chat.completions.create(   model=model,   messages=messages,).choices[0].message.contentresult = call_openai( messages, langsmith_extra={"run_id": run_id,},)print("Traceable Run ID: ", run_id)
```

```
import OpenAI from"openai";import{ RunTree }from"langsmith";import{v4 as uuidv4}from"uuid";const client =newOpenAI();const messages =[{role:"system", content:"You are a helpful assistant."},{role:"user", content:"Is sunshine food for you?"}];const runId =uuidv4();const rt =newRunTree({ run_type:"llm", name:"OpenAI Call RunTree", inputs:{ messages }, id: runId})await rt.postRun();const chatCompletion =await client.chat.completions.create({ model:"gpt-3.5-turbo", messages: messages,});rt.end(chatCompletion)await rt.patchRun()console.log("Run ID: ", runId);
```

```
import openaiimport requestsfrom datetime import datetimefrom uuid import uuid4# Send your API Key in the request headersheaders ={"x-api-key":"ls__..."}messages =[{"role":"system","content":"You are a helpful assistant. Please respond to the user's request only based on the given context."},{"role":"user","content":"Is sunshine good for you?"}]start_time = datetime.utcnow().isoformat()client = openai.Client()chat_completion = client.chat.completions.create( model="gpt-3.5-turbo", messages=messages)run_id = uuid4()requests.post("https://api.smith.langchain.com/runs", json={"id": run_id.hex,"name":"OpenAI Call","run_type":"llm","inputs":{"messages": messages},"start_time": start_time,"outputs":{"answer": chat_completion.choices[0].message.content},"end_time": datetime.utcnow().isoformat(),}, headers=headers)print("API Run ID: ", run_id)
```

## Getting the URL of a logged run[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#getting-the-url-of-a-logged-run "Direct link to Getting the URL of a logged run")
Runs are logged to whichever project you have configured ("default" if none is set), and you can view them by opening the corresponding project details page. To programmatically access the run's URL, you can use the LangSmith client. Below is an example. To get the run ID of a run, you can follow the guide [here](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#getting-the-run-id-of-a-logged-run).
  * Python
  * TypeScript


```
from langsmith import Clientclient = Client()run = client.read_run("<run_id>")print(run.url)
```

```
import{ Client }from"langsmith";const client =newClient();const runUrl =await client.getRunUrl({runId:"<run_id>"});console.log(runUrl);
```

## Deleting traces in a project[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#deleting-traces-in-a-project "Direct link to Deleting traces in a project")
You can delete a project, along with all its associated traces and other information, in the UI or by using the LangSmith client.
Below is an example using the SDK:
  * Python
  * TypeScript


```
from langsmith import Clientclient = Client()client.delete_project(project_name="<project_name>")
```

```
import{ Client }from"langsmith";const client =newClient();await client.deleteProject({projectName:"<project_name>"});
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow-To Guides](https://docs.smith.langchain.com/old/tracing/faq)[NextCustomize Trace Attributes](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes)
  * [Annotating your code for tracing](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#annotating-your-code-for-tracing)
    * [Using `@traceable` / `traceable`](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#using-traceable--traceable)
    * [Wrapping the OpenAI client](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#wrapping-the-openai-client)
    * [Using the `RunTree` API](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#using-the-runtree-api)
  * [Viewing Traces](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#viewing-traces)
  * [Setting a sampling rate for tracing](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#setting-a-sampling-rate-for-tracing)
  * [Distributed Tracing](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#distributed-tracing)
  * [Turning off tracing](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#turning-off-tracing)
  * [Getting the run ID of a logged run](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#getting-the-run-id-of-a-logged-run)
  * [Getting the URL of a logged run](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#getting-the-url-of-a-logged-run)
  * [Deleting traces in a project](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#deleting-traces-in-a-project)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/faq/logging_feedback

[Skip to main content](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
      * [Core Functionality](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing)
      * [Customize Trace Attributes](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes)
      * [Querying Traces](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)
      * [Capturing Feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback)
      * [Querying Feedback](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback)
      * [LangChain-Specific Guides](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides)
      * [Token Counting for Custom LLMs](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
  * Capturing Feedback


On this page
# How to Collect Feedback for Traces
Feedback allows you to understand how your users are experiencing your application and helps draw attention to problematic traces. LangSmith makes it easy to collect feedback for traces and view it in the context of the trace, as well as filter traces based on feedback. For more information on how to score your application programmatically using a testing workflow, see the [Evaluation](https://docs.smith.langchain.com/old/evaluation) section.
### Capturing feedback programmatically[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback#capturing-feedback-programmatically "Direct link to Capturing feedback programmatically")
It's often helpful to expose a simple mechanism (such as a thumbs-up, thumbs-down button) to collect user feedback for your application responses. You can then use the LangSmith SDK or API to send feedback for a trace. To get the run_id of a logged run, see [this guide](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing#getting-the-run-id-of-a-logged-run).
  * Python
  * TypeScript
  * API (Using Python Requests)


```
from langsmith import Clientclient = Client()# ... Run your application and get the run_id...# This information can be the result of a user-facing feedback formclient.create_feedback( run_id, key="feedback-key", score=1.0, comment="comment",)
```

```
import{ Client }from"langsmith";const client =newClient();// ... Run your application and get the run_id...// This information can be the result of a user-facing feedback formawait client.createFeedback(runId,"feedback-key",{ score:1.0, comment:"comment",});
```

```
import requests# Send your API Key in the request headersheaders ={"x-api-key":"ls__...",}requests.post("https://api.smith.langchain.com/feedback", headers=headers, json={"run_id": run_id.hex,"key":"feedback-key","score":1.0,"comment":"comment",},)
```

### Annotating traces with feedback[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback#annotating-traces-with-feedback "Direct link to Annotating traces with feedback")
LangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user's comment or a note about a specific issue. You can annotate a trace either inline or by sending the trace to an Annotation Queue, which allows you closely inspect and log feedbacks to runs one at a time. Feedback tags are associated with your tenant.
You can click the top right corner of the trace to annotate it inline ![Annotate Inline](https://docs.smith.langchain.com/assets/images/annotate_inline-8265b73b74e39abdded8de7367c2ec6b.png)
Or you can send the trace to the Annotation Queue ![Send to Queue](https://docs.smith.langchain.com/assets/images/send_to_annotation_queue-1cb8835960457a63722042e79c2d2526.png)
You can annotate the trace in an Annotation Queue using one of the feedback tags associated with your tenant (or create a new one) ![Annotation In Queue](https://docs.smith.langchain.com/assets/images/annotate_in_queue-af2caee1bfd5a61268c629d9dde1a714.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousQuerying Traces](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)[NextQuerying Feedback](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback)
  * [Capturing feedback programmatically](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback#capturing-feedback-programmatically)
  * [Annotating traces with feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback#annotating-traces-with-feedback)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/faq/querying_feedback

[Skip to main content](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
      * [Core Functionality](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing)
      * [Customize Trace Attributes](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes)
      * [Querying Traces](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)
      * [Capturing Feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback)
      * [Querying Feedback](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback)
      * [LangChain-Specific Guides](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides)
      * [Token Counting for Custom LLMs](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
  * Querying Feedback


On this page
# Querying Feedback
LangSmith makes it easy to fetch feedback associated with your runs. The `Run` object itself has aggregate `feedback_stats` on its body, which may satisfy your needs. If you want the additional feedback metadata (or full list of feedback objects), you can use the SDK or API to list feedback objects based on run IDs, feedback keys, and feedback source types.
Using the `list_feedback` method in the SDK or [`/feedback`](https://api.smith.langchain.com/redoc#tag/feedback/operation/list_feedback_feedback_get) endpoint in the API, you can fetch feedback to analyze. Most simple requests can be satisfied using the following arguments:
  * `run_ids`: Fetch feedback for specific runs by providing their IDs.
  * `feedback_keys`: Filter feedback by specific keys, such as 'correctness' or 'quality'.
  * `feedback_source_types`: Filter feedback by the source type, such as 'model' for model-generated feedback or 'api' for feedback submitted via the API.


All the examples below assume you have created a LangSmith client and configured it with your API key to connect to the LangSmith server.
  * Python
  * TypeScript


```
from langsmith import Clientclient = Client()
```

```
import{ Client, Feedback }from"langsmith";const client =newClient();
```

Below are some examples of ways to list feedback using the available arguments:
#### List all feedback for a specific run[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback#list-all-feedback-for-a-specific-run "Direct link to List all feedback for a specific run")
  * Python
  * TypeScript


```
run_feedback = client.list_feedback(run_ids=["<run_id>"])
```

```
const runFeedback: Feedback[]=[];forawait(const feedback of client.listFeedback({runIds:["<run_id>"],})){runFeedback.push(feedback);}
```

#### List all feedback with a specific key[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback#list-all-feedback-with-a-specific-key "Direct link to List all feedback with a specific key")
  * Python
  * TypeScript


```
correctness_feedback = client.list_feedback(feedback_key=["correctness"])
```

```
const correctnessFeedback: Feedback[]=[];forawait(const feedback of client.listFeedback({feedbackKeys:["correctness"],})){correctnessFeedback.push(feedback);}
```

#### List all model-generated feedback[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback#list-all-model-generated-feedback "Direct link to List all model-generated feedback")
  * Python
  * TypeScript


```
model_feedback = client.list_feedback(feedback_source_type=["model"])
```

```
const modelFeedback: Feedback[]=[];forawait(const feedback of client.listFeedback({feedbackSourceTypes:["model"],})){modelFeedback.push(feedback);}
```

## Use Cases[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback#use-cases "Direct link to Use Cases")
Here are a few common use cases for querying feedback:
#### Compare model-generated and human feedback for a set of runs[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback#compare-model-generated-and-human-feedback-for-a-set-of-runs "Direct link to Compare model-generated and human feedback for a set of runs")
After querying for a set of runs, you can compare the model-generated and human feedback for those specific runs:
  * Python
  * TypeScript


```
# Query for runsruns = client.list_runs(project_name="<your_project>",filter='gt(start_time, "2023-07-15T00:00:00Z")')# Extract run IDsrun_ids =[run.idfor run in runs]# Fetch model-generated feedback for the runsmodel_feedback = client.list_feedback(run_ids=run_ids, feedback_source_type=["model"])# Fetch human feedback for the runshuman_feedback = client.list_feedback(run_ids=run_ids, feedback_source_type=["api"])
```

```
// Query for runsconst runs: Run[]=[];forawait(const run of client.listRuns({projectName:"<your_project>",filter:'gt(start_time, "2023-07-15T00:00:00Z")',})){runs.push(run);}// Extract run IDsconst runIds = runs.map(run => run.id);// Fetch model-generated feedback for the runsconst modelFeedback: Feedback[]=[];forawait(const feedback of client.listFeedback({runIds,feedbackSourceTypes:["model"],})){modelFeedback.push(feedback);}// Fetch human feedback for the runsconst humanFeedback: Feedback[]=[];forawait(const feedback of client.listFeedback({runIds,feedbackSourceTypes:["api"],})){humanFeedback.push(feedback);}
```

#### Analyze feedback for a specific key and set of runs[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback#analyze-feedback-for-a-specific-key-and-set-of-runs "Direct link to Analyze feedback for a specific key and set of runs")
If you're interested in analyzing feedback for a specific key, such as 'correctness' or 'quality', for a set of runs, you can query for the runs and then filter the feedback by key:
  * Python
  * TypeScript


```
# Query for runsruns = client.list_runs(project_name="<your_project>",filter='gt(start_time, "2023-07-15T00:00:00Z")')# Extract run IDs run_ids =[run.idfor run in runs]# Fetch correctness feedback for the runscorrectness_feedback = client.list_feedback(run_ids=run_ids, feedback_key=["correctness"])# Analyze the correctness scoresscores =[feedback.score for feedback in correctness_feedback]
```

```
// Query for runsconst runs: Run[]=[];forawait(const run of client.listRuns({projectName:"<your_project>",filter:'gt(start_time, "2023-07-15T00:00:00Z")',})){runs.push(run);}// Extract run IDsconst runIds = runs.map(run => run.id);// Fetch correctness feedback for the runsconst correctnessFeedback: Feedback[]=[];forawait(const feedback of client.listFeedback({runIds,feedbackKeys:["correctness"],})){correctnessFeedback.push(feedback);}// Analyze the correctness scoresconst scores = correctnessFeedback.map(feedback => feedback.score);
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousCapturing Feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback)[NextLangChain-Specific Guides](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides)
  * [Use Cases](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback#use-cases)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/faq/querying_traces

[Skip to main content](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
      * [Core Functionality](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing)
      * [Customize Trace Attributes](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes)
      * [Querying Traces](https://docs.smith.langchain.com/old/tracing/faq/querying_traces)
      * [Capturing Feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback)
      * [Querying Feedback](https://docs.smith.langchain.com/old/tracing/faq/querying_feedback)
      * [LangChain-Specific Guides](https://docs.smith.langchain.com/old/tracing/faq/langchain_specific_guides)
      * [Token Counting for Custom LLMs](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
  * Querying Traces


On this page
# How to query traces and runs
LangSmith makes it easy to query for traces and runs. In addition to the filtering experience presented in the UI, you can also use the SDK or API to query for traces and runs.
Using the `list_runs` method in the SDK or [`/runs/query`](https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_runs_query_post) endpoint in the API, you can filter runs to analyze and export. Most simple requests can be satisfied using simple top level arguments:
Keys| Description  
---|---  
`project_id` / `project_name`| The project(s) to fetch runs from - can be a single project or a list of projects.  
`trace_id`| Fetch runs that are part of a specific trace.  
`run_type`| The type of run to get, such as `llm`, `chain`, `tool`, `retriever`, etc.  
`dataset_name` / `dataset_id`| Fetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.  
`reference_example_id`| Fetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.  
`parent_run_id`| Fetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.  
`error`| Fetch runs that errored or did not error.  
`run_ids`| Fetch runs with a given list of run ids. Note: **This will ignore all other filtering arguments.**  
`filter`|  Fetch runs that match a given structured filter statement. See the [run filtering guide](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#run-filtering) below for more information.  
`trace_filter`| Filter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of the root run within a trace.  
`tree_filter`| Filter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of any run within a trace.  
`is_root`| Only return root runs.  
`select`| Select the fields to return in the response. By default, all fields are returned.  
`query` (_experimental_)| Query the experimental natural language API, which translates your query into a filter statement.  
## Using keyword arguments[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#using-keyword-arguments "Direct link to Using keyword arguments")
For simple queries, such as filtering by project, run time, name, or run ID's, you can directly use keyword arguments in the list_runs method. These correspond directly to query params in the REST API. All the examples below assume you have created a LangSmith client and configured it with your API key to connect to the LangSmith server.
  * Python
  * TypeScript


```
from langsmith import Clientclient = Client()
```

```
import{ Client, Run }from"langsmith";const client =newClient();
```

Below are some examples of ways to list runs using keyword arguments:
#### List all runs in a project[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-in-a-project "Direct link to List all runs in a project")
  * Python
  * TypeScript


```
project_runs = client.list_runs(project_name="<your_project>")
```

```
// Download runs in a projectconst projectRuns: Run[]=[];forawait(const run of client.listRuns({projectName:"<your_project>",})){projectRuns.push(run);};
```

#### List LLM and Chat runs in the last 24 hours[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-llm-and-chat-runs-in-the-last-24-hours "Direct link to List LLM and Chat runs in the last 24 hours")
  * Python
  * TypeScript


```
todays_llm_runs = client.list_runs( project_name="<your_project>", start_time=datetime.now()- timedelta(days=1), run_type="llm",)
```

```
const todaysLlmRuns: Run[]=[];forawait(const run of client.listRuns({projectName:"<your_project>",startTime:newDate(Date.now()-1000*60*60*24),runType:"llm",})){todaysLlmRuns.push(run);};
```

#### List traces in a project[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-traces-in-a-project "Direct link to List traces in a project")
Root runs (or run traces), are runs that have no parents. These are assigned an 'execution_order' of 1. You can use this to filter for root runs.
  * Python
  * TypeScript


```
root_runs = client.list_runs( project_name="<your_project>", is_root=True)
```

```
const rootRuns: Run[]=[];forawait(const run of client.listRuns({projectName:"<your_project>",isRoot:1,})){rootRuns.push(run);};
```

#### List runs without errors[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-runs-without-errors "Direct link to List runs without errors")
  * Python
  * TypeScript


```
correct_runs = client.list_runs(project_name="<your_project>", error=False)
```

```
const correctRuns: Run[]=[];forawait(const run of client.listRuns({projectName:"<your_project>",error:false,})){correctRuns.push(run);};
```

#### List runs by run ID[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-runs-by-run-id "Direct link to List runs by run ID")
If you have a list of run IDs, you can list them directly:
  * Python
  * TypeScript


```
run_ids =['a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836','9398e6be-964f-4aa4-8ae9-ad78cd4b7074']selected_runs = client.list_runs(id=run_ids)
```

```
const runIds =["a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836","9398e6be-964f-4aa4-8ae9-ad78cd4b7074",];const selectedRuns: Run[]=[];forawait(const run of client.listRuns({id: runIds,})){selectedRuns.push(run);};
```

Ignores Other Arguments
If you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like `project_name`, `run_type`, etc. and directly return the runs matching the given IDs.
## Run Filtering[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#run-filtering "Direct link to Run Filtering")
Listing runs with query params is useful for simple queries, but doesn't support many common needs, such as filtering by metadata, tags, or other fields.
LangSmith supports a filter query language to permit more complex filtering operations when fetching runs. This guide will provide a high level overview of the grammar as well as a few examples of when it can be useful.
If you'd prefer a more visual guide, you can get a taste of the language by viewing the table of runs on any of your projects' pages. We provide some recommended filters to get you started that you can copy and use the SDK.
### Grammar[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#grammar "Direct link to Grammar")
The filtering grammar is based on common comparators on fields in the run object. Supported comparators include:
  * `gte` (greater than or equal to)
  * `gt` (greater than)
  * `lte` (less than or equal to)
  * `lt` (less than)
  * `eq` (equal to)
  * `neq` (not equal to)
  * `has` (check if run contains a tag or metadata json blob)
  * `search` (search for a substring in a string field)


Additionally, you can combine multiple comparisons through `and` and `or` operators.
These can be applied on fields of the run object, such as its `id`, `name`, `run_type`, `start_time` / `end_time`, `latency`, `total_tokens`, `error`, `execution_order`, `tags`, and any associated feedback through `feedback_key` and `feedback_score`.
### Examples[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#examples "Direct link to Examples")
The following examples assume you have configured your environment appropriately and have runs stored in LangSmith.
#### List all runs called "extractor" whose root of the trace was assigned feedback "user_score" score of 1[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-called-extractor-whose-root-of-the-trace-was-assigned-feedback-user_score-score-of-1 "Direct link to List all runs called "extractor" whose root of the trace was assigned feedback "user_score" score of 1")
  * Python
  * TypeScript


```
client.list_runs( project_name="<your_project>",filter='eq(name, "extractor")', trace_filter='and(eq(feedback_key, "user_score"), eq(feedback_score, 1))')
```

```
client.listRuns({projectName:"<your_project>",filter:'eq(name, "extractor")',traceFilter:'and(eq(feedback_key, "user_score"), eq(feedback_score, 1))'})
```

#### List runs with "star_rating" key whose score is greater than 4[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-runs-with-star_rating-key-whose-score-is-greater-than-4 "Direct link to List runs with "star_rating" key whose score is greater than 4")
  * Python
  * TypeScript


```
client.list_runs( project_name="<your_project>",filter='and(eq(feedback_key, "star_rating"), gt(feedback_score, 4))')
```

```
client.listRuns({projectName:"<your_project>",filter:'and(eq(feedback_key, "star_rating"), gt(feedback_score, 4))'})
```

#### List runs that took longer than 5 seconds to complete[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-runs-that-took-longer-than-5-seconds-to-complete "Direct link to List runs that took longer than 5 seconds to complete")
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='gt(latency, "5s")')
```

```
client.listRuns({projectName:"<your_project>", filter:'gt(latency, "5s")'})
```

#### List all runs where `total_tokens` is greater than 5000[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-where-total_tokens-is-greater-than-5000 "Direct link to list-all-runs-where-total_tokens-is-greater-than-5000")
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='gt(total_tokens, 5000)')
```

```
client.listRuns({projectName:"<your_project>", filter:'gt(total_tokens, 5000)'})
```

#### List all runs that have "error" not equal to null[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-that-have-error-not-equal-to-null "Direct link to List all runs that have "error" not equal to null")
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='neq(error, null)')
```

```
client.listRuns({projectName:"<your_project>", filter:'neq(error, null)'})
```

#### List all runs where `start_time` is greater than a specific timestamp[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-where-start_time-is-greater-than-a-specific-timestamp "Direct link to list-all-runs-where-start_time-is-greater-than-a-specific-timestamp")
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='gt(start_time, "2023-07-15T12:34:56Z")')
```

```
client.listRuns({projectName:"<your_project>", filter:'gt(start_time, "2023-07-15T12:34:56Z")'})
```

#### List all runs that contain the string "substring"[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-that-contain-the-string-substring "Direct link to List all runs that contain the string "substring"")
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='search("substring")')
```

```
client.listRuns({projectName:"<your_project>", filter:'search("substring")'})
```

#### List all runs that are tagged with the git hash "2aa1cf4"[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-that-are-tagged-with-the-git-hash-2aa1cf4 "Direct link to List all runs that are tagged with the git hash "2aa1cf4"")
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='has(tags, "2aa1cf4")')
```

```
client.listRuns({projectName:"<your_project>", filter:'has(tags, "2aa1cf4")'})
```

#### List all "chain" type runs that took more than 10 seconds and[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-chain-type-runs-that-took-more-than-10-seconds-and "Direct link to List all "chain" type runs that took more than 10 seconds and")
had `total_tokens` greater than 5000
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='and(eq(run_type, "chain"), gt(latency, 10), gt(total_tokens, 5000))')
```

```
client.listRuns({projectName:"<your_project>",filter:'and(eq(run_type, "chain"), gt(latency, 10), gt(total_tokens, 5000))'})
```

#### List all runs that started after a specific timestamp and either[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-that-started-after-a-specific-timestamp-and-either "Direct link to List all runs that started after a specific timestamp and either")
have "error" not equal to null or a "Correctness" feedback score equal to 0
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='and(gt(start_time, "2023-07-15T12:34:56Z"), or(neq(error, null), and(eq(feedback_key, "Correctness"), eq(feedback_score, 0.0))))')
```

```
client.listRuns({projectName:"<your_project>",filter:'and(gt(start_time, "2023-07-15T12:34:56Z"), or(neq(error, null), and(eq(feedback_key, "Correctness"), eq(feedback_score, 0.0))))'})
```

#### Complex query: List all runs where `tags` include "experimental" or "beta" and[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#complex-query-list-all-runs-where-tags-include-experimental-or-beta-and "Direct link to complex-query-list-all-runs-where-tags-include-experimental-or-beta-and")
`latency` is greater than 2 seconds
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='and(or(has(tags, "experimental"), has(tags, "beta")), gt(latency, 2))')
```

```
client.listRuns({projectName:"<your_project>",filter:'and(or(has(tags, "experimental"), has(tags, "beta")), gt(latency, 2))'})
```

#### Search trace trees by full text You can use the `search()` function without[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#search-trace-trees-by-full-text-you-can-use-the-search-function-without "Direct link to search-trace-trees-by-full-text-you-can-use-the-search-function-without")
any specific field to do a full text search across all string fields in a run. This allows you to quickly find traces that match a search term.
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='search("image classification")')
```

```
client.listRuns({projectName:"<your_project>",filter:'search("image classification")'})
```

#### Check for presence of metadata[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#check-for-presence-of-metadata "Direct link to Check for presence of metadata")
If you want to check for the presence of metadata, you can use the `eq` operator, optionally with an `and` statement to match by value. This is useful if you want to log more structured information about your runs.
  * Python
  * TypeScript


```
to_search ={"user_id":""}# Check for any run with the "user_id" metadata keyclient.list_runs(project_name="default",filter="eq(metadata_key, 'user_id')")# Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.list_runs(project_name="default",filter="and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))")
```

```
// Check for any run with the "user_id" metadata keyclient.listRuns({projectName:'default',filter:`eq(metadata_key, 'user_id')`});// Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.listRuns({projectName:'default',filter:`and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))`});
```

#### Check for environment details in metadata.[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#check-for-environment-details-in-metadata "Direct link to Check for environment details in metadata.")
A common pattern is to add environment information to your traces via metadata. If you want to filter for runs containing environment metadata, you can use the same pattern as above:
  * Python
  * TypeScript


```
client.list_runs(project_name="default",filter="and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))")
```

```
client.listRuns({projectName:'default',filter:`and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))`});
```

#### Check for conversation ID in metadata[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#check-for-conversation-id-in-metadata "Direct link to Check for conversation ID in metadata")
Another common way to associate traces in the same conversation is by using a shared conversation ID. If you want to filter runs based on a conversation ID in this way, you can search for that ID in the metadata.
  * Python
  * TypeScript


```
client.list_runs(project_name="default",filter="and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))")
```

```
client.listRuns({projectName:'default',filter:`and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))`});
```

#### Combine multiple filters[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#combine-multiple-filters "Direct link to Combine multiple filters")
If you want to combine multiple conditions to refine your search, you can use the `and` operator along with other filtering functions. Here's how you can search for runs named "ChatOpenAI" that also have a specific `conversation_id` in their metadata:
  * Python
  * TypeScript


```
client.list_runs(project_name="default",filter="and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))")
```

```
client.listRuns({projectName:'default',filter:`and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))`});
```

#### Tree Filter[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#tree-filter "Direct link to Tree Filter")
List all runs named "RetrieveDocs" whose root run has a "user_score" feedback of 1 and any run in the full trace is named "ExpandQuery".
This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace.
  * Python
  * TypeScript


```
client.list_runs( project_name="<your_project>",filter='eq(name, "RetrieveDocs")', trace_filter='and(eq(feedback_key, "user_score"), eq(feedback_score, 1))', tree_filter='eq(name, "ExpandQuery")')
```

```
client.listRuns({projectName:"<your_project>",filter:'eq(name, "RetrieveDocs")',traceFilter:'and(eq(feedback_key, "user_score"), eq(feedback_score, 1))',treeFilter:'eq(name, "ExpandQuery")'})
```

## Advanced Examples[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#advanced-examples "Direct link to Advanced Examples")
#### Export flattened trace view with child tool usage[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#export-flattened-trace-view-with-child-tool-usage "Direct link to Export flattened trace view with child tool usage")
The following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace. This can be used to analyze the behavior of your agents across multiple traces.
This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information.
To optimize the query, the example:
  1. Selects only the necessary fields when querying tool runs to reduce query time.
  2. Fetches root runs in batches while processing tool runs concurrently.


  * Python


```
from collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltafrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name ="my-project"num_days =30# List all tool runstool_runs = client.list_runs( project_name=project_name, start_time=datetime.now()- timedelta(days=num_days), run_type="tool",# We don't need to fetch inputs, outputs, and other values that # may increase the query time select=["trace_id","name","run_type"],)data =[]futures:list[Future]=[]trace_cursor =0trace_batch_size =50tool_runs_by_parent = defaultdict(lambda: defaultdict(set))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2)as executor:# Group tool runs by parent run IDfor run in tqdm(tool_runs):# Collect all tools invoked within a given trace   tool_runs_by_parent[run.trace_id]["tools_involved"].add(run.name)# maybe send a batch of parent run IDs to the server# this lets us query for the root runs in batches# while still processing the tool runsiflen(tool_runs_by_parent)% trace_batch_size ==0:if this_batch :=list(tool_runs_by_parent.keys())[       trace_cursor : trace_cursor + trace_batch_size]:       trace_cursor += trace_batch_size       futures.append(         executor.submit(           client.list_runs,           project_name=project_name,           run_ids=this_batch,           select=["name","inputs","outputs","run_type"],))if this_batch :=list(tool_runs_by_parent.keys())[trace_cursor:]:   futures.append(     executor.submit(       client.list_runs,       project_name=project_name,       run_ids=this_batch,       select=["name","inputs","outputs","run_type"],))for future in tqdm(futures): root_runs = future.result()for root_run in root_runs:   root_data = tool_runs_by_parent[root_run.id]   data.append({"run_id": root_run.id,"run_name": root_run.name,"run_type": root_run.run_type,"inputs": root_run.inputs,"outputs": root_run.outputs,"tools_involved":list(root_data["tools_involved"]),})# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()
```

#### Export retriever inputs/outputs for traces with a specific feedback score[‚Äã](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#export-retriever-inputsoutputs-for-traces-with-a-specific-feedback-score "Direct link to Export retriever inputs/outputs for traces with a specific feedback score")
This query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior. The following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score.
  * Python


```
from collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltaimport pandas as pdfrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name ="your-project-name"num_days =1# List all tool runsretriever_runs = client.list_runs( project_name=project_name, start_time=datetime.now()- timedelta(days=num_days), run_type="retriever",# This time we do want to fetch the inputs and outputs, since they# may be adjusted by query expansion steps. select=["trace_id","name","run_type","inputs","outputs"], trace_filter='eq(feedback_key, "user_score")',)data =[]futures:list[Future]=[]trace_cursor =0trace_batch_size =50retriever_runs_by_parent = defaultdict(lambda: defaultdict(list))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2)as executor:# Group retriever runs by parent run IDfor run in tqdm(retriever_runs):# Collect all retriever calls invoked within a given tracefor k, v in run.inputs.items():     retriever_runs_by_parent[run.trace_id][f"retriever.inputs.{k}"].append(v)for k, v in(run.outputs or{}).items():# Extend the docs     retriever_runs_by_parent[run.trace_id][f"retriever.outputs.{k}"].extend(v)# maybe send a batch of parent run IDs to the server# this lets us query for the root runs in batches# while still processing the retriever runsiflen(retriever_runs_by_parent)% trace_batch_size ==0:if this_batch :=list(retriever_runs_by_parent.keys())[       trace_cursor : trace_cursor + trace_batch_size]:       trace_cursor += trace_batch_size       futures.append(         executor.submit(           client.list_runs,           project_name=project_name,           run_ids=this_batch,           select=["name","inputs","outputs","run_type","feedback_stats",],))if this_batch :=list(retriever_runs_by_parent.keys())[trace_cursor:]:   futures.append(     executor.submit(       client.list_runs,       project_name=project_name,       run_ids=this_batch,       select=["name","inputs","outputs","run_type"],))for future in tqdm(futures): root_runs = future.result()for root_run in root_runs:   root_data = retriever_runs_by_parent[root_run.id]   feedback ={f"feedback.{k}": v.get("avg")for k, v in(root_run.feedback_stats or{}).items()}   inputs ={f"inputs.{k}": v for k, v in root_run.inputs.items()}   outputs ={f"outputs.{k}": v for k, v in(root_run.outputs or{}).items()}   data.append({"run_id": root_run.id,"run_name": root_run.name,**inputs,**outputs,**feedback,**root_data,})# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousCustomize Trace Attributes](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes)[NextCapturing Feedback](https://docs.smith.langchain.com/old/tracing/faq/logging_feedback)
  * [Using keyword arguments](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#using-keyword-arguments)
    * [List all runs in a project](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-in-a-project)
    * [List LLM and Chat runs in the last 24 hours](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-llm-and-chat-runs-in-the-last-24-hours)
    * [List traces in a project](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-traces-in-a-project)
    * [List runs without errors](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-runs-without-errors)
    * [List runs by run ID](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-runs-by-run-id)
  * [Run Filtering](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#run-filtering)
    * [Grammar](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#grammar)
    * [Examples](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#examples)
      * [List all runs called "extractor" whose root of the trace was assigned feedback "user_score" score of 1](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-called-extractor-whose-root-of-the-trace-was-assigned-feedback-user_score-score-of-1)
      * [List runs with "star_rating" key whose score is greater than 4](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-runs-with-star_rating-key-whose-score-is-greater-than-4)
      * [List runs that took longer than 5 seconds to complete](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-runs-that-took-longer-than-5-seconds-to-complete)
      * [List all runs where `total_tokens` is greater than 5000](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-where-total_tokens-is-greater-than-5000)
      * [List all runs that have "error" not equal to null](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-that-have-error-not-equal-to-null)
      * [List all runs where `start_time` is greater than a specific timestamp](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-where-start_time-is-greater-than-a-specific-timestamp)
      * [List all runs that contain the string "substring"](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-that-contain-the-string-substring)
      * [List all runs that are tagged with the git hash "2aa1cf4"](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-that-are-tagged-with-the-git-hash-2aa1cf4)
      * [List all "chain" type runs that took more than 10 seconds and](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-chain-type-runs-that-took-more-than-10-seconds-and)
      * [List all runs that started after a specific timestamp and either](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#list-all-runs-that-started-after-a-specific-timestamp-and-either)
      * [Complex query: List all runs where `tags` include "experimental" or "beta" and](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#complex-query-list-all-runs-where-tags-include-experimental-or-beta-and)
      * [Search trace trees by full text You can use the `search()` function without](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#search-trace-trees-by-full-text-you-can-use-the-search-function-without)
      * [Check for presence of metadata](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#check-for-presence-of-metadata)
      * [Check for environment details in metadata.](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#check-for-environment-details-in-metadata)
      * [Check for conversation ID in metadata](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#check-for-conversation-id-in-metadata)
      * [Combine multiple filters](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#combine-multiple-filters)
      * [Tree Filter](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#tree-filter)
  * [Advanced Examples](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#advanced-examples)
    * [Export flattened trace view with child tool usage](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#export-flattened-trace-view-with-child-tool-usage)
    * [Export retriever inputs/outputs for traces with a specific feedback score](https://docs.smith.langchain.com/old/tracing/faq/querying_traces#export-retriever-inputsoutputs-for-traces-with-a-specific-feedback-score)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/integrations

[Skip to main content](https://docs.smith.langchain.com/old/tracing/integrations#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/integrations)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
      * [Python](https://docs.smith.langchain.com/old/tracing/integrations/python)
      * [TypeScript](https://docs.smith.langchain.com/old/tracing/integrations/typescript)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * Integrations


# Integrations
This section includes examples and techniques for how you can use LangSmith's tracing capabilities to integrate with a variety of frameworks and SDKs, as well as arbitrary functions.
## [üìÑÔ∏è PythonLangSmith allows you to log traces in various ways.](https://docs.smith.langchain.com/old/tracing/integrations/python)## [üìÑÔ∏è TypeScriptLangChain.js](https://docs.smith.langchain.com/old/tracing/integrations/typescript)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/tracing/integrations%3E).
[PreviousToken Counting for Custom LLMs](https://docs.smith.langchain.com/old/tracing/faq/custom_llm_token_counting)[NextPython](https://docs.smith.langchain.com/old/tracing/integrations/python)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/integrations/python

[Skip to main content](https://docs.smith.langchain.com/old/tracing/integrations/python#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/integrations/python)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
      * [Python](https://docs.smith.langchain.com/old/tracing/integrations/python)
      * [TypeScript](https://docs.smith.langchain.com/old/tracing/integrations/typescript)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
  * Python


On this page
# Python Integrations
LangSmith allows you to log traces in various ways.
## LangChain[‚Äã](https://docs.smith.langchain.com/old/tracing/integrations/python#langchain "Direct link to LangChain")
To log traces with [LangChain](https://python.langchain.com/), all you need to do is set an environment variable.
```
export LANGCHAIN_API_KEY=<your-api-key>export LANGCHAIN_TRACING_V2=true
```

After that, you can use LangChain as you normally would and all traces will get logged to LangSmith!
## OpenAI SDK[‚Äã](https://docs.smith.langchain.com/old/tracing/integrations/python#openai-sdk "Direct link to OpenAI SDK")
We provide a convenient wrapper for the [OpenAI SDK](https://platform.openai.com/docs/api-reference).
In order to use, you first need to set your LangSmith API key.
```
export LANGCHAIN_API_KEY=<your-api-key>
```

Next, you will need to install the LangSmith SDK:
```
pip install -U langsmith
```

After that, you can wrap the OpenAI client:
```
from openai import OpenAIfrom langsmith import wrappersclient = wrappers.wrap_openai(OpenAI())
```

Now, you can use the OpenAI client as you normally would, but now everything is logged to LangSmith!
```
client.chat.completions.create(  model="gpt-4",  messages=[{"role":"user","content":"Say this is a test"}],)
```

Oftentimes, you use the OpenAI client inside of other functions. You can get nested traces by using this wrapped client and decorating those functions with `@traceable`. See [this documentation](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing) for more documentation how to use this decorator
```
from langsmith import traceable@traceabledefmy_function(text:str):return client.chat.completions.create(    model="gpt-4",    messages=[{"role":"user","content":f"Say {text}"}],)my_function("hello world")
```

## Instructor[‚Äã](https://docs.smith.langchain.com/old/tracing/integrations/python#instructor "Direct link to Instructor")
We provide a convenient integration with [Instructor](https://jxnl.github.io/instructor/).
In order to use, you first need to set your LangSmith API key.
```
export LANGCHAIN_API_KEY=<your-api-key>
```

Next, you will need to install the LangSmith SDK:
```
pip install -U langsmith
```

After that, you can wrap the OpenAI client:
```
from openai import OpenAIfrom langsmith import wrappersclient = wrappers.wrap_openai(OpenAI())
```

After this, you can patch the wrapped OpenAI client using `instructor`:
```
import instructorclient = instructor.patch(client)
```

Now, you can use `instructor` as you normally would, but now everything is logged to LangSmith!
```
from pydantic import BaseModelclassUserDetail(BaseModel):  name:str  age:intuser = client.chat.completions.create(  model="gpt-3.5-turbo",  response_model=UserDetail,  messages=[{"role":"user","content":"Extract Jason is 25 years old"},])
```

Oftentimes, you use `instructor` inside of other functions. You can get nested traces by using this wrapped client and decorating those functions with `@traceable`. See [this documentation](https://docs.smith.langchain.com/old/tracing/faq/logging_and_viewing) for more documentation how to use this decorator
```
# You can customize the run name with the `name` keyword argument@traceable(name="Extract User Details")defmy_function(text:str)-> UserDetail:return client.chat.completions.create(    model="gpt-3.5-turbo",    response_model=UserDetail,    messages=[{"role":"user","content":f"Extract {text}"},])my_function("Jason is 25 years old")
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/tracing/integrations/python%3E).
[PreviousIntegrations](https://docs.smith.langchain.com/old/tracing/integrations)[NextTypeScript](https://docs.smith.langchain.com/old/tracing/integrations/typescript)
  * [LangChain](https://docs.smith.langchain.com/old/tracing/integrations/python#langchain)
  * [OpenAI SDK](https://docs.smith.langchain.com/old/tracing/integrations/python#openai-sdk)
  * [Instructor](https://docs.smith.langchain.com/old/tracing/integrations/python#instructor)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/integrations/typescript

[Skip to main content](https://docs.smith.langchain.com/old/tracing/integrations/typescript#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/integrations/typescript)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
      * [Python](https://docs.smith.langchain.com/old/tracing/integrations/python)
      * [TypeScript](https://docs.smith.langchain.com/old/tracing/integrations/typescript)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
  * TypeScript


On this page
# TypeScript Integrations
## LangChain.js[‚Äã](https://docs.smith.langchain.com/old/tracing/integrations/typescript#langchainjs "Direct link to LangChain.js")
To log traces with [LangChain.js](https://js.langchain.com/), you'll need to set a few environment variables.
```
export LANGCHAIN_API_KEY=<your-api-key>export LANGCHAIN_TRACING_V2=true
```

And optionally:
```
LANGCHAIN_PROJECT=<some-custom-session-name>
```

After that, you can use LangChain as you normally would and all traces will get logged to LangSmith!
## OpenAI SDK[‚Äã](https://docs.smith.langchain.com/old/tracing/integrations/typescript#openai-sdk "Direct link to OpenAI SDK")
The easiest way to trace calls from the [OpenAI SDK](https://platform.openai.com/docs/api-reference) with LangSmith is using the `wrapOpenAI` wrapper function available in LangSmith 0.1.3 and up.
In order to use, you first need to set your LangSmith API key:
```
export LANGCHAIN_API_KEY=<your-api-key>
```

Next, you will need to install the LangSmith SDK and the OpenAI SDK:
```
npm install langsmith openai
```

After that, initialize your OpenAI client and wrap the client with `wrapOpenAI` method to enable tracing for the completions and chat completions methods:
```
import{ OpenAI }from"openai";import{ wrapOpenAI }from"langsmith/wrappers";const openai =wrapOpenAI(newOpenAI());
```

This new client takes the same exact arguments and has the same return type as the original method, but will log everything to LangSmith.
```
await openai.chat.completions.create({ model:"gpt-3.5-turbo", messages:[{ content:"Hi there!", role:"user"}],});
```

```
{ id: 'chatcmpl-8sOWEOYVyehDlyPcBiaDtTxWvr9v6', object: 'chat.completion', created: 1707974654, model: 'gpt-3.5-turbo-0613', choices: [  {   index: 0,   message: { role: 'assistant', content: 'Hello! How can I help you today?' },   logprobs: null,   finish_reason: 'stop'  } ], usage: { prompt_tokens: 10, completion_tokens: 9, total_tokens: 19 }, system_fingerprint: null}
```

Alternatively, you can use the `traceable` function to wrap the client methods you want to use:
```
import{ OpenAI }from"openai";import{ traceable }from"langsmith/traceable";const openai =newOpenAI();const createCompletion =traceable( openai.chat.completions.create.bind(openai.chat.completions),{ name:"OpenAI Chat Completion", run_type:"llm"});awaitcreateCompletion({ model:"gpt-3.5-turbo", messages:[{ content:"Hi there!", role:"user"}],});
```

Note the use of `.bind` to preserve the function's context. The `run_type` field in the extra config object marks the function as an LLM call, and enables token usage tracking for OpenAI.
This also works for streaming:
```
const stream =awaitcreateCompletion({ model:"gpt-3.5-turbo", stream:true, messages:[{ content:"Hi there!", role:"user"}],});
```

```
forawait(const chunk of stream){console.log(chunk);}
```

### Nested tracing[‚Äã](https://docs.smith.langchain.com/old/tracing/integrations/typescript#nested-tracing "Direct link to Nested tracing")
Oftentimes, you use the OpenAI client inside of other functions or as part of a longer sequence. You can automatically get nested traces by using this wrapped method within other functions wrapped with `traceable`.
```
const nestedTrace =traceable(async(text:string)=>{const completion =awaitcreateCompletion({  model:"gpt-3.5-turbo",  messages:[{ content: text, role:"user"}],});return completion;});awaitnestedTrace("Why is the sky blue?");
```

```
{ "id": "chatcmpl-8sPToJQLLVepJvyeTfzZMOMVIKjMo", "object": "chat.completion", "created": 1707978348, "model": "gpt-3.5-turbo-0613", "choices": [  {   "index": 0,   "message": {    "role": "assistant",    "content": "The sky appears blue because of a phenomenon known as Rayleigh scattering. The Earth's atmosphere is composed of tiny molecules, such as nitrogen and oxygen, which are much smaller than the wavelength of visible light. When sunlight interacts with these molecules, it gets scattered in all directions. However, shorter wavelengths of light (blue and violet) are scattered more compared to longer wavelengths (red, orange, and yellow). \n\nAs a result, when sunlight passes through the Earth's atmosphere, the blue and violet wavelengths are scattered in all directions, making the sky appear blue. This scattering of shorter wavelengths is also responsible for the vibrant colors observed during sunrise and sunset, when the sunlight has to pass through a thicker portion of the atmosphere, causing the longer wavelengths to dominate the scattered light."   },   "logprobs": null,   "finish_reason": "stop"  } ], "usage": {  "prompt_tokens": 13,  "completion_tokens": 154,  "total_tokens": 167 }, "system_fingerprint": null}
```

tip
[Click here](https://smith.langchain.com/public/4af46ef6-b065-46dc-9cf0-70f1274edb01/r) to see an example LangSmith trace of the above.
## Next.js[‚Äã](https://docs.smith.langchain.com/old/tracing/integrations/typescript#nextjs "Direct link to Next.js")
You can use the `traceable` wrapper function in Next.js apps to wrap arbitrary functions much like in the example above.
One neat trick you can use for Next.js and other similar server frameworks is to wrap the entire exported handler for a route to group traces for the any sub-runs. Here's an example:
```
import{ NextRequest, NextResponse }from"next/server";import{ OpenAI }from"openai";import{ traceable }from"langsmith/traceable";exportconst runtime ="edge";const handler =traceable(asyncfunction(){const openai =newOpenAI();const createCompletion =traceable(   openai.chat.completions.create.bind(openai.chat.completions),{ name:"OpenAI Chat Completion", run_type:"llm"});const completion =awaitcreateCompletion({   model:"gpt-3.5-turbo",   messages:[{ content:"Why is the sky blue?", role:"user"}],});const response1 = completion.choices[0].message.content;const completion2 =awaitcreateCompletion({   model:"gpt-3.5-turbo",   messages:[{ content:"Why is the sky blue?", role:"user"},{ content: response1, role:"assistant"},{ content:"Cool thank you!", role:"user"},],});const response2 = completion2.choices[0].message.content;return{   text: response2,};},{  name:"Simple Next.js handler",});exportasyncfunctionPOST(req: NextRequest){const result =awaithandler();return NextResponse.json(result);}
```

The two OpenAI calls within the handler will be traced with appropriate inputs, outputs, and token usage information.
tip
[Click here](https://smith.langchain.com/public/faaf26ad-8c59-4622-bcfe-b7d896733ca6/r) to see an example LangSmith trace of the above.
## Vercel AI SDK[‚Äã](https://docs.smith.langchain.com/old/tracing/integrations/typescript#vercel-ai-sdk "Direct link to Vercel AI SDK")
The [Vercel AI SDK](https://sdk.vercel.ai/docs) contains integrations with a variety of model providers. Here's an example of how you can trace outputs in a Next.js handler:
```
import{ traceable }from"langsmith/traceable";import{ OpenAIStream, StreamingTextResponse }from"ai";// Note: There are no types for the Mistral API client yet.import MistralClient from"@mistralai/mistralai";const client =newMistralClient(process.env.MISTRAL_API_KEY||"");exportasyncfunctionPOST(req: Request){// Extract the `messages` from the body of the requestconst{ messages }=await req.json();const mistralChatStream =traceable(client.chatStream.bind(client),{  name:"Mistral Stream",  run_type:"llm",});const response =awaitmistralChatStream({  model:"mistral-tiny",  maxTokens:1000,  messages,});// Convert the response into a friendly text-stream. The Mistral client responses are// compatible with the Vercel AI SDK OpenAIStream adapter.const stream =OpenAIStream(response asany);// Respond with the streamreturnnewStreamingTextResponse(stream);}
```

See the [AI SDK docs](https://sdk.vercel.ai/docs) for more examples.
## Arbitrary SDKs[‚Äã](https://docs.smith.langchain.com/old/tracing/integrations/typescript#arbitrary-sdks "Direct link to Arbitrary SDKs")
You can use the generic `wrapSDK` method to add tracing for arbitrary SDKs.
Do note that this will trace ALL methods in the SDK, not just chat completion endpoints. If the SDK you are wrapping has other methods, we recommend using it for only LLM calls.
Here's an example using the Anthropic SDK:
```
import{ wrapSDK }from"langsmith/wrappers";import{ Anthropic }from"@anthropic-ai/sdk";const originalSDK =newAnthropic();const sdkWithTracing =wrapSDK(originalSDK);const response =await sdkWithTracing.messages.create({ messages:[{   role:"user",   content:`What is 1 + 1? Respond only with "2" and nothing else.`,},], model:"claude-3-sonnet-20240229", max_tokens:1024,});
```

tip
[Click here](https://smith.langchain.com/public/0e7248af-bbed-47cf-be9f-5967fea1dec1/r) to see an example LangSmith trace of the above.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/tracing/integrations/typescript%3E).
[PreviousPython](https://docs.smith.langchain.com/old/tracing/integrations/python)[NextUse Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [LangChain.js](https://docs.smith.langchain.com/old/tracing/integrations/typescript#langchainjs)
  * [OpenAI SDK](https://docs.smith.langchain.com/old/tracing/integrations/typescript#openai-sdk)
    * [Nested tracing](https://docs.smith.langchain.com/old/tracing/integrations/typescript#nested-tracing)
  * [Next.js](https://docs.smith.langchain.com/old/tracing/integrations/typescript#nextjs)
  * [Vercel AI SDK](https://docs.smith.langchain.com/old/tracing/integrations/typescript#vercel-ai-sdk)
  * [Arbitrary SDKs](https://docs.smith.langchain.com/old/tracing/integrations/typescript#arbitrary-sdks)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/quick_start

[Skip to main content](https://docs.smith.langchain.com/old/tracing/quick_start#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/quick_start)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * Quick Start


# Tracing Quick Start
You can get started with LangSmith tracing using either LangChain, the Python SDK, the TypeScript SDK, or the API. The following sections provide a quick start guide for each of these options.
First, create an API key by navigating to the [settings page](https://smith.langchain.com/settings), then follow the instructions below:
  * Python SDK
  * TypeScript SDK
  * LangChain
  * API


## 1. Install the LangSmith library
Start by installing the Python library.
  * Shell


```
pip install langsmith
```

## 2. Configure your environment
  * Shell


```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key># This example uses OpenAI, but you can use any LLM provider of choiceexport OPENAI_API_KEY=<your-openai-api-key>
```

## 3. Log a trace
We provide multiple ways to log traces to LangSmith. Below, we'll highlight how to use our simple `@traceable` decorator. See more in the [Integrations](https://docs.smith.langchain.com/old/tracing/integrations) section.
```
import openaifrom langsmith import wrappers, traceable# Auto-trace LLM calls in-contextclient = wrappers.wrap_openai(openai.Client())@traceable# Auto-trace this functiondefpipeline(user_input:str):  result = client.chat.completions.create(    messages=[{"role":"user","content": user_input}],    model="gpt-4o-mini")return result.choices[0].message.contentpipeline("Hello, world!")# Out: Hello there! How can I assist you today?
```

## 4. View the trace
By default, the trace will be logged to the project with the name `default`. You can change the project you log to by following the instructions 
[here](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#logging-to-a-specific-project)
. An example of a trace logged using the above code is made public and can be viewed [ here ](https://smith.langchain.com/public/279036f1-a06c-487a-bf74-d5d72af5fd9f/r).
## 1. Install the LangSmith library
Start by installing the TypeScript library.
  * npm
  * yarn
  * pnpm
  * bun


```
npm install langsmith
```

```
yarn add langsmith
```

```
pnpm add langsmith
```

```
bun add langsmith
```

## 2. Configure your environment
  * Shell


```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key># This example uses OpenAI, but you can use any LLM provider of choiceexport OPENAI_API_KEY=<your-openai-api-key>
```

## 3. Log a Trace
We provide multiple ways to log traces to LangSmith. Below, we'll highlight how to use our simple `traceable` higher order function (HOF). See more in the [Integrations](https://docs.smith.langchain.com/old/tracing/integrations) section.
```
import{ OpenAI }from"openai";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers";// Auto-trace LLM calls in-contextconst client =wrapOpenAI(newOpenAI());// Auto-trace this functionconst pipeline =traceable(async(user_input)=>{const result =await client.chat.completions.create({    messages:[{ role:"user", content: user_input }],    model:"gpt-4o-mini",});return result.choices[0].message.content;});awaitpipeline("Hello, world!")// Out: Hello there! How can I assist you today?
```

## 4. View the trace
By default, the trace will be logged to the project with the name `default`. You can change the project you log to by following the instructions 
[here](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#logging-to-a-specific-project)
. An example of a trace logged using the above code is made public and can be viewed [ here ](https://smith.langchain.com/public/90baed21-d367-442b-9448-5066a3fe71ab/r).
## 1. Install or upgrade LangChain
  * pip
  * yarn
  * npm
  * pnpm


```
pip install langchain_openai langchain_core
```

```
yarn add @langchain/openai @langchain/core
```

```
npm install @langchain/openai @langchain/core
```

```
pnpm add @langchain/openai @langchain/core
```

## 2. Configure your environment
  * Shell


```
export LANGSMITH_TRACING=true     export LANGSMITH_API_KEY=<your-api-key>     # This example uses OpenAI, but you can use any LLM provider of choice     export OPENAI_API_KEY=<your-openai-api-key>
```

## 3. Log a trace
No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would.
  * Python
  * TypeScript


```
from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([("system","You are a helpful assistant. Please respond to the user's request only based on the given context."),("user","Question: {question}\nContext: {context}")])model = ChatOpenAI(model="gpt-4o-mini")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion ="Can you summarize this morning's meetings?"context ="During this morning's meeting, we solved all world conflict."chain.invoke({"question": question,"context": context})
```

```
import{ ChatOpenAI }from"@langchain/openai";import{ ChatPromptTemplate }from"@langchain/core/prompts";import{ StringOutputParser }from"@langchain/core/output_parsers";const prompt = ChatPromptTemplate.fromMessages([["system","You are a helpful assistant. Please respond to the user's request only based on the given context."],["user","Question: {question}\nContext: {context}"],]);const model =newChatOpenAI({ modelName:"gpt-4o-mini"});const outputParser =newStringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question ="Can you summarize this morning's meetings?"const context ="During this morning's meeting, we solved all world conflict."await chain.invoke({ question: question, context: context });
```

## 4. View the trace
By default, the trace will be logged to the project with the name `default`. You can change the project you log to by following the instructions 
[here](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#logging-to-a-specific-project)
. An example of a trace logged using the above code is made public and can be viewed [ here ](https://smith.langchain.com/public/e6a46eb2-d785-4804-a1e3-23f167a04300/r).
## 1. Log a trace
Log a trace using the [LangSmith API](https://api.smith.langchain.com/redoc).
Here, we'll show you to use the `requests` library in Python to log a trace, but you can use any HTTP client in any language.
```
import openaiimport requestsfrom datetime import datetimefrom uuid import uuid4defpost_run(run_id, name, run_type, inputs, parent_id=None):"""Function to post a new run to the API."""  data ={"id": run_id.hex,"name": name,"run_type": run_type,"inputs": inputs,"start_time": datetime.utcnow().isoformat(),}if parent_id:    data["parent_run_id"]= parent_id.hex  requests.post("https://api.smith.langchain.com/runs",    json=data,    headers=headers)defpatch_run(run_id, outputs):"""Function to patch a run with outputs."""  requests.patch(f"https://api.smith.langchain.com/runs/{run_id}",    json={"outputs": outputs,"end_time": datetime.utcnow().isoformat(),},    headers=headers,)# Send your API Key in the request headersheaders ={"x-api-key":"<YOUR API KEY>"}# This can be a user input to your appquestion ="Can you summarize this morning's meetings?"# This can be retrieved in a retrieval stepcontext ="During this morning's meeting, we solved all world conflict."messages =[{"role":"system","content":"You are a helpful assistant. Please respond to the user's request only based on the given context."},{"role":"user","content":f"Question: {question}\nContext: {context}"}]# Create parent runparent_run_id = uuid4()post_run(parent_run_id,"Chat Pipeline","chain",{"question": question})# Create child runchild_run_id = uuid4()post_run(child_run_id,"OpenAI Call","llm",{"messages": messages}, parent_run_id)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(model="gpt-4o-mini", messages=messages)# End runspatch_run(child_run_id, chat_completion.dict())patch_run(parent_run_id,{"answer": chat_completion.choices[0].message.content})
```

## 2. View the trace
By default, the trace will be logged to the project with the name `default`. You can change the project you log to by following the instructions 
[here](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#logging-to-a-specific-project)
. An example of a trace logged using the above code is made public and can be viewed [ here ](https://smith.langchain.com/public/c0fcc401-f163-4546-aad8-1641e9fd6e91/r).
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousOverview](https://docs.smith.langchain.com/old/tracing)[NextConcepts](https://docs.smith.langchain.com/old/tracing/concepts)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/use_cases

[Skip to main content](https://docs.smith.langchain.com/old/tracing/use_cases#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/use_cases)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
      * [Monitor application sentiment](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment)
      * [Summarize app usage](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage)
      * [Few-shot prompting with LangSmith datasets](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * Use Cases


# Use Cases Guides
The following guides are provided to serve as examples for how you can use LangSmith's tracing capabilities to answer common questions about your application. These are not meant to be exhaustive, nor are they optimized for your use case. They are meant as a reference to help you get started.
## [üìÑÔ∏è Monitor application sentimentIn this guide, you will use create an evaluator to predict the sentiment of user queries in your production application. This technique can be flexibly applied to traced runs to add additional measurements to your unstructured data.](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment)## [üìÑÔ∏è Summarize app usageMost usage of LLM applications is in the form of unstructured data. LangChain can be used to](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage)## [üìÑÔ∏è Few-shot prompting with LangSmith datasetsDatasets are useful for more than just testing, evaluation, and fine-tuning. They can also be used to curate examples for few-shot "learning."](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/tracing/use_cases%3E).
[PreviousTypeScript](https://docs.smith.langchain.com/old/tracing/integrations/typescript)[NextMonitor application sentiment](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets

[Skip to main content](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
      * [Monitor application sentiment](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment)
      * [Summarize app usage](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage)
      * [Few-shot prompting with LangSmith datasets](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * Few-shot prompting with LangSmith datasets


On this page
# Few-shot prompting with LangSmith datasets
Datasets are useful for more than just testing, evaluation, and fine-tuning. They can also be used to curate examples for few-shot "learning." The overall flow for this looks something like:
  * Capture samples by tracing a chain or LLM, either in a live deployment or on example data.
  * Filter samples based on user feedback or other custom criteria (did/didn't error, AI-assisted evaluation, etc.)
  * Create dataset(s) from filtered samples.
  * Use within ExampleSelectors and FewShotPromptTemplates to improve in-context learning.
  * Repeat :) This can help improve the quality of the model, permit usage of cheaper models, and more. As a motivating example, we will show how we can use a few examples from a chain powered by OpenAI's `gpt4` model to improve the quality of a "history Q&A" bot powered by the local `GPT4All` model.


## Setup[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#setup "Direct link to Setup")
First, we will need to install LangChain and GPT4All
```
pip install -U langchain langchain_openai langsmith gpt4all
```

Then configure your environment:
```
export LANGCHAIN_API_KEY=<your key>export LANGCHAIN_TRACING_V2_ENABLED=trueexport OPENAI_API_KEY=<your key>
```

## Step 1: Trace Prototype Model[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#step-1-trace-prototype-model "Direct link to Step 1: Trace Prototype Model")
Run your chain prototype to collect example traces. This data could come from a live deployment, staging environment, prototyping dataset, or any other source. If you already have some traced data you believe to be good candidates for few-shot prompting, you can skip this step. To get started quickly, we will use some example questions we want to ask our chain.
#### Create a dataset[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#create-a-dataset "Direct link to Create a dataset")
The 'History of Flanders' dataset here is used mainly for convenience. We use it in the `evaluate` method to quickly capture traces. You can also select runs from any project for the few-shot dataset.
```
from langsmith import Clientclient = Client()questions =["When and where was the printing press invented by Johannes Gutenberg?","What were the most significant contributions of Flemish painters of the Renaissance?","What are the main characteristics of the Flemish language?","What were the main causes and consequences of the Flemish Revolt?","What are some of the most important works of Flemish literature in the 19th and 20th centuries?","What are some of the most important films produced in Flanders in the 20th century?","What were the key goals of the Flemish Movement?","What are the main responsibilities of the Flemish Community?","What are the main drivers of the Flemish economy in the 21st century?","What are the main arguments for and against Flemish independence?","What are the main challenges facing Flanders in the 21st century?","How did the invention of the printing press by Johannes Gutenberg impact Flemish history?","How did the development of Flemish painting in the 15th and 16th centuries impact Flemish history?","How did the development of the Flemish language impact Flemish history?","How did the Flemish Revolt (1568-1609) impact Flemish history?","How did the development of Flemish literature in the 19th and 20th centuries impact Flemish history?","How did the development of Flemish cinema in the 20th century impact Flemish history?","How did the development of the Flemish Movement (19th-20th centuries) impact Flemish history?","How did the creation of the Flemish Community (1970) impact Flemish history?","How did the development of the Flemish economy in the 21st century impact Flemish history?","How did the rise of Flemish nationalism in the 21st century impact Flemish history?",]shared_dataset_name ="History of Flanders"ds = client.create_dataset(  dataset_name=shared_dataset_name, description="Some questions about Flanders",)client.create_examples(  inputs=[{"input": q}for q in questions], dataset_id=ds.id)
```

#### Run chain over dataset[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#run-chain-over-dataset "Direct link to Run chain over dataset")
We will use a gpt-4 powered LLMChain and the `evaluate` method to kick things off.
```
from langchain_openai import ChatOpenAIfrom langchain_core.prompts import PromptTemplatefrom langsmith.evaluation import evaluatedefprototype_predict(inputs):  llm = ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.0)  prompt = PromptTemplate.from_template(    template="Help out as best you can.\nQuestion: {input}\nResponse: ",)  chain = prompt | llmreturn chain.invoke({"input": inputs["input"]})prototype_project_name ="History Prototype Test"prototype_results = evaluate(  prototype_predict,  data=shared_dataset_name,  experiment_prefix=prototype_project_name,)
```

## Step 2: Create Few-Shot Dataset[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#step-2-create-few-shot-dataset "Direct link to Step 2: Create Few-Shot Dataset")
The traces you've captured can be used for few-shot example prompting! Below, add the inputs and outputs from the traced runs to a dataset. You can review the data in the web app to delete or edit examples you don't like. You could also add or filter by feedback to make sure the dataset(s) capture the style you want to use. For instance, you may want to put good and bad examples in separate datasets and include examples of both in your final prompt.
```
few_shot_dataset_name ="History Few Shot Dataset"few_shot_dataset = client.create_dataset(few_shot_dataset_name)runs = client.list_runs(  project_name=prototype_results.experiment_name,  run_type="chain",)for run in runs:  client.create_example_from_run(run, dataset_id=few_shot_dataset.id)
```

## Step 3: Establish Baseline[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#step-3-establish-baseline "Direct link to Step 3: Establish Baseline")
We're about ready to try out our few-shot prompting model. But first, we want to establish a baseline. Otherwise, we won't be able to tell if it actually helped! Let's take Nomic's GPT4All model to power a private, local history QA bot. To see how well it does, we will create a "development dataset" to see its responses. Once we have a baseline, we can try changing the prompts, models, or other parameters to get better results.
```
dev_questions =["What was the significance of the adoption of Christianity in Armenia in 301 AD?","Who was responsible for the creation of the Armenian Alphabet in 405 AD and why was it significant?","Can you describe some of the major accomplishments during the Golden Age of Armenian Art and Literature?","What is unique about the architecture of the Zvartnots Cathedral in Armenia?","Who wrote 'The Knight in the Panther's Skin' and why is it considered a masterpiece of Georgian literature?","What cultural developments took place in Georgia during the rule of Queen Tamar?","Can you describe the style and significance of Armenian Miniature Painting developed in the 13th to 14th centuries?","What are some key characteristics of the Georgian Renaissance in terms of architecture and arts?","What makes the Svetitskhoveli Cathedral an iconic symbol of Georgian architectural style?","How did the Mkhitarist Order contribute to the preservation of Armenian culture and literature?","How did the creation of the Armenian alphabet impact the cultural and literary development of Armenia?","What factors contributed to the Golden Age of Armenian Art and Literature?","What was the cultural and religious significance of the Zvartnots Cathedral in Armenia?","How did 'The Knight in the Panther's Skin' reflect Georgian cultural values and beliefs?","What kind of cultural and artistic advancements were made during the rule of Queen Tamar in Georgia?","What is the significance of Armenian Miniature Painting in the context of medieval art?","How did the Georgian Renaissance influence subsequent architectural and artistic styles in Georgia?","What are some of the key architectural features of the Svetitskhoveli Cathedral in Georgia?","How did the establishment of the Mkhitarist Order impact Armenian diaspora communities in Europe?","How did the religious and cultural changes in Armenia and Georgia from the 4th to the 18th centuries influence their respective art and architecture?"]dev_dataset_name ="History Dev Set"dev_dataset = client.create_dataset(  dataset_name=dev_dataset_name, description="Some history questions.",)client.create_examples(inputs=[{"input": q}for q in dev_questions], dataset_id=dev_dataset.id)
```

#### Define the chain to benchmark[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#define-the-chain-to-benchmark "Direct link to Define the chain to benchmark")
```
from langchain_community.llms import GPT4Allfrom langchain_core.prompts import PromptTemplategpt4all_model = GPT4All(model="orca-mini-3b.ggmlv3.q4_0.bin")base_prompt = PromptTemplate.from_template(  template="Help out as best you can.\nQuestion: {input}\nResponse: ",)defbenchmark_predict(inputs):return(base_prompt | gpt4all_model).invoke(inputs)
```

#### Run the chain[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#run-the-chain "Direct link to Run the chain")
Run the chain on the dev dataset and inspect the results. Define some AI-assisted evaluators to give some visibility on the quality here.
```
from langsmith.evaluation import LangChainStringEvaluatorfrom langsmith.schemas import Example, Rundeflabeled_criteria_data(run: Run, example: Example):return{"prediction": run.outputs["text"],"input": example.inputs["input"],}helpfulness_evaluator = LangChainStringEvaluator("labeled_criteria",  config={"criteria":"helpfulness"},  prepare_data=labeled_criteria_data,)completeness_evaluator = LangChainStringEvaluator("labeled_criteria",  config={"criteria":"Is the submission complete, providing adequate depth in its answer?"},  prepare_data=labeled_criteria_data,)original_dev_res = evaluate(  benchmark_predict,  data=dev_dataset_name,  evaluators=[helpfulness_evaluator, completeness_evaluator],  experiment_prefix="zero-shot chain test",)
```

You can view some of the evaluation feedback by reading the project.
```
# More details are visible in the web app, but you can see feedback stats# directly using the SDKoriginal_dev_res.aggregate_feedback
```

## Step 4: Create Few-Shot Example Model[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#step-4-create-few-shot-example-model "Direct link to Step 4: Create Few-Shot Example Model")
That did reasonably well, but I think we can do better. We liked the style of response provided by the prototype model and want to achieve a similar style with the local example. We will accomplish this with a few-shot example selector. In the example below, fetch the examples from the few-shot dataset created previously and add them to a vector store.
```
from langchain_core.prompts import SemanticSimilarityExampleSelectorfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.vectorstores import Chromaembeddings = OpenAIEmbeddings()examples =[{"input": example.inputs["input"],"output": example.outputs["text"]}for example in  client.list_examples(dataset_name=few_shot_dataset_name)]to_vectorize =[" ".join(example.values())for example in examples]vectorstore = Chroma.from_texts(  to_vectorize, embeddings, metadatas=examples)example_selector = SemanticSimilarityExampleSelector(  vectorstore=vectorstore)
```

**The[FewShotPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.few_shot.FewShotPromptTemplate.html#langchain.prompts.few_shot.FewShotPromptTemplate) is used to inject examples into the prompt using the example_selector created above.** In this case, we used a SemanticSimilarityExampleSelector, which returns examples that are the most 'similar' based on vector similarity. The overall flow looks something like:
  * An input is passed to the LLMChain.
  * The input is passed from the FewShotPromptTemplate to the example_selector, which returns the top N most 'similar' examples.
  * The `example_prompt` is used to format each selected example.
  * The selected examples are inserted between the prefix and suffix to form the final prompt.


```
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate# Define how each selected example will be formatted# when inserted into the whole promptexample_prompt = PromptTemplate(  input_variables=["input","output"],  template="Question: {input}\nResponse: {output}\n\n",)# Define the overall prompt.few_shot_prompt = FewShotPromptTemplate(  example_selector=example_selector,  example_prompt=example_prompt,  prefix="Help out as best you can.\n",  suffix="Question: {input}\nResponse: ",  input_variables=["input"])
```

#### Benchmark the few-shot model[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#benchmark-the-few-shot-model "Direct link to Benchmark the few-shot model")
Now we can run the chain again, this time using the few-shot prompt template. We will use the same evaluators as before to compare the results.
```
deffewshot_predict(inputs):return(few_shot_prompt | gpt4all_model).invoke(inputs)few_shot_dev_res = evaluate(  fewshot_predict,  data=dev_dataset_name,  evaluators=[helpfulness_evaluator, completeness_evaluator],  experiment_prefix="few-shot chain test",)
```

#### Compare results[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#compare-results "Direct link to Compare results")
Both the helpfulness and completeness of the responses improved!
```
few_shot_dev_res.aggregate_feedback
```

## Review[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#review "Direct link to Review")
In this example, we used LangSmith datasets to curate examples and connected them to a few-shot example selector to improve the quality of the prompt used with a smaller local model. This tactic can be extended in many ways to help improve the quality, style, API awareness, and other characteristics of your chain or agent without having to fine-tune the underlying LLM weights.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/tracing/use_cases/few-shot-datasets%3E).
[PreviousSummarize app usage](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage)[NextOverview](https://docs.smith.langchain.com/old/evaluation)
  * [Setup](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#setup)
  * [Step 1: Trace Prototype Model](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#step-1-trace-prototype-model)
    * [Create a dataset](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#create-a-dataset)
    * [Run chain over dataset](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#run-chain-over-dataset)
  * [Step 2: Create Few-Shot Dataset](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#step-2-create-few-shot-dataset)
  * [Step 3: Establish Baseline](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#step-3-establish-baseline)
    * [Define the chain to benchmark](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#define-the-chain-to-benchmark)
    * [Run the chain](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#run-the-chain)
  * [Step 4: Create Few-Shot Example Model](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#step-4-create-few-shot-example-model)
    * [Benchmark the few-shot model](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#benchmark-the-few-shot-model)
    * [Compare results](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#compare-results)
  * [Review](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets#review)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage

[Skip to main content](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
      * [Monitor application sentiment](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment)
      * [Summarize app usage](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage)
      * [Few-shot prompting with LangSmith datasets](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * Summarize app usage


On this page
# Summarize App Usage
Most usage of LLM applications is in the form of unstructured data. LangChain can be used to cluster and summarize how people are using an application so you can build a better product.
In this example, you will perform simple k-means clustering and an LLM call to summarize logged user activity.
## Prerequisites[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage#prerequisites "Direct link to Prerequisites")
This example assumes you've configured your environment to connect to LangSmith.
```
export LANGCHAIN_API_KEY=<your api key>
```

It also uses OpenAI for the embeddings and Anthropic to run the LLM. You can replace these with your models of choice or install the following:
```
pip install -U langchain openai anthropicexport OPENAI_API_KEY=<your api key>export ANTHROPIC_API_KEY=<your api key>
```

## Step 1: Sample recent runs[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage#step-1-sample-recent-runs "Direct link to Step 1: Sample recent runs")
For this example, we will randomly sample from the past day's runs. You can take a much larger sample, depending on your needs.
```
import randomfrom datetime import datetime, timedeltafrom langsmith import Client# Update these values to match your project# and run schema.project_name ="my-project"input_key ="question"max_to_analyze =10_000yesterday = datetime.now()- timedelta(days=1)client = Client()runs =list(  client.list_runs(    project_name=project_name,    start_time=yesterday,    execution_order=1,# Ignore child runs))inputs =[str(run.inputs[input_key])for run in runs if input_key in run.inputs]sampled = inputsiflen(inputs)> max_to_analyze:  sampled = random.sample(sampled, max_to_analyze)
```

## Step 2: Embed the inputs for clustering[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage#step-2-embed-the-inputs-for-clustering "Direct link to Step 2: Embed the inputs for clustering")
Embeddings provide vector representations of text that we can use to cluster semantically similar inputs.
```
import numpy as npfrom langchain.embeddings import OpenAIEmbeddingsembedder = OpenAIEmbeddings()embeddings = embedder.embed_documents(sampled)arr = np.stack(embeddings)
```

Now, cluster the embeddings:
```
from sklearn.cluster import KMeans# Reduce n_clusters in case you're trying this walkthrough out on a small projectn_clusters =min(10,len(sampled)//4)kmeans = KMeans(n_clusters=n_clusters, n_init='auto', init="k-means++", random_state=42)kmeans.fit(arr)
```

## Step 3: Summarize each cluster[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage#step-3-summarize-each-cluster "Direct link to Step 3: Summarize each cluster")
Now it's time to produce the summaries! We will use Claude-2 using the `ChatAnthropic` model with a 100k token context window. You can check out the prompt we're using [on the hub](https://smith.langchain.com/hub/wfh/summarize_logs).
```
import anthropicfrom langchain import hubfrom langchain.chat_models import ChatAnthropicfrom langchain.schema.output_parser import StrOutputParserdef_truncate_inputs(input_logs:list)->str:  max_prompt_tokens =80_000# Truncate to max_prompt_tokens  tokenizer = anthropic.Anthropic().get_tokenizer()  inputs ="\n\n".join(input_logs)  truncated_ids = tokenizer.encode(inputs).ids[:max_prompt_tokens]return{"logs": tokenizer.decode(truncated_ids)}prompt = hub.pull("wfh/summarize_logs")chain =(  _truncate_inputs| prompt| ChatAnthropic(    model="claude-2",    temperature=1,    max_tokens_to_sample=1000,)| StrOutputParser())input_arr = np.array(sampled)batch_inputs =[  input_arr[kmeans.labels_ == i]for i inrange(n_clusters)]summaries = chain.batch(batch_inputs,{"max_concurrency":2})for summary in summaries:print(summary)
```

Congratulations! You have now clustered and summarized user activity for an application.
This is just a simple example of how to use LangChain to analyze your application logs. You will likely want to combine this with other techniques, such as [run filtering](https://docs.smith.langchain.com/old/tracing/faq/querying_traces) to select relevant examples. You can also update the prompt to ask more targed questions or to provide additional context.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/tracing/use_cases/summarize-usage%3E).
[PreviousMonitor application sentiment](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment)[NextFew-shot prompting with LangSmith datasets](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets)
  * [Prerequisites](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage#prerequisites)
  * [Step 1: Sample recent runs](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage#step-1-sample-recent-runs)
  * [Step 2: Embed the inputs for clustering](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage#step-2-embed-the-inputs-for-clustering)
  * [Step 3: Summarize each cluster](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage#step-3-summarize-each-cluster)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment

[Skip to main content](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
    * [Quick Start](https://docs.smith.langchain.com/old/tracing/quick_start)
    * [Concepts](https://docs.smith.langchain.com/old/tracing/concepts)
    * [How-To Guides](https://docs.smith.langchain.com/old/tracing/faq)
    * [Integrations](https://docs.smith.langchain.com/old/tracing/integrations)
    * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
      * [Monitor application sentiment](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment)
      * [Summarize app usage](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage)
      * [Few-shot prompting with LangSmith datasets](https://docs.smith.langchain.com/old/tracing/use_cases/few-shot-datasets)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Use Cases](https://docs.smith.langchain.com/old/tracing/use_cases)
  * Monitor application sentiment


On this page
# Monitor Application Sentiment
In this guide, you will use create an evaluator to predict the sentiment of user queries in your production application. This technique can be flexibly applied to traced runs to add additional measurements to your unstructured data.
Bias in Sentiment Analysis
Predicting sentiment from text has known [biases](https://arxiv.org/abs/1805.04508) and limitations. It's important to understand the implications of this before deciding whether it's appropriate to use sentiment and other NLP metrics in instrumenting your application.
## Prerequisites[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment#prerequisites "Direct link to Prerequisites")
This guide assumes you have already deployed an LLM application and are logging traces in a monitoring project. While all the steps should work in a debug project as well, the signal is likely to be less useful.
## Step 1: Define Evaluator[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment#step-1-define-evaluator "Direct link to Step 1: Define Evaluator")
You will be using an LLMChain to do the sentiment classification on the run's inputs. We will use a GPT-3.5 turbo model here, but you can use any model you like.
```
from langsmith.evaluation import EvaluationResult, RunEvaluatorfrom langsmith.schemas import Example, Runfrom langchain.chains import LLMChainfrom langchain_openai import ChatOpenAIclassSentimentEvaluator(RunEvaluator):def__init__(self):    prompt ="""Is the predominant sentiment in the following statement positive, negative, or neutral?---------Statement: {input}---------Respond in one word: positive, negative, or neutral.Sentiment:"""    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.0)    self.chain = LLMChain.from_string(llm=llm, template=prompt)defevaluate_run(self, run: Run, example: Example)-> EvaluationResult:    input_str =str(list(run.inputs.values())[0])    prediction = self.chain.run(input_str)# Strip the prompt    prediction = prediction.strip()    score ={"positive":1,"negative":-1,"neutral":0}.get(prediction)return EvaluationResult(      key="sentiment",      value=prediction,      score=score,)
```

## Step 2: Evaluate[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment#step-2-evaluate "Direct link to Step 2: Evaluate")
Now that you've defined your evaluator, use it to predict the sentiment of run inputs. You can do this by iterating over the runs in your project and evaluating each run with the object you just defined.
```
from langsmith import Clientclient = Client()evaluator = SentimentEvaluator()for run in client.list_runs(  project_name="my-project",  execution_order=1,# Do not return child / nested runs):    client.evaluate_run(run, evaluator)
```

## Step 3: View aggregate feedback[‚Äã](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment#step-3-view-aggregate-feedback "Direct link to Step 3: View aggregate feedback")
The sentiment predictions are logged as feedback and can be viewed in the application UI. You can view the aggregate sentiment scores over the project and filter by feedback values.
You can use the client to filter and export run and feedback information.
```
# Get the aggregate stats for the projectproject = client.read_project(project_name="my-project")aggregate_feedback = project.feedback_stats# Get feedback per runrowwise_feedback =[  run.feedback_statsfor run in client.list_runs(    project_name="my-project",    execution_order=1,)]# Get runs that were scored as negativenegative_runs = client.list_runs(  project_name="my-project",  query="and(eq(feedback_key, "sentiment"), eq(feedback_score, -1))",)
```

If you use [tags or metadata](https://docs.smith.langchain.com/old/tracing/faq/customizing_trace_attributes#adding-metadata-and-tags-to-traces) to organize runs within a project, you can use these for subset and cohort analysis.
```
# Get feedback for runs# with the tag "my-tag"rowwise_feedback =[  run.feedback_statsfor run in client.list_runs(    project_name="my-project",    execution_order=1,    query='has(tags, "my-tag")')]
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/old/tracing/use_cases/track-sentiment%3E).
[PreviousUse Cases](https://docs.smith.langchain.com/old/tracing/use_cases)[NextSummarize app usage](https://docs.smith.langchain.com/old/tracing/use_cases/summarize-usage)
  * [Prerequisites](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment#prerequisites)
  * [Step 1: Define Evaluator](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment#step-1-define-evaluator)
  * [Step 2: Evaluate](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment#step-2-evaluate)
  * [Step 3: View aggregate feedback](https://docs.smith.langchain.com/old/tracing/use_cases/track-sentiment#step-3-view-aggregate-feedback)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/old/user_guide

[Skip to main content](https://docs.smith.langchain.com/old/user_guide#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/old/user_guide)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Quick Start](https://docs.smith.langchain.com/old)
  * [User Guide](https://docs.smith.langchain.com/old/user_guide)
  * [Tracing](https://docs.smith.langchain.com/old/tracing)
  * [Evaluation](https://docs.smith.langchain.com/old/evaluation)
  * [Production Monitoring & Automations](https://docs.smith.langchain.com/old/monitoring)
  * [Prompt Hub](https://docs.smith.langchain.com/old/category/prompt-hub)
  * [Proxy](https://docs.smith.langchain.com/old/category/proxy)
  * [Pricing](https://docs.smith.langchain.com/old/pricing)
  * [Self-Hosting](https://docs.smith.langchain.com/old/category/self-hosting)


This is outdated documentation for ü¶úÔ∏èüõ†Ô∏è LangSmith, which is no longer actively maintained.
For up-to-date documentation, see the **[latest version](https://docs.smith.langchain.com/)**.
  * [](https://docs.smith.langchain.com/)
  * User Guide


On this page
# LangSmith User Guide
LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.
![Lifecycle](https://docs.smith.langchain.com/assets/images/LangSmith_Diagram-GA-final-1261e0cb2b8337f0a2c8732482957deb.png)
## Prototyping[‚Äã](https://docs.smith.langchain.com/old/user_guide#prototyping "Direct link to Prototyping")
Prototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters. The ability to rapidly understand how the model is performing ‚Äî and debug where it is failing ‚Äî is incredibly important for this phase.
#### Debugging[‚Äã](https://docs.smith.langchain.com/old/user_guide#debugging "Direct link to Debugging")
When developing new LLM applications, we suggest having LangSmith tracing enabled by default. Oftentimes, it isn‚Äôt necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it‚Äôs extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues. We provide native rendering of chat messages, functions, and retrieve documents.
#### Initial Test Set[‚Äã](https://docs.smith.langchain.com/old/user_guide#initial-test-set "Direct link to Initial Test Set")
While many developers still ship an initial version of their application based on ‚Äúvibe checks‚Äù, we‚Äôve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications. These test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.
#### Comparison View[‚Äã](https://docs.smith.langchain.com/old/user_guide#comparison-view "Direct link to Comparison View")
When prototyping different versions of your applications and making changes, it‚Äôs important to see whether or not you‚Äôve regressed with respect to your initial test cases. Oftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application. In order to get a sense for which variant is performing better, it‚Äôs useful to be able to view results for different configurations on the same datapoints side-by-side. We‚Äôve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.
#### Playground[‚Äã](https://docs.smith.langchain.com/old/user_guide#playground "Direct link to Playground")
LangSmith provides a playground environment for rapid iteration and experimentation. This allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace. Every playground run is logged in the system and can be used to create test cases or compare with other runs.
## Beta Testing[‚Äã](https://docs.smith.langchain.com/old/user_guide#beta-testing "Direct link to Beta Testing")
Beta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it‚Äôs important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it‚Äôs breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.
#### Capturing Feedback[‚Äã](https://docs.smith.langchain.com/old/user_guide#capturing-feedback "Direct link to Capturing Feedback")
When launching your application to an initial set of users, it‚Äôs important to gather human feedback on the responses it‚Äôs producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.
#### Annotating Traces[‚Äã](https://docs.smith.langchain.com/old/user_guide#annotating-traces "Direct link to Annotating Traces")
LangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.
#### Adding Runs to a Dataset[‚Äã](https://docs.smith.langchain.com/old/user_guide#adding-runs-to-a-dataset "Direct link to Adding Runs to a Dataset")
As your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.
## Production[‚Äã](https://docs.smith.langchain.com/old/user_guide#production "Direct link to Production")
Closely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you‚Äôll also want to do once your app hits production.
However, especially at the production stage, it‚Äôs crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.
Online evaluations and automations allow you to process and score production traces in near real-time.
Additionally, threads provide a seamless way to group traces from a single conversation, making it easier to track the performance of your application across multiple turns.
#### Monitoring and A/B Testing[‚Äã](https://docs.smith.langchain.com/old/user_guide#monitoring-and-ab-testing "Direct link to Monitoring and A/B Testing")
LangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period ‚Äî this is especially handy for debugging production issues.
LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.
#### Automations[‚Äã](https://docs.smith.langchain.com/old/user_guide#automations "Direct link to Automations")
Automations are a powerful feature in LangSmith that allow you to perform actions on traces in near real-time. This can be used to automatically score traces, send them to annotation queues, or send them to datasets.
To define an automation, simply provide a filter condition, a sampling rate, and an action to perform. Automations are particularly helpful for processing traces at production scale.
#### Threads[‚Äã](https://docs.smith.langchain.com/old/user_guide#threads "Direct link to Threads")
Many LLM applications are multi-turn, meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation together, making it easier to track the performance of and annotate your application across multiple turns.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousQuick Start](https://docs.smith.langchain.com/old)[NextOverview](https://docs.smith.langchain.com/old/tracing)
  * [Prototyping](https://docs.smith.langchain.com/old/user_guide#prototyping)
  * [Beta Testing](https://docs.smith.langchain.com/old/user_guide#beta-testing)
  * [Production](https://docs.smith.langchain.com/old/user_guide#production)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/concepts

[Skip to main content](https://docs.smith.langchain.com/administration/concepts#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/concepts)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * Conceptual Guide


On this page
# Concepts
This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith.
## Resource Hierarchy[‚Äã](https://docs.smith.langchain.com/administration/concepts#resource-hierarchy "Direct link to Resource Hierarchy")
### Organizations[‚Äã](https://docs.smith.langchain.com/administration/concepts#organizations "Direct link to Organizations")
An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the [setup guide](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization).
When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join. There are a few important differences between your personal organization and shared organizations:
Feature| Personal| Shared  
---|---|---  
Maximum workspaces| 1| Variable, depending on plan (see [pricing page](https://www.langchain.com/pricing-langsmith)  
Collaboration| Cannot invite users| Can invite users  
Billing: paid plans| Developer plan only| All other plans available  
### Workspaces[‚Äã](https://docs.smith.langchain.com/administration/concepts#workspaces "Direct link to Workspaces")
info
Workspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition.
A workspace is a logical grouping of users and resources within an organization. A workspace separates trust boundaries for resources and access control. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the [setup guide](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace).
It is recommended to create a separate workspace for each team within your organization. To organize resources even further, you can use [Resource Tags](https://docs.smith.langchain.com/administration/concepts#resource-tags) to group resources within a workspace.
The following image shows a sample workspace settings page: ![Sample Workspace](https://docs.smith.langchain.com/assets/images/sample_workspace-64231f08f21d5c5e0907940b11440196.png)
The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace:
See the table below for details on which features are available in which scope (organization or workspace):
Resource/Setting| Scope  
---|---  
Trace Projects| Workspace  
Annotation Queues| Workspace  
Deployments| Workspace  
Datasets & Experiments| Workspace  
Prompts| Workspace  
Resource Tags| Workspace  
API Keys| Workspace  
Settings including Secrets, Feedback config, Models, Rules, and Shared URLs| Workspace  
User management: Invite User to Workspace| Workspace  
RBAC: Assigning Workspace Roles| Workspace  
Data Retention, Usage Limits| Workspace*  
Plans and Billing, Credits, Invoices| Organization  
User management: Invite User to Organization| Organization**  
Adding Workspaces| Organization  
Assigning Organization Roles| Organization  
RBAC: Creating/Editing/Deleting Custom Roles| Organization  
* Data retention settings and usage limits will be available soon for the organization level as well ** Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag. See the [self-hosted user management docs](https://docs.smith.langchain.com/self_hosting/configuration/user_management) for details.
### Resource tags[‚Äã](https://docs.smith.langchain.com/administration/concepts#resource-tags "Direct link to Resource tags")
Resource tags allow you to organize resources within a workspaces. Each tag is a key-value pair that can be assigned to a resource. Tags can be used to filter workspace-scoped resources in the UI and API: Projects, Datasets, Annotation Queues, Deployments, and Experiments.
Each new workspace comes with two default tag keys: `Application` and `Environment`; as the names suggest, these tags can be used to categorize resources based on the application and environment they belong to. More tags can be added as needed.
LangSmith resource tags are very similar to tags in cloud services like [AWS](https://docs.aws.amazon.com/tag-editor/latest/userguide/tagging.html).
![Sample Resource Tags](https://docs.smith.langchain.com/assets/images/resource_tags-699b7da108a8d853cba974d38b25bc53.png)
## User Management and RBAC[‚Äã](https://docs.smith.langchain.com/administration/concepts#user-management-and-rbac "Direct link to User Management and RBAC")
### Users[‚Äã](https://docs.smith.langchain.com/administration/concepts#users "Direct link to Users")
A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations.
Organization members are managed in organization settings:
![Sample Organization Members](https://docs.smith.langchain.com/assets/images/org_members_settings-9752ee7ba61a60dffdf1c8e9cc2c6e4f.png)
And workspace members are managed in workspace settings:
![Sample Workspace Members](https://docs.smith.langchain.com/assets/images/org_settings_workspaces_tab-b9d1cae86681fcf118048a9a56a73911.png)
### API keys[‚Äã](https://docs.smith.langchain.com/administration/concepts#api-keys "Direct link to API keys")
Legacy Keys deprecated as of October 22, 2024
We ended support for legacy API keys prefixed with `ls__` on October 22, 2024 in favor of personal access tokens (PATs) and service keys. We require using PATs and service keys for all new integrations. API keys prefixed with `ls__` will no longer work as of October 22, 2024.
#### Personal Access Tokens (PATs)[‚Äã](https://docs.smith.langchain.com/administration/concepts#personal-access-tokens-pats "Direct link to Personal Access Tokens \(PATs\)")
Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it.
PATs are prefixed with `lsv2_pt_`
#### Service keys[‚Äã](https://docs.smith.langchain.com/administration/concepts#service-keys "Direct link to Service keys")
Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account.
Service keys are prefixed with `lsv2_sk_`
note
To see how to create a service key or Personal Access Token, see the [setup guide](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
### Organization roles[‚Äã](https://docs.smith.langchain.com/administration/concepts#organization-roles "Direct link to Organization roles")
Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple [workspaces](https://docs.smith.langchain.com/administration/concepts#workspaces). Your organization role determines your workspace membership characteristics and your organization-level permissions. See the [organization setup guide](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#organization-roles) for more information.
The organization role selected also impacts workspace membership as described here:
  * `Organization Admin` grants full access to manage all organization configuration, users, billing, and workspaces. **An`Organization Admin` has `Admin` access to all workspaces in an organization**
  * `Organization User` may read organization information but cannot execute any write actions at the organization level. **An`Organization User` can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.**


info
The `Organization User` role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are `Organization Admins`. Custom organization-scoped roles are not available yet.
See the table below for all organization permissions:
Organization User| Organization Admin  
---|---  
View organization configuration| ‚úÖ| ‚úÖ  
View organization roles| ‚úÖ| ‚úÖ  
View organization members| ‚úÖ| ‚úÖ  
View data retention settings| ‚úÖ| ‚úÖ  
View usage limits| ‚úÖ| ‚úÖ  
Admin access to all workspaces| ‚úÖ  
Manage billing settings| ‚úÖ  
Create workspaces| ‚úÖ  
Create, edit, and delete organization roles| ‚úÖ  
Invite new users to organization| ‚úÖ  
Delete user invites| ‚úÖ  
Remove users from an organization| ‚úÖ  
Update data retention settings*| ‚úÖ  
Update usage limits*| ‚úÖ  
### Workspace roles (RBAC)[‚Äã](https://docs.smith.langchain.com/administration/concepts#workspace-roles "Direct link to Workspace roles \(RBAC\)")
note
RBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev Other plans default to using the Admin role for all users.
Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited:
  * `Admin` - has full access to all resources within the workspace
  * `Viewer` - has read-only access to all resources within the workspace
  * `Editor` - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys)


Organization admins can also create/edit custom roles with specific permissions for different resources.
Roles can be managed in organization settings under the `Roles` tab:
![Roles](https://docs.smith.langchain.com/assets/images/roles_tab_rbac-6861d230dc1e19d14ddad970a80804f8.png)
For more details on assigning and creating roles, see the [access control setup guide](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control).
## Best Practices[‚Äã](https://docs.smith.langchain.com/administration/concepts#best-practices "Direct link to Best Practices")
### Environment Separation[‚Äã](https://docs.smith.langchain.com/administration/concepts#environment-separation "Direct link to Environment Separation")
Use [resource tags](https://docs.smith.langchain.com/administration/concepts#resource-tags) to organize resources by environment using the default tag key `Environment` and different values for the environment (e.g. `dev`, `staging`, `prod`). This tagging structure will allow you to organize your tracing projects today and easily enforce permissions when we release attribute based access control (ABAC). ABAC on the resource tag will provide a fine-grained way to restrict access to production tracing projects, for example. We do not recommend that you use Workspaces for environment separation as you cannot share resources across Workspaces. If you would like to promote a prompt from `staging` to `prod`, we recommend you use prompt tags instead. See [docs](https://docs.smith.langchain.com/prompt_engineering/concepts#tags) for more information.
## Usage and Billing[‚Äã](https://docs.smith.langchain.com/administration/concepts#usage-and-billing "Direct link to Usage and Billing")
### Data Retention[‚Äã](https://docs.smith.langchain.com/administration/concepts#data-retention "Direct link to Data Retention")
In May 2024, LangSmith introduced a maximum data retention period on traces of 400 days. In June 2024, LangSmith introduced a new data retention based pricing model where customers can configure a shorter data retention period on traces in exchange for savings up to 10x. On this page, we'll go through how data retention works and is priced in LangSmith.
#### Why retention matters[‚Äã](https://docs.smith.langchain.com/administration/concepts#why-retention-matters "Direct link to Why retention matters")
  * **Privacy** : Many data privacy regulations, such as GDPR in Europe or CCPA in California, require organizations to delete personal data once it's no longer necessary for the purposes for which it was collected. Setting retention periods aids in compliance with such regulations.
  * **Cost** : LangSmith charges less for traces that have low data retention. See our tutorial on how to [optimize spend](https://docs.smith.langchain.com/administration/tutorials/manage_spend) for details.


#### How it works[‚Äã](https://docs.smith.langchain.com/administration/concepts#how-it-works "Direct link to How it works")
LangSmith now has two tiers of traces based on Data Retention with the following characteristics:
Base| Extended  
---|---  
**Price**|  $.50 / 1k traces| $5 / 1k traces  
**Retention Period**|  14 days| 400 days  
**Data deletion after retention ends**
After the specified retention period, traces are no longer accessible via the runs table or API. All user data associated with the trace (e.g. inputs and outputs) is deleted from our internal systems within a day thereafter. Some metadata associated with each trace may be retained indefinitely for analytics and billing purposes.
**Data retention auto-upgrades**
caution
Auto upgrades can have an impact on your bill. Please read this section carefully to fully understand your estimated LangSmith tracing costs.
When you use certain features with `base` tier traces, their data retention will be automatically upgraded to `extended` tier. This will increase both the retention period, and the cost of the trace.
The complete list of scenarios in which a trace will upgrade when:
  * **Feedback** is added to any run on the trace
  * An **Annotation Queue** receives any run from the trace
  * A **Run Rule** matches any run within a trace


**Why auto-upgrade traces?**
We have two reasons behind the auto-upgrade model for tracing:
  1. We think that traces that match any of these conditions are fundamentally more interesting than other traces, and therefore it is good for users to be able to keep them around longer.
  2. We philosophically want to charge customers an order of magnitude lower for traces that may not be interacted with meaningfully. We think auto-upgrades align our pricing model with the value that LangSmith brings, where only traces with meaningful interaction are charged at a higher rate.


If you have questions or concerns about our pricing model, please feel free to reach out to support@langchain.dev and let us know your thoughts!
**How does data retention affect downstream features?**
  * **Annotation Queues, Run Rules, and Feedback** : Traces that use these features will be [auto-upgraded](https://docs.smith.langchain.com/administration/concepts#data-retention-auto-upgrades).
  * **Monitoring** : The monitoring tab will continue to work even after a base tier trace's data retention period ends. It is powered by trace metadata that exists for >30 days, meaning that your monitoring graphs will continue to stay accurate even on `base` tier traces.
  * **Datasets** : Datasets have an indefinite data retention period. Restated differently, if you add a trace's inputs and outputs to a dataset, they will never be deleted. We suggest that if you are using LangSmith for data collection, you take advantage of the datasets feature.


#### Billing model[‚Äã](https://docs.smith.langchain.com/administration/concepts#billing-model "Direct link to Billing model")
**Billable metrics**
On your LangSmith invoice, you will see two metrics that we charge for:
  * LangSmith Traces (Base Charge)
  * LangSmith Traces (Extended Data Retention Upgrades).


The first metric includes all traces, regardless of tier. The second metric just counts the number of extended retention traces.
**Why measure all traces + upgrades instead of base and extended traces?**
A natural question to ask when considering our pricing is why not just show the number of `base` tier and `extended` tier traces directly on the invoice?
While we understand this would be more straightforward, it doesn't fit trace upgrades properly. Consider a `base` tier trace that was recorded on June 30, and upgraded to `extended` tier on July 3. The `base` tier trace occurred in the June billing period, but the upgrade occurred in the July billing period. Therefore, we need to be able to measure these two events independently to properly bill our customers.
If your trace was recorded as an extended retention trace, then the `base` and `extended` metrics will both be recorded with the same timestamp.
**Cost breakdown**
The Base Charge for a trace is .05¬¢ per trace. We priced the upgrade such that an `extended` retention trace costs 10x the price of a base tier trace (.50¬¢ per trace) including both metrics. Thus, each upgrade costs .45¬¢.
### Rate Limits[‚Äã](https://docs.smith.langchain.com/administration/concepts#rate-limits "Direct link to Rate Limits")
LangSmith has rate limits which are designed to ensure the stability of the service for all users.
To ensure access and stability, LangSmith will respond with HTTP Status Code 429 indicating that rate or usage limits have been exceeded under the following circumstances:
#### Scenarios[‚Äã](https://docs.smith.langchain.com/administration/concepts#scenarios "Direct link to Scenarios")
###### Temporary throughput limit over a 1 minute period at our application load balancer[‚Äã](https://docs.smith.langchain.com/administration/concepts#temporary-throughput-limit-over-a-1-minute-period-at-our-application-load-balancer "Direct link to Temporary throughput limit over a 1 minute period at our application load balancer")
This 429 is the the result of exceeding a fixed number of API calls over a 1 minute window on a per API key/access token basis. The start of the window will vary slightly ‚Äî it is not guaranteed to start at the start of a clock minute ‚Äî and may change depending on application deployment events.
After the max events are received we will respond with a 429 until 60 seconds from the start of the evaluation window has been reached and then the process repeats.
This 429 is thrown by our application load balancer and is a mechanism in place for all LangSmith users independent of plan tier to ensure continuity of service for all users.
Method| Endpoint| Limit| Window  
---|---|---|---  
DELETE| Sessions| 30| 1 minute  
POST OR PATCH| Runs| 5000| 1 minute  
POST| Feedback| 5000| 1 minute  
*| *| 2000| 1 minute  
note
The LangSmith SDK takes steps to minimize the likelihood of reaching these limits on run-related endpoints by batching up to 100 runs from a single session ID into a single API call.
###### Plan-level hourly trace event limit[‚Äã](https://docs.smith.langchain.com/administration/concepts#plan-level-hourly-trace-event-limit "Direct link to Plan-level hourly trace event limit")
This 429 is the result of reaching your maximum hourly events ingested and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour.
An event in this context is the creation or update of a run. So if run is created, then subsequently updated in the same hourly window, that will count as 2 events against this limit.
This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use.
Plan| Limit| Window  
---|---|---  
Developer (no payment on file)| 50,000 events| 1 hour  
Developer (with payment on file)| 250,000 events| 1 hour  
Startup/Plus| 500,000 events| 1 hour  
Enterprise| Custom| Custom  
###### Plan-level hourly trace data ingest limit[‚Äã](https://docs.smith.langchain.com/administration/concepts#plan-level-hourly-trace-data-ingest-limit "Direct link to Plan-level hourly trace data ingest limit")
This 429 is the result of reaching the maximum amount of data ingested across your trace inputs, outputs, and metadata and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour.
Typically, inputs, outputs, and metadata are send on both run creation and update events. So if a run is created and is 2.0MB in size at creation, and 3.0MB in size when updated in the same hourly window, that will count as 5.0MB of storage against this limit.
This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use.
Plan| Limit| Window  
---|---|---  
Developer (no payment on file)| 500MB| 1 hour  
Developer (with payment on file)| 2.5GB| 1 hour  
Startup/Plus| 5.0GB| 1 hour  
Enterprise| Custom| Custom  
###### Plan-level monthly unique traces limit[‚Äã](https://docs.smith.langchain.com/administration/concepts#plan-level-monthly-unique-traces-limit "Direct link to Plan-level monthly unique traces limit")
This 429 is the result of reaching your maximum monthly traces ingested and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month.
This is thrown by our application and applies only to the Developer Plan Tier when there is no payment method on file.
Plan| Limit| Window  
---|---|---  
Developer (no payment on file)| 5,000 traces| 1 month  
###### Self-configured monthly usage limits[‚Äã](https://docs.smith.langchain.com/administration/concepts#self-configured-monthly-usage-limits "Direct link to Self-configured monthly usage limits")
This 429 is the result of reaching your usage limit as configured by your organization admin and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month.
This is thrown by our application and varies by organization based on their configured settings.
#### Handling 429s responses in your application[‚Äã](https://docs.smith.langchain.com/administration/concepts#handling-429s-responses-in-your-application "Direct link to Handling 429s responses in your application")
Since some 429 responses are temporary and may succeed on a successive call, if you are directly calling the LangSmith API in your application we recommend implementing retry logic with exponential backoff and jitter.
For convenience, LangChain applications built with the LangSmith SDK has this capability built-in.
note
It is important to note that if you are saturating the endpoints for extended periods of time, retries may not be effective as your application will eventually run large enough backlogs to exhaust all retries.
If that is the case, we would like to discuss your needs more specifically. Please reach out to LangSmith Support with details about your applications throughput needs and sample code and we can work with you to better understand whether the best approach is fixing a bug, changes to your application code, or a different LangSmith plan.
### Usage Limits[‚Äã](https://docs.smith.langchain.com/administration/concepts#usage-limits "Direct link to Usage Limits")
LangSmith lets you configure usage limits on tracing. Note that these are _usage_ limits, not _spend_ limits, which mean they let you limit the quantity of occurrences of some event rather than the total amount you will spend.
LangSmith lets you set two different monthly limits, mirroring our Billable Metrics discussed in the aforementioned data retention guide:
  * All traces limit
  * Extended data retention traces limit


These let you limit the number of total traces, and extended data retention traces respectively.
#### Properties of usage limiting[‚Äã](https://docs.smith.langchain.com/administration/concepts#properties-of-usage-limiting "Direct link to Properties of usage limiting")
Usage limiting is approximate, meaning that we do not guarantee the exactness of the limit. In rare cases, there may be a small period of time where additional traces are processed above the limit threshold before usage limiting begins to apply.
#### Side effects of extended data retention traces limit[‚Äã](https://docs.smith.langchain.com/administration/concepts#side-effects-of-extended-data-retention-traces-limit "Direct link to Side effects of extended data retention traces limit")
The extended data retention traces limit has side effects. If the limit is already reached, any feature that could cause an auto-upgrade of tracing tiers becomes inaccessible. This is because an auto-upgrade of a trace would cause another extended retention trace to be created, which in turn should not be allowed by the limit. Therefore, you can no longer:
  1. match run rules
  2. add feedback to traces
  3. add runs to annotation queues


Each of these features may cause an auto upgrade, so we shut them off when the limit is reached.
#### Updating usage limits[‚Äã](https://docs.smith.langchain.com/administration/concepts#updating-usage-limits "Direct link to Updating usage limits")
Usage limits can be updated from the `Settings` page under `Usage and Billing`. Limit values are cached, so it may take a minute or two before the new limits apply.
### Related content[‚Äã](https://docs.smith.langchain.com/administration/concepts#related-content "Direct link to Related content")
  * Tutorial on how to [optimize spend](https://docs.smith.langchain.com/administration/tutorials/manage_spend)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousSAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)[NextSelf-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Resource Hierarchy](https://docs.smith.langchain.com/administration/concepts#resource-hierarchy)
    * [Organizations](https://docs.smith.langchain.com/administration/concepts#organizations)
    * [Workspaces](https://docs.smith.langchain.com/administration/concepts#workspaces)
    * [Resource tags](https://docs.smith.langchain.com/administration/concepts#resource-tags)
  * [User Management and RBAC](https://docs.smith.langchain.com/administration/concepts#user-management-and-rbac)
    * [Users](https://docs.smith.langchain.com/administration/concepts#users)
    * [API keys](https://docs.smith.langchain.com/administration/concepts#api-keys)
    * [Organization roles](https://docs.smith.langchain.com/administration/concepts#organization-roles)
    * [Workspace roles (RBAC)](https://docs.smith.langchain.com/administration/concepts#workspace-roles)
  * [Best Practices](https://docs.smith.langchain.com/administration/concepts#best-practices)
    * [Environment Separation](https://docs.smith.langchain.com/administration/concepts#environment-separation)
  * [Usage and Billing](https://docs.smith.langchain.com/administration/concepts#usage-and-billing)
    * [Data Retention](https://docs.smith.langchain.com/administration/concepts#data-retention)
    * [Rate Limits](https://docs.smith.langchain.com/administration/concepts#rate-limits)
    * [Usage Limits](https://docs.smith.langchain.com/administration/concepts#usage-limits)
    * [Related content](https://docs.smith.langchain.com/administration/concepts#related-content)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/how_to_guides

[Skip to main content](https://docs.smith.langchain.com/administration/how_to_guides#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/how_to_guides)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * How-to Guides


On this page
# Administration how-to guides
Step-by-step guides that cover key tasks and operations in LangSmith.
## Organization Management[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides#organization-management "Direct link to Organization Management")
See the following guides to set up your LangSmith account.
  * [Create an account and API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
  * [Set up an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
    * [Create an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#create-an-organization)
    * [Manage and navigate workspaces](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#manage-and-navigate-workspaces)
    * [Manage users](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#manage-users)
    * [Manage your organization using the API](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)
  * [Set up a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
    * [Create a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace#create-a-workspace)
    * [Manage users](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace#manage-users)
    * [Configure workspace settings](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace#configure-workspace-settings)
  * [Set up billing](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
  * [Update invoice email, tax id and, business information](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
  * [Set up access control (enterprise only)](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
    * [Create a role](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control#create-a-role)
    * [Assign a role to a user](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control#assign-a-role-to-a-user)
  * [Set up resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
    * [Create a tag](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#create-a-tag)
    * [Assign a tag to a resource](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#assign-a-tag-to-a-resource)
    * [Delete a tag](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#delete-a-tag)
    * [Filter resources by tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#filter-resources-by-tags)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousOptimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)[NextCreate an account and API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
  * [Organization Management](https://docs.smith.langchain.com/administration/how_to_guides#organization-management)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key

[Skip to main content](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
        * [Create an account and API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
        * [Set up an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
        * [Set up a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
        * [Set up billing for your LangSmith account](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
        * [Update invoice email, tax id and, business information](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
        * [Manage your organization using the API](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)
        * [Set up access control](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
        * [Set up resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
        * [SAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
  * Setup
  * Create an account and API key


On this page
# Create an account and API key
## Create an account[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key#create-an-account "Direct link to Create an account")
To get started with LangSmith, you need to create an account. You can sign up for a free account [here](https://smith.langchain.com). We support logging in with Google, GitHub, Discord, and email.
![](https://docs.smith.langchain.com/assets/images/create_account-0f1187e246adb36940a04b89ac2c28a4.png)
## API keys[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key#api-keys "Direct link to API keys")
LangSmith supports two types of API keys: Service Keys and Personal Access Tokens. Both types of tokens can be used to authenticate requests to the LangSmith API, but they have different use cases.
Read more about the differences between Service Keys and Personal Access Tokens under [admin concepts](https://docs.smith.langchain.com/administration/concepts)
## Create an API key[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key#create-an-api-key "Direct link to Create an API key")
To log traces and run evaluations with LangSmith, you will need to create an API key to authenticate your requests. Currently, an API key is scoped to a workspace, so you will need to create an API key for each workspace you want to use.
To create either type of API key head to the [Settings page](https://smith.langchain.com/settings), then scroll to the **API Keys** section. Then click **Create API Key.**
note
The API key will be shown only once, so make sure to copy it and store it in a safe place.
![](https://docs.smith.langchain.com/assets/images/create_api_key-5fab98924105622c8db8d23924f0a1dc.png)
## Configure the SDK[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key#configure-the-sdk "Direct link to Configure the SDK")
You may set the following environment variables in addition to `LANGSMITH_API_KEY`. These are only required if using the EU instance.
`LANGSMITH_ENDPOINT=``https://api.smith.langchain.com`
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousAdministration how-to guides](https://docs.smith.langchain.com/administration/how_to_guides)[NextSet up an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
  * [Create an account](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key#create-an-account)
  * [API keys](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key#api-keys)
  * [Create an API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key#create-an-api-key)
  * [Configure the SDK](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key#configure-the-sdk)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api

[Skip to main content](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
        * [Create an account and API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
        * [Set up an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
        * [Set up a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
        * [Set up billing for your LangSmith account](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
        * [Update invoice email, tax id and, business information](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
        * [Manage your organization using the API](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)
        * [Set up access control](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
        * [Set up resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
        * [SAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
  * Setup
  * Manage your organization using the API


On this page
# Manage your organization using the API
LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are [noted below](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#user-only-endpoints).
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Conceptual guide on organizations and workspaces](https://docs.smith.langchain.com/administration/concepts)
  * [Organization setup how-to guild](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)


note
There are a few limitations that will be lifted soon:
  * The LangSmith SDKs do not support these organization management actions yet.
  * [Service Keys](https://docs.smith.langchain.com/administration/concepts#api-keys) don't have access to newly-added workspaces yet (we're adding support soon). We recommend using a PAT of an Organization Admin for now, which by default has the required permissions for these actions.


Some commonly-used endpoints and use cases are listed below. For a complete list of available endpoints, see the [API docs](https://api.smith.langchain.com/redoc). **The`X-Organization-Id` header should be present on all requests, and `X-Tenant-Id` header should be present on requests that are scoped to a particular workspace.**
## Workspaces[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#workspaces "Direct link to Workspaces")
  * [List workspaces](https://api.smith.langchain.com/redoc#tag/workspaces/operation/list_workspaces_api_v1_workspaces_get)
  * [Create workspace](https://api.smith.langchain.com/redoc#tag/workspaces/operation/create_workspace_api_v1_workspaces_post)
  * [Update workspace name](https://api.smith.langchain.com/redoc#tag/workspaces/operation/patch_workspace_api_v1_workspaces__workspace_id__patch)


## User management[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#user-management "Direct link to User management")
### RBAC[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#rbac "Direct link to RBAC")
  * [List roles](https://api.smith.langchain.com/redoc#tag/orgs/operation/list_organization_roles_api_v1_orgs_current_roles_get)
  * [List permissions](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_organization_roles_api_v1_orgs_current_roles__role_id__patch)
  * [Create role](https://api.smith.langchain.com/redoc#tag/orgs/operation/create_organization_roles_api_v1_orgs_current_roles_post)
  * [Update role](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_organization_roles_api_v1_orgs_current_roles__role_id__patch)


### Membership management[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#membership-management "Direct link to Membership management")
`List roles` under [RBAC](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#rbac) should be used for retrieving role IDs of these operations. `List [organization|workspace] members` endpoints (below) response `"id"`s should be used as `identity_id` in these operations.
Organization level:
  * [List organization members](https://api.smith.langchain.com/redoc#tag/orgs/operation/get_current_org_members_api_v1_orgs_current_members_get)
  * [Invite a user to the organization and one or more workspaces](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post) . This should be used when the user is not already a member in the organization.
  * [Update a user‚Äôs organization role](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post)
  * [Remove someone from the organization](https://api.smith.langchain.com/redoc#tag/orgs/operation/remove_member_from_current_org_api_v1_orgs_current_members__identity_id__delete)


Workspace level:
  * [List workspace members](https://api.smith.langchain.com/redoc#tag/workspaces/operation/get_current_workspace_members_api_v1_workspaces_current_members_get)
  * [Add a member to a workspace that is already part of the organization](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post)
  * [Update a user‚Äôs workspace role](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post)
  * [Remove someone from a workspace](https://api.smith.langchain.com/redoc#tag/workspaces/operation/delete_current_workspace_member_api_v1_workspaces_current_members__identity_id__delete)


note
These params should be omitted: `read_only` (deprecated), `password` and `full_name` ([basic auth](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods) only)
## API Keys[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#api-keys "Direct link to API Keys")
note
Use the `X-Tenant-Id` header to specify which workspace to target. If the header is not present, operations will default to the workspace the API key was initially created in.
  * [Create a service key](https://api.smith.langchain.com/redoc#tag/api-key/operation/generate_api_key_api_v1_api_key_post)
  * [Delete a service key](https://api.smith.langchain.com/redoc#tag/api-key/operation/delete_api_key_api_v1_api_key__api_key_id__delete)


## Security Settings[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#security-settings "Direct link to Security Settings")
note
"Shared resources" in this context refer to [public prompts](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt#save-your-prompt), [shared runs](https://docs.smith.langchain.com/observability/how_to_guides/share_trace), and [shared datasets](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset).
  * [Update organization sharing settings](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_current_organization_info_api_v1_orgs_current_info_patch)
    * use `unshare_all` to unshare **ALL** shared resources in the organization - use `disable_public_sharing` to prevent future sharing of resources


## User-Only Endpoints[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#user-only-endpoints "Direct link to User-Only Endpoints")
These endpoints are user-scoped and require a logged-in user's JWT, so they should only be executed through the UI.
  * `/api-key/current` endpoints: these are related a user's PATs
  * `/sso/email-verification/send` (Cloud-only): this endpoint is related to [SAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)


## Sample Code[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#sample-code "Direct link to Sample Code")
The sample code below goes through a few common workflows related to organization management. Make sure to make necessary replacements wherever `<replace_me>` is in the code.
```
import osimport requestsdefmain():  api_key = os.environ["LANGSMITH_API_KEY"]# LANGSMITH_ORGANIZATION_ID is not a standard environment variable in the SDK, just used for this example  organization_id = os.environ["LANGSMITH_ORGANIZATION_ID"]  base_url = os.environ.get("LANGSMITH_ENDPOINT")or"https://api.smith.langchain.com"  headers ={"Content-Type":"application/json","X-API-Key": api_key,"X-Organization-Id": organization_id,}  session = requests.Session()  session.headers.update(headers)  workspaces_path =f"{base_url}/api/v1/workspaces"  orgs_path =f"{base_url}/api/v1/orgs/current"  api_keys_path =f"{base_url}/api/v1/api-key"# Create a workspace  workspace_res = session.post(workspaces_path, json={"display_name":"My Workspace"})  workspace_res.raise_for_status()  workspace = workspace_res.json()  workspace_id = workspace["id"]  new_workspace_headers ={"X-Tenant-Id": workspace_id,}# Grab roles - this includes both organization and workspace roles  roles_res = session.get(f"{orgs_path}/roles")  roles_res.raise_for_status()  roles = roles_res.json()# system org roles are 'Organization Admin', 'Organization User'# system workspace roles are 'Admin', 'Editor', 'Viewer'  org_roles_by_name ={role["display_name"]: role for role in roles if role["access_scope"]=="organization"}  ws_roles_by_name ={role["display_name"]: role for role in roles if role["access_scope"]=="workspace"}# Invite a user to the org and the new workspace, as an Editor.# workspace_role_id is only allowed if RBAC is enabled (an enterprise feature).  new_user_email ="<replace_me>"  new_user_res = session.post(f"{orgs_path}/members",    json={"email": new_user_email,"role_id": org_roles_by_name["Organization User"]["id"],"workspace_ids":[workspace_id],"workspace_role_id": ws_roles_by_name["Editor"]["id"],},)  new_user_res.raise_for_status()# Add a user that already exists in the org to the new workspace, as a Viewer.# workspace_role_id is only allowed if RBAC is enabled (an enterprise feature).  existing_user_email ="<replace_me>"  org_members_res = session.get(f"{orgs_path}/members")  org_members_res.raise_for_status()  org_members = org_members_res.json()  existing_org_member =next((member for member in org_members["members"]if member["email"]== existing_user_email),None)  existing_user_res = session.post(f"{workspaces_path}/current/members",    json={"user_id": existing_org_member["user_id"],"workspace_ids":[workspace_id],"workspace_role_id": ws_roles_by_name["Viewer"]["id"],},    headers=new_workspace_headers,)  existing_user_res.raise_for_status()# List all members of the workspace  members_res = session.get(f"{workspaces_path}/current/members", headers=new_workspace_headers)  members_res.raise_for_status()  members = members_res.json()  workspace_member =next((member for member in members["members"]if member["email"]== existing_user_email),None)# Update the user's workspace role to Admin (enterprise-only)  existing_user_id = workspace_member["id"]  update_res = session.patch(f"{workspaces_path}/current/members/{existing_user_id}",    json={"role_id": ws_roles_by_name["Admin"]["id"]},    headers=new_workspace_headers,)  update_res.raise_for_status()# Update the user's organization role to Organization Admin  update_res = session.patch(f"{orgs_path}/members/{existing_org_member['id']}",    json={"role_id": org_roles_by_name["Organization Admin"]["id"]},)  update_res.raise_for_status()# Create a new Service key  api_key_res = session.post(    api_keys_path,    json={"description":"my key"},    headers=new_workspace_headers,)  api_key_res.raise_for_status()  api_key_json = api_key_res.json()  api_key = api_key_json["key"]if __name__ =="__main__":  main()
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousUpdate invoice email, tax id and, business information](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)[NextSet up access control](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
  * [Workspaces](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#workspaces)
  * [User management](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#user-management)
    * [RBAC](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#rbac)
    * [Membership management](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#membership-management)
  * [API Keys](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#api-keys)
  * [Security Settings](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#security-settings)
  * [User-Only Endpoints](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#user-only-endpoints)
  * [Sample Code](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api#sample-code)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control

[Skip to main content](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
        * [Create an account and API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
        * [Set up an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
        * [Set up a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
        * [Set up billing for your LangSmith account](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
        * [Update invoice email, tax id and, business information](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
        * [Manage your organization using the API](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)
        * [Set up access control](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
        * [Set up resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
        * [SAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
    * [Evaluation](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)


  * [](https://docs.smith.langchain.com/)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
  * Setup
  * Set up access control


On this page
# Set up access control
note
RBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev Other plans default to using the Admin role for all users. Read more about roles under [admin concepts](https://docs.smith.langchain.com/administration/concepts)
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Conceptual guide on organizations and workspaces](https://docs.smith.langchain.com/administration/concepts)


LangSmith relies on RBAC to manage user permissions **within a workspace**. This allows you to control who can access your LangSmith workspace and what they can do within it. Only users with the `workspace:manage` permission can manage access control settings for a workspace.
## Create a role[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control#create-a-role "Direct link to Create a role")
By default, LangSmith comes with a set of system roles:
  * `Admin` - has full access to all resources within the workspace
  * `Viewer` - has read-only access to all resources within the workspace
  * `Editor` - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys)


If these do not fit your access model, Organization Admins can create custom roles to suit your needs.
To create a role, navigate to the `Roles` tab in the `Members and roles` section of the [Organization settings page](https://smith.langchain.com/settings). Note that new roles that you create will be usable across all workspaces within your organization.
Click on the `Create Role` button to create a new role. You should see a form like the one below:
![Create Role](https://docs.smith.langchain.com/assets/images/create_role-c9ffbce3c79cd705cc31b9c94a00034c.png)
Assign permissions for the different LangSmith resources that you want to control access to.
## Assign a role to a user[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control#assign-a-role-to-a-user "Direct link to Assign a role to a user")
Once you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the `Workspace members` tab in the `Workspaces` section of the [Organization settings page](https://smith.langchain.com/settings)
Each user will have a `Role` dropdown that you can use to assign a role to them.
![Assign Role](https://docs.smith.langchain.com/assets/images/assign_role-3b4bb67dc9efc08575f6598fe6b8cdd3.png)
You can also invite new users with a given role.
![Invite User](https://docs.smith.langchain.com/assets/images/invite_user-a33d24bf3f1ccb8807373c7d2576848b.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/administration/how_to_guides/organization_management/set_up_access_control%3E).
[PreviousManage your organization using the API](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)[NextSet up resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
  * [Create a role](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control#create-a-role)
  * [Assign a role to a user](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control#assign-a-role-to-a-user)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing

[Skip to main content](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
        * [Create an account and API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
        * [Set up an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
        * [Set up a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
        * [Set up billing for your LangSmith account](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
        * [Update invoice email, tax id and, business information](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
        * [Manage your organization using the API](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)
        * [Set up access control](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
        * [Set up resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
        * [SAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
    * [Evaluation](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)


  * [](https://docs.smith.langchain.com/)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
  * Setup
  * Set up billing for your LangSmith account


On this page
# Set up billing for your LangSmith account
note
If you are interested in the [Enterprise](https://www.langchain.com/pricing) plan, please [contact sales](https://www.langchain.com/contact-sales). This guide is only for our self-serve billing plans.
note
If you created your LangSmith organization before pricing was introduced on April 2nd, 2024, please [skip to the final section](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#set-up-billing-for-accounts-created-before-pricing-was-introduced-on-april-2-2024).
To set up billing for your LangSmith organization, head to the [Usage and Billing](https://smith.langchain.com/settings/payments) page under Settings. Depending on your organization's settings, you will be given a different walkthrough to get started.
## Developer Plan: set up billing on your personal organization[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#developer-plan-set-up-billing-on-your-personal-organization "Direct link to Developer Plan: set up billing on your personal organization")
Personal organizations are limited to 5000 traces per month until a credit card is added. You can add a credit card on the Plans and Billing page as follows:
### 1. Click `Set up Billing`[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#1-click-set-up-billing "Direct link to 1-click-set-up-billing")
![](https://docs.smith.langchain.com/assets/images/free_tier_billing_page-61bd5906c134b67290189e33a14f6803.png)
### 2. Add your credit card info[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#2-add-your-credit-card-info "Direct link to 2. Add your credit card info")
After this step, you will no longer be rate limited to 5000 traces, and will be charged for any excess traces at rates specified on our [pricing](https://www.langchain.com/pricing-langsmith) page.
## Plus Plan: set up billing on a shared organization[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#plus-plan-set-up-billing-on-a-shared-organization "Direct link to Plus Plan: set up billing on a shared organization")
If you have not yet created an organization, please do so by following [this guide](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization). This walkthrough assumes you are already in a new organization.
note
New organizations are not usable until a credit card is entered. After you complete the following steps, you will gain complete access to LangSmith.
### 1. Click `Subscribe` on the Plus page[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#1-click-subscribe-on-the-plus-page "Direct link to 1-click-subscribe-on-the-plus-page")
note
If you are a startup building with AI, please instead click `Apply Now` on our Startup Plan. You may be eligible for discounted prices and a generous free, monthly trace allotment.
![](https://docs.smith.langchain.com/assets/images/new_org_billing_page-ddeaafe34aa7594e594ea30dcae53a48.png)
### 2. Review your existing members[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#2-review-your-existing-members "Direct link to 2. Review your existing members")
Before subscribing, LangSmith lets you remove any added users that you would not like to be charged for.
![](https://docs.smith.langchain.com/assets/images/new_org_manage_spend-1bd91423f03123975897803bce2010ea.png)
### 3. Enter your credit card info[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#3-enter-your-credit-card-info "Direct link to 3. Enter your credit card info")
#### Enter business information, invoice email and tax id[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#enter-business-information-invoice-email-and-tax-id "Direct link to Enter business information, invoice email and tax id")
If this organization belongs to a business. Please check the "This is a business" checkbox and enter the information accordingly.
For more information refer to [this guide](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
Once this step is complete, your org should now have access to the rest of LangSmith!
## Set up billing for accounts created before pricing was introduced on April 2, 2024[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#set-up-billing-for-accounts-created-before-pricing-was-introduced-on-april-2-2024 "Direct link to Set up billing for accounts created before pricing was introduced on April 2, 2024")
If you joined LangSmith before pricing was introduced April 2, 2024, you have the option to upgrade your existing account to setup billing. If you did not set up billing by July 8, 2024, then your account is now rate limited to a maximum of 5,000 traces per month.
### 1. Head to the [Settings page](https://smith.langchain.com/settings) page under Settings[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#1-head-to-the--page-under-settings "Direct link to 1-head-to-the--page-under-settings")
### 2. Click `Set up Billing`[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#2-click-set-up-billing "Direct link to 2-click-set-up-billing")
![](https://docs.smith.langchain.com/assets/images/setup_billing_legacy-7159d49e60d87f07be8dccea1c1ca454.png)
### 3. Enter your credit card info[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#3-enter-your-credit-card-info-1 "Direct link to 3. Enter your credit card info")
If you are on a Personal Organization, this will add you to the Developer plan. If you are on a shared Organization, this will add you to the Plus plan. For more information, please view the above walkthroughs for Developer or Plus respectively, starting at step 2.
### 4. Claim free credits as a thank you for being an early LangSmith user[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#4-claim-free-credits-as-a-thank-you-for-being-an-early-langsmith-user "Direct link to 4. Claim free credits as a thank you for being an early LangSmith user")
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/administration/how_to_guides/organization_management/set_up_billing%3E).
[PreviousSet up a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)[NextUpdate invoice email, tax id and, business information](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
  * [Developer Plan: set up billing on your personal organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#developer-plan-set-up-billing-on-your-personal-organization)
    * [1. Click `Set up Billing`](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#1-click-set-up-billing)
    * [2. Add your credit card info](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#2-add-your-credit-card-info)
  * [Plus Plan: set up billing on a shared organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#plus-plan-set-up-billing-on-a-shared-organization)
    * [1. Click `Subscribe` on the Plus page](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#1-click-subscribe-on-the-plus-page)
    * [2. Review your existing members](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#2-review-your-existing-members)
    * [3. Enter your credit card info](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#3-enter-your-credit-card-info)
  * [Set up billing for accounts created before pricing was introduced on April 2, 2024](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#set-up-billing-for-accounts-created-before-pricing-was-introduced-on-april-2-2024)
    * [1. Head to the page under Settings](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#1-head-to-the--page-under-settings)
    * [2. Click `Set up Billing`](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#2-click-set-up-billing)
    * [3. Enter your credit card info](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#3-enter-your-credit-card-info-1)
    * [4. Claim free credits as a thank you for being an early LangSmith user](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing#4-claim-free-credits-as-a-thank-you-for-being-an-early-langsmith-user)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization

[Skip to main content](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
        * [Create an account and API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
        * [Set up an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
        * [Set up a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
        * [Set up billing for your LangSmith account](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
        * [Update invoice email, tax id and, business information](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
        * [Manage your organization using the API](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)
        * [Set up access control](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
        * [Set up resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
        * [SAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
    * [Evaluation](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)


  * [](https://docs.smith.langchain.com/)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
  * Setup
  * Set up an organization


On this page
# Set up an organization
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Conceptual guide on organizations and workspaces](https://docs.smith.langchain.com/administration/concepts)


note
If you're interested in managing your organization and workspaces programmatically, see [this how-to guide](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api).
## Create an organization[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#create-an-organization "Direct link to Create an organization")
When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.
To do this, head to the [Settings page](https://smith.langchain.com/settings) and click **Create Organization**. Shared organizations require a credit card before they can be used. You will need to [set up billing](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing) to proceed.
![](https://docs.smith.langchain.com/assets/images/create_organization-aac1dccd362fb4fa6137552362ce987d.png)
## Manage and navigate workspaces[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#manage-and-navigate-workspaces "Direct link to Manage and navigate workspaces")
Once you've subscribed to a plan that allows for multiple users per organization, you can [set up workspaces](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace) to collaborate more effectively and isolate LangSmith resources between different groups of users. To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:
![](https://docs.smith.langchain.com/assets/images/select_workspace-b56235ac87fae4b49a46f161cfa5d887.png)
## Manage users[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#manage-users "Direct link to Manage users")
Manage membership in your shared organization in the [Settings page](https://smith.langchain.com/settings) `Members and roles` tab. Here you can
  * Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace role
  * Edit a user's organization role
  * Remove users from your organization


![](https://docs.smith.langchain.com/assets/images/organization_members_and_roles-52964d637277b463665a15c2cb3bb8c9.png)
Organizations on the Enterprise plan may set up custom workspace roles in the `Roles` tab here. See the [access control setup guide](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control) for more details.
### Organization roles[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#organization-roles "Direct link to Organization roles")
These are organization-scoped roles that are used to determine access to organization settings. The role selected also impacts workspace membership as described here:
  * `Organization Admin` grants full access to manage all organization configuration, users, billing, and workspaces. **Any`Organization Admin` has `Admin` access to all workspaces in an organization**
  * `Organization User` may read organization information but cannot execute any write actions at the organization level. **An`Organization User` can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.**


info
The `Organization User` role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are `Organization Admins`. Custom organization-scoped roles are not available yet.
See [this conceptual guide](https://docs.smith.langchain.com/administration/concepts#organization-roles) for a full list of permissions associated with each role.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/administration/how_to_guides/organization_management/set_up_organization%3E).
[PreviousCreate an account and API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)[NextSet up a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
  * [Create an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#create-an-organization)
  * [Manage and navigate workspaces](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#manage-and-navigate-workspaces)
  * [Manage users](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#manage-users)
    * [Organization roles](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#organization-roles)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags

[Skip to main content](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
        * [Create an account and API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
        * [Set up an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
        * [Set up a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
        * [Set up billing for your LangSmith account](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
        * [Update invoice email, tax id and, business information](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
        * [Manage your organization using the API](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)
        * [Set up access control](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
        * [Set up resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
        * [SAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
    * [Evaluation](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)


  * [](https://docs.smith.langchain.com/)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
  * Setup
  * Set up resource tags


On this page
# Set up resource tags
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Conceptual guide on organizations and workspaces](https://docs.smith.langchain.com/administration/concepts)


Availability
Resource tags are available for Plus and Enterprise plans. This feature will be rolling out week of August 19th.
While workspaces help separate trust boundaries and access control, tags help you organize resources within a workspace. Tags are key-value pairs that you can attach to resources.
## Create a tag[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#create-a-tag "Direct link to Create a tag")
To create a tag, head to the workspace settings and click on the "Resource Tags" tab. Here, you'll be able to see the existing tag values, grouped by key. Two keys `Application` and `Environment` are created by default.
To create a new tag, click on the "New Tag" button. You'll be prompted to enter a key and a value for the tag. Note that you can use an existing key or create a new one.
![](https://docs.smith.langchain.com/assets/images/create_tag-448b16994c22f18dec1b19d7eee7d9f1.png)
## Assign a tag to a resource[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#assign-a-tag-to-a-resource "Direct link to Assign a tag to a resource")
Within the same side panel for creating a new tag, you can also create assign resources to tags. Search for corresponding resources in the "Assign Resources" section and select the resources you want to tag.
note
You can only tag workspace-scoped resources with resource tags. This includes Tracing Projects, Annotation Queues, Deployments, Experiments, Datasets, and Prompts.
You can also assign tags to resources from the resource's detail page. Click on the Resource tags button to open up the tag panel and assign tags.
![](https://docs.smith.langchain.com/assets/images/assign_tag-d026365eff8316f646c8bf5f6e3b77df.png)
To un-assign a tag from a resource, click on the Trash icon next to the tag, both in the tag panel and the resource tag panel.
## Delete a tag[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#delete-a-tag "Direct link to Delete a tag")
You can delete either a key or a value of a tag from the [workspace settings page](https://smith.langchain.com/settings/workspaces/resource_tags). To delete a key, click on the Trash icon next to the key. To delete a value, click on the Trash icon next to the value.
Note that if you delete a key, all values associated with that key will also be deleted. When you delete a value, you will lose all associations between that value and resources.
![](https://docs.smith.langchain.com/assets/images/delete_tag-a5e025548ada6e813d2b57ebdd98bb7e.png)
## Filter resources by tags[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#filter-resources-by-tags "Direct link to Filter resources by tags")
You can use resource tags to organize your experience navigating resources in the workspace.
To filter resources by tags in your workspace, open up the left-hand side panel and click on the tags icon. Here, you can select the tags you want to filter by.
In the homepage, you can see updated counts for resources based on the tags you've selected.
As you navigate through the different product surfaces, you will _only_ see resources that match the tags you've selected. At any time, you can clear the tags to see all resources in the workspace or select different tags to filter by.
![](https://docs.smith.langchain.com/assets/images/filter_by_tags-3539ae829b39ac88611679aa6e4f1b88.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/administration/how_to_guides/organization_management/set_up_resource_tags%3E).
[PreviousSet up access control](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)[NextSAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
  * [Create a tag](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#create-a-tag)
  * [Assign a tag to a resource](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#assign-a-tag-to-a-resource)
  * [Delete a tag](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#delete-a-tag)
  * [Filter resources by tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags#filter-resources-by-tags)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso

[Skip to main content](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
        * [Create an account and API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
        * [Set up an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
        * [Set up a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
        * [Set up billing for your LangSmith account](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
        * [Update invoice email, tax id and, business information](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
        * [Manage your organization using the API](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)
        * [Set up access control](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
        * [Set up resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
        * [SAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
    * [Evaluation](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)


  * [](https://docs.smith.langchain.com/)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
  * Setup
  * SAML SSO


On this page
# SAML SSO
Single Sign-On (SSO) functionality is available for Enterprise Cloud customers to access LangSmith through a single authentication source. This allows administrators to centrally manage team access and keeps information more secure.
LangSmith's SSO configuration is built using the SAML (Security Assertion Markup Language) 2.0 standard. SAML 2.0 enables connecting an Identity Provider (IdP) to your organization for an easier, more secure login experience.
note
SAML SSO is available for organizations on the [Enterprise plan](https://www.langchain.com/pricing-langsmith). Please [contact sales](https://www.langchain.com/contact-sales) to learn more.
## What is SAML SSO?[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#what-is-saml-sso "Direct link to What is SAML SSO?")
SSO services permit a user to use one set of credentials (for example, a name or email address and password) to access multiple applications. The service authenticates the end user only once for all the applications the user has been given rights to and eliminates further prompts when the user switches applications during the same session.
## Benefits of SSO[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#benefits-of-sso "Direct link to Benefits of SSO")
  * Streamlines user management across systems for organization owners.
  * Enables organizations to enforce their own security policies (e.g. MFA)
  * Removes the need for end-users to remember and manage multiple passwords. Simplifies end-users experience by allowing them to sign in at one single access point and enjoy a seamless experience across multiple applications.


## Set up SAML SSO for your organization[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#set-up-saml-sso-for-your-organization "Direct link to Set up SAML SSO for your organization")
### Prerequisites[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#prerequisites "Direct link to Prerequisites")
  * Your organization must be on an Enterprise plan
  * Your Identity Provider (IdP) must support the SAML 2.0 standard
  * Only [Organization Admins](https://docs.smith.langchain.com/administration/concepts#organization-roles) can configure SAML SSO


### Initial configuration[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#initial-configuration "Direct link to Initial configuration")
note
See IdP-specific instructions [below](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#identity-provider-idp-setup)
note
The URLs are different for the US and EU. Please make sure to select your region from the dropdown in the top right.
  1. In your IdP: Configure a SAML application with the following details, then copy the metadata URL or XML for step 3 below 
    1. Single sign-on URL a.k.a. ACS URL: <https://auth.langchain.com/auth/v1/sso/saml/acs>
    2. Audience URI a.k.a. SP Entity ID: <https://auth.langchain.com/auth/v1/sso/saml/metadata>
    3. Name ID format: email address
    4. Application username: email address
    5. Required claims: `sub` and `email`
  2. In LangSmith: Go to `Settings` -> `Members and roles` -> `SSO Configuration`
    1. Fill in the required information and submit to activate SSO login 
      1. Fill in either the `SAML metadata URL` or `SAML metadata XML`
      2. Select the `Default workspace role` and `Default workspaces`. New users logging in via SSO will be added to the specified workspaces with the selected role.


### Editing SAML SSO settings[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#editing-saml-sso-settings "Direct link to Editing SAML SSO settings")
  * `Default workspace role` and `Default workspaces` are editable. The updated settings will apply to new users only, not existing users.
  * (Coming soon) `SAML metadata URL` and `SAML metadata XML` are editable. This is usually only necessary when cryptographic keys are rotated/expired or the metadata URL has changed but the same IdP is still used.


## Just-in-time (JIT) provisioning[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#just-in-time-jit-provisioning "Direct link to Just-in-time \(JIT\) provisioning")
LangSmith supports Just-in-Time provisioning when using SAML SSO. This allows someone signing in via SAML SSO to join the organization and selected workspaces automatically as a member.
note
JIT provisioning only runs for new users i.e. users who do not already have access to the organization with the same email address via a [different login method](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#cloud)
## Login methods and access[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#login-methods-and-access "Direct link to Login methods and access")
Once you have completed your configuration of SAML SSO for your organization, users will be able to login via SAML SSO in addition to [other login methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#cloud) such as username/password and Google Authentication.
  * When logged in via SAML SSO, users can only access the corresponding organization with SAML SSO configured.
  * Users with SAML SSO as their only login method do not have [personal organizations](https://docs.smith.langchain.com/administration/concepts#organizations)
  * When logged in via any other method, users can access the organization with SAML SSO configured along with any other organizations they are a part of


## Enforce SAML SSO only[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#enforce-saml-sso-only "Direct link to Enforce SAML SSO only")
To ensure users can only access the organization when logged in using SAML SSO and no other method, check the `Login via SSO only` checkbox and click `Save`. Once this happens, users accessing the organization that are logged-in via a non-SSO login method are required to log back in using SAML SSO. This setting can be switched back to allow all login methods by unselecting the checkbox and clicking `Save`.
note
You must be logged in via SAML SSO in order to update this setting to `Only SAML SSO`. This is to ensure the SAML settings are valid and avoid locking users out of your organization.
## Support and troubleshooting[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#support-and-troubleshooting "Direct link to Support and troubleshooting")
If you have issues setting up SAML SSO, please reach out to support@langchain.dev.
### FAQ[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#faq "Direct link to FAQ")
#### _How do I change a SAML SSO user's email address?_[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#how-do-i-change-a-saml-sso-users-email-address "Direct link to how-do-i-change-a-saml-sso-users-email-address")
Some identity providers retain the original `User ID` through an email change while others do not, so we recommend that you follow these steps to avoid duplicate users in LangSmith:
  1. Remove the user from the organization (see [here](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#manage-users))
  2. Change their email address in the IdP
  3. Have them login to LangSmith again via SAML SSO - this will trigger the usual [JIT provisioning](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#just-in-time-jit-provisioning) flow with their new email address


#### _How do I fix "405 method not allowed"?_[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#how-do-i-fix-405-method-not-allowed "Direct link to how-do-i-fix-405-method-not-allowed")
Ensure you're using the correct ACS URL: <https://auth.langchain.com/auth/v1/sso/saml/acs>
## Identity Provider (IdP) Setup[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#identity-provider-idp-setup "Direct link to Identity Provider \(IdP\) Setup")
These are instructions for setting up LangSmith SAML SSO with Entra ID (formerly Azure), Google, and Okta. If you use a different Identity Provider and need assistance with configuration, please contact our support team.
### Entra ID (Azure)[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#entra-id-azure "Direct link to Entra ID \(Azure\)")
For additional information, see Microsoft's [documentation](https://learn.microsoft.com/en-us/entra/identity/enterprise-apps/add-application-portal-setup-sso).
**Step 1: Create a new Entra ID application integration**
  1. Log in to the [Azure portal](https://portal.azure.com/#home) with a privileged role (e.g. Global Administrator). On the left navigation pane, select the `Entra ID` service.
  2. Navigate to Enterprise Applications and then select All Applications.
  3. Click `Create your own application`.
  4. In the Create your own application window: 
    1. Enter a name for your application (e.g. `LangSmith`)
    2. Select `Integrate any other application you don't find in the gallery (Non-gallery)`.
  5. Click `Create`.


**Step 2: Configure the Entra ID application and obtain the SAML Metadata**
  1. Open the enterprise application that you created.
  2. In the left-side navigation, select `Manage > Single sign-on`.
  3. On the Single sign-on page, click `SAML`.
  4. Update the `Basic SAML Configuration`
    1. `Identifier (Entity ID)`: <https://auth.langchain.com/auth/v1/sso/saml/metadata>
    2. `Reply URL (Assertion Consumer Service URL)`: <https://auth.langchain.com/auth/v1/sso/saml/acs>
    3. Leave `Relay State`, `Logout Url`, and `Sign on URL` empty
    4. Click `Save`
  5. Ensure required claims are present with `Namespace`: `http://schemas.xmlsoap.org/ws/2005/05/identity/claims`
    1. `sub`: `user.objectid`
    2. `emailaddress`: `user.userprincipalname` or `user.mail` (if using the latter, ensure all users have the `Email` field filled in under `Contact Information`)
  6. On the SAML-based Sign-on page, under `SAML Certificates`, copy the `App Federation Metadata Url`.


**Step 3: Set up LangSmith SSO Configuration**
Follow the instructions under [initial configuration](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#initial-configuration) in the `Fill in required information` step, using the metadata URL from the previous step.
**Step 4: Verify the SSO setup**
  1. Assign the application to users/groups in Entra ID 
    1. Select `Manage > Users and groups`
    2. Click `Add user/group`
    3. In the Add Assignment window: 
      1. Under Users, click `None Selected`.
      2. Search for the user you want to assign to the enterprise application, and then click `Select`.
      3. Verify that the user is selected, and click `Assign`.
  2. Have the user sign in via the unique login URL from the `SSO Configuration` page, or go to `Manage > Single sign-on` and select `Test single sign-on with <application name>`


### Google[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#google "Direct link to Google")
For additional information, see Google's [documentation](https://support.google.com/a/answer/6087519).
**Step 1: Create and configure the Google Workspace SAML application**
  1. Make sure you're signed into an administrator account with the appropriate permissions.
  2. In the Admin console, go to `Menu -> Apps -> Web and mobile apps`.
  3. Click `Add App` and then `Add custom SAML app`.
  4. Enter the app name and, optionally, upload an icon. Click `Continue`.
  5. On the Google Identity Provider details page, download the `IDP metadata` and save it for Step 2 below. Click Continue.
  6. In the `Service Provider Details` window, enter: 
    1. `ACS URL`: <https://auth.langchain.com/auth/v1/sso/saml/acs>
    2. `Entity ID`: <https://auth.langchain.com/auth/v1/sso/saml/metadata>
    3. Leave `Start URL` and the `Signed response` box empty.
    4. Set `Name ID` format to `EMAIL` and leave `Name ID` as the default (`Basic Information > Primary email`).
    5. Click `Continue`.
  7. Use `Add mapping` to ensure required claims are present: 
    1. `Basic Information > Primary email` -> `email`


**Step 2: Set up LangSmith SSO Configuration**
Follow the instructions under [initial configuration](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#initial-configuration) in the `Fill in required information` step, using the `IDP metadata` from the previous step as the metadata XML.
**Step 3: Turn on the SAML app in Google**
  1. Select the SAML app under `Menu -> Apps -> Web and mobile apps`
  2. Click `User access`.
  3. Turn on the service: 
    1. To turn the service on for everyone in your organization, click `On for everyone`, and then click `Save`.
    2. To turn the service on for an organizational unit: 
      1. At the left, select the organizational unit then `On`.
      2. If the Service status is set to `Inherited` and you want to keep the updated setting, even if the parent setting changes, click `Override`.
      3. If the Service status is set to `Overridden`, either click `Inherit` to revert to the same setting as its parent, or click `Save` to keep the new setting, even if the parent setting changes.
    3. To turn on a service for a set of users across or within organizational units, select an access group. For details, go to [Use groups to customize service access](https://support.google.com/a/answer/9050643).
  4. Ensure that the email addresses your users use to sign in to LangSmith match the email addresses they use to sign in to your Google domain.


**Step 4: Verify the SSO setup**
Have a user with access sign in via the unique login URL from the `SSO Configuration` page, or go to the SAML application page in Google and click `TEST SAML LOGIN`.
### Okta[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#okta "Direct link to Okta")
For additional information, see Okta's [documentation](https://help.okta.com/en-us/content/topics/apps/apps_app_integration_wizard_saml.htm).
**Step 1: Create and configure the Okta SAML application**
  1. Log in to Okta as an administrator, and go to the `Okta Admin console`.
  2. Under `Applications > Applications` click `Create App Integration`
  3. Select `SAML 2.0`
  4. Enter an `App name` (e.g. `LangSmith`) and optionally an `App logo`, then click `Next`
  5. Enter the following information in the `Configure SAML` page: 
    1. `Single sign-on URL` a.k.a. `ACS URL`: <https://auth.langchain.com/auth/v1/sso/saml/acs>. Keep `Use this for Recipient URL and Destination URL` checked.
    2. `Audience URI (SP Entity ID)`: <https://auth.langchain.com/auth/v1/sso/saml/metadata>
    3. `Name ID format`: `EmailAddress`
    4. `Application username`: `email`
    5. Leave the rest of the fields empty or set to their default.
    6. Click `Next
  6. Click `Finish`
  7. Copy the `Metadata URL` from the `Sign On` page to use in the next step


**Step 2: Set up LangSmith SSO Configuration**
Follow the instructions under [initial configuration](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#initial-configuration) in the `Fill in required information` step, using the metadata URL from the previous step.
**Step 3: Assign users to LangSmith in Okta**
  1. Under `Applications > Applications`, select the SAML application created in Step 1
  2. Under the `Assignments` tab, click `Assign` then either `Assign to People` or `Assign to Groups`
  3. Make the desired selection(s), then `Assign` and `Done`


**Step 4: Verify the SSO setup**
Have a user with access sign in via the unique login URL from the `SSO Configuration` page, or have a user select the application from their Okta dashboard.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/administration/how_to_guides/organization_management/set_up_saml_sso%3E).
[PreviousSet up resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)[NextConceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [What is SAML SSO?](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#what-is-saml-sso)
  * [Benefits of SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#benefits-of-sso)
  * [Set up SAML SSO for your organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#set-up-saml-sso-for-your-organization)
    * [Prerequisites](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#prerequisites)
    * [Initial configuration](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#initial-configuration)
    * [Editing SAML SSO settings](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#editing-saml-sso-settings)
  * [Just-in-time (JIT) provisioning](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#just-in-time-jit-provisioning)
  * [Login methods and access](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#login-methods-and-access)
  * [Enforce SAML SSO only](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#enforce-saml-sso-only)
  * [Support and troubleshooting](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#support-and-troubleshooting)
    * [FAQ](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#faq)
  * [Identity Provider (IdP) Setup](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#identity-provider-idp-setup)
    * [Entra ID (Azure)](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#entra-id-azure)
    * [Google](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#google)
    * [Okta](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso#okta)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace

[Skip to main content](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
        * [Create an account and API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
        * [Set up an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
        * [Set up a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
        * [Set up billing for your LangSmith account](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
        * [Update invoice email, tax id and, business information](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
        * [Manage your organization using the API](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)
        * [Set up access control](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
        * [Set up resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
        * [SAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
    * [Evaluation](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)


  * [](https://docs.smith.langchain.com/)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
  * Setup
  * Set up a workspace


On this page
# Set up a workspace
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Conceptual guide on organizations and workspaces](https://docs.smith.langchain.com/administration/concepts)


When you log in for the first time, a default [workspace](https://docs.smith.langchain.com/administration/concepts#workspaces) will be created for you automatically in your [personal organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#personal-vs-shared-organizations). Workspaces are often used to separate resources between different teams or business units, ensuring clear trust boundaries between them. Within each workspace, Role-Based Access Control (RBAC) is implemented to manage permissions and access levels, ensuring that users only have access to the resources and settings necessary for their role. Most LangSmith activity happens in the context of a workspace, each of which has its own settings and access controls.
To organize resources _within_ a workspace, you can use [resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags).
## Create a workspace[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace#create-a-workspace "Direct link to Create a workspace")
To create a new workspace, head to the [Settings page](https://smith.langchain.com/settings) `Workspaces` tab in your shared organization and click **Add Workspace**. Once your workspace has been created, you can manage its members and other configuration by selecting it on this page.
![](https://docs.smith.langchain.com/assets/images/create_workspace-8acf7883bef46bd556eea8c5a83584dc.png)
note
Different plans have different limits placed on the number of workspaces that can be used in an organization. Please see the [pricing page](https://www.langchain.com/pricing-langsmith) for more information.
## Manage users[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace#manage-users "Direct link to Manage users")
info
Only workspace `Admins` may manage workspace membership and, if RBAC is enabled, change a user's workspace role.
For users that are already members of an organization, a workspace admin may add them to a workspace in the `Workspace members` tab under [workspace settings page](https://smith.langchain.com/settings/workspaces). Users may also be invited directly to one or more workspaces when they are [invited to an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization#manage-users).
## Configure workspace settings[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace#configure-workspace-settings "Direct link to Configure workspace settings")
Workspace configuration exists in the [workspace settings page](https://smith.langchain.com/settings/workspaces) tab. Select the workspace to configure and then the desired configuration sub-tab. The example below shows the `API keys`, and other configuration options including secrets, models, and shared URLs are available here as well.
![](https://docs.smith.langchain.com/assets/images/workspace_settings-0dc7f4a80b3620ca403d04d97978b513.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/administration/how_to_guides/organization_management/set_up_workspace%3E).
[PreviousSet up an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)[NextSet up billing for your LangSmith account](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
  * [Create a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace#create-a-workspace)
  * [Manage users](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace#manage-users)
  * [Configure workspace settings](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace#configure-workspace-settings)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info

[Skip to main content](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
        * [Create an account and API key](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key)
        * [Set up an organization](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization)
        * [Set up a workspace](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_workspace)
        * [Set up billing for your LangSmith account](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)
        * [Update invoice email, tax id and, business information](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
        * [Manage your organization using the API](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)
        * [Set up access control](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control)
        * [Set up resource tags](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_resource_tags)
        * [SAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
    * [Evaluation](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info)


  * [](https://docs.smith.langchain.com/)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
  * Setup
  * Update invoice email, tax id and, business information


On this page
# Update invoice email, tax id and, business information
To update business information for your LangSmith organization, head to the
[Usage and Billing](https://smith.langchain.com/settings/payments) page under Settings and click on the [Plans and Billing](https://smith.langchain.com/settings/payments?tab=2) tab.
note
Business information, tax id and invoice email can only be updated for the plus and start up plans. Free and developer plans cannot update this information.
## Update invoice email[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info#update-invoice-email "Direct link to Update invoice email")
![](https://docs.smith.langchain.com/assets/images/update_invoice_email-0176d7a1bdcf1b1eec8400d293322864.png)
To update the email address where your invoices are sent, follow these steps:
  1. Navigate to the Plans and Billing tab.
  2. Locate the section beneath the payment method, where the current invoice email is displayed.
  3. Enter the new email address you want invoices to be sent to in the provided field.
  4. The new email address will be automatically saved.


This ensures that all future invoices will be sent to the updated email address.
## Update business information and tax id[‚Äã](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info#update-business-information-and-tax-id "Direct link to Update business information and tax id")
note
In certain jurisdictions, LangSmith is required to collect sales tax. If you are a business, providing your Tax ID may qualify you for a sales tax exemption.
![](https://docs.smith.langchain.com/assets/images/update_business_info-8fa3d60877ad424951ac5d2215fa0c38.png)
To update your organization's business information, follow these steps:
  1. Navigate to the Plans and Billing tab.
  2. Below the invoice email section, you will find a checkbox labeled Business.
  3. Check the Business checkbox if your organization belongs to a business.
  4. A business information section will appear, allowing you to enter or update the following details: 
     * Business Name
     * Address
     * Tax ID for applicable jurisdictions
  5. Tax ID field will appear for applicable jurisdictions after you select a country.
  6. After entering the necessary information, click the Save button to save your changes.


This ensures that your business information is up-to-date and accurate for billing and tax purposes.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/administration/how_to_guides/organization_management/update_business_info%3E).
[PreviousSet up billing for your LangSmith account](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_billing)[NextManage your organization using the API](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api)
  * [Update invoice email](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info#update-invoice-email)
  * [Update business information and tax id](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/update_business_info#update-business-information-and-tax-id)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/tutorials

[Skip to main content](https://docs.smith.langchain.com/administration/tutorials#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/tutorials)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/tutorials)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/administration/tutorials)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/administration/tutorials)
    * [Evaluation](https://docs.smith.langchain.com/administration/tutorials)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/administration/tutorials)


  * [](https://docs.smith.langchain.com/)
  * Administration


# Tutorials
New to LangSmith or to LLM app development in general? Read this material to quickly get up and running.
  * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/administration/tutorials%3E).
[PreviousDeployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)[NextTutorials](https://docs.smith.langchain.com/administration/tutorials)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/administration/tutorials/manage_spend

[Skip to main content](https://docs.smith.langchain.com/administration/tutorials/manage_spend#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
    * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
      * [Optimize tracing spend on LangSmith](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [How-to Guides](https://docs.smith.langchain.com/administration/how_to_guides)
      * [Setup](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [Conceptual Guide](https://docs.smith.langchain.com/administration/concepts)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
    * [Evaluation](https://docs.smith.langchain.com/administration/tutorials/manage_spend)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/administration/tutorials/manage_spend)


  * [](https://docs.smith.langchain.com/)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Tutorials](https://docs.smith.langchain.com/administration/tutorials)
  * Optimize tracing spend on LangSmith


On this page
# Optimize tracing spend on LangSmith
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Data Retention Conceptual Docs](https://docs.smith.langchain.com/administration/concepts#data-retention)
  * [Usage Limiting Conceptual Docs](https://docs.smith.langchain.com/administration/concepts#usage-limit)


note
Some of the features mentioned in this guide are not currently available in Enterprise plan due to its custom nature of billing. If you are on Enterprise plan and have questions about cost optimization, please reach out to your sales rep or support@langchain.dev.
This tutorial walks through optimizing your spend on LangSmith. In it, we will learn how to optimize existing spend and prevent future overspend on a realistic real-world example. We will use an existing LangSmith organization with high usage. Concepts can be transferred to your own organization.
## Problem Setup[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#problem-setup "Direct link to Problem Setup")
In this tutorial, we take an existing organization that has three workspaces, one for each deployment stage (Dev, Staging, and Prod):
![](https://docs.smith.langchain.com/assets/images/workspaces-a0ac918a9cf0ac7c37608f55e421b089.png)
## Understand your current usage[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#understand-your-current-usage "Direct link to Understand your current usage")
The first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph and Invoices.
### Usage Graph[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#usage-graph "Direct link to Usage Graph")
The usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show spend (which we will see later on our draft invoice).
We can navigate to the Usage Graph under `Settings` -> `Usage and Billing` -> `Usage Graph`.
![](https://docs.smith.langchain.com/assets/images/p1usagegraph_v2-4be4875dfec4a0ec60344bbe3c62fd9d.png)
We see in the graph above that there are two usage metrics that LangSmith charges for:
  * LangSmith Traces (Base Charge)
  * LangSmith Traces (Extended Data Retention Upgrades).


The first metric tracks all traces that you send to LangSmith. The second tracks all traces that also have our Extended 400 Day Data Retention. For more details, see our [data retention conceptual docs](https://docs.smith.langchain.com/administration/concepts#data-retention). Notice that these graphs look identical, which will come into play later in the tutorial.
LangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in our example), or teams within an organization. As a LangSmith administrator, we want to understand spend granularly per each of these units. In this case where we just want to cut spend, we can focus on the environment responsible for the majority of costs first for the greatest savings.
note
LangSmith's Usage Graph and Invoice use the term `tenant_id` to refer to a workspace ID. They are interchangeable.
In the above image, the vast majority of usage is in the workspace with ID `c27dd32c-7c80-4e8c-acde-bfcb67a90ab2`. We can go to `Settings` -> `Workspaces`, and hover our mouse over the `Workspace ID` button to find the one with a matching ID. In this case, it's the Prod workspace:
![](https://docs.smith.langchain.com/assets/images/p1findworkspaceid_v2-823e45ef409fc43da50843690099f821.png)
### Invoices[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#invoices "Direct link to Invoices")
We understand what usage looks like in terms of traces, but we now need to translate that into spend. To do so, we head to the `Invoices` tab. The first invoice that will appear on screen is a draft of your current month's invoice, which shows your running spend thus far this month.
![](https://docs.smith.langchain.com/assets/images/invoice_investigation_v2-bb51cf36cb68c049dc0a6e7d7cd2d242.gif)
In the above GIF, we see that the charges for LangSmith Traces are broken up by "tenant_id" (i.e. Workspace ID), meaning we can track tracing spend on each of our workspaces. In the first few days of June, the vast majority of the total spend of ~$2,000 is in our production workspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades.
These upgrades occur for two reasons:
  1. You use extended data retention tracing, meaning that, by default, your traces are retained for 400 days
  2. You use base data retention tracing, and use a feature that automatically extends the data retention of a trace ([see our Auto-Upgrade conceptual docs](https://docs.smith.langchain.com/administration/concepts#data-retention))


Given that the number of total traces per day is equal to the number of extended retention traces per day, it's most likely the case that this org is using extended data retention tracing everywhere. As such, we start by optimizing our retention settings.
## Optimization 1: manage data retention[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#optimization-1-manage-data-retention "Direct link to Optimization 1: manage data retention")
LangSmith charges differently based on a trace's data retention (see our [data retention conceptual docs](https://docs.smith.langchain.com/administration/concepts#data-retention)), where short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, we will show how to get optimal settings for data retention without sacrificing historical observability, and show the effect it has on our bill.
### Change org level retention defaults for new projects[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#change-org-level-retention-defaults-for-new-projects "Direct link to Change org level retention defaults for new projects")
We navigate to the `Usage configuration` tab, and look at our organization level retention settings. Modifying this setting affects all **new projects** that are created going forward in all workspaces in our org.
note
For backwards compatibility, older organizations may have this defaulted to Extended. Organizations created after June 3rd have this defaulted to Base.
![](https://docs.smith.langchain.com/assets/images/p1orgretention_v2-3855a68ca98aee1b023ef79dd8200d81.png)
### Change project level retention defaults[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#change-project-level-retention-defaults "Direct link to Change project level retention defaults")
Our existing projects have not changed their data retention settings, so we can change these on the individual project pages.
We navigate to `Projects` -> `<your project name>`, click the data retention drop down, and modify it to base retention. As with the organization level setting, this will only affect retention (and pricing) for traces going forward.
![](https://docs.smith.langchain.com/assets/images/p1projectretention-e0544167f2d217e3ccf58da299ce740d.png)
### Keep around a percentage of traces for extended data retention[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#keep-around-a-percentage-of-traces-for-extended-data-retention "Direct link to Keep around a percentage of traces for extended data retention")
We may not want all our traces to expire after 14 days if we care about historical debugging. As such, we can take advantage of LangSmith's built in ability to do server side sampling for extended data retention.
Choosing the right percentage of runs to sample depends on your use case. We will arbitrarily pick 10% of runs here, but will leave it to the user to find the right value that balances collecting rare events and cost constraints.
LangSmith automatically upgrades the data retention for any trace that matches a run rule in our automations product (see our [run rules docs](https://docs.smith.langchain.com/observability/how_to_guides/rules)). On the projects page, click `Rules -> Add Rule`, and configure the rule as follows:
![](https://docs.smith.langchain.com/assets/images/P2SampleTraces-2e21e456daa11a6b2d371f1997cefe13.png)
Run rules match on runs rather than traces. Runs are single units of work within an LLM application's API handling. Traces are end to end API calls (learn more about [tracing concepts in LangSmith](https://docs.smith.langchain.com/observability/concepts)). This means a trace can be thought of as a tree of runs making up an API call. When a run rule matches any run within a trace, the trace's full run tree upgrades to be retained for 400 days.
Therefore, to make sure we have the proper sampling rate on traces, we take advantage of the [filtering](https://docs.smith.langchain.com/observability/how_to_guides/rules#step-2-define-the-filter) functionality of run rules.
We add add a filter condition to only match the "root" run in the run tree. This is distinct per trace, so our 10% sampling will upgrade 10% of traces, rather 10% of runs, which could correspond to more than 10% of traces. If desired, we can optionally add any other filtering conditions required (e.g. specific tags/metadata attached to our traces) for more pointed data retention extension. For the sake of this tutorial, we will stick with the simplest condition, and leave more advanced filtering as an exercise to the user.
note
If you want to keep a subset of traces for longer than 400 days for data collection purposes, you can create another run rule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.
### See results after 7 days[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#see-results-after-7-days "Direct link to See results after 7 days")
While the total amount of traces per day stayed the same, the extended data retention traces was cut heavily.
![](https://docs.smith.langchain.com/assets/images/p1endresultgraph_v2-b4e025baad3bfa84f9f0d9ea009e56e9.png)
This translates to the invoice, where we've only spent about $900 in the last 7 days, as opposed to $2,000 in the previous 4. That's a cost reduction of nearly 75% per day!
![](https://docs.smith.langchain.com/assets/images/p1endresultinvoice_v2-e2dc147344fcff6054c0ac9e7b4af02b.png)
## Optimization 2: limit usage[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#optimization-2-limit-usage "Direct link to Optimization 2: limit usage")
In the previous section, we managed data retention settings to _optimize existing spend_. In this section, we will use usage limits to _prevent future overspend_.
LangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we've been tracking on our usage graph. We can use these in tandem to have granular control over spend.
To set limits, we navigate back to `Settings` -> `Usage and Billing` -> `Usage configuration`. There is a table at the bottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along with a cost estimate:
![](https://docs.smith.langchain.com/assets/images/p2usagelimitsempty_v2-88aa6701942f71348900d0b7f3bd8d49.png)
Lets start by setting limits on our production usage, since that is where the majority of spend comes from.
### Setting a good total traces limit[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#setting-a-good-total-traces-limit "Direct link to Setting a good total traces limit")
Picking the right "total traces" limit depends on the expected load of traces that you will send to LangSmith. You should clearly think about your assumptions before setting a limit.
For example:
  * **Current Load** : Our gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it, meaning we log around 100,000-130,000 traces per day
  * **Expected Growth in Load** : We expect to double in size in the near future.


From these assumptions, we can do a quick back-of-the-envelope calculation to get a good limit of:
```
limit = current_load_per_day * expected_growth * days/month   = 130,000 * 2 * 30   = 7,800,000 traces / month
```

We click on the edit icon on the right side of the table for our Prod row, and can enter this limit as follows:
![](https://docs.smith.langchain.com/assets/images/p2alllimitonly_v2-145644f03c894ffe68596de17e1b88cc.png)
note
When set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.
### Cutting maximum spend with an extended data retention limit[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#cutting-maximum-spend-with-an-extended-data-retention-limit "Direct link to Cutting maximum spend with an extended data retention limit")
If we are not a big enterprise, we may shudder at the ~$40k per month bill.
We saw from [Optimization 1](https://docs.smith.langchain.com/administration/tutorials/manage_spend#optimization-1-manage-data-retention) that the easiest way to cut cost was through managing data retention. The same can be said for limits. If we only want to keep ~10% of traces to be around more than 14 days, we can set a limit on the maximum high retention traces we can keep. That would be `.10 * 7,800,000 = 780,000`.
![](https://docs.smith.langchain.com/assets/images/p2bothlimits_v2-47813bbfc90f88540f0de6db5dd7aafc.png)
As we can see, the maximum cost is cut from ~40k per month to ~7.5k per month, because we no longer allow as many expensive data retention upgrades. This lets us be confident that new users on the platform will not accidentally cause cost to balloon.
note
The extended data retention limit can cause features other than traces to stop working once reached. If you plan to use this feature, please read more about its functionality [here](https://docs.smith.langchain.com/administration/concepts#side-effects-of-extended-data-retention-traces-limit).
### Set dev/staging limits and view total spent limit across workspaces[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#set-devstaging-limits-and-view-total-spent-limit-across-workspaces "Direct link to Set dev/staging limits and view total spent limit across workspaces")
Following the same logic for our dev and staging environments, we set limits at 10% of the production limit on usage for each workspace.
While this works with our usage pattern, setting good dev and staging limits may vary depending on your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may want to be more liberal with your usage limits to avoid test failures.
Now that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces:
![](https://docs.smith.langchain.com/assets/images/p2totalspendlimits_v2-8932659a5b62eada73977bfda80c7f7e.png)
With this estimator, we can be confident that we will not end up with an unexpected credit card bill at the end of the month.
## Summary[‚Äã](https://docs.smith.langchain.com/administration/tutorials/manage_spend#summary "Direct link to Summary")
In this tutorial, we learned how to:
  1. Cut down our existing costs with data retention policies
  2. Prevent future overspend with usage limits


If you have questions about further optimizing your spend, please reach out to support@langchain.dev.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/administration/tutorials/manage_spend%3E).
[PreviousTutorials](https://docs.smith.langchain.com/administration/tutorials)[NextAdministration how-to guides](https://docs.smith.langchain.com/administration/how_to_guides)
  * [Problem Setup](https://docs.smith.langchain.com/administration/tutorials/manage_spend#problem-setup)
  * [Understand your current usage](https://docs.smith.langchain.com/administration/tutorials/manage_spend#understand-your-current-usage)
    * [Usage Graph](https://docs.smith.langchain.com/administration/tutorials/manage_spend#usage-graph)
    * [Invoices](https://docs.smith.langchain.com/administration/tutorials/manage_spend#invoices)
  * [Optimization 1: manage data retention](https://docs.smith.langchain.com/administration/tutorials/manage_spend#optimization-1-manage-data-retention)
    * [Change org level retention defaults for new projects](https://docs.smith.langchain.com/administration/tutorials/manage_spend#change-org-level-retention-defaults-for-new-projects)
    * [Change project level retention defaults](https://docs.smith.langchain.com/administration/tutorials/manage_spend#change-project-level-retention-defaults)
    * [Keep around a percentage of traces for extended data retention](https://docs.smith.langchain.com/administration/tutorials/manage_spend#keep-around-a-percentage-of-traces-for-extended-data-retention)
    * [See results after 7 days](https://docs.smith.langchain.com/administration/tutorials/manage_spend#see-results-after-7-days)
  * [Optimization 2: limit usage](https://docs.smith.langchain.com/administration/tutorials/manage_spend#optimization-2-limit-usage)
    * [Setting a good total traces limit](https://docs.smith.langchain.com/administration/tutorials/manage_spend#setting-a-good-total-traces-limit)
    * [Cutting maximum spend with an extended data retention limit](https://docs.smith.langchain.com/administration/tutorials/manage_spend#cutting-maximum-spend-with-an-extended-data-retention-limit)
    * [Set dev/staging limits and view total spent limit across workspaces](https://docs.smith.langchain.com/administration/tutorials/manage_spend#set-devstaging-limits-and-view-total-spent-limit-across-workspaces)
  * [Summary](https://docs.smith.langchain.com/administration/tutorials/manage_spend#summary)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation

[Skip to main content](https://docs.smith.langchain.com/evaluation#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * Evaluation


# Evaluation Quick Start
Evaluations are a quantitative way to measure performance of LLM applications, which is important beacause LLMs don't always behave predictably ‚Äî small changes in prompts, models, or inputs can significantly impact results. Evaluations provide a structured way to identify failures, compare changes across different versions of your application, and build more reliable AI applications.
Evaluations are made up of three components:
  1. A [dataset](https://docs.smith.langchain.com/evaluation/concepts#datasets) with test inputs and optionally expected outputs.
  2. A [target function](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target) that defines what you're evaluating. For example, this may be one LLM call that includes the new prompt you are testing, a part of your application or your end to end application.
  3. [Evaluators](https://docs.smith.langchain.com/evaluation/concepts#evaluators) that score your target function's outputs.


This quick start guides you through running a simple evaluation to test the correctness of LLM responses with the LangSmith SDK or UI.
  * SDK
  * UI


## 1. Install Dependencies[‚Äã](https://docs.smith.langchain.com/evaluation#1-install-dependencies "Direct link to 1. Install Dependencies")
  * Python
  * TypeScript


```
pip install -U langsmith openai pydantic
```

```
yarn add langsmith openai zod
```

## 2. Create an API key[‚Äã](https://docs.smith.langchain.com/evaluation#2-create-an-api-key "Direct link to 2. Create an API key")
To create an API key head to the [Settings page](https://smith.langchain.com/settings). Then click **Create API Key.**
## 3. Set up your environment[‚Äã](https://docs.smith.langchain.com/evaluation#3-set-up-your-environment "Direct link to 3. Set up your environment")
  * Shell


```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY="<your-langchain-api-key>"# The example uses OpenAI, but it's not necessary in generalexport OPENAI_API_KEY="<your-openai-api-key>"
```

## 4. Import dependencies[‚Äã](https://docs.smith.langchain.com/evaluation#4-import-dependencies "Direct link to 4. Import dependencies")
  * Python
  * TypeScript


```
from langsmith import wrappers, Clientfrom pydantic import BaseModel, Fieldfrom openai import OpenAIclient = Client()openai_client = wrappers.wrap_openai(OpenAI())
```

```
import{ Client }from"langsmith";import OpenAI from"openai";import{ z }from"zod";import{ zodResponseFormat }from"openai/helpers/zod";importtype{ EvaluationResult }from"langsmith/evaluation";import{ evaluate }from"langsmith/evaluation";const client =newClient();const openai =newOpenAI();
```

## 5. Create a dataset[‚Äã](https://docs.smith.langchain.com/evaluation#5-create-a-dataset "Direct link to 5. Create a dataset")
  * Python
  * TypeScript


```
# For other dataset creation methods, see:# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application# Programmatically create a dataset in LangSmithdataset = client.create_dataset( dataset_name="Sample dataset", description="A sample dataset in LangSmith.")# Create examplesexamples =[{"inputs":{"question":"Which country is Mount Kilimanjaro located in?"},"outputs":{"answer":"Mount Kilimanjaro is located in Tanzania."},},{"inputs":{"question":"What is Earth's lowest point?"},"outputs":{"answer":"Earth's lowest point is The Dead Sea."},},]# Add examples to the datasetclient.create_examples(dataset_id=dataset.id, examples=examples)
```

```
// For other dataset creation methods, see: // https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically // https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application// Create inputs and reference outputsconst examples:[string,string][]=[["Which country is Mount Kilimanjaro located in?","Mount Kilimanjaro is located in Tanzania.",],["What is Earth's lowest point?","Earth's lowest point is The Dead Sea.",],];const inputs = examples.map(([inputPrompt])=>({question: inputPrompt,}));const outputs = examples.map(([, outputAnswer])=>({answer: outputAnswer,}));// Programmatically create a dataset in LangSmithconst dataset =await client.createDataset("Sample dataset",{description:"A sample dataset in LangSmith.",});// Add examples to the datasetawait client.createExamples({inputs,outputs,datasetId: dataset.id,});
```

## 6. Define what you're evaluating[‚Äã](https://docs.smith.langchain.com/evaluation#6-define-what-youre-evaluating "Direct link to 6. Define what you're evaluating")
  * Python
  * TypeScript


```
# Define the application logic you want to evaluate inside a target function# The SDK will automatically send the inputs from the dataset to your target functiondeftarget(inputs:dict)->dict: response = openai_client.chat.completions.create(   model="gpt-4o-mini",   messages=[{"role":"system","content":"Answer the following question accurately"},{"role":"user","content": inputs["question"]},],)return{"response": response.choices[0].message.content.strip()}
```

```
// Define the application logic you want to evaluate inside a target function// The SDK will automatically send the inputs from the dataset to your target functionasyncfunctiontarget(inputs:string):Promise<{ response:string}>{const response =await openai.chat.completions.create({ model:"gpt-4o-mini", messages:[{ role:"system", content:"Answer the following question accurately"},{ role:"user", content: inputs },],});return{ response: response.choices[0].message.content?.trim()||""};}
```

## 7. Define evaluator[‚Äã](https://docs.smith.langchain.com/evaluation#7-define-evaluator "Direct link to 7. Define evaluator")
  * Python
  * TypeScript


```
# Define instructions for the LLM judge evaluatorinstructions ="""Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: - False: No conceptual match and similarity- True: Most or full conceptual match and similarity- Key criteria: Concept should match, not exact wording."""# Define output schema for the LLM judgeclassGrade(BaseModel): score:bool= Field(   description="Boolean that indicates whether the response is accurate relative to the reference answer")# Define LLM judge that grades the accuracy of the response relative to reference outputdefaccuracy(outputs:dict, reference_outputs:dict)->bool: response = openai_client.beta.chat.completions.parse(   model="gpt-4o-mini",   messages=[{"role":"system","content": instructions},{"role":"user","content":f"""Ground Truth answer: {reference_outputs["answer"]};        Student's Answer: {outputs["response"]}"""},],   response_format=Grade,)return response.choices[0].message.parsed.score
```

```
// Define instructions for the LLM judge evaluatorconst instructions =`Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: - False: No conceptual match and similarity- True: Most or full conceptual match and similarity- Key criteria: Concept should match, not exact wording.`;// Define context for the LLM judge evaluatorconst context =`Ground Truth answer: {reference}; Student's Answer: {prediction}`;// Define output schema for the LLM judgeconst ResponseSchema = z.object({score: z.boolean().describe("Boolean that indicates whether the response is accurate relative to the reference answer"),});// Define LLM judge that grades the accuracy of the response relative to reference outputasyncfunctionaccuracy({outputs,referenceOutputs,}:{outputs?: Record<string,string>;referenceOutputs?: Record<string,string>;}):Promise<EvaluationResult>{const response =await openai.chat.completions.create({ model:"gpt-4o-mini", messages:[{ role:"system", content: instructions },{ role:"user", content: context.replace("{prediction}", outputs?.answer ||"").replace("{reference}", referenceOutputs?.answer ||"")}], response_format:zodResponseFormat(ResponseSchema,"response")});return{ key:"accuracy", score: ResponseSchema.parse(JSON.parse(response.choices[0].message.content ||"")).score,};}
```

## 8. Run and view results[‚Äã](https://docs.smith.langchain.com/evaluation#8-run-and-view-results "Direct link to 8. Run and view results")
  * Python
  * TypeScript


```
# After running the evaluation, a link will be provided to view the results in langsmithexperiment_results = client.evaluate( target, data="Sample dataset", evaluators=[   accuracy,# can add multiple evaluators here], experiment_prefix="first-eval-in-langsmith", max_concurrency=2,)
```

```
// After running the evaluation, a link will be provided to view the results in langsmithawaitevaluate((exampleInput)=>{returntarget(exampleInput.question);},{ data:"Sample dataset", evaluators:[  accuracy,// can add multiple evaluators here], experimentPrefix:"first-eval-in-langsmith", maxConcurrency:2,});
```

Click the link printed out by your evaluation run to access the LangSmith Experiments UI, and explore the results of your evaluation.
![](https://docs.smith.langchain.com/assets/images/view_experiment-6328bb0fb0d033a49b381d84a3f9b1e8.gif)
## Next steps[‚Äã](https://docs.smith.langchain.com/evaluation#next-steps "Direct link to Next steps")
tip
To learn more about running experiments in LangSmith, read the [evaluation conceptual guide](https://docs.smith.langchain.com/evaluation/concepts).
  * See the [How-to guides](https://docs.smith.langchain.com/evaluation/how_to_guides) for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.
  * For end-to-end walkthroughs see [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials).
  * For comprehensive descriptions of every class and function see the [API reference](https://langsmith-sdk.readthedocs.io/en/latest/evaluation.html).


If you prefer video tutorials, check out the [Datasets, Evaluators, and Experiments videos](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.
## 1. Navigate to the Playground[‚Äã](https://docs.smith.langchain.com/evaluation#1-navigate-to-the-playground "Direct link to 1. Navigate to the Playground")
LangSmith's [Prompt Playground](https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-playground) makes it possible run evaluations over diffrent prompts, new models or test different model configuration. Go to LangSmith's **Playground** in the UI.
## 2. Create a prompt[‚Äã](https://docs.smith.langchain.com/evaluation#2-create-a-prompt "Direct link to 2. Create a prompt")
Modify the system prompt to:
```
Answer the following question accurately:
```

## 3. Create a dataset[‚Äã](https://docs.smith.langchain.com/evaluation#3-create-a-dataset "Direct link to 3. Create a dataset")
Click **Set up Evaluation** , then use the **+ New** button in the dropdown to create a new dataset.
Add the following examples to the dataset:
Inputs| Reference Outputs  
---|---  
question: Which country is Mount Kilimanjaro located in?| output: Mount Kilimanjaro is located in Tanzania.  
question: What is Earth's lowest point?| output: Earth's lowest point is The Dead Sea.  
Press **Save** to save your newly created dataset.
## 4. Add an evaluator[‚Äã](https://docs.smith.langchain.com/evaluation#4-add-an-evaluator "Direct link to 4. Add an evaluator")
Click **+Evaluator**. Select **Correctness** from the pre-built evaluator options. Press **Save**.
## 5. Run your evaluation[‚Äã](https://docs.smith.langchain.com/evaluation#5-run-your-evaluation "Direct link to 5. Run your evaluation")
Press **Start** on the top right to run your evaluation. Running this evaluation will create an experiment that you can view in full by clicking the experiment name.
![](https://docs.smith.langchain.com/assets/images/evals-quick-start-63f56f94f3e44bec97c25c2401b9f175.gif)
## Next steps[‚Äã](https://docs.smith.langchain.com/evaluation#next-steps-1 "Direct link to Next steps")
tip
To learn more about running experiments in LangSmith, read the [evaluation conceptual guide](https://docs.smith.langchain.com/evaluation/concepts).
See the [How-to guides](https://docs.smith.langchain.com/evaluation/how_to_guides) for answers to ‚ÄúHow do I‚Ä¶.?‚Äù format questions.
  * Learn how to [create and manage datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#set-up-your-dataset)
  * Learn how to [run an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)


If you prefer video tutorials, check out the [Playground videos](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousConceptual Guide](https://docs.smith.langchain.com/observability/concepts)[NextQuick Start](https://docs.smith.langchain.com/evaluation)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/concepts

[Skip to main content](https://docs.smith.langchain.com/evaluation/concepts#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/concepts)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/concepts)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/concepts)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/concepts)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/concepts)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * Conceptual Guide


On this page
# Evaluation concepts
The quality and development speed of AI applications is often limited by high-quality evaluation datasets and metrics, which enable you to both optimize and test your applications.
LangSmith makes building high-quality evaluations easy. This guide explains the LangSmith evaluation framework and AI evaluation techniques more broadly. The building blocks of the LangSmith framework are:
  * [**Datasets** :](https://docs.smith.langchain.com/evaluation/concepts#datasets) Collections of test inputs and reference outputs.
  * [**Evaluators**](https://docs.smith.langchain.com/evaluation/concepts#evaluators): Functions for scoring outputs.


## Datasets[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#datasets "Direct link to Datasets")
A dataset is a collection of examples used for evaluating an application. An example is a test input, reference output pair.
![Dataset](https://docs.smith.langchain.com/assets/images/dataset_concept-8237578f80243a73b10a5451fb20660a.png)
### Examples[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#examples "Direct link to Examples")
Each example consists of:
  * **Inputs** : a dictionary of input variables to pass to your application.
  * **Reference outputs** (optional): a dictionary of reference outputs. These do not get passed to your application, they are only used in evaluators.
  * **Metadata** (optional): a dictionary of additional information that can be used to create filtered views of a dataset.


![Example](https://docs.smith.langchain.com/assets/images/example_concept-05b9ba24b73114b4209aa8ce0eb1f738.png)
### Dataset curation[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#dataset-curation "Direct link to Dataset curation")
There are various ways to build datasets for evaluation, including:
#### Manually curated examples[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#manually-curated-examples "Direct link to Manually curated examples")
This is how we typically recommend people get started creating datasets. From building your application, you probably have some idea of what types of inputs you expect your application to be able to handle, and what "good" responses may be. You probably want to cover a few different common edge cases or situations you can imagine. Even 10-20 high-quality, manually-curated examples can go a long way.
#### Historical traces[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#historical-traces "Direct link to Historical traces")
Once you have an application in production, you start getting valuable information: how are users actually using it? These real-world runs make for great examples because they're, well, the most realistic!
If you're getting a lot of traffic, how can you determine which runs are valuable to add to a dataset? There are a few techniques you can use:
  * **User feedback** : If possible - try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future.
  * **Heuristics** : You can also use other heuristics to identify "interesting" datapoints. For example, runs that took a long time to complete could be interesting to look at and add to a dataset.
  * **LLM feedback** : You can use another LLM to detect noteworthy runs. For example, you could use an LLM to label chatbot conversations where the user had to rephrase their question or correct the model in some way, indicating the chatbot did not initially respond correctly.


#### Synthetic data[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#synthetic-data "Direct link to Synthetic data")
Once you have a few examples, you can try to artificially generate some more. It's generally advised to have a few good hand-crafted examples before this, as this synthetic data will often resemble them in some way. This can be a useful way to get a lot of datapoints, quickly.
### Splits[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#splits "Direct link to Splits")
When setting up your evaluation, you may want to partition your dataset into different splits. For example, you might use a smaller split for many rapid and cheap iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately.
Learn how to [create and manage dataset splits](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-and-manage-dataset-splits).
### Versions[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#versions "Direct link to Versions")
Datasets are [versioned](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets) such that every time you add, update, or delete examples in your dataset, a new version of the dataset is created. This makes it easy to inspect and revert changes to your dataset in case you make a mistake. You can also [tag versions](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets#tag-a-version) of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset's history.
You can run evaluations on specific versions of a dataset. This can be useful when running evaluations in CI, to make sure that a dataset update doesn't accidentally break your CI pipelines.
## Evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#evaluators "Direct link to Evaluators")
Evaluators are functions that score how well your application performs on a particular example.
#### Evaluator inputs[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#evaluator-inputs "Direct link to Evaluator inputs")
Evaluators receive these inputs:
  * [Example](https://docs.smith.langchain.com/evaluation/concepts#examples): The example(s) from your [Dataset](https://docs.smith.langchain.com/evaluation/concepts#datasets). Contains inputs, (reference) outputs, and metadata.
  * [Run](https://docs.smith.langchain.com/observability/concepts#runs): The actual outputs and intermediate steps (child runs) from passing the example inputs to the application.


#### Evaluator outputs[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#evaluator-outputs "Direct link to Evaluator outputs")
An evaluator returns one or more metrics. These should be returned as a dictionary or list of dictionaries of the form:
  * `key`: The name of the metric.
  * `score` | `value`: The value of the metric. Use `score` if it's a numerical metric and `value` if it's categorical.
  * `comment` (optional): The reasoning or additional string information justifying the score.


#### Defining evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#defining-evaluators "Direct link to Defining evaluators")
There are a number of ways to define and run evaluators:
  * **Custom code** : Define [custom evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator) as Python or TypeScript functions and run them client-side using the SDKs or server-side via the UI.
  * **Built-in evaluators** : LangSmith has a number of built-in evaluators that you can configure and run via the UI.


You can run evaluators using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python) and [TypeScript](https://docs.smith.langchain.com/reference/js)), via the [Prompt Playground](https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-playground), or by configuring [Rules](https://docs.smith.langchain.com/observability/how_to_guides/rules) to automatically run them on particular tracing projects or datasets.
#### Evaluation techniques[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#evaluation-techniques "Direct link to Evaluation techniques")
There are a few high-level approaches to LLM evaluation:
### Human[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#human "Direct link to Human")
Human evaluation is [often a great starting point for evaluation](https://hamel.dev/blog/posts/evals/#looking-at-your-traces). LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps).
LangSmith's [annotation queues](https://docs.smith.langchain.com/evaluation/concepts#annotation-queues) make it easy to get human feedback on your application's outputs.
### Heuristic[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#heuristic "Direct link to Heuristic")
Heuristic evaluators are deterministic, rule-based functions. These are good for simple checks like making sure that a chatbot's response isn't empty, that a snippet of generated code can be compiled, or that a classification is exactly correct.
### LLM-as-judge[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#llm-as-judge "Direct link to LLM-as-judge")
LLM-as-judge evaluators use LLMs to score the application's output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference output (e.g., check if the output is factually accurate relative to the reference).
With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often it is helpful to write these as few-shot evaluators, where you provide examples of inputs, outputs, and expected grades as part of the grader prompt.
Learn about [how to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge).
### Pairwise[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#pairwise "Direct link to Pairwise")
Pairwise evaluators allow you to compare the outputs of two versions of an application. Think [LMSYS Chatbot Arena](https://chat.lmsys.org/) - this is the same concept, but applied to AI applications more generally, not just models! This can use either a heuristic ("which response is longer"), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples).
**When should you use pairwise evaluation?**
Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs. This can be the case for tasks like summarization - it may be hard to give a summary an absolute score, but easy to choose which of two summaries is more informative.
Learn [how run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise).
## Experiment[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#experiment "Direct link to Experiment")
Each time we evaluate an application on a dataset, we are conducting an experiment. An experiment contains the results of running a specific version of your application on the dataset. To understand how to use the LangSmith experiment view, see [how to analyze experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment).
![Experiment view](https://docs.smith.langchain.com/assets/images/experiment_view-5e544109c90f6ea9682bc70ebde47701.png)
Typically, we will run multiple experiments on a given dataset, testing different configurations of our application (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset. Additionally, you can [compare multiple experiments in a comparison view](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results).
![Comparison view](https://docs.smith.langchain.com/assets/images/comparison_view-eca0ade727e51ed17dc156f402836597.png)
## Annotation queues[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#annotation-queues "Direct link to Annotation queues")
Human feedback is often the most valuable feedback you can gather on your application. With [annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues) you can flag runs of your application for annotation. Human annotators then have a streamlined view to review and provide feedback on the runs in a queue. Often (some subset of) these annotated runs are then transferred to a [dataset](https://docs.smith.langchain.com/evaluation/concepts#datasets) for future evaluations. While you can always [annotate runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline), annotation queues provide another option to group runs together, specify annotation criteria, and configure permissions.
Learn more about [annotation queues and human feedback](https://docs.smith.langchain.com/evaluation/how_to_guides#annotation-queues-and-human-feedback).
## Offline evaluation[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#offline-evaluation "Direct link to Offline evaluation")
Evaluating an application on a dataset is what we call "offline" evaluation. It is offline because we're evaluating on a pre-compiled set of data. An online evaluation, on the other hand, is one in which we evaluate a deployed application's outputs on real traffic, in near realtime. Offline evaluations are used for testing a version(s) of your application pre-deployment.
You can run offline evaluations client-side using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python) and [TypeScript](https://docs.smith.langchain.com/reference/js)). You can run them server-side via the [Prompt Playground](https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-playground) or by configuring [automations](https://docs.smith.langchain.com/observability/how_to_guides/rules) to run certain evaluators on every new experiment against a specific dataset.
![Offline](https://docs.smith.langchain.com/assets/images/offline-6c332e4bdb64400d46d3f29ff9fb68e5.png)
### Benchmarking[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#benchmarking "Direct link to Benchmarking")
Perhaps the most common type of offline evaluation is one in which we curate a dataset of representative inputs, define the key performance metrics, and benchmark multiple versions of our application to find the best one. Benchmarking can be laborious because for many use cases you have to curate a dataset with gold-standard reference outputs and design good metrics for comparing experimental outputs to them. For a RAG Q&A bot this might look like a dataset of questions and reference answers, and an LLM-as-judge evaluator that determines if the actual answer is semantically equivalent to the reference answer. For a ReACT agent this might look like a dataset of user requests and a reference set of all the tool calls the model is supposed to make, and a heuristic evaluator that checks if all of the reference tool calls were made.
### Unit tests[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#unit-tests "Direct link to Unit tests")
Unit tests are used in software development to verify the correctness of individual system components. [Unit tests in the context of LLMs are often rule-based assertions](https://hamel.dev/blog/posts/evals/#level-1-unit-tests) on LLM inputs or outputs (e.g., checking that LLM-generated code can be compiled, JSON can be loaded, etc.) that validate basic functionality.
Unit tests are often written with the expectation that they should always pass. These types of tests are nice to run as part of CI. Note that when doing so it is useful to set up a cache to minimize LLM calls (because those can quickly rack up!).
### Regression tests[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#regression-tests "Direct link to Regression tests")
Regression tests are used to measure performance across versions of your application over time. They are used to, at the very least, ensure that a new app version does not regress on examples that your current version correctly handles, and ideally to measure how much better your new version is relative to the current. Often these are triggered when you are making app updates (e.g. updating models or architectures) that are expected to influence the user experience.
LangSmith's comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline. Regressions are highlighted red, improvements green.
![Comparison view](https://docs.smith.langchain.com/assets/images/comparison_view-eca0ade727e51ed17dc156f402836597.png)
### Backtesting[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#backtesting "Direct link to Backtesting")
Backtesting is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs.
This is commonly used to evaluate new model versions. Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model. Then compare those results to what actually happened in production.
### Pairwise evaluation[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#pairwise-evaluation "Direct link to Pairwise evaluation")
For some tasks [it is easier](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/) for a human or LLM grader to determine if "version A is better than B" than to assign an absolute score to either A or B. Pairwise evaluations are just this ‚Äî a scoring of the outputs of two versions against each other as opposed to against some reference output or absolute criteria. Pairwise evaluations are often useful when using LLM-as-judge evaluators on more general tasks. For example, if you have a summarizer application, it may be easier for an LLM-as-judge to determine "Which of these two summaries is more clear and concise?" than to give an absolute score like "Give this summary a score of 1-10 in terms of clarity and concision."
Learn [how run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise).
## Online evaluation[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#online-evaluation "Direct link to Online evaluation")
Evaluating a deployed application's outputs in (roughly) realtime is what we call "online" evaluation. In this case there is no dataset involved and no possibility of reference outputs ‚Äî we're running evaluators on real inputs and real outputs as they're produced. This is useful for monitoring your application and flagging unintended behavior. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can be later used to curate a dataset for offline evaluation.
Online evaluators are generally intended to be run server-side. LangSmith has built-in [LLM-as-judge evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge) that you can configure, or you can define custom code evaluators that are also run within LangSmith.
![Online](https://docs.smith.langchain.com/assets/images/online-dd2d48ea3204a44705cd014b48ec67d0.png)
## Testing[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#testing "Direct link to Testing")
### Evaluations vs testing[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#evaluations-vs-testing "Direct link to Evaluations vs testing")
Testing and evaluation are very similar and overlapping concepts that often get confused.
**An evaluation measures performance according to a metric(s).** Evaluation metrics can be fuzzy or subjective, and are more useful in relative terms than absolute ones. That is, they're often used to compare two systems against each other rather than to assert something about an individual system.
**Testing asserts correctness.** A system can only be deployed if it passes all tests.
Evaluation metrics can be _turned into_ tests. For example, you can write regression tests to assert that any new version of a system must outperform some baseline version of the system on the relevant evaluation metrics.
It can also be more resource efficient to run tests and evaluations together if your system is expensive to run and you have overlapping datasets for your tests and evaluations.
You can also choose to write evaluations using standard software testing tools like `pytest` or `vitest/jest` out of convenience.
### Using `pytest` and `vitest/jest`[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#using-pytest-and-vitestjest "Direct link to using-pytest-and-vitestjest")
The LangSmith SDKs come with integrations for [pytest](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest) and [`vitest/jest`](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest). These make it easy to:
  * Track test results in LangSmith
  * Write evaluations as tests


Tracking test results in LangSmith makes it easy to share results, compare systems, and debug failing tests.
Writing evaluations as tests can be useful when each example you want to evaluate on requires custom logic for running the application and/or evaluators. The standard evaluation flows assume that you can run your application and evaluators in the same way on every example in a dataset. But for more complex systems or comprehensive evals, you may want to evaluate specific subsets of your system with specific types of inputs and metrics. These types of heterogenous evals are much easier to write as a suite of distinct test cases that all get tracked together rather than using the standard evaluate flow.
Using testing tools is also helpful when you want to _both_ evaluate your system's outputs _and_ assert some basic things about them.
## Application-specific techniques[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#application-specific-techniques "Direct link to Application-specific techniques")
Below, we will discuss evaluation of a few specific, popular LLM applications.
### Agents[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#agents "Direct link to Agents")
[LLM-powered autonomous agents](https://lilianweng.github.io/posts/2023-06-23-agent/) combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents [use tool calling](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/) with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. [Tool calling](https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling/) allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required.
![Tool use](https://docs.smith.langchain.com/assets/images/tool_use-ffe4e2defc68f34db45e0b05b4a4903a.png)
Below is a tool-calling agent in [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/). The `assistant node` is an LLM that determines whether to invoke a tool based upon the input. The `tool condition` sees if a tool was selected by the `assistant node` and, if so, routes to the `tool node`. The `tool node` executes the tool and returns the output as a tool message to the `assistant node`. This loop continues until as long as the `assistant node` selects a tool. If no tool is selected, then the agent directly returns the LLM response.
![Agent](https://docs.smith.langchain.com/assets/images/langgraph_agent-5ce2976f349a12fc55e99b1b2a0b3235.png)
This sets up three general types of agent evaluations that users are often interested in:
  * `Final Response`: Evaluate the agent's final response.
  * `Single step`: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).
  * `Trajectory`: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer.


![Agent-eval](https://docs.smith.langchain.com/assets/images/agent_eval-46c50aa604c69f6146319814d72f28d9.png)
Below we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this. Note that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!
#### Evaluating an agent's final response[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#evaluating-an-agents-final-response "Direct link to Evaluating an agent's final response")
One way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done.
The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don't need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time.
The output should be the agent's final response.
The evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response.
However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics.
#### Evaluating a single step of an agent[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#evaluating-a-single-step-of-an-agent "Direct link to Evaluating a single step of an agent")
Agents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do.
The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps.
The outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next.
The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string.
There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristic evaluation of the selected tool relative to the reference tool. One downside is that they don't capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent's trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses).
#### Evaluating an agent's trajectory[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#evaluating-an-agents-trajectory "Direct link to Evaluating an agent's trajectory")
Evaluating an agent's trajectory involves evaluating all the steps an agent took.
The inputs are again the inputs to the overall agent (the user input, and optionally a list of tools).
The outputs are a list of tool calls, which can be formulated as an "exact" trajectory (e.g., an expected sequence of tool calls) or simply a set of tool calls that are expected (in any order).
The evaluator here is some function over the steps taken. Assessing the "exact" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong.
To address these flaws, evaluation metrics can focused on the number of "incorrect" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order.
However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent's trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with.
### Retrieval augmented generation (RAG)[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#retrieval-augmented-generation-rag "Direct link to Retrieval augmented generation \(RAG\)")
Retrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.
info
For a comprehensive review of RAG concepts, see our [`RAG From Scratch` series](https://github.com/langchain-ai/rag-from-scratch).
#### Dataset[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#dataset "Direct link to Dataset")
When evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).
#### Evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#evaluator "Direct link to Evaluator")
`LLM-as-judge` is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.
![rag-types.png](https://docs.smith.langchain.com/assets/images/rag-types-4ff470c7c97ea68ec46956326513fb52.png)
When evaluating RAG applications, you can have evaluators that require reference outputs and those that don't:
  1. **Require reference output** : Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.
  2. **Don't require reference output** : Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).


#### Applying RAG Evaluation[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#applying-rag-evaluation "Direct link to Applying RAG Evaluation")
When applying RAG evaluation, consider the following approaches:
  1. `Offline evaluation`: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.
  2. `Online evaluation`: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.
  3. `Pairwise evaluation`: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.


#### RAG evaluation summary[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#rag-evaluation-summary "Direct link to RAG evaluation summary")
Evaluator| Detail| Needs reference output| LLM-as-judge?| Pairwise relevant  
---|---|---|---|---  
Document relevance| Are documents relevant to the question?| No| Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-document-relevance)| No  
Answer faithfulness| Is the answer grounded in the documents?| No| Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-answer-hallucination)| No  
Answer helpfulness| Does the answer help address the question?| No| Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-answer-helpfulness)| No  
Answer correctness| Is the answer consistent with a reference answer?| Yes| Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-answer-vs-reference)| No  
Pairwise comparison| How do multiple answer versions compare?| No| Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-rag)| Yes  
### Summarization[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#summarization "Direct link to Summarization")
Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria.
`Developer curated examples` of texts to summarize are commonly used for evaluation (see a dataset example [here](https://smith.langchain.com/public/659b07af-1cab-4e18-b21a-91a69a4c3990/d)). However, `user logs` from a production (summarization) app can be used for online evaluation with any of the `Reference-free` evaluation prompts below.
`LLM-as-judge` is typically used for evaluation of summarization (as well as other types of writing) using `Reference-free` prompts that follow provided criteria to grade a summary. It is less common to provide a particular `Reference` summary, because summarization is a creative task and there are many possible correct answers.
`Online` or `Offline` evaluation are feasible because of the `Reference-free` prompt used. `Pairwise` evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs):
Use Case| Detail| Needs reference output| LLM-as-judge?| Pairwise relevant  
---|---|---|---|---  
Factual accuracy| Is the summary accurate relative to the source documents?| No| Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/summary-accurancy-evaluator)| Yes  
Faithfulness| Is the summary grounded in the source documents (e.g., no hallucinations)?| No| Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/summary-hallucination-evaluator)| Yes  
Helpfulness| Is summary helpful relative to user need?| No| Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/summary-helpfulness-evaluator)| Yes  
### Classification and tagging[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#classification-and-tagging "Direct link to Classification and tagging")
Classification and tagging apply a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification/tagging evaluation typically employs the following components, which we will review in detail below:
A central consideration for classification/tagging evaluation is whether you have a dataset with `reference` labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a classification/tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc).
If ground truth reference labels are provided, then it's common to simply define a [custom heuristic evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator) to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use `LLM-as-judge` to perform the classification/tagging of an input based upon specified criteria (without a ground truth reference).
`Online` or `Offline` evaluation is feasible when using `LLM-as-judge` with the `Reference-free` prompt used. In particular, this is well suited to `Online` evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc).
Use Case| Detail| Needs reference output| LLM-as-judge?| Pairwise relevant  
---|---|---|---|---  
Accuracy| Standard definition| Yes| No| No  
Precision| Standard definition| Yes| No| No  
Recall| Standard definition| Yes| No| No  
## Experiment configuration[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#experiment-configuration "Direct link to Experiment configuration")
LangSmith supports a number of experiment configurations which make it easier to run your evals in the manner you want.
### Repetitions[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#repetitions "Direct link to Repetitions")
Running an experiment multiple times can be helpful since LLM outputs are not deterministic and can differ from one repetition to the next. By running multiple repetitions, you can get a more accurate estimate of the performance of your system.
Repetitions can be configured by passing the `num_repetitions` argument to `evaluate` / `aevaluate` ([Python](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate), [TypeScript](https://docs.smith.langchain.com/reference/js/interfaces/evaluation.EvaluateOptions#numrepetitions)). Repeating the experiment involves both re-running the target function to generate outputs and re-running the evaluators.
To learn more about running repetitions on experiments, read the [how-to-guide](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition).
### Concurrency[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#concurrency "Direct link to Concurrency")
By passing the `max_concurrency` argument to `evaluate` / `aevaluate`, you can specify the concurrency of your experiment. The `max_concurrency` argument has slightly different semantics depending on whether you are using `evaluate` or `aevaluate`.
#### `evaluate`[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#evaluate "Direct link to evaluate")
The `max_concurrency` argument to `evaluate` specifies the maximum number of concurrent threads to use when running the experiment. This is both for when running your target function as well as your evaluators.
#### `aevaluate`[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#aevaluate "Direct link to aevaluate")
The `max_concurrency` argument to `aevaluate` is fairly similar to `evaluate`, but instead uses a semaphore to limit the number of concurrent tasks that can run at once. `aevaluate` works by creating a task for each example in the dataset. Each task consists of running the target function as well as all of the evaluators on that specific example. The `max_concurrency` argument specifies the maximum number of concurrent tasks, or put another way - examples, to run at once.
### Caching[‚Äã](https://docs.smith.langchain.com/evaluation/concepts#caching "Direct link to Caching")
Lastly, you can also cache the API calls made in your experiment by setting the `LANGSMITH_TEST_CACHE` to a valid folder on your device with write access. This will cause the API calls made in your experiment to be cached to disk, meaning future experiments that make the same API calls will be greatly sped up.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/concepts%3E).
[PreviousHow to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)[NextQuick Start (UI)](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Datasets](https://docs.smith.langchain.com/evaluation/concepts#datasets)
    * [Examples](https://docs.smith.langchain.com/evaluation/concepts#examples)
    * [Dataset curation](https://docs.smith.langchain.com/evaluation/concepts#dataset-curation)
    * [Splits](https://docs.smith.langchain.com/evaluation/concepts#splits)
    * [Versions](https://docs.smith.langchain.com/evaluation/concepts#versions)
  * [Evaluators](https://docs.smith.langchain.com/evaluation/concepts#evaluators)
    * [Human](https://docs.smith.langchain.com/evaluation/concepts#human)
    * [Heuristic](https://docs.smith.langchain.com/evaluation/concepts#heuristic)
    * [LLM-as-judge](https://docs.smith.langchain.com/evaluation/concepts#llm-as-judge)
    * [Pairwise](https://docs.smith.langchain.com/evaluation/concepts#pairwise)
  * [Experiment](https://docs.smith.langchain.com/evaluation/concepts#experiment)
  * [Annotation queues](https://docs.smith.langchain.com/evaluation/concepts#annotation-queues)
  * [Offline evaluation](https://docs.smith.langchain.com/evaluation/concepts#offline-evaluation)
    * [Benchmarking](https://docs.smith.langchain.com/evaluation/concepts#benchmarking)
    * [Unit tests](https://docs.smith.langchain.com/evaluation/concepts#unit-tests)
    * [Regression tests](https://docs.smith.langchain.com/evaluation/concepts#regression-tests)
    * [Backtesting](https://docs.smith.langchain.com/evaluation/concepts#backtesting)
    * [Pairwise evaluation](https://docs.smith.langchain.com/evaluation/concepts#pairwise-evaluation)
  * [Online evaluation](https://docs.smith.langchain.com/evaluation/concepts#online-evaluation)
  * [Testing](https://docs.smith.langchain.com/evaluation/concepts#testing)
    * [Evaluations vs testing](https://docs.smith.langchain.com/evaluation/concepts#evaluations-vs-testing)
    * [Using `pytest` and `vitest/jest`](https://docs.smith.langchain.com/evaluation/concepts#using-pytest-and-vitestjest)
  * [Application-specific techniques](https://docs.smith.langchain.com/evaluation/concepts#application-specific-techniques)
    * [Agents](https://docs.smith.langchain.com/evaluation/concepts#agents)
    * [Retrieval augmented generation (RAG)](https://docs.smith.langchain.com/evaluation/concepts#retrieval-augmented-generation-rag)
    * [Summarization](https://docs.smith.langchain.com/evaluation/concepts#summarization)
    * [Classification and tagging](https://docs.smith.langchain.com/evaluation/concepts#classification-and-tagging)
  * [Experiment configuration](https://docs.smith.langchain.com/evaluation/concepts#experiment-configuration)
    * [Repetitions](https://docs.smith.langchain.com/evaluation/concepts#repetitions)
    * [Concurrency](https://docs.smith.langchain.com/evaluation/concepts#concurrency)
    * [Caching](https://docs.smith.langchain.com/evaluation/concepts#caching)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * How-to Guides


On this page
# Evaluation how-to guides
These guides answer ‚ÄúHow do I‚Ä¶?‚Äù format questions. They are goal-oriented and concrete, and are meant to help you complete a specific task. For conceptual explanations see the [Conceptual guide](https://docs.smith.langchain.com/evaluation/concepts). For end-to-end walkthroughs see [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials). For comprehensive descriptions of every class and function see the [API reference](https://langsmith-sdk.readthedocs.io/en/latest/evaluation.html).
## Key features[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides#key-features "Direct link to Key features")
  * Create a dataset [with the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset) or [from the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#set-up-your-dataset)
  * Run offline evaluations [with the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application) or [from the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
  * Run online evaluations with [LLM-as-judge](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#configure-llm-as-judge-evaluators) and [custom code](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#configure-custom-code-evaluators) evaluators
  * [Analyze evaluation results](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment) in the UI
  * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback) from your app
  * Log expert feedback [with annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)


## Offline evaluation[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides#offline-evaluation "Direct link to Offline evaluation")
Evaluate and improve your application before deploying it.
### Run an evaluation[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides#run-an-evaluation "Direct link to Run an evaluation")
  * [Define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
  * [Run an evaluation with the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
  * [Run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
  * [Run an evaluation comparing two experiments](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
  * [Evaluate a `langchain` runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
  * [Evaluate a `langgraph` graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
  * [Evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
  * [Run an evaluation from the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
  * [Run an evaluation via the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
  * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
  * [Set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)


### Define an evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides#define-an-evaluator "Direct link to Define an evaluator")
  * [Define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
  * [Define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
  * [Define a pairwise evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
  * [Define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
  * [Use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
  * [Evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
  * [Return multiple metrics in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
  * [Return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)


### Configure the evaluation data[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides#configure-the-evaluation-data "Direct link to Configure the evaluation data")
  * [Evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
  * [Evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)


### Configure an evaluation job[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides#configure-an-evaluation-job "Direct link to Configure an evaluation job")
  * [Evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
  * [Handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
  * [Print detailed logs (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
  * [Run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)


### Add default evaluators to a dataset[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides#add-default-evaluators-to-a-dataset "Direct link to Add default evaluators to a dataset")
Set up evaluators that automatically run for all experiments against a dataset.
  * [Set up an auto-evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
  * [Create a few-shot evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)


## Testing integrations[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides#testing-integrations "Direct link to Testing integrations")
Run evals using your favorite testing tools.
  * [Run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
  * [Run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)


## Online evaluation[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides#online-evaluation "Direct link to Online evaluation")
Evaluate and monitor your system's live performance on production data.
  * [Set up an online evaluator](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#get-started-with-online-evaluators)
  * [Create a few-shot evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)


## Analyzing experiment results[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides#analyzing-experiment-results "Direct link to Analyzing experiment results")
Use the UI & API to understand your experiment results.
  * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
  * [Compare experiments with the comparison view](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
  * [Filter experiments](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
  * [View pairwise experiments](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#view-pairwise-experiments)
  * [Fetch experiment results in the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
  * [Upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
  * [Download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
  * [Audit and correct evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
  * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)


## Dataset management[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides#dataset-management "Direct link to Dataset management")
Manage datasets in LangSmith used by your evaluations.
  * [Create a dataset from the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#set-up-your-dataset)
  * [Export a dataset from the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#export-a-dataset)
  * [Create a dataset split from the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-and-manage-dataset-splits)
  * [Filter examples from the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#filter-examples)
  * [Create a dataset with the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset)
  * [Fetch a dataset with the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#fetch-datasets)
  * [Update a dataset with the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#update-examples)
  * [Version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
  * [Dataset sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
  * [Export filtered traces from an experiment to a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)


## Annotation queues and human feedback[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides#annotation-queues-and-human-feedback "Direct link to Annotation queues and human feedback")
Collect feedback from subject matter experts and users to improve your applications.
  * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
  * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
  * [Set up a new feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
  * [Annotate traces inline in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
  * [Audit and correct evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousTest a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)[NextAnalyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
  * [Key features](https://docs.smith.langchain.com/evaluation/how_to_guides#key-features)
  * [Offline evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides#offline-evaluation)
    * [Run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides#run-an-evaluation)
    * [Define an evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides#define-an-evaluator)
    * [Configure the evaluation data](https://docs.smith.langchain.com/evaluation/how_to_guides#configure-the-evaluation-data)
    * [Configure an evaluation job](https://docs.smith.langchain.com/evaluation/how_to_guides#configure-an-evaluation-job)
    * [Add default evaluators to a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides#add-default-evaluators-to-a-dataset)
  * [Testing integrations](https://docs.smith.langchain.com/evaluation/how_to_guides#testing-integrations)
  * [Online evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides#online-evaluation)
  * [Analyzing experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides#analyzing-experiment-results)
  * [Dataset management](https://docs.smith.langchain.com/evaluation/how_to_guides#dataset-management)
  * [Annotation queues and human feedback](https://docs.smith.langchain.com/evaluation/how_to_guides#annotation-queues-and-human-feedback)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * Analyze a single experiment


On this page
# Analyze a single experiment
After running an experiment, you can use LangSmith's experiment view to analyze the results and draw insights about how your experiment performed.
This guide will walk you through viewing the results of an experiment and highlights the features available in the experiments view.
## Open the experiment view[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#open-the-experiment-view "Direct link to Open the experiment view")
To open the experiment view, select the relevant Dataset from the Dataset & Experiments page and then select the experiment you want to view.
![Open experiment view](https://docs.smith.langchain.com/assets/images/select_experiment-8c8ba7c4fe01ad5c9c8e5e14eb9ee1ad.png)
## View experiment results[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#view-experiment-results "Direct link to View experiment results")
This table displays your experiment results. This includes the input, output, and reference output for each [example](https://docs.smith.langchain.com/evaluation/concepts#examples) in the dataset. It also shows each configured feedback key in separate columns alongside its corresponding feedback score.
Out of the box metrics (latency, status, cost, and token count) will also be displayed in individual columns.
In the columns dropdown, you can choose which columns to hide and which to show.
![Experiment view](https://docs.smith.langchain.com/assets/images/experiment_view-876562a9613e8ba4044aebe107b03a4a.png)
## Heatmap view[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#heatmap-view "Direct link to Heatmap view")
The experiment view defaults to a heatmap view, where feedback scores for each run are highlighted in a color. Red indicates a lower score, while green indicates a higher score. The heatmap visualization makes it easy to identify patterns, spot outliers, and understand score distributions across your dataset at a glance.
![Heatmap view](https://docs.smith.langchain.com/assets/images/heatmap-788c827412f373adf1128c44ebce1c6d.png)
## Sort and filter[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#sort-and-filter "Direct link to Sort and filter")
To sort or filter feedback scores, you can use the actions in the column headers.
![Sort and filter](https://docs.smith.langchain.com/assets/images/sort_filter-53db36c12a84ec02cf807480a1010998.png)
## Table views[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#table-views "Direct link to Table views")
Depending on the view most useful for your analysis, you can change the formatting of the table by toggling between a compact view, a full, view, and a diff view.
  * The `Compact` view shows each run as a one-line row, for ease of comparing scores at a glance.
  * The `Full` view shows the full output for each run for digging into the details of individual runs.
  * The `Diff` view shows the text difference between the reference output and the output for each run.


![Diff view](https://docs.smith.langchain.com/assets/images/diff_mode-211ab56443819b6e8d41d0c78ee794fe.png)
## View the traces[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#view-the-traces "Direct link to View the traces")
Hover over any of the output cells, and click on the trace icon to view the trace for that run. This will open up a trace in the side panel.
To view the entire tracing project, click on the "View Project" button in the top right of the header.
![View trace](https://docs.smith.langchain.com/assets/images/view_trace-5ca69528c296cf6a802fa5a004b67ada.png)
## View evaluator runs[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#view-evaluator-runs "Direct link to View evaluator runs")
For evaluator scores, you can view the source run by hovering over the evaluator score cell and clicking on the arrow icon. This will open up a trace in the side panel. If you're running a LLM-as-a-judge evaluator, you can view the prompt used for the evaluator in this run. If your experiment has [repetitions](https://docs.smith.langchain.com/evaluation/concepts#repetitions), you can click on the aggregate average score to find links to all of the individual runs.
![View evaluator runs](https://docs.smith.langchain.com/assets/images/evaluator_run-beeb1702c25c4e07577812a8b0cfe904.png)
## Group results by metadata[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#group-results-by-metadata "Direct link to Group results by metadata")
You can add metadata to examples to categorize and organize them. For example, if you're evaluating factual accuracy on a question answering dataset, the metadata might include which subject area each question belongs to. Metadata can be added either [via the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#edit-example-metadata) or [via the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#update-single-example).
To analyze results by metadata, use the "Group by" dropdown in the top right corner of the experiment view and select your desired metadata key. This displays average feedback scores, latency, total tokens, and cost for each metadata group.
info
You will only be able to group by example metadata on experiments created after February 20th, 2025. Any experiments before that date can still be grouped by metadata, but only if the metadata is on the experiment traces themselves.
![Group by](https://docs.smith.langchain.com/assets/images/group_by-dd6672f38d782df98d8c66bdb307c63e.gif)
## Repetitions[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#repetitions "Direct link to Repetitions")
If you've run your experiment with [repetitions](https://docs.smith.langchain.com/evaluation/concepts#repetitions), there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view.
When you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.
![Repetitions](https://docs.smith.langchain.com/assets/images/repetitions-5ed8e2031f73454e46c64ecce1ecb166.png)
## Compare to another experiment[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#compare-to-another-experiment "Direct link to Compare to another experiment")
In the top right of the experiment view, you can select another experiment to compare to. This will open up a comparison view, where you can see how the two experiments compare. To learn more about the comparison view, see [how to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results).
![Compare](https://docs.smith.langchain.com/assets/images/compare_to_another-7fbee3c23adbc9e0fb8f43029972ddfb.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/analyze_single_experiment%3E).
[PreviousEvaluation how-to guides](https://docs.smith.langchain.com/evaluation/how_to_guides)[NextLog user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
  * [Open the experiment view](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#open-the-experiment-view)
  * [View experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#view-experiment-results)
  * [Heatmap view](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#heatmap-view)
  * [Sort and filter](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#sort-and-filter)
  * [Table views](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#table-views)
  * [View the traces](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#view-the-traces)
  * [View evaluator runs](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#view-evaluator-runs)
  * [Group results by metadata](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#group-results-by-metadata)
  * [Repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#repetitions)
  * [Compare to another experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment#compare-to-another-experiment)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * Annotate traces and runs inline


# Annotate traces and runs inline
LangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user's comment or a note about a specific issue. You can annotate a trace either inline or by sending the trace to an annotation queue, which allows you closely inspect and log feedbacks to runs one at a time. Feedback tags are associated with your [workspace](https://docs.smith.langchain.com/administration/concepts#workspaces).
note
You can attach user feedback to ANY intermediate run (span) of the trace, not just the root span. This is useful for critiquing specific parts of the LLM application, such as the retrieval step or generation step of the RAG pipeline.
To annotate a trace inline, click on the `Annotate` in the upper right corner of trace view for any particular run that is part of the trace.
![](https://docs.smith.langchain.com/assets/images/annotate_trace_inline-632a5b3e248e1838a384afb0256d446c.png)
This will open up a pane that allows you to choose from feedback tags associated with your workspace and add a score for particular tags. You can also add a standalone comment. Follow [this guide](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria) to set up feedback tags for your workspace. You can also set up new feedback criteria from within the pane itself.
![](https://docs.smith.langchain.com/assets/images/annotation_sidebar-f2d92eeed575f79637b3ad5562016118.png)
You can use the labeled keyboard shortcuts to streamline the annotation process.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/annotate_traces_inline%3E).
[PreviousSet up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)[NextHow to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * Use annotation queues


On this page
# Use annotation queues
Annotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs. While you can always [annotate runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline), annotation queues provide another option to group runs together, then have annotators review and provide feedback on them.
## Create an annotation queue[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#create-an-annotation-queue "Direct link to Create an annotation queue")
![](https://docs.smith.langchain.com/assets/images/create_annotation_queue-6fcbcf31308cfa8297287f7d7cb5fa7c.png)
To create an annotation queue, navigate to the **Annotation queues** section through the homepage or left-hand navigation bar. Then click **+ New annotation queue** in the top right corner.
![](https://docs.smith.langchain.com/assets/images/create_annotation_queue_new-4ef470bd39073c0b24a047583d9989f1.png)
### Basic Details[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#basic-details "Direct link to Basic Details")
Fill in the form with the **name** and **description** of the queue. You can also assign a **default dataset** to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace.
### Annotation Rubric[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#annotation-rubric "Direct link to Annotation Rubric")
Begin by drafting some high-level instructions for your annotators, which will be shown in the sidebar on every run.
Next, click "+ Desired Feedback" to add feedback keys to your annotation queue. Annotators will be presented with these feedback keys on each run. Add a description for each, as well as a short description of each category if the feedback is categorical.
![annotation queue rubric](https://docs.smith.langchain.com/assets/images/create_annotation_rubric-cd88030818ddf6f7e2cef2f38eb95f4a.png)
Reviewers will see this:
![rubric for annotators](https://docs.smith.langchain.com/assets/images/rubric_for_annotators-1a7b3e09a1dfa4504a0f468ae96124e7.png)
### Collaborator Settings[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#collaborator-settings "Direct link to Collaborator Settings")
There are a few settings related to multiple annotators:
  * **Number of reviewers per run** : This determines the number of reviewers that must mark a run as "Done" for it to be removed from the queue. If you check "All workspace members review each run," then a run will remain in the queue until all workspace members have marked it "Done".
  * **Enable reservations on runs** : We recommend enabling reservations. This will prevent multiple annotators from reviewing the same run at the same time.


  1. **How do reservations work?**


When a reviewer views a run, the run is reserved for that reviewer for the specified "reservation length". If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time.
  1. **What happens if time runs out?**


If a reviewer has viewed a run and then leaves the run without marking it "Done", the reservation will expire after the specified "reservation length". The run is then released back into the queue and can be reserved by another reviewer.
note
Clicking "Requeue at end" will only move the current run to the end of the current user's queue; it won't affect the queue order of any other user. It will also release the reservation that the current user has on that run.
Because of these settings, it's possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue as well as anyone else's queue size.
You can update these settings at any time by clicking on the pencil icon in the **Annotation Queues** section.
![](https://docs.smith.langchain.com/assets/images/annotation_queue_edit-24378216c29bef3569544238feef0b55.png)
## Assign runs to an annotation queue[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#assign-runs-to-an-annotation-queue "Direct link to Assign runs to an annotation queue")
To assign runs to an annotation queue, either:
  1. Click on **Add to Annotation Queue** in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span. ![](https://docs.smith.langchain.com/assets/images/add_to_annotation_queue-a536d57618587d8cd8b7d7e56f2465cf.png)
  2. Select multiple runs in the runs table then click **Add to Annotation Queue** at the bottom of the page. ![](https://docs.smith.langchain.com/assets/images/multi_select_annotation_queue-efeab98023dccbd3f8dc6da86f138f87.png)
  3. [Set up an automation rule](https://docs.smith.langchain.com/observability/how_to_guides/rules) that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue.
  4. Select one or multiple experiments from the dataset page and click **Annotate**. From the resulting popup, you may either create a new queue or add the runs to an existing one: ![](https://docs.smith.langchain.com/assets/images/annotate_experiment-8911b495c3ba92db1660af23cb314097.png)


tip
It is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction. To learn more about how to capture user feedback from your LLM application, follow [this guide](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback).
## Review runs in an annotation queue[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#review-runs-in-an-annotation-queue "Direct link to Review runs in an annotation queue")
To review runs in an annotation queue, navigate to the **Annotation Queues** section through the homepage or left-hand navigation bar. Then click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review.
You can attach a comment, attach a score for a particular feedback criteria, add the run a dataset and/or mark the run as reviewed. You can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the **Trash** icon next to "View run".
The keyboard shortcuts shown can help streamline the review process.
![](https://docs.smith.langchain.com/assets/images/review_runs-6ee82b92d6a07141bd2fcf0fc0638ba2.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/annotation_queues%3E).
[PreviousHow to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)[NextDataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
  * [Create an annotation queue](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#create-an-annotation-queue)
    * [Basic Details](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#basic-details)
    * [Annotation Rubric](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#annotation-rubric)
    * [Collaborator Settings](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#collaborator-settings)
  * [Assign runs to an annotation queue](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#assign-runs-to-an-annotation-queue)
  * [Review runs in an annotation queue](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues#review-runs-in-an-annotation-queue)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/async

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/async#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/async)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to run an evaluation asynchronously


On this page
# How to run an evaluation asynchronously
Key concepts
[Evaluations](https://docs.smith.langchain.com/evaluation/concepts#applying-evaluations) | [Evaluators](https://docs.smith.langchain.com/evaluation/concepts#evaluators) | [Datasets](https://docs.smith.langchain.com/evaluation/concepts#datasets) | [Experiments](https://docs.smith.langchain.com/evaluation/concepts#experiments)
We can run evaluations asynchronously via the SDK using [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate), which accepts all of the same arguments as [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) but expects the application function to be asynchronous. You can learn more about how to use the `evaluate()` function [here](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application).
Python only
This guide is only relevant when using the Python SDK. In JS/TS the `evaluate()` function is already async. You can see how to use it [here](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application).
## Use `aevaluate()`[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/async#use-aevaluate "Direct link to use-aevaluate")
  * Python


Requires `langsmith>=0.3.13`
```
from langsmith import wrappers, Clientfrom openai import AsyncOpenAI# Optionally wrap the OpenAI client to trace all model calls.oai_client = wrappers.wrap_openai(AsyncOpenAI())# Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.@traceableasyncdefresearcher_app(inputs:dict)->str:  instructions ="""You are an excellent researcher. Given a high-level research idea, \list 5 concrete questions that should be investigated to determine if the idea is worth pursuing."""  response =await oai_client.chat.completions.create(    model="gpt-4o-mini",    messages=[{"role":"system","content": instructions},{"role":"user","content": inputs["idea"]},],)return response.choices[0].message.content# Evaluator functions can be sync or asyncdefconcise(inputs:dict, outputs:dict)->bool:returnlen(outputs["output"])<3*len(inputs["idea"])ls_client = Client()ideas =["universal basic income","nuclear fusion","hyperloop","nuclear powered rockets",]dataset = ls_client.create_dataset("research ideas")ls_client.create_examples(  dataset_name=dataset.name,  examples=[{"inputs":{"idea": i}}for i in ideas],)# Can equivalently use the 'aevaluate' function directly:# from langsmith import aevaluate# await aevaluate(...)results =await ls_client.aevaluate(  researcher_app,  data=dataset,  evaluators=[concise],# Optional, add concurrency.  max_concurrency=2,# Optional, add concurrency.  experiment_prefix="gpt-4o-mini-baseline"# Optional, random by default.)
```

## Related[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/async#related "Direct link to Related")
  * [Run an evaluation (synchronously)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
  * [Handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/async%3E).
[PreviousHow to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)[NextHow to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
  * [Use `aevaluate()`](https://docs.smith.langchain.com/evaluation/how_to_guides/async#use-aevaluate)
  * [Related](https://docs.smith.langchain.com/evaluation/how_to_guides/async#related)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * Log user feedback


# Log user feedback
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Conceptual guide on tracing and feedback](https://docs.smith.langchain.com/observability/concepts)
  * [Reference guide on feedback data format](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)


In many applications, but even more so for LLM applications, it is important to collect user feedback to understand how your application is performing in real-world scenarios. The ability to observe user feedback along with trace data can be very powerful to drill down into the most interesting datapoints, then send those datapoints for further review, automatic evaluation, or even datasets. To learn more about how to filter traces based on various attributes, including user feedback, see [this guide](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
LangSmith makes it easy to attach user feedback to traces. It's often helpful to expose a simple mechanism (such as a thumbs-up, thumbs-down button) to collect user feedback for your application responses. You can then use the LangSmith SDK or API to send feedback for a trace. To get the `run_id` of a logged run, see [this guide](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span).
note
You can attach user feedback to ANY intermediate run (span) of the trace, not just the root span. This is useful for critiquing specific parts of the LLM application, such as the retrieval step or generation step of the RAG pipeline.
  * Python
  * TypeScript


```
from langsmith import Clientclient = Client()# ... Run your application and get the run_id...# This information can be the result of a user-facing feedback formclient.create_feedback( run_id, key="feedback-key", score=1.0, comment="comment",)
```

```
import{ Client }from"langsmith";const client =newClient();// ... Run your application and get the run_id...// This information can be the result of a user-facing feedback formawait client.createFeedback( runId,"feedback-key",{   score:1.0,   comment:"comment",});
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousAnalyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)[NextHow to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to audit evaluator scores


On this page
# How to audit evaluator scores
LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK.
## In the comparison view[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores#in-the-comparison-view "Direct link to In the comparison view")
In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the "edit" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under "Make correction". If you would like, you may also attach an explanation to your correction. This is useful if you are using a [few-shot evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators) and will be automatically inserted into your few-shot examples in place of the `few_shot_explanation` prompt variable.
![Audit Evaluator Comparison View](https://docs.smith.langchain.com/assets/images/corrections_comparison_view-2a14c3ed9bc9cf527a436e3abefe0eb2.png)
## In the runs table[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores#in-the-runs-table "Direct link to In the runs table")
In the runs table, find the "Feedback" column and click on the feedback tag to bring up the feedback details. Again, click the "edit" icon on the right to bring up the corrections view.
![Audit Evaluator Runs Table](https://docs.smith.langchain.com/assets/images/corrections_runs_table-90fb6a2f9788b4a36f5f14883bde4e89.png)
## In the SDK[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores#in-the-sdk "Direct link to In the SDK")
Corrections can be made via the SDK's `update_feedback` function, with the `correction` dict. You must specify a `score` key which corresponds to a number for it to be rendered in the UI.
  * Python
  * TypeScript


```
import langsmithclient = langsmith.Client()client.update_feedback( my_feedback_id, correction={"score":1,},)
```

```
import{ Client }from'langsmith';const client =newClient();await client.updateFeedback( myFeedbackId,{   correction:{     score:1,}})
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousRun pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)[NextHow to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
  * [In the comparison view](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores#in-the-comparison-view)
  * [In the runs table](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores#in-the-runs-table)
  * [In the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores#in-the-sdk)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to bind an evaluator to a dataset in the UI


On this page
# How to bind an evaluator to a dataset in the UI
While you can specify evaluators to grade the results of your experiments programmatically (see [this guide](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application) for more information), you can also bind evaluators to a dataset in the UI. This allows you to configure automatic evaluators that grade your experiment results. We have support for both LLM-based evaluators, and custom python code evaluators.
The process for configuring this is very similar to the process for configuring an [online evaluator](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations) for traces.
Only affects subsequent experiment runs
When you configure an evaluator for a dataset, it will only affect the experiment runs that are created after the evaluator is configured. It will not affect the evaluation of experiment runs that were created before the evaluator was configured.
  1. **Navigate to the dataset details page** by clicking **Datasets and Testing** in the sidebar and selecting the dataset you want to configure the evaluator for.
  2. **Click on the`Add Auto-Evaluator` button** to add an evaluator to the dataset. This will open a modal you can use to configure the evaluator.


The next steps vary based on the evaluator type.
## LLM as judge evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset#llm-as-judge-evaluators "Direct link to LLM as judge evaluators")
  1. **Select the LLM as judge type evaluator**
  2. **Give your evaluator a name** and **set an inline prompt or load a prompt from the prompt hub** that will be used to evaluate the results of the runs in the experiment.


![Add evaluator name and prompt](https://docs.smith.langchain.com/assets/images/create_evaluator-38b8ca3ad9bb615ba3d660da426f611d.png)
Importantly, evaluator prompts can only contain the following input variables:
  * `input` (required): the input to the target you are evaluating
  * `output` (required): the output of the target you are evaluating
  * `reference`: the reference output, taken from the dataset


note
Automatic evaluators you configure in the application will only work if the `inputs` to your evaluation target, `outputs` from your evaluation target, and `examples` in your dataset are all single-key dictionaries. LangSmith will automatically extract the values from the dictionaries and pass them to the evaluator.
LangSmith currently doesn't support setting up evaluators in the application that act on multiple keys in the `inputs` or `outputs` or `examples` dictionaries.
You can specify the scoring criteria in the "schema" field. In this example, we are asking the LLM to grade on "correctness" of the output with respect to the reference, with a boolean output of 0 or 1. The name of the field in the schema will be interpreted as the feedback key and the type will be the type of the score.
![Evaluator prompt](https://docs.smith.langchain.com/assets/images/evaluator_prompt-96b358edf79006d44a5955accf643dec.png)
  1. **Save the evaluator** and navigate back to the dataset details page. Each **subsequent** experiment run from the dataset will now be evaluated by the evaluator you configured. Note that in the below image, each run in the experiment has a "correctness" score.


![Playground evaluator results](https://docs.smith.langchain.com/assets/images/playground_evaluator_results-c60a255f465396f27d6c1a3dc6a76c42.png)
## Custom code evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset#custom-code-evaluators "Direct link to Custom code evaluators")
  1. **Select the`Custom code` type evaluator**
  2. **Write your evaluation function**


Custom code evaluators restrictions.
**Allowed Libraries** : You can import all standard library functions, as well as the following public packages:
```
 numpy (v2.2.2): "numpy" pandas (v1.5.2): "pandas" jsonschema (v4.21.1): "jsonschema" scipy (v1.14.1): "scipy" sklearn (v1.26.4): "scikit-learn"
```

**Network Access** : You cannot access the internet from a custom code evaluator.
In the UI, you will see a panel that lets you write your code inline, with some starter code:
![](https://docs.smith.langchain.com/assets/images/code-autoeval-popup-9c0641db5e678a94f62fa78dd56e462b.png)
Custom Code evaluators take in two arguments:
  * A `Run` ([reference](https://docs.smith.langchain.com/reference/data_formats/run_data_format)). This represents the new run in your experiment. For example, if you ran an experiment via SDK, this would contain the input/output from your chain or model you are testing.
  * An `Example`. This represents the reference example in your dataset that the chain or model you are testing uses. The `inputs` to the Run and Example should be the same. If your Example has a reference `outputs`, then you can use this to compare to the run's output for scoring.


They return a single value:
  * Feedback(s) Dictionary: A dictionary whose keys are the type of feedback you want to return, and values are the score you will give for that feedback key. For example, `{"correctness": 1, "silliness": 0}` would create two types of feedback on the run in your experiment, one saying it is correct, and the other saying it is not silly.


In the below screenshot, you can see an example of a simple function that validates that each run in the experiment has a known json field:
```
import jsondef perform_eval(run, example): output_to_validate = run['outputs'] is_valid_json = 0 # assert you can serialize/deserialize as json try:  json.loads(json.dumps(output_to_validate)) except Exception as e:  return { "formatted": False } # assert output facts exist if "facts" not in output_to_validate:  return { "formatted": False } # assert required fields exist if "years_mentioned" not in output_to_validate["facts"]:  return { "formatted": False } return {"formatted": True}
```

  1. **Test and save your evaluation function**


Before saving, you can click test, and LangSmith will run your code over a past run+example pair to make sure your evaluation code works.
This auto evaluator will now leave feedback on any type of future experiment, whether it be from the SDK or via Playground.
  1. **See the results in action**


To visualize the feedback left on new experiments, try running a new experiment via playground.
On the dataset, if you now click to the `experiments` tab -> `+ Experiment` -> `Run in Playground`, you can see the results in action. Your runs in your experiments will be automatically marked with the key specified in your code sample above (here, `formatted`):
![](https://docs.smith.langchain.com/assets/images/show-feedback-from-autoeval-code-8bcc7909c6424dcab57d7b24d754d9aa.png)
And if you navigate back to your dataset, you'll see summary stats for said experiment in the `experiments` tab:
![](https://docs.smith.langchain.com/assets/images/experiments-tab-code-results-002cddccd9c524c6852cfce5cd023dfb.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/bind_evaluator_to_dataset%3E).
[PreviousRenaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)[NextHow to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
  * [LLM as judge evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset#llm-as-judge-evaluators)
  * [Custom code evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset#custom-code-evaluators)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to compare experiment results


On this page
# How to compare experiment results
Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments.
LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments.
![](https://docs.smith.langchain.com/assets/images/compare-92cf87a92a0592831d6b8ff89372020e.gif)
## Open the comparison view[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#open-the-comparison-view "Direct link to Open the comparison view")
To open the experiment comparison view, click the **Dataset & Experiments** page, select the relevant Dataset, select two or more experiments on the Experiments tab and click compare.
![](https://docs.smith.langchain.com/assets/images/compare_select-a65688e7942d0d4bf824473a8bd9bf18.png)
## Adjust the table display[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#adjust-the-table-display "Direct link to Adjust the table display")
You can toggle between different views by clicking "Full" or "Compact" at the top of the page.
Toggling Full Text will show the full text of the input, output and reference output for each run. If the reference output is too long to display in the table, you can click on expand to view the full content.
You can also select and hide individual feedback keys or individual metrics in the display settings dropdown to isolate the information you want to see.
![](https://docs.smith.langchain.com/assets/images/toggle_views-44fb854d566e54a2b5014601cf1f1353.gif)
## View regressions and improvements[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#view-regressions-and-improvements "Direct link to View regressions and improvements")
In the LangSmith comparison view, runs that _regressed_ on your specified feedback key against your baseline experiment will be highlighted in red, while runs that _improved_ will be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment.
![Regressions](https://docs.smith.langchain.com/assets/images/regression_view-6445db75bfcaa79c88446da1429045dd.png)
## Filter on regressions or improvements[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#filter-on-regressions-or-improvements "Direct link to Filter on regressions or improvements")
Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.
![Regressions Filter](https://docs.smith.langchain.com/assets/images/filter_to_regressions-a40ec775f25c1c7179a3ec8a4df8f732.png)
## Update baseline experiment and metric[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#update-baseline-experiment-and-metric "Direct link to Update baseline experiment and metric")
In order to track regressions, you need to:
  1. Select a baseline experiment against which to compare and a metric to measure. By default, the newest experiment is selected as the baseline.
  2. Select feedback key (evaluation metric) you want to focus compare against. One will be assigned by default, but you can adjust as needed.
  3. Configure whether a higher score is better for the selected feedback key. This preference will be stored.


![Baseline](https://docs.smith.langchain.com/assets/images/select_baseline-eb990d76f6ee4e5c76509e693a5e6227.png)
## Open a trace[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#open-a-trace "Direct link to Open a trace")
If the example you're evaluating is from an ingested [run](https://docs.smith.langchain.com/observability/concepts#runs), you can hover over the output cell and click on the trace icon to open the trace view for that run. This will open up a trace in the side panel.
![](https://docs.smith.langchain.com/assets/images/open_source_trace-426f75ecb6815cb9ab38ab55ee99d6cf.png)
## Expand detailed view[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#expand-detailed-view "Direct link to Expand detailed view")
From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores.
![](https://docs.smith.langchain.com/assets/images/expanded_view-73880f6556e9f2c4152fda9b269f0641.png)
## View summary charts[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#view-summary-charts "Direct link to View summary charts")
You can also view summary charts by clicking on the "Charts" tab at the top of the page.
![](https://docs.smith.langchain.com/assets/images/charts_tab-f9b42b53119df27d2c7b0b4fc07d325b.png)
## Use experiment metadata as chart labels[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#use-experiment-metadata-as-chart-labels "Direct link to Use experiment metadata as chart labels")
You can configure the x-axis labels for the charts based on [experiment metadata](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui#background-add-metadata-to-your-experiments).
Select a metadata key to see change the x-axis labels of the charts.
![](https://docs.smith.langchain.com/assets/images/metadata_in_charts-54244735eea874ca500038f71dd05153.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)[NextDynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
  * [Open the comparison view](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#open-the-comparison-view)
  * [Adjust the table display](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#adjust-the-table-display)
  * [View regressions and improvements](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#view-regressions-and-improvements)
  * [Filter on regressions or improvements](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#filter-on-regressions-or-improvements)
  * [Update baseline experiment and metric](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#update-baseline-experiment-and-metric)
  * [Open a trace](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#open-a-trace)
  * [Expand detailed view](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#expand-detailed-view)
  * [View summary charts](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#view-summary-charts)
  * [Use experiment metadata as chart labels](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#use-experiment-metadata-as-chart-labels)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to improve your evaluator with few-shot examples


On this page
# How to improve your evaluator with few-shot examples
Using LLM-as-a-judge evaluators can be very helpful when you can't evaluate your system programmatically. However, their effectiveness depends on their quality and how well they align with human reviewer feedback. LangSmith provides the ability to improve the alignment of LLM-as-a-judge evaluator to human preferences using [human corrections](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#make-corrections).
Human corrections are automatically inserted into your evaluator prompt using few-shot examples. Few-shot examples is a technique inspired by [few-shot prompting](https://www.promptingguide.ai/techniques/fewshot) that guides the models output with a few high-quality examples.
This guide covers how to set up few-shot examples as part of your LLM-as-a-judge evaluator and apply corrections to feedback scores.
## How few-shot examples work[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#how-few-shot-examples-work "Direct link to How few-shot examples work")
  * Few-shot examples are added to your evaluator prompt using the `{{Few-shot examples}}` variable
  * Creating an evaluator with few-shot examples, will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections
  * At runtime, these examples will inserted into the evaluator to serve as a guide for its outputs - this will help the evaluator to better align with human preferences


## Configure your evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#configure-your-evaluator "Direct link to Configure your evaluator")
note
Few-shot examples are not currently supported in LLM-as-a-judge evaluators that use the prompt hub and are only compatible with prompts that use mustache formatting.
Before enabling few-shot examples, set up your LLM-as-a-judge evaluator. If you haven't done this yet, follow the steps in the [LLM-as-a-judge evaluator guide](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge).
### 1. Configure variable mapping[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#1-configure-variable-mapping "Direct link to 1. Configure variable mapping")
Each few-shot example is formatted according to the variable mapping specified in the configuration. The variable mapping for few-shot examples, should contain the same variables as your main prompt, plus a `few_shot_explanation` and a `score` variable which should have the same name as your feedback key.
For example, if your main prompt has variables `question` and `response`, and your evaluator outputs a `correctness` score, then your few-shot prompt should have the vartiables `question`, `response`, `few_shot_explanation`, and `correctness`.
### 2. Specify the number of few-shot examples to use[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#2-specify-the-number-of-few-shot-examples-to-use "Direct link to 2. Specify the number of few-shot examples to use")
You may also specify the number of few-shot examples to use. The default is 5. If your examples are very long, you may want to set this number lower to save tokens - whereas if your examples tend to be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you.
## Make corrections[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#make-corrections "Direct link to Make corrections")
Key concepts
[Audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
As you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you [make corrections to these scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores), you will begin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the `few_shot_explanation` variable.
The inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset. The outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset:
![Few-shot example](https://docs.smith.langchain.com/assets/images/few_shot_example-4f3b0845f355b640df232ccc2d550927.png)
Note that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!
## View your corrections dataset[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#view-your-corrections-dataset "Direct link to View your corrections dataset")
In order to view your corrections dataset:
  * **Online evaluators** : Select your run rule and click **Edit Rule**
  * **Offline evaluators** : Select your evaluator and click **Edit Evaluator**


![Edit Evaluator](https://docs.smith.langchain.com/assets/images/edit_evaluator-386e00fc1f41f249c114ca98ca754fcd.png)
Head to your dataset of corrections linked in the the **Improve evaluator accuracy using few-shot examples** section. You can view and update your few-shot examples in the dataset.
![View few-shot dataset](https://docs.smith.langchain.com/assets/images/view_few_shot_ds-28fc666004d1338dbeec99cdb9332c55.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)[NextHow to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
  * [How few-shot examples work](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#how-few-shot-examples-work)
  * [Configure your evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#configure-your-evaluator)
    * [1. Configure variable mapping](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#1-configure-variable-mapping)
    * [2. Specify the number of few-shot examples to use](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#2-specify-the-number-of-few-shot-examples-to-use)
  * [Make corrections](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#make-corrections)
  * [View your corrections dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#view-your-corrections-dataset)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to define a custom evaluator


On this page
# How to define a custom evaluator
Key concepts
  * [Evaluators](https://docs.smith.langchain.com/evaluation/concepts#evaluators)


Custom evaluators are just functions that take a dataset example and the resulting application output, and return one or more metrics. These functions can be passed directly into [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate).
## Basic example[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator#basic-example "Direct link to Basic example")
  * Python
  * TypeScript


Requires `langsmith>=0.2.0`
```
from langsmith import evaluatedefcorrect(outputs:dict, reference_outputs:dict)->bool:"""Check if the answer exactly matches the expected answer."""return outputs["answer"]== reference_outputs["answer"]defdummy_app(inputs:dict)->dict:return{"answer":"hmm i'm not sure","reasoning":"i didn't understand the question"}results = evaluate(  dummy_app,  data="dataset_name",  evaluators=[correct])
```

Requires `langsmith>=0.2.9`
```
importtype{ EvaluationResult }from"langsmith/evaluation";const correct =async({ outputs, referenceOutputs }:{ outputs: Record<string,any>; referenceOutputs?: Record<string,any>;}):Promise<EvaluationResult>=>{const score = outputs?.answer === referenceOutputs?.answer;return{ key:"correct", score };}
```

## Evaluator args[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator#evaluator-args "Direct link to Evaluator args")
Custom evaluator functions must have specific argument names. They can take any subset of the following arguments:
  * `run: Run`: The full [Run](https://docs.smith.langchain.com/reference/data_formats/run_data_format) object generated by the application on the given example.
  * `example: Example`: The full dataset [Example](https://docs.smith.langchain.com/reference/data_formats/example_data_format), including the example inputs, outputs (if available), and metdata (if available).
  * `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.
  * `outputs: dict`: A dictionary of the outputs generated by the application on the given `inputs`.
  * `reference_outputs/referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.


For most use cases you'll only need `inputs`, `outputs`, and `reference_outputs`. `run` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.
When using JS/TS these should all be passed in as part of a single object argument.
## Evaluator output[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator#evaluator-output "Direct link to Evaluator output")
Custom evaluators are expected to return one of the following types:
Python and JS/TS
  * `dict`: dicts of the form `{"score" | "value": ..., "key": ...}` allow you to customize the metric type ("score" for numerical and "value" for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.


Python only
  * `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.
  * `str`: this is intepreted as a categorical metric. The function name is used as the name of the metric.
  * `list[dict]`: return multiple metrics using a single function.


## Additional examples[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator#additional-examples "Direct link to Additional examples")
  * Python
  * TypeScript


Requires `langsmith>=0.2.0`
```
from langsmith import evaluate, wrappersfrom langsmith.schemas import Run, Examplefrom openai import AsyncOpenAI# Assumes you've installed pydantic.from pydantic import BaseModel# We can still pass in Run and Example objects if we'd likedefcorrect_old_signature(run: Run, example: Example)->dict:"""Check if the answer exactly matches the expected answer."""return{"key":"correct","score": run.outputs["answer"]== example.outputs["answer"]}# Just evaluate actual outputsdefconcision(outputs:dict)->int:"""Score how concise the answer is. 1 is the most concise, 5 is the least concise."""returnmin(len(outputs["answer"])//1000,4)+1# Use an LLM-as-a-judgeoai_client = wrappers.wrap_openai(AsyncOpenAI())asyncdefvalid_reasoning(inputs:dict, outputs:dict)->bool:"""Use an LLM to judge if the reasoning and the answer are consistent."""  instructions ="""\Given the following question, answer, and reasoning, determine if the reasoning for the \answer is logically valid and consistent with question and the answer."""classResponse(BaseModel):    reasoning_is_valid:bool  msg =f"Question: {inputs['question']}\nAnswer: {outputs['answer']}\nReasoning: {outputs['reasoning']}"  response =await oai_client.beta.chat.completions.parse(    model="gpt-4o-mini",    messages=[{"role":"system","content": instructions,},{"role":"user","content": msg}],    response_format=Response)return response.choices[0].message.parsed.reasoning_is_validdefdummy_app(inputs:dict)->dict:return{"answer":"hmm i'm not sure","reasoning":"i didn't understand the question"}results = evaluate(  dummy_app,  data="dataset_name",  evaluators=[correct_old_signature, concision, valid_reasoning])
```

```
import{ Client }from"langsmith";import{ evaluate }from"langsmith/evaluation";import{ Run, Example }from"langsmith/schemas";import OpenAI from"openai";// Type definitionsinterfaceAppInputs{  question:string;}interfaceAppOutputs{  answer:string;  reasoning:string;}interfaceResponse{  reasoning_is_valid:boolean;}// Old signature evaluatorfunctioncorrectOldSignature(run: Run, example: Example){return{    key:"correct",    score: run.outputs?.["answer"]=== example.outputs?.["answer"],};}// Output-only evaluatorfunctionconcision({ outputs }:{ outputs: AppOutputs }){return{    key:"concision",    score: Math.min(Math.floor(outputs.answer.length /1000),4)+1,};}// LLM-as-judge evaluatorconst openai =newOpenAI();asyncfunctionvalidReasoning({  inputs,  outputs}:{  inputs: AppInputs;  outputs: AppOutputs;}){const instructions =`   Given the following question, answer, and reasoning, determine if the reasoning for the    answer is logically valid and consistent with question and the answer.`;const msg =`Question: ${inputs.question}Answer: ${outputs.answer}\nReasoning: ${outputs.reasoning}`;const response =await openai.chat.completions.create({    model:"gpt-4",    messages:[{ role:"system", content: instructions },{ role:"user", content: msg }],    response_format:{ type:"json_object"},    functions:[{    name:"parse_response",    parameters:{      type:"object",      properties:{      reasoning_is_valid:{        type:"boolean",        description:"Whether the reasoning is valid"}},      required:["reasoning_is_valid"]}}]});const parsed =JSON.parse(response.choices[0].message.content ??"{}")as Response;return{    key:"valid_reasoning",    score: parsed.reasoning_is_valid ?1:0};}// Example applicationfunctiondummyApp(inputs: AppInputs): AppOutputs {return{    answer:"hmm i'm not sure",    reasoning:"i didn't understand the question"};}const results =awaitevaluate(dummyApp,{    data:"dataset_name",    evaluators:[correctOldSignature, concision, validReasoning],    client:newClient()});
```

## Related[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator#related "Direct link to Related")
  * [Evaluate aggregate experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/summary): Define summary evaluators, which compute metrics for an entire experiment.
  * [Run an evaluation comparing two experiments](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise): Define pairwise evaluators, which compute metrics by comparing two (or more) experiments against each other.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)[NextHow to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
  * [Basic example](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator#basic-example)
  * [Evaluator args](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator#evaluator-args)
  * [Evaluator output](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator#evaluator-output)
  * [Additional examples](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator#additional-examples)
  * [Related](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator#related)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to evaluate on a split / filtered view of a dataset


On this page
# How to evaluate on a split / filtered view of a dataset
Recommended reading
Before diving into this content, it might be helpful to read:
  * [guide on fetching examples](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#fetch-examples).
  * [guide on creating/managing dataset splits](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-and-manage-dataset-splits)


## Evaluate on a filtered view of a dataset[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset#evaluate-on-a-filtered-view-of-a-dataset "Direct link to Evaluate on a filtered view of a dataset")
You can use the `list_examples` / `listExamples` method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples.
One common workflow is to fetch examples that have a certain metadata key-value pair.
  * Python
  * TypeScript


```
from langsmith import evaluateresults = evaluate(lambda inputs: label_text(inputs["text"]),  data=client.list_examples(dataset_name=dataset_name, metadata={"desired_key":"desired_value"}),  evaluators=[correct_label],  experiment_prefix="Toxic Queries",)
```

```
import{ evaluate }from"langsmith/evaluation";awaitevaluate((inputs)=>labelText(inputs["input"]),{ data: langsmith.listExamples({  datasetName: datasetName,  metadata:{"desired_key":"desired_value"},}), evaluators:[correctLabel], experimentPrefix:"Toxic Queries",});
```

For more advanced filtering capabilities see this [how-to guide](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-examples-by-structured-filter).
## Evaluate on a dataset split[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset#evaluate-on-a-dataset-split "Direct link to Evaluate on a dataset split")
You can use the `list_examples` / `listExamples` method to evaluate on one or multiple splits of your dataset. The `splits` param takes a list of the splits you would like to evaluate.
  * Python
  * TypeScript


```
from langsmith import evaluateresults = evaluate(lambda inputs: label_text(inputs["text"]),  data=client.list_examples(dataset_name=dataset_name, splits=["test","training"]),  evaluators=[correct_label],  experiment_prefix="Toxic Queries",)
```

```
import{ evaluate }from"langsmith/evaluation";awaitevaluate((inputs)=>labelText(inputs["input"]),{ data: langsmith.listExamples({  datasetName: datasetName,  splits:["test","training"],}), evaluators:[correctLabel], experimentPrefix:"Toxic Queries",});
```

## Related[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset#related "Direct link to Related")
  * Learn more about how to fetch views of a dataset [here](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#fetch-datasets)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)[NextHow to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
  * [Evaluate on a filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset#evaluate-on-a-filtered-view-of-a-dataset)
  * [Evaluate on a dataset split](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset#evaluate-on-a-dataset-split)
  * [Related](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset#related)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to evaluate on a specific dataset version


On this page
# How to evaluate on a specific dataset version
Recommended reading
Before diving into this content, it might be helpful to read the [guide on versioning datasets](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets). Additionally, it might be helpful to read the [guide on fetching examples](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#fetch-examples).
## Using `list_examples`[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version#using-list_examples "Direct link to using-list_examples")
You can take advantage of the fact that `evaluate` / `aevaluate` allows passing in an iterable of examples to evaluate on a particular version of a dataset. Simply use `list_examples` / `listExamples` to fetch examples from a particular version tag using `as_of` / `asOf` and pass that in to the `data` argument.
  * Python
  * TypeScript


```
from langsmith import Clientls_client = Client()# Assumes actual outputs have a 'class' key.# Assumes example outputs have a 'label' key.defcorrect(outputs:dict, reference_outputs:dict)->bool:return outputs["class"]== reference_outputs["label"]results = ls_client.evaluate(lambda inputs:{"class":"Not toxic"},# Pass in filtered data here:  data=ls_client.list_examples(   dataset_name="Toxic Queries",   as_of="latest",# specify version here),  evaluators=[correct],)
```

```
import{ evaluate }from"langsmith/evaluation";awaitevaluate((inputs)=>labelText(inputs["input"]),{ data: langsmith.listExamples({  datasetName: datasetName,  asOf:"latest",}), evaluators:[correctLabel],});
```

## Related[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version#related "Direct link to Related")
  * Learn more about how to fetch views of a dataset [here](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#fetch-datasets)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)[NextHow to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
  * [Using `list_examples`](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version#using-list_examples)
  * [Related](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version#related)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/define_target

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to define a target function to evaluate


On this page
# How to define a target function to evaluate
There are three main pieces need to run an evaluation:
  1. A [dataset](https://docs.smith.langchain.com/evaluation/concepts#datasets) of test inputs and expected outputs.
  2. A target function which is what you're evaluating.
  3. [Evaluators](https://docs.smith.langchain.com/evaluation/concepts#evaluators) that score your target function's outputs.


This guide shows you how to define the target function depending on the part of your application you are evaluating. See here for [how to create a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically) and [how to define evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator), and here for an [end-to-end example of running an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application).
## Target function signature[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target#target-function-signature "Direct link to Target function signature")
In order to evaluate an application in code, we need a way to run the application. When using `evaluate()` ([Python](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.evaluate)/[TypeScript](https://docs.smith.langchain.com/reference/js/functions/evaluation.evaluate))we'll do this by passing in a _target function_ argument. This is a function that takes in a dataset [Example's](https://docs.smith.langchain.com/evaluation/concepts#examples) inputs and returns the application output as a dict. Within this function we can call our application however we'd like. We can also format the output however we'd like. The key is that any evaluator functions we define should work with the output format we return in our target function.
```
from langsmith import Client# 'inputs' will come from your dataset.defdummy_target(inputs:dict)->dict:return{"foo":1,"bar":"two"}# 'inputs' will come from your dataset.# 'outputs' will come from your target function.defevaluator_one(inputs:dict, outputs:dict)->bool:return outputs["foo"]==2defevaluator_two(inputs:dict, outputs:dict)->bool:returnlen(outputs["bar"])<3client = Client()results = client.evaluate(  dummy_target,# <-- target function  data="your-dataset-name",  evaluators=[evaluator_one, evaluator_two],...)
```

Automatic tracing
`evaluate()` will automatically trace your target function. This means that if you run any traceable code within your target function, this will also be traced as child runs of the target trace.
## Example: Single LLM call[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target#example-single-llm-call "Direct link to Example: Single LLM call")
When we're iterating on a prompt or comparing models it can be useful to evaluate a single LLM call:
  * Python
  * TypeScript
  * Python (LangChain)
  * TypeScript (LangChain)


Set env var `OPENAI_API_KEY` and install deps `pip install -U openai langsmith`.
```
from langsmith import wrappersfrom openai import OpenAI# Optionally wrap the OpenAI client to automatically # trace all model calls.oai_client = wrappers.wrap_openai(OpenAI())deftarget(inputs:dict)->dict:# This assumes your dataset has inputs with a 'messages' key.# You can update to match your dataset schema. messages = inputs["messages"] response = oai_client.chat.completions.create(   messages=messages,   model="gpt-4o-mini",)return{"answer": response.choices[0].message.content}
```

Set env var `OPENAI_API_KEY` and install `openai` and `langsmith`.
```
import OpenAI from'openai';import{ wrapOpenAI }from"langsmith/wrappers";const client =wrapOpenAI(newOpenAI());// This is the function you will evaluate.consttarget=async(inputs)=>{// This assumes your dataset has inputs with a `messages` keyconst messages = inputs.messages;const response =await client.chat.completions.create({   messages: messages,   model:'gpt-4o-mini',});return{ answer: response.choices[0].message.content };}
```

Set env var `OPENAI_API_KEY` and install deps `pip install -U langchain[openai]`.
```
from langchain.chat_models import init_chat_modelllm = init_chat_model("openai:gpt-4o-mini")deftarget(inputs:dict)->dict:# This assumes your dataset has inputs with a `messages` key messages = inputs["messages"] response = llm.invoke(messages)return{"answer": response.content}
```

Set env var `OPENAI_API_KEY` and install `@langchain/openai`.
```
import{ ChatOpenAI }from'@langchain/openai';// This is the function you will evaluate.consttarget=async(inputs)=>{// This assumes your dataset has inputs with a `messages` keyconst messages = inputs.messages;const model =newChatOpenAI({ model:"gpt-4o-mini"});const response =await model.invoke(messages);return{"answer": response.content};}
```

## Example: Non-LLM component[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target#example-non-llm-component "Direct link to Example: Non-LLM component")
Sometimes, you may want to evaluate a step of your application that doesn't involve an LLM. This includes but is not limited to:
  * A retrieval step in a RAG application
  * Execution of a tool


In this example we show how to test a simple calculator tool. In practice evaluations are useful for components that have more complex and hard-to-unit-test behavior, like a retriever or an online research tool.
  * Python
  * TypeScript


```
from langsmith import traceable# Optionally decorate with '@traceable' to trace all invocations of this function.@traceabledefcalculator_tool(operation:str, number1:float, number2:float)->str:if operation =="add":returnstr(number1 + number2)elif operation =="subtract":returnstr(number1 - number2)elif operation =="multiply":returnstr(number1 * number2)elif operation =="divide":returnstr(number1 / number2)else:raise ValueError(f"Unrecognized operation: {operation}.")# This is the function you will evaluate.deftarget(inputs:dict)->dict:# This assumes your dataset has inputs with `operation`, `num1`, and `num2` keys. operation = inputs["operation"] number1 = inputs["num1"] number2 = inputs["num2"] result = calculator_tool(operation, number1, number2)return{"result": result}
```

```
import{ traceable }from"langsmith/traceable";// Optionally wrap in 'traceable' to trace all invocations of this function. const calculatorTool =traceable(async({ operation, number1, number2 })=>{// Functions must return stringsif(operation ==="add"){return(number1 + number2).toString();}elseif(operation ==="subtract"){return(number1 - number2).toString();}elseif(operation ==="multiply"){return(number1 * number2).toString();}elseif(operation ==="divide"){return(number1 / number2).toString();}else{thrownewError("Invalid operation.");}});// This is the function you will evaluate.consttarget=async(inputs)=>{// This assumes your dataset has inputs with `operation`, `num1`, and `num2` keysconst result =await calculatorTool.invoke({ operation: inputs.operation, number1: inputs.num1, number2: inputs.num2,});return{ result };}
```

## Example: Application or agent[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target#example-application-or-agent "Direct link to Example: Application or agent")
Evaluating the complete output of your agentic application captures the interactions between multiple components, providing a more realistic picture of end-to-end performance. End-to-end evaluations may also uncover integration and error handling issues that might be missed when testing isolated functions or individual LLM calls.```
  * Python
  * TypeScript


```
from my_agent import agent# This is the function you will evaluate.deftarget(inputs:dict)->dict:# This assumes your dataset has inputs with a `messages` key messages = inputs["messages"]# Replace `invoke` with whatever you use to call your agent response = agent.invoke({"messages": messages})# This assumes your agent output is in the right formatreturn response
```

```
import{ agent }from'my_agent';// This is the function you will evaluate.consttarget=async(inputs)=>{// This assumes your dataset has inputs with a `messages` keyconst messages = inputs.messages;// Replace `invoke` with whatever you use to call your agentconst response =await agent.invoke({ messages });// This assumes your agent output is in the right formatreturn response;}
```

LangGraph / LangChain targets
If you have a LangGraph/LangChain agent that accepts the inputs defined in your dataset and that returns the output format you want to use in your evaluators, you can pass that object in as the target directly:
```
from my_agent import agentfrom langsmith import Clientclient = Client()client.evaluate(agent,...)
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)[NextHow to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
  * [Target function signature](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target#target-function-signature)
  * [Example: Single LLM call](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target#example-single-llm-call)
  * [Example: Non-LLM component](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target#example-non-llm-component)
  * [Example: Application or agent](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target#example-application-or-agent)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to download experiment results as a CSV


On this page
# How to download experiment results as a CSV
LangSmith lets you download experiment results as a CSV file, making it easy to analyze and share your results.
## Download experiment results as a CSV[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv#download-experiment-results-as-a-csv "Direct link to Download experiment results as a CSV")
To download your experiment results as a CSV, click the download icon at the top of the experiment view. The icon is directly to the left of the ["Compact" toggle](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results#adjust-the-table-display).
![Download CSV](https://docs.smith.langchain.com/assets/images/download_experiment_results_as_csv-2819611f32ba57fe2273fc27d53276d8.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/download_experiment_results_as_csv%3E).
[PreviousHow to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)[NextRun an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
  * [Download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv#download-experiment-results-as-a-csv)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to evaluate an existing experiment (Python only)


# How to evaluate an existing experiment (Python only)
Evaluation of existing experiments is currently only supported in the Python SDK.
If you have already run an experiment and want to add additional evaluation metrics, you can apply any evaluators to the experiment using the `evaluate()` / `aevaluate()` methods as before. Just pass in the experiment name / ID instead of a target function:
```
from langsmith import evaluatedefalways_half(inputs:dict, outputs:dict)->float:return0.5experiment_name ="my-experiment:abc"# Replace with an actual experiment name or IDevaluate(experiment_name, evaluators=[always_half])
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/evaluate_existing_experiment%3E).
[PreviousDynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)[NextHow to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to run an evaluation


On this page
# How to run an evaluation
Key concepts
[Evaluations](https://docs.smith.langchain.com/evaluation/concepts#applying-evaluations) | [Evaluators](https://docs.smith.langchain.com/evaluation/concepts#evaluators) | [Datasets](https://docs.smith.langchain.com/evaluation/concepts#datasets)
In this guide we'll go over how to evaluate an application using the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) method in the LangSmith SDK.
Running large jobs
For larger evaluation jobs in Python we recommend using [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate), the asynchronous version of [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate). It is still worthwhile to read this guide first, as the two have identical interfaces, before reading the how-to guide on [running an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async).
In JS/TS evaluate() is already asynchronous so no separate method is needed.
It is also important to configure the `max_concurrency`/`maxConcurrency` arg when running large jobs. This parallelizes evaluation by effectively splitting the dataset across threads.
## Define an application[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#define-an-application "Direct link to Define an application")
First we need an application to evaluate. Let's create a simple toxicity classifier for this example.
  * Python
  * TypeScript


```
from langsmith import traceable, wrappersfrom openai import OpenAI# Optionally wrap the OpenAI client to trace all model calls.oai_client = wrappers.wrap_openai(OpenAI())# Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.@traceabledeftoxicity_classifier(inputs:dict)->dict:  instructions =("Please review the user query below and determine if it contains any form of toxic behavior, ""such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does ""and 'Not toxic' if it doesn't.")  messages =[{"role":"system","content": instructions},{"role":"user","content": inputs["text"]},]  result = oai_client.chat.completions.create(    messages=messages, model="gpt-4o-mini", temperature=0)return{"class": result.choices[0].message.content}
```

```
import{ OpenAI }from"openai";import{ wrapOpenAI }from"langsmith/wrappers";import{ traceable }from"langsmith/traceable";# Optionally wrap the OpenAI client to trace all model calls.const oaiClient =wrapOpenAI(newOpenAI());# Optionally add the 'traceable' wrapper to trace the inputs/outputs ofthisfunction.const toxicityClassifier =traceable(async(text:string)=>{const result =await oaiClient.chat.completions.create({   messages:[{     role:"system",     content:"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.",},{ role:"user", content: text },],   model:"gpt-4o-mini",   temperature:0,});return result.choices[0].message.content;},{ name:"toxicityClassifier"});
```

We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to [this guide](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code).
## Create or select a dataset[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#create-or-select-a-dataset "Direct link to Create or select a dataset")
We need a [Dataset](https://docs.smith.langchain.com/evaluation/concepts#datasets) to evaluate our application on. Our dataset will contain labeled [examples](https://docs.smith.langchain.com/evaluation/concepts#examples) of toxic and non-toxic text.
  * Python
  * TypeScript


Requires `langsmith>=0.3.13`
```
from langsmith import Clientls_client = Client()examples =[{"inputs":{"text":"Shut up, idiot"},"outputs":{"label":"Toxic"},},{"inputs":{"text":"You're a wonderful person"},"outputs":{"label":"Not toxic"},},{"inputs":{"text":"This is the worst thing ever"},"outputs":{"label":"Toxic"},},{"inputs":{"text":"I had a great day today"},"outputs":{"label":"Not toxic"},},{"inputs":{"text":"Nobody likes you"},"outputs":{"label":"Toxic"},},{"inputs":{"text":"This is unacceptable. I want to speak to the manager."},"outputs":{"label":"Not toxic"},},]dataset = ls_client.create_dataset(dataset_name="Toxic Queries)ls_client.create_examples( dataset_id=dataset.id, examples=examples,)
```

```
import{ Client }from"langsmith";const langsmith =newClient();// create a datasetconst labeledTexts =[["Shut up, idiot","Toxic"],["You're a wonderful person","Not toxic"],["This is the worst thing ever","Toxic"],["I had a great day today","Not toxic"],["Nobody likes you","Toxic"],["This is unacceptable. I want to speak to the manager.","Not toxic"],];const[inputs, outputs]= labeledTexts.reduce<[Array<{ input:string}>,Array<{ outputs:string}>]>(([inputs, outputs], item)=>[[...inputs,{ input: item[0]}],[...outputs,{ outputs: item[1]}],],[[],[]]);const datasetName ="Toxic Queries";const toxicDataset =await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });
```

See [here](https://docs.smith.langchain.com/evaluation/how_to_guides/#dataset-management) for more on dataset management.
## Define an evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#define-an-evaluator "Direct link to Define an evaluator")
tip
You can also check out LangChain's open source evaluation package [openevals](https://github.com/langchain-ai/openevals) for common pre-built evaluators.
[Evaluators](https://docs.smith.langchain.com/evaluation/concepts#evaluators) are functions for scoring your application's outputs. They take in the example inputs, actual outputs, and, when present, the reference outputs. Since we have labels for this task, our evaluator can directly check if the actual outputs match the reference outputs.
  * Python
  * TypeScript


Requires `langsmith>=0.3.13`
```
defcorrect(inputs:dict, outputs:dict, reference_outputs:dict)->bool:return outputs["class"]== reference_outputs["label"]
```

Requires `langsmith>=0.2.9`
```
importtype{ EvaluationResult }from"langsmith/evaluation";functioncorrect({ outputs, referenceOutputs,}:{ outputs: Record<string,any>; referenceOutputs?: Record<string,any>;}): EvaluationResult {const score = outputs.output === referenceOutputs?.outputs;return{ key:"correct", score };}
```

See [here](https://docs.smith.langchain.com/evaluation/how_to_guides/#define-an-evaluator) for more on how to define evaluators.
## Run the evaluation[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#run-the-evaluation "Direct link to Run the evaluation")
We'll use the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) methods to run the evaluation.
The key arguments are:
  * a target function that takes an input dictionary and returns an output dictionary. The `example.inputs` field of each [Example](https://docs.smith.langchain.com/reference/data_formats/example_data_format) is what gets passed to the target function. In this case our `toxicity_classifier` is already set up to take in example inputs so we can use it directly.
  * `data` - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examples
  * `evaluators` - a list of evaluators to score the outputs of the function


  * Python
  * TypeScript


Requires `langsmith>=0.3.13`
```
# Can equivalently use the 'evaluate' function directly:# from langsmith import evaluate; evaluate(...)results = ls_client.evaluate(  toxicity_classifier,  data=dataset.name,  evaluators=[correct],  experiment_prefix="gpt-4o-mini, baseline",# optional, experiment name prefix  description="Testing the baseline system.",# optional, experiment description  max_concurrency=4,# optional, add concurrency)
```

```
import{ evaluate }from"langsmith/evaluation";awaitevaluate((inputs)=>toxicityClassifier(inputs["input"]),{ data: datasetName, evaluators:[correct], experimentPrefix:"gpt-4o-mini, baseline",// optional, experiment name prefix maxConcurrency:4,// optional, add concurrency});
```

See [here](https://docs.smith.langchain.com/evaluation/how_to_guides/#run-an-evaluation) for other ways to kick off evaluations and [here](https://docs.smith.langchain.com/evaluation/how_to_guides/#configure-an-evaluation-job) for how to configure evaluation jobs.
## Explore the results[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#explore-the-results "Direct link to Explore the results")
Each invocation of `evaluate()` creates an [Experiment](https://docs.smith.langchain.com/evaluation/concepts#experiments) which can be viewed in the LangSmith UI or queried via the SDK. Evaluation scores are stored against each actual output as feedback.
_If you've annotated your code for tracing, you can open the trace of each row in a side panel view._
![](https://docs.smith.langchain.com/assets/images/view_experiment-6328bb0fb0d033a49b381d84a3f9b1e8.gif)
## Reference code[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#reference-code "Direct link to Reference code")
Click to see a consolidated code snippet
  * Python
  * TypeScript


Requires `langsmith>=0.3.13`
```
from langsmith import Client, traceable, wrappersfrom openai import OpenAI# Step 1. Define an applicationoai_client = wrappers.wrap_openai(OpenAI())@traceabledeftoxicity_classifier(inputs:dict)->str:  system =("Please review the user query below and determine if it contains any form of toxic behavior, ""such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does ""and 'Not toxic' if it doesn't.")  messages =[{"role":"system","content": system},{"role":"user","content": inputs["text"]},]  result = oai_client.chat.completions.create(    messages=messages, model="gpt-4o-mini", temperature=0)return result.choices[0].message.content# Step 2. Create a datasetls_client = Client()dataset = ls_client.create_dataset(dataset_name="Toxic Queries)examples =[{"inputs":{"text":"Shut up, idiot"},"outputs":{"label":"Toxic"},},{"inputs":{"text":"You're a wonderful person"},"outputs":{"label":"Not toxic"},},{"inputs":{"text":"This is the worst thing ever"},"outputs":{"label":"Toxic"},},{"inputs":{"text":"I had a great day today"},"outputs":{"label":"Not toxic"},},{"inputs":{"text":"Nobody likes you"},"outputs":{"label":"Toxic"},},{"inputs":{"text":"This is unacceptable. I want to speak to the manager."},"outputs":{"label":"Not toxic"},},]ls_client.create_examples( dataset_id=dataset.id, examples=examples,)# Step 3. Define an evaluatordefcorrect(inputs:dict, outputs:dict, reference_outputs:dict)->bool:return outputs["output"]== reference_outputs["label"]# Step 4. Run the evaluation# Client.evaluate() and evaluate() behave the same.results = ls_client.evaluate(  toxicity_classifier,  data=dataset.name,  evaluators=[correct],  experiment_prefix="gpt-4o-mini, simple",# optional, experiment name prefix  description="Testing the baseline system.",# optional, experiment description  max_concurrency=4,# optional, add concurrency)
```

```
import{ OpenAI }from"openai";import{ Client }from"langsmith";import{ evaluate, EvaluationResult }from"langsmith/evaluation";importtype{ Run, Example }from"langsmith/schemas";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers";const oaiClient =wrapOpenAI(newOpenAI());const toxicityClassifier =traceable(async(text:string)=>{const result =await oaiClient.chat.completions.create({   messages:[{     role:"system",     content:"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.",},{ role:"user", content: text },],   model:"gpt-4o-mini",   temperature:0,});return result.choices[0].message.content;},{ name:"toxicityClassifier"});const langsmith =newClient();// create a datasetconst labeledTexts =[["Shut up, idiot","Toxic"],["You're a wonderful person","Not toxic"],["This is the worst thing ever","Toxic"],["I had a great day today","Not toxic"],["Nobody likes you","Toxic"],["This is unacceptable. I want to speak to the manager.","Not toxic"],];const[inputs, outputs]= labeledTexts.reduce<[Array<{ input:string}>,Array<{ outputs:string}>]>(([inputs, outputs], item)=>[[...inputs,{ input: item[0]}],[...outputs,{ outputs: item[1]}],],[[],[]]);const datasetName ="Toxic Queries";const toxicDataset =await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });// Row-level evaluatorfunctioncorrect({ outputs, referenceOutputs,}:{ outputs: Record<string,any>; referenceOutputs?: Record<string,any>;}): EvaluationResult {const score = outputs.output === referenceOutputs?.outputs;return{ key:"correct", score };}awaitevaluate((inputs)=>toxicityClassifier(inputs["input"]),{ data: datasetName, evaluators:[correct], experimentPrefix:"gpt-4o-mini, simple",// optional, experiment name prefix maxConcurrency:4,// optional, add concurrency});
```

## Related[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#related "Direct link to Related")
  * [Run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
  * [Run an evaluation via the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
  * [Run an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousLog user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)[NextCreating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
  * [Define an application](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#define-an-application)
  * [Create or select a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#create-or-select-a-dataset)
  * [Define an evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#define-an-evaluator)
  * [Run the evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#run-the-evaluation)
  * [Explore the results](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#explore-the-results)
  * [Reference code](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#reference-code)
  * [Related](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application#related)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to evaluate an application's intermediate steps


On this page
# How to evaluate an application's intermediate steps
While, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline.
For example, for retrieval-augmented generation (RAG), you might want to
  1. Evaluate the retrieval step to ensure that the correct documents are retrieved w.r.t the input query.
  2. Evaluate the generation step to ensure that the correct answer is generated w.r.t the retrieved documents.


In this guide, we will use a simple, fully-custom evaluator for evaluating criteria 1 and an LLM-based evaluator for evaluating criteria 2 to highlight both scenarios.
In order to evaluate the intermediate steps of your pipeline, your evaluator function should traverse and process the `run`/`rootRun` argument, which is a `Run` object that contains the intermediate steps of your pipeline.
## 1. Define your LLM pipeline[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps#1-define-your-llm-pipeline "Direct link to 1. Define your LLM pipeline")
The below RAG pipeline consists of 1) generating a Wikipedia query given the input question, 2) retrieving relevant documents from Wikipedia, and 3) generating an answer given the retrieved documents.
  * Python
  * TypeScript


```
pip install -U langsmith langchain[openai] wikipedia
```

```
yarn add langsmith langchain @langchain/openai wikipedia
```

  * Python
  * TypeScript


Requires `langsmith>=0.3.13`
```
import wikipedia as wpfrom openai import OpenAIfrom langsmith import traceable, wrappersoai_client = wrappers.wrap_openai(OpenAI())@traceabledefgenerate_wiki_search(question:str)->str:"""Generate the query to search in wikipedia."""  instructions =("Generate a search query to pass into wikipedia to answer the user's question. ""Return only the search query and nothing more. ""This will passed in directly to the wikipedia search engine.")  messages =[{"role":"system","content": instructions},{"role":"user","content": question}]  result = oai_client.chat.completions.create(    messages=messages,    model="gpt-4o-mini",    temperature=0,)return result.choices[0].message.content@traceable(run_type="retriever")defretrieve(query:str)->list:"""Get up to two search wikipedia results."""  results =[]for term in wp.search(query, results =10):try:      page = wp.page(term, auto_suggest=False)      results.append({"page_content": page.summary,"type":"Document","metadata":{"url": page.url}})except wp.DisambiguationError:passiflen(results)>=2:return results@traceabledefgenerate_answer(question:str, context:str)->str:"""Answer the question based on the retrieved information."""  instructions =f"Answer the user's question based ONLY on the content below:\n\n{context}"  messages =[{"role":"system","content": instructions},{"role":"user","content": question}]  result = oai_client.chat.completions.create(    messages=messages,    model="gpt-4o-mini",    temperature=0)return result.choices[0].message.content@traceabledefqa_pipeline(question:str)->str:"""The full pipeline."""  query = generate_wiki_search(question)  context ="\n\n".join([doc["page_content"]for doc in retrieve(query)])return generate_answer(question, context)
```

```
import OpenAI from"openai";import wiki from"wikipedia";import{ Client }from"langsmith";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers";const openai =wrapOpenAI(newOpenAI());const generateWikiSearch =traceable(async(input:{ question:string})=>{const messages =[{    role:"system"asconst,    content:"Generate a search query to pass into Wikipedia to answer the user's question. Return only the search query and nothing more. This will be passed in directly to the Wikipedia search engine.",},{ role:"user"asconst, content: input.question },];const chatCompletion =await openai.chat.completions.create({   model:"gpt-4o-mini",   messages: messages,   temperature:0,});return chatCompletion.choices[0].message.content ??"";},{ name:"generateWikiSearch"});const retrieve =traceable(async(input:{ query:string; numDocuments:number})=>{const{ results }=await wiki.search(input.query,{ limit:10});const finalResults:Array<{   page_content:string;   type:"Document";   metadata:{ url:string};}>=[];for(const result of results){if(finalResults.length >= input.numDocuments){// Just return the top 2 pages for nowbreak;}const page =await wiki.page(result.title,{ autoSuggest:false});const summary =await page.summary();   finalResults.push({    page_content: summary.extract,    type:"Document",    metadata:{ url: page.fullurl },});}return finalResults;},{ name:"retrieve", run_type:"retriever"});const generateAnswer =traceable(async(input:{ question:string; context:string})=>{const messages =[{    role:"system"asconst,    content:`Answer the user's question based only on the content below:\n\n${input.context}`,},{ role:"user"asconst, content: input.question },];const chatCompletion =await openai.chat.completions.create({   model:"gpt-4o-mini",   messages: messages,   temperature:0,});return chatCompletion.choices[0].message.content ??"";},{ name:"generateAnswer"});const ragPipeline =traceable(async({ question }:{ question:string}, numDocuments:number=2)=>{const query =awaitgenerateWikiSearch({ question });const retrieverResults =awaitretrieve({ query, numDocuments });const context = retrieverResults.map((result)=> result.page_content).join("\n\n");const answer =awaitgenerateAnswer({ question, context });return answer;},{ name:"ragPipeline"});
```

This pipeline will produce a trace that looks something like: ![](https://docs.smith.langchain.com/assets/images/evaluation_intermediate_trace-b5d866a6f04b1648a6a2f9bbb3f20003.png)
## 2. Create a dataset and examples to evaluate the pipeline[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps#2-create-a-dataset-and-examples-to-evaluate-the-pipeline "Direct link to 2. Create a dataset and examples to evaluate the pipeline")
We are building a very simple dataset with a couple of examples to evaluate the pipeline.
  * Python
  * TypeScript


Requires `langsmith>=0.3.13`
```
from langsmith import Clientls_client = Client()dataset_name ="Wikipedia RAG"ifnot ls_client.has_dataset(dataset_name=dataset_name):  dataset = ls_client.create_dataset(dataset_name=dataset_name)  examples =[{"inputs":{"question":"What is LangChain?"}},{"inputs":{"question":"What is LangSmith?"}},]  ls_client.create_examples(   dataset_id=dataset.id,   examples=examples,)
```

```
import{ Client }from"langsmith";const client =newClient();const examples =[["What is LangChain?","LangChain is an open-source framework for building applications using large language models.",],["What is LangSmith?","LangSmith is an observability and evaluation tool for LLM products, built by LangChain Inc.",],];const datasetName ="Wikipedia RAG";const inputs = examples.map(([input, _])=>({ input }));const outputs = examples.map(([_, expected])=>({ expected }));const dataset =await client.createDataset(datasetName);await client.createExamples({ datasetId: dataset.id, inputs, outputs });
```

## 3. Define your custom evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps#3-define-your-custom-evaluators "Direct link to 3. Define your custom evaluators")
As mentioned above, we will define two evaluators: one that evaluates the relevance of the retrieved documents w.r.t the input query and another that evaluates the hallucination of the generated answer w.r.t the retrieved documents. We will be using LangChain LLM wrappers, along with [`with_structured_output`](https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/) to define the evaluator for hallucination.
The key here is that the evaluator function should traverse the `run` / `rootRun` argument to access the intermediate steps of the pipeline. The evaluator can then process the inputs and outputs of the intermediate steps to evaluate according to the desired criteria.
  * Python
  * TypeScript


Example uses `langchain` for convenience, this is not required.
```
from langchain.chat_models import init_chat_modelfrom langsmith.schemas import Runfrom pydantic import BaseModel, Fielddefdocument_relevance(run: Run)->bool:"""Checks if retriever input exists in the retrieved docs."""  qa_pipeline_run =next(    r for run in run.child_runs if r.name =="qa_pipeline")  retrieve_run =next(    r for run in qa_pipeline_run.child_runs if r.name =="retrieve")  page_contents ="\n\n".join(    doc["page_content"]for doc in retrieve_run.outputs["output"])return retrieve_run.inputs["query"]in page_contents# Data modelclassGradeHallucinations(BaseModel):"""Binary score for hallucination present in generation answer."""  is_grounded:bool= Field(..., description="True if the answer is grounded in the facts, False otherwise.")# LLM with structured outputs for grading hallucinations# For more see: https://python.langchain.com/docs/how_to/structured_output/grader_llm= init_chat_model("gpt-4o-mini", temperature=0).with_structured_output(  GradeHallucinations,  method="json_schema",  strict=True,)defno_hallucination(run: Run)->bool:"""Check if the answer is grounded in the documents.  Return True if there is no hallucination, False otherwise.  """# Get documents and answer  qa_pipeline_run =next(    r for r in run.child_runs if r.name =="qa_pipeline")  retrieve_run =next(    r for r in qa_pipeline_run.child_runs if r.name =="retrieve")  retrieved_content ="\n\n".join(    doc["page_content"]for doc in retrieve_run.outputs["output"])# Construct prompt  instructions =("You are a grader assessing whether an LLM generation is grounded in / ""supported by a set of retrieved facts. Give a binary score 1 or 0, ""where 1 means that the answer is grounded in / supported by the set of facts.")  messages =[{"role":"system","content": instructions},{"role":"user","content":f"Set of facts:\n{retrieved_content}\n\nLLM generation: {run.outputs['answer']}"},]  grade = grader_llm.invoke(messages)return grade.is_grounded
```

```
import{ EvaluationResult }from"langsmith/evaluation";import{ Run, Example }from"langsmith/schemas";import{ ChatPromptTemplate }from"@langchain/core/prompts";import{ ChatOpenAI }from"@langchain/openai";import{ z }from"zod";functionfindNestedRun(run: Run,search:(run: Run)=>boolean): Run |null{const queue: Run[]=[run];while(queue.length >0){const currentRun = queue.shift()!;if(search(currentRun))return currentRun;  queue.push(...currentRun.child_runs);}returnnull;}// A very simple evaluator that checks to see if the input of the retrieval step exists// in the retrieved docs.functiondocumentRelevance(rootRun: Run, example: Example): EvaluationResult {const retrieveRun =findNestedRun(rootRun,(run)=> run.name ==="retrieve");const docs:Array<{ page_content:string}>|undefined=  retrieveRun.outputs?.outputs;const pageContents = docs?.map((doc)=> doc.page_content).join("\n\n");const score = pageContents.includes(retrieveRun.inputs?.query);return{ key:"simple_document_relevance", score };}asyncfunctionhallucination( rootRun: Run, example: Example):Promise<EvaluationResult>{const rag =findNestedRun(rootRun,(run)=> run.name ==="ragPipeline");const retrieve =findNestedRun(rootRun,(run)=> run.name ==="retrieve");const docs:Array<{ page_content:string}>|undefined=  retrieve.outputs?.outputs;const documents = docs?.map((doc)=> doc.page_content).join("\n\n");const prompt = ChatPromptTemplate.fromMessages<{  documents:string;  generation:string;}>([["system",[`You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n`,`Give a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.`,].join("\n"),],["human","Set of facts: \n\n {documents} \n\n LLM generation: {generation}",],]);const llm =newChatOpenAI({  model:"gpt-4o-mini",  temperature:0,}).withStructuredOutput(  z.object({    binary_score: z.number().describe("Answer is grounded in the facts, 1 or 0"),}).describe("Binary score for hallucination present in generation answer."));const grader = prompt.pipe(llm);const score =await grader.invoke({  documents,  generation: rag.outputs?.outputs,});return{ key:"answer_hallucination", score: score.binary_score };}
```

## 4. Evaluate the pipeline[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps#4-evaluate-the-pipeline "Direct link to 4. Evaluate the pipeline")
Finally, we'll run `evaluate` with the custom evaluators defined above.
  * Python
  * TypeScript


```
defqa_wrapper(inputs:dict)->dict:"""Wrap the qa_pipeline so it can accept the Example.inputs dict as input."""return{"answer": qa_pipeline(inputs["question"])}experiment_results = ls_client.evaluate(  qa_wrapper,  data=dataset_name,  evaluators=[document_relevance, no_hallucination],  experiment_prefix="rag-wiki-oai")
```

```
import{ evaluate }from"langsmith/evaluation";awaitevaluate((inputs)=>ragPipeline({ question: inputs.input }),{ data: datasetName, evaluators:[hallucination, documentRelevance], experimentPrefix:"rag-wiki-oai",});
```

The experiment will contain the results of the evaluation, including the scores and comments from the evaluators: ![](https://docs.smith.langchain.com/assets/images/evaluation_intermediate_experiment-2f22b28000582fdb501f9a6e1144b06c.png)
## Related[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps#related "Direct link to Related")
  * [Evaluate a `langgraph` graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/evaluate_on_intermediate_steps%3E).
[PreviousAnnotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)[NextHow to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
  * [1. Define your LLM pipeline](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps#1-define-your-llm-pipeline)
  * [2. Create a dataset and examples to evaluate the pipeline](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps#2-create-a-dataset-and-examples-to-evaluate-the-pipeline)
  * [3. Define your custom evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps#3-define-your-custom-evaluators)
  * [4. Evaluate the pipeline](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps#4-evaluate-the-pipeline)
  * [Related](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps#related)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * Run pairwise evaluations


On this page
# How to run pairwise evaluations
Key concepts
  * [Pairwise evaluations](https://docs.smith.langchain.com/evaluation/concepts#pairwise)


LangSmith supports evaluating **existing** experiments in a comparative manner. This allows you to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. Think [LMSYS Chatbot Arena](https://chat.lmsys.org/) - this is the same concept! To do this, use the [evaluate()](https://langsmith-docs-git-bagatur-rfcbuiltinsdkref-langchain.vercel.app/reference/python/evaluation/langsmith.evaluation._runner.evaluate) function with two existing experiments.
If you haven't already created experiments to compare, check out our [quick start](https://docs.smith.langchain.com/evaluation/) or our [how-to guide](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application) to get started with evaluations.
## `evaluate()` comparative args[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#evaluate-comparative-args "Direct link to evaluate-comparative-args")
info
This guide requires `langsmith` Python version `>=0.2.0` or JS version `>=0.2.9`.
At its simplest, `evaluate` / `aevaluate` function takes the following arguments:
Argument| Description  
---|---  
`target`| A list of the two **existing experiments** you would like to evaluate against each other. These can be uuids or experiment names.  
`evaluators`| A list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these.  
Along with these, you can also pass in the following optional args:
Argument| Description  
---|---  
`randomize_order` / `randomizeOrder`| An optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.  
`experiment_prefix` / `experimentPrefix`| A prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.  
`description`| A description of the pairwise experiment. Defaults to None.  
`max_concurrency` / `maxConcurrency`| The maximum number of concurrent evaluations to run. Defaults to 5.  
`client`| The LangSmith client to use. Defaults to None.  
`metadata`| Metadata to attach to your pairwise experiment. Defaults to None.  
`load_nested` / `loadNested`| Whether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False.  
## Define a pairwise evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#define-a-pairwise-evaluator "Direct link to Define a pairwise evaluator")
Pairwise evaluators are just functions with an expected signature.
### Evaluator args[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#evaluator-args "Direct link to Evaluator args")
Custom evaluator functions must have specific argument names. They can take any subset of the following arguments:
  * `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.
  * `outputs: list[dict]`: A two-item list of the dict outputs produced by each experiment on the given inputs.
  * `reference_outputs` / `referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.
  * `runs: list[Run]`: A two-item list of the full [Run](https://docs.smith.langchain.com/reference/data_formats/run_data_format) objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.
  * `example: Example`: The full dataset [Example](https://docs.smith.langchain.com/reference/data_formats/example_data_format), including the example inputs, outputs (if available), and metdata (if available).


For most use cases you'll only need `inputs`, `outputs`, and `reference_outputs` / `referenceOutputs`. `run` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.
### Evaluator output[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#evaluator-output "Direct link to Evaluator output")
Custom evaluators are expected to return one of the following types:
Python and JS/TS
  * `dict`: dictionary with keys: 
    * `key`, which represents the feedback key that will be logged
    * `scores`, which is a mapping from run ID to score for that run.
    * `comment`, which is a string. Most commonly used for model reasoning.


Currently Python only
  * `list[int | float | bool]`: a two-item list of scores. The list is assumed to have the same order as the `runs` / `outputs` evaluator args. The evaluator function name is used for the feedback key.


Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with `pairwise_` or `ranked_`.
## Run a pairwise evaluation[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#run-a-pairwise-evaluation "Direct link to Run a pairwise evaluation")
The following example uses [a prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2) which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.
Optional LangChain Usage
In the Python example below, we are pulling [this structured prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2) from the [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub) and using it with a LangChain chat model wrapper.
**Usage of LangChain is totally optional.** To illustrate this point, the TypeScript example uses the OpenAI SDK directly.
  * Python
  * TypeScript


Requires `langsmith>=0.2.0`
```
from langchain import hubfrom langchain.chat_models import init_chat_modelfrom langsmith import evaluate# See the prompt: https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2prompt = hub.pull("langchain-ai/pairwise-evaluation-2")model = init_chat_model("gpt-4o")chain = prompt | modeldefranked_preference(inputs:dict, outputs:list[dict])->list:# Assumes example inputs have a 'question' key and experiment# outputs have an 'answer' key.  response = chain.invoke({"question": inputs["question"],"answer_a": outputs[0].get("answer","N/A"),"answer_b": outputs[1].get("answer","N/A"),})if response["Preference"]==1:    scores =[1,0]elif response["Preference"]==2:    scores =[0,1]else:    scores =[0,0]return scoresevaluate(("experiment-1","experiment-2"),# Replace with the names/IDs of your experiments  evaluators=[ranked_preference],  randomize_order=True,  max_concurrency=4,)
```

Requires `langsmith>=0.2.9`
```
import{ evaluate}from"langsmith/evaluation";import{ Run }from"langsmith/schemas";import{ wrapOpenAI }from"langsmith/wrappers";import OpenAI from"openai";import{ z }from"zod";const openai =wrapOpenAI(newOpenAI());asyncfunctionrankedPreference({ inputs, runs,}:{ inputs: Record<string,any>; runs: Run[];}){const scores: Record<string,number>={};const[runA, runB]= runs;if(!runA ||!runB)thrownewError("Expected at least two runs");const payload ={  question: inputs.question,  answer_a: runA?.outputs?.output ??"N/A",  answer_b: runB?.outputs?.output ??"N/A",};const output =await openai.chat.completions.create({  model:"gpt-4-turbo",  messages:[{    role:"system",    content:["Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.","You should choose the assistant that follows the user's instructions and answers the user's question better.","Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.","Begin your evaluation by comparing the two responses and provide a short explanation.","Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.","Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.",].join(" "),},{    role:"user",    content:[`[User Question] ${payload.question}`,`[The Start of Assistant A's Answer] ${payload.answer_a} [The End of Assistant A's Answer]`,`The Start of Assistant B's Answer] ${payload.answer_b} [The End of Assistant B's Answer]`,].join("\n\n"),},],  tool_choice:{   type:"function",function:{ name:"Score"},},  tools:[{    type:"function",function:{     name:"Score",     description:[`After providing your explanation, output your final verdict by strictly following this format:`,`Output "1" if Assistant A answer is better based upon the factors above.`,`Output "2" if Assistant B answer is better based upon the factors above.`,`Output "0" if it is a tie.`,].join(" "),     parameters:{      type:"object",      properties:{       Preference:{        type:"integer",        description:"Which assistant answer is preferred?",},},},},},],});const{ Preference }= z.object({ Preference: z.number()}).parse(JSON.parse(output.choices[0].message.tool_calls[0].function.arguments));if(Preference ===1){  scores[runA.id]=1;  scores[runB.id]=0;}elseif(Preference ===2){  scores[runA.id]=0;  scores[runB.id]=1;}else{  scores[runA.id]=0;  scores[runB.id]=0;}return{ key:"ranked_preference", scores };}awaitevaluate(["earnest-name-40","reflecting-pump-91"],{ evaluators:[rankedPreference],});
```

## View pairwise experiments[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#view-pairwise-experiments "Direct link to View pairwise experiments")
Navigate to the "Pairwise Experiments" tab from the dataset page:
![Pairwise Experiments Tab](https://docs.smith.langchain.com/assets/images/pairwise_from_dataset-199fcd2a84e3d2c3a0038bac3798dc84.png)
Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View:
![Pairwise Comparison View](https://docs.smith.langchain.com/assets/images/pairwise_comparison_view-b2488aea85cc61989df7cb4105344831.png)
You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:
![Pairwise Filtering](https://docs.smith.langchain.com/assets/images/filter_pairwise-38195d09c91b78c4bf08ae6472c9c7c6.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)[NextHow to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
  * [`evaluate()` comparative args](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#evaluate-comparative-args)
  * [Define a pairwise evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#define-a-pairwise-evaluator)
    * [Evaluator args](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#evaluator-args)
    * [Evaluator output](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#evaluator-output)
  * [Run a pairwise evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#run-a-pairwise-evaluation)
  * [View pairwise experiments](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise#view-pairwise-experiments)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * Run an evaluation with large file inputs


On this page
# Run an evaluation with large file inputs
LangSmith supports creating dataset examples with file attachments, which you can consume when running evals over that dataset.
Attachments are most useful when working with LLM applications that require multimodal inputs or produce multimodal outputs. While multimodal data can be base64 encoded and uploaded as part of an example's inputs/outputs, base64 encodings are fairly space inefficient relative to the underlying binary data making them slower to upload/download to/from LangSmith. By using attachments you can speed up uploads/downloads and get nicer renderings of different file types in the LangSmith UI.
## Create examples with attachments[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#create-examples-with-attachments "Direct link to Create examples with attachments")
### Using the SDK[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#using-the-sdk "Direct link to Using the SDK")
To upload examples with attachments using the SDK, use the [create_examples](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_examples) / [update_examples](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.update_examples) Python methods or the [uploadExamplesMultipart](https://docs.smith.langchain.com/reference/js/classes/client.Client#uploadexamplesmultipart) / [updateExamplesMultipart](https://docs.smith.langchain.com/reference/js/classes/client.Client#updateexamplesmultipart) TypeScript methods.
  * Python
  * TypeScript


Requires `langsmith>=0.3.13`
```
import requestsimport uuidfrom pathlib import Pathfrom langsmith import Client# Publicly available test filespdf_url ="https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"wav_url ="https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"# Fetch the files as bytespdf_bytes = requests.get(pdf_url).contentwav_bytes = requests.get(wav_url).content# Create the datasetls_client = Client()dataset_name ="attachment-test-dataset"dataset = ls_client.create_dataset( dataset_name=dataset_name, description="Test dataset for evals with publicly available attachments",)# Define an example with attachmentsexample_id = uuid.uuid4()example ={"id": example_id,"inputs":{"audio_question":"What is in this audio clip?","image_question":"What is in this image?",},"outputs":{"audio_answer":"The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.","image_answer":"A mug with a blanket over it.",},"attachments":{"my_pdf":{"mime_type":"application/pdf","data": pdf_bytes},"my_wav":{"mime_type":"audio/wav","data": wav_bytes),# Example of an attachment specified via a local file path:# "my_img": {"mime_type": "image/png", "data": Path(__file__).parent / "my_img.png"},},)# Create the examplels_client.create_examples( dataset_id=dataset.id, examples=[example],# Uncomment this flag if you'd like to upload attachments from local files:# dangerously_allow_filesystem=True)
```

Requires version >= 0.2.13
You can use the `uploadExamplesMultipart` method to upload examples with attachments.
Note that this is a different method from the standard `createExamples` method, which currently does not support attachments. Each attachment requires either a `Uint8Array` or an `ArrayBuffer` as the data type.
  * `Uint8Array`: Useful for handling binary data directly.
  * `ArrayBuffer`: Represents fixed-length binary data, which can be converted to `Uint8Array` as needed.


Note that you cannot directly pass in a file path in the TypeScript SDK, as accessing local files is not supported in all runtime environments.
```
import{ Client }from"langsmith";import{ v4 as uuid4 }from"uuid";// Publicly available test filesconst pdfUrl ="https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf";const wavUrl ="https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav";const pngUrl ="https://www.w3.org/Graphics/PNG/nurbcup2si.png";// Helper function to fetch file as ArrayBufferasyncfunctionfetchArrayBuffer(url:string):Promise<ArrayBuffer>{const response =awaitfetch(url);if(!response.ok){thrownewError(`Failed to fetch ${url}: ${response.statusText}`);}return response.arrayBuffer();}// Fetch files as ArrayBufferconst pdfArrayBuffer =awaitfetchArrayBuffer(pdfUrl);const wavArrayBuffer =awaitfetchArrayBuffer(wavUrl);const pngArrayBuffer =awaitfetchArrayBuffer(pngUrl);// Create the LangSmith client (Ensure LANGSMITH_API_KEY is set in env)const langsmithClient =newClient();// Create a unique dataset nameconst datasetName ="attachment-test-dataset:"+uuid4().substring(0,8);// Create the datasetconst dataset =await langsmithClient.createDataset(datasetName,{ description:"Test dataset for evals with publicly available attachments",});// Define the example with attachmentsconst exampleId =uuid4();const example ={ id: exampleId, inputs:{   audio_question:"What is in this audio clip?",   image_question:"What is in this image?",}, outputs:{   audio_answer:"The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.",   image_answer:"A mug with a blanket over it.",},attachments:{my_pdf:{mimeType:"application/pdf",data: pdfArrayBuffer},my_wav:{mimeType:"audio/wav",data: wavArrayBuffer},my_img:{mimeType:"image/png",data: pngArrayBuffer},},};// Upload the example with attachments to the datasetawait langsmithClient.uploadExamplesMultipart(dataset.id,[example]);
```

Uploading from filesystem
Along with being passed in as bytes, attachments can be specified as paths to local files. To do so pass in a path for the attachment `data` value and specify arg `dangerously_allow_filesystem=True`:
```
client.create_examples(..., dangerously_allow_filesystem=True)
```

Once you upload examples with attachments, you can view them in the LangSmith UI. Each attachment will be rendered as a file with a preview, making it easy to inspect the contents. ![](https://docs.smith.langchain.com/assets/images/attachments_with_examples-971a7937743ea63a81622231e6d2fb6b.png)
### From the UI[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#from-the-ui "Direct link to From the UI")
#### From existing runs[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#from-existing-runs "Direct link to From existing runs")
When adding runs to a LangSmith dataset, attachments can be selectively propagated from the source run to the destination example. To learn more, please see [this guide](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#add-runs-from-the-tracing-project-ui).
![](https://docs.smith.langchain.com/assets/images/add_trace_with_attachments_to_dataset-69d8221a53c7faf28fa19784ae34e30e.png)
#### From scratch[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#from-scratch "Direct link to From scratch")
You can also upload examples with attachments directly from the LangSmith UI. You can do so by clicking the `+ Example` button in the `Examples` tab of the dataset UI. You can then upload the attachments that you want by using the "Upload Files" button:
![](https://docs.smith.langchain.com/assets/images/create_example_with_attachments-880895fd13f299567af768374c0f2356.png)
## Run evaluations[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#run-evaluations "Direct link to Run evaluations")
Once you have a dataset that contains examples with file attachments, you can run evaluations that process these attachments.
### Define a target function[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#define-a-target-function "Direct link to Define a target function")
Now that we have a dataset that includes examples with attachments, we can define a target function to run over these examples. The following example simply uses OpenAI's GPT-4o model to answer questions about an image and an audio clip.
  * Python
  * TypeScript


The target function you are evaluating must have two positional arguments in order to consume the attachments associated with the example, the first must be called `inputs` and the second must be called `attachments`.
  * The `inputs` argument is a dictionary that contains the input data for the example, excluding the attachments.
  * The `attachments` argument is a dictionary that maps the attachment name to a dictionary containing a presigned url, mime_type, and a reader of the bytes content of the file. You can use either the presigned url or the reader to get the file contents. Each value in the attachments dictionary is a dictionary with the following structure:

```
{
  "presigned_url": str,
  "mime_type": str,
  "reader": BinaryIO
}

```

```
from langsmith.wrappers import wrap_openaiimport base64from openai import OpenAIclient = wrap_openai(OpenAI())# Define target function that uses attachmentsdeffile_qa(inputs, attachments):# Read the audio bytes from the reader and encode them in base64 audio_reader = attachments["my_wav"]["reader"] audio_b64 = base64.b64encode(audio_reader.read()).decode('utf-8') audio_completion = client.chat.completions.create( model="gpt-4o-audio-preview", messages=[{"role":"user","content":[{"type":"text","text": inputs["audio_question"]},{"type":"input_audio","input_audio":{"data": audio_b64,"format":"wav"}}]}]# Most models support taking in an image URL directly in addition to base64 encoded images# You can pipe the image pre-signed URL directly to the model image_url = attachments["my_img"]["presigned_url"] image_completion = client.chat.completions.create(   model="gpt-4o-mini",   messages=[{"role":"user","content":[{"type":"text","text": inputs["image_question"]},{"type":"image_url","image_url":{"url": image_url,},},],}],)return{"audio_answer": audio_completion.choices[0].message.content,"image_answer": image_completion.choices[0].message.content,}
```

In the TypeScript SDK, the `config` argument is used to pass in the attachments to the target function if `includeAttachments` is set to `true`.
The `config` will contain `attachments` which is an object mapping the attachment name to an object of the form:
```
{
presigned_url: string,
mime_type: string,
}

```

```
import OpenAI from"openai";import{ wrapOpenAI }from"langsmith/wrappers";const client:any=wrapOpenAI(newOpenAI());asyncfunctionfileQA(inputs: Record<string,any>, config?: Record<string,any>){const presignedUrl = config?.attachments?.["my_wav"]?.presigned_url;if(!presignedUrl){thrownewError("No presigned URL provided for audio.");}const response =awaitfetch(presignedUrl);if(!response.ok){thrownewError(`Failed to fetch audio: ${response.statusText}`);}const arrayBuffer =await response.arrayBuffer();const uint8Array =newUint8Array(arrayBuffer);const audioB64 = Buffer.from(uint8Array).toString("base64");const audioCompletion =await client.chat.completions.create({model:"gpt-4o-audio-preview",messages:[{role:"user",content:[{ type:"text", text: inputs["audio_question"]},{type:"input_audio",input_audio:{data: audioB64,format:"wav",},},],},],});const imageUrl = config?.attachments?.["my_img"]?.presigned_urlconst imageCompletion =await client.chat.completions.create({model:"gpt-4o-mini",messages:[{role:"user",content:[{ type:"text", text: inputs["image_question"]},{type:"image_url",image_url:{url: imageUrl,},},],},],});return{audio_answer: audioCompletion.choices[0].message.content,image_answer: imageCompletion.choices[0].message.content,};}
```

### Define custom evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#define-custom-evaluators "Direct link to Define custom evaluators")
In addition to using attachments inside of your target function, you can also use them inside of your evaluators as follows. The exact same rules apply as above to determine whether the evaluator should receive attachments.
The evaluator below uses an LLM to judge if the reasoning and the answer are consistent. To learn more about how to define llm-based evaluators, please see [this guide](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge).
  * Python
  * TypeScript


```
# Assumes you've installed pydanticfrom pydantic import BaseModeldefvalid_image_description(outputs:dict, attachments:dict)->bool:"""Use an LLM to judge if the image description and ime are consistent.""" instructions =""" Does the description of the following image make sense? Please carefully review the image and the description to determine if the description is valid."""classResponse(BaseModel):   description_is_valid:bool image_url = attachments["my_img"]["presigned_url"] response = client.beta.chat.completions.parse(   model="gpt-4o",   messages=[{"role":"system","content": instructions},{"role":"user","content":[{"type":"image_url","image_url":{"url": image_url}},{"type":"text","text": outputs["image_answer"]}]}],   response_format=Response)return response.choices[0].message.parsed.description_is_validls_client.evaluate( file_qa, data=dataset_name, evaluators=[valid_image_description],)
```

```
import{ zodResponseFormat }from'openai/helpers/zod';import{ z }from'zod';import{ evaluate }from"langsmith/evaluation";const DescriptionResponse = z.object({description_is_valid: z.boolean(),});asyncfunctionvalidImageDescription({outputs,attachments,}:{outputs?:any;attachments?:any;}):Promise<{ key:string; score:boolean}>{const instructions =`Does the description of the following image make sense?Please carefully review the image and the description to determine if the description is valid.`;const imageUrl = attachments?.["my_img"]?.presigned_urlconst completion =await client.beta.chat.completions.parse({   model:"gpt-4o",   messages:[{       role:"system",       content: instructions,},{       role:"user",       content:[{ type:"image_url", image_url:{ url: imageUrl }},{ type:"text", text: outputs?.image_answer },],},],   response_format:zodResponseFormat(DescriptionResponse,'imageResponse'),});const score:boolean= completion.choices[0]?.message?.parsed?.description_is_valid ??false;return{ key:"valid_image_description", score };}const resp =awaitevaluate(fileQA,{data: datasetName,// Need to pass flag to include attachmentsincludeAttachments:true,evaluators:[validImageDescription],client: langsmithClient});
```

## Update examples with attachments[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#update-examples-with-attachments "Direct link to Update examples with attachments")
### Using the SDK[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#using-the-sdk-1 "Direct link to Using the SDK")
In the code [above](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#create-examples-with-attachments), we showed how to add examples with attachments to a dataset. It is also possible to update these same examples using the SDK.
As with existing examples, datasets are versioned when you update them with attachments. Therefore, you can navigate to the dataset version history to see the changes made to each example. To learn more, please see [this guide](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application).
When updating an example with attachments, you can update attachments in a few different ways:
  * Pass in new attachments
  * Rename existing attachments
  * Delete existing attachments


Note that:
  * Any existing attachments that are not explicitly renamed or retained **will be deleted**.
  * An error will be raised if you pass in a non-existent attachment name to `retain` or `rename`.
  * New attachments take precedence over existing attachments in case the same attachment name appears in the `attachments` and `attachment_operations` fields.


  * Python
  * TypeScript


```
example_update ={"id": example_id,"attachments":{# These are net new attachments"my_new_file":("text/plain",b"foo bar"),},# Any attachments not in rename/retain will be deleted.# In this case, that would be "my_img" if we uploaded it."attachments_operations":(# Retained attachments will stay exactly the same"retain":["my_pdf"],# Renaming attachments preserves the original data"rename":{"my_wav":"my_new_wav",}),)ls_client.update_examples(dataset_id=dataset.id, updates=[example_update])
```

```
import{ ExampleUpdateWithAttachments }from"langsmith/schemas";const exampleUpdate: ExampleUpdateWithAttachments ={id: exampleId,attachments:{// These are net new attachments"my_new_file":{mimeType:"text/plain",data: Buffer.from("foo bar")},},attachments_operations:{// Retained attachments will stay exactly the sameretain:["my_img"],// Renaming attachments preserves the original datarename:{"my_wav":"my_new_wav",},// Any attachments not in rename/retain will be deleted// In this case, that would be "my_pdf"},};await langsmithClient.updateExamplesMultipart(dataset.id,[exampleUpdate],);
```

### From the UI[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#from-the-ui-1 "Direct link to From the UI")
Attachment Size Limit
Attachments are limited to 20MB in size in the UI.
When editing an example in the UI, you can upload new attachments, rename and delete attachemnts, and there is also a quick reset button to restore the attachments to what previously existed on the example. No changes will be saved until you click submit.
![](https://docs.smith.langchain.com/assets/images/attachment_editing-fb89402a5b7c7723da79281b0a73763b.gif)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)[NextHow to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
  * [Create examples with attachments](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#create-examples-with-attachments)
    * [Using the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#using-the-sdk)
    * [From the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#from-the-ui)
  * [Run evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#run-evaluations)
    * [Define a target function](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#define-a-target-function)
    * [Define custom evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#define-custom-evaluators)
  * [Update examples with attachments](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#update-examples-with-attachments)
    * [Using the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#using-the-sdk-1)
    * [From the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments#from-the-ui-1)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to export filtered traces from experiment to dataset


On this page
# How to export filtered traces from experiment to dataset
After running an offline evaluation in LangSmith, you may want to export traces that met some evaluation criteria to a dataset.
## View experiment traces[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset#view-experiment-traces "Direct link to View experiment traces")
![Export filtered traces](https://docs.smith.langchain.com/assets/images/export-filtered-trace-to-dataset-d37b79d8aaaf286e32d870d8d282baba.png)
To do so, first click on the arrow next to your experiment name. This will direct you to a project that contains the traces generated from your experiment.
![Export filtered traces](https://docs.smith.langchain.com/assets/images/experiment-tracing-project-9ab7812833896611732a5a125e0074b6.png)
From there, you can filter the traces based on your evaluation criteria. In this example, I want to filter for all traces that received an accuracy score greater than 0.5.
![Export filtered traces](https://docs.smith.langchain.com/assets/images/filtered-traces-from-experiment-5d406cea3c104f91000192cd1963c1ba.png)
Afte applying the filter on my project, I can multi-select runs I'd like to add to my dataset, and click the 'Add to Dataset' at the bottom of my screen.
![Export filtered traces](https://docs.smith.langchain.com/assets/images/add-filtered-traces-to-dataset-072a2a52719330955023d5de72030508.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/export_filtered_traces_to_dataset%3E).
[PreviousHow to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)[NextHow to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
  * [View experiment traces](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset#view-experiment-traces)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to fetch performance metrics for an experiment


# How to fetch performance metrics for an experiment
Experiments, Projects, and Sessions
Tracing projects and experiments use the same underlying data structure in our backend, which is called a "session."
You might see these terms interchangeably in our documentation, but they all refer to the same underlying data structure.
We are working on unifying the terminology across our documentation and APIs.
When you run an experiment using `evaluate` with the Python or TypeScript SDK, you can fetch the performance metrics for the experiment using the `read_project`/`readProject` methods.
The payload for experiment details includes the following values:
```
{"start_time":"2024-06-06T01:02:51.299960","end_time":"2024-06-06T01:03:04.557530+00:00","extra":{"metadata":{"git":{"tags":null,"dirty":true,"branch":"ankush/agent-eval","commit":"...","repo_name":"...","remote_url":"...","author_name":"Ankush Gola","commit_time":"...","author_email":"..."},"revision_id":null,"dataset_splits":["base"],"dataset_version":"2024-06-05T04:57:01.535578+00:00","num_repetitions":3}},"name":"SQL Database Agent-ae9ad229","description":null,"default_dataset_id":null,"reference_dataset_id":"...","id":"...","run_count":9,"latency_p50":7.896,"latency_p99":13.09332,"first_token_p50":null,"first_token_p99":null,"total_tokens":35573,"prompt_tokens":32711,"completion_tokens":2862,"total_cost":0.206485,"prompt_cost":0.163555,"completion_cost":0.04293,"tenant_id":"...","last_run_start_time":"2024-06-06T01:02:51.366397","last_run_start_time_live":null,"feedback_stats":{"cot contextual accuracy":{"n":9,"avg":0.6666666666666666,"values":{"CORRECT":6,"INCORRECT":3}}},"session_feedback_stats":{},"run_facets":[],"error_rate":0,"streaming_rate":0,"test_run_number":11}
```

From here, you can extract performance metrics such as:
  * `latency_p50`: The 50th percentile latency in seconds.
  * `latency_p99`: The 99th percentile latency in seconds.
  * `total_tokens`: The total number of tokens used.
  * `prompt_tokens`: The number of prompt tokens used.
  * `completion_tokens`: The number of completion tokens used.
  * `total_cost`: The total cost of the experiment.
  * `prompt_cost`: The cost of the prompt tokens.
  * `completion_cost`: The cost of the completion tokens.
  * `feedback_stats`: The feedback statistics for the experiment.
  * `error_rate`: The error rate for the experiment.
  * `first_token_p50`: The 50th percentile latency for the time to generate the first token (if using streaming).
  * `first_token_p99`: The 99th percentile latency for the time to generate the first token (if using streaming).


Here is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs.
First, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript. Please view the [how-to guide](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application) on evaluation for more details.
```
from langsmith import Clientclient = Client()# Create a datasetdataset_name ="HelloDataset"dataset = client.create_dataset(dataset_name=dataset_name)examples =[{"inputs":{"input":"Harrison"},"outputs":{"expected":"Hello Harrison"},},{"inputs":{"input":"Ankush"},"outputs":{"expected":"Hello Ankush"},},]client.create_examples(dataset_id=dataset.id, examples=examples)Next, we will create an experiment, retrieve the experiment name from the result of `evaluate`, then fetch the performance metrics for the experiment.<CodeTabs groupId="client-language" tabs={[  PythonBlock(`from langsmith.schemas import Example, Run\ndataset_name ="HelloDataset"\ndeffoo_label(root_run: Run, example: Example)->dict:return{"score":1,"key":"foo"}\nfrom langsmith import evaluate\nresults = evaluate(lambda inputs:"Hello "+ inputs["input"],  data=dataset_name,  evaluators=[foo_label],  experiment_prefix="Hello",)\nresp = client.read_project(project_name=results.experiment_name, include_stats=True)\nprint(resp.json(indent=2))`),  TypeScriptBlock(`import{ Client }from"langsmith";import{ evaluate }from"langsmith/evaluation";importtype{ EvaluationResult }from"langsmith/evaluation";importtype{ Run, Example }from"langsmith/schemas";\n// Row-level evaluatorfunction fooLabel(rootRun: Run, example: Example): EvaluationResult {return{score:1, key:"foo"};}\nconst client = new Client();\nconst results =await evaluate((inputs)=>{return{ output:"Hello "+ inputs.input};},{ data:"HelloDataset", experimentPrefix:"Hello", evaluators:[fooLabel],});\nconst resp =await client.readProject({ projectName: results.experimentName, includeStats: true })console.log(JSON.stringify(resp, null,2))`),]}/>
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)[NextHow to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to filter experiments in the UI


On this page
# How to filter experiments in the UI
LangSmith lets you filter your previous experiments by feedback scores and metadata to make it easy to find only the experiments you care about.
## Background: add metadata to your experiments[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui#background-add-metadata-to-your-experiments "Direct link to Background: add metadata to your experiments")
When you run an experiment in the SDK, you can attach metadata to make it easier to filter in UI. This is helpful if you know what axes you want to drill down into when running experiments.
In our example, we are going to attach metadata to our experiment around the model used, the model provider, and a known ID of the prompt:
  * Python


```
models ={"openai-gpt-4o": ChatOpenAI(model="gpt-4o", temperature=0),"openai-gpt-4o-mini": ChatOpenAI(model="gpt-4o-mini", temperature=0),"anthropic-claude-3-sonnet-20240229": ChatAnthropic(temperature=0, model_name="claude-3-sonnet-20240229")}prompts ={"singleminded":"always answer questions with the word banana.","fruitminded":"always discuss fruit in your answers.","basic":"you are a chatbot."}defanswer_evaluator(run, example)->dict:  llm = ChatOpenAI(model="gpt-4o", temperature=0)  answer_grader = hub.pull("langchain-ai/rag-answer-vs-reference")| llm   score = answer_grader.invoke({"question": example.inputs["question"],"correct_answer": example.outputs["answer"],"student_answer": run.outputs,})return{"key":"correctness","score": score["Score"]}dataset_name ="Filterable Dataset"for model_type, model in models.items():for prompt_type, prompt in prompts.items():defpredict(example):return model.invoke([("system", prompt),("user", example["question"])])    model_provider = model_type.split("-")[0]    model_name = model_type[len(model_provider)+1:]    evaluate(      predict,      data=dataset_name,      evaluators=[answer_evaluator],# ADD IN METADATA HERE!!      metadata={"model_provider": model_provider,"model_name": model_name,"prompt_id": prompt_type})
```

## Filter experiments in the UI[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui#filter-experiments-in-the-ui "Direct link to Filter experiments in the UI")
In the UI, we see all experiments that have been run by default.
![](https://docs.smith.langchain.com/assets/images/filter-all-experiments-306b8e56c617e1fb6c1fb06062e28ac0.png)
If we, say, have a preference for openai models, we can easily filter down and see scores within just openai models first:
![](https://docs.smith.langchain.com/assets/images/filter-openai-506a47b07cbb1714573030fcce87d129.png)
We can stack filters, allowing us to filter out low scores on correctness to make sure we only compare relevant experiments:
![](https://docs.smith.langchain.com/assets/images/filter-feedback-bad38e89ce3c6df224825d47f6d32408.png)
Finally, we can clear and reset filters. For example, if we see there is clear there's a winner with the `singleminded` prompt, we can change filtering settings to see if any other model providers' models work as well with it:
![](https://docs.smith.langchain.com/assets/images/filter-singleminded-9f92972779f380390354f8e33425d51b.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousRun an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)[NextHow to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
  * [Background: add metadata to your experiments](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui#background-add-metadata-to-your-experiments)
  * [Filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui#filter-experiments-in-the-ui)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * Dynamic few shot example selection


On this page
# Dynamic few shot example selection
note
This feature is in open beta. It is only available to paid team plans. Please reach out to support@langchain.dev if you have questions about enablement.
Configure your datasets so that you can search for few shot examples based on an incoming request.
## Pre-conditions[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection#pre-conditions "Direct link to Pre-conditions")
  1. Your dataset must use the KV store data type (we do not currently support chat model or LLM type datasets)
  2. You must have an input schema defined for your dataset. See our docs on setting up schema validation [in our UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#dataset-schema-validation) for details.
  3. You must be on a paid team plan (e.g. Plus plan)
  4. You must be on LangSmith cloud


## Index your dataset for few shot search[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection#index-your-dataset-for-few-shot-search "Direct link to Index your dataset for few shot search")
Navigate to the datasets UI, and click the new `Few-Shot search` tab. Hit the `Start sync` button, which will create a new index on your dataset to make it searchable.
![](https://docs.smith.langchain.com/assets/images/few_shot_tab_unsynced-5b185911867b171d156ef21860d88c72.png)
By default, we sync to the latest version of your dataset. That means when new examples are added to your dataset, they will automatically be added to your index. This process runs every few minutes, so there should be a very short delay for indexing new examples. You can see whether your index is up to date under `Few-shot index` on the lefthand side of the screen in the next section.
## Test search quality in the few shot playground[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection#test-search-quality-in-the-few-shot-playground "Direct link to Test search quality in the few shot playground")
Now that you have turned on indexing for your dataset, you will see the new few shot playground.
![](https://docs.smith.langchain.com/assets/images/few_shot_synced_empty_state-a53161ba4242273a06e824ae5bb8f199.png)
You can type in a sample input, and check which results would be returned by our search API.
![](https://docs.smith.langchain.com/assets/images/few_shot_search_results-1600112964494d4b6c2a641191117076.png)
Each result will have a score and a link to the example in the dataset. The scoring system works such that 0 is a completely random result, and higher scores are better. Results will be sorted in descending order according to score.
note
Search uses a BM25-like algorithm for keyword based similarity scores. The actual score is subject to change as we improve the search algorithm, so we recommend not relying on the scores themselves, as their meaning may evolve over time. They are simply used for convenience in vibe-testing outputs in the playground.
## Adding few shot search to your application[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection#adding-few-shot-search-to-your-application "Direct link to Adding few shot search to your application")
Click the `Get Code Snippet` button in the previous diagram, you'll be taken to a screen that has code snippets from our LangSmith SDK in different languages.
![](https://docs.smith.langchain.com/assets/images/few_shot_code_snippet-bac04bbcaa5bbd844bae6c84d248a92e.png)
For code samples on using few shot search in LangChain python applications, please see our [how-to guide in the LangChain docs](https://python.langchain.com/v0.2/docs/how_to/example_selectors_langsmith/).
### Code snippets[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection#code-snippets "Direct link to Code snippets")
note
Please ensure you are using the python SDK with version >= 1.101 or the typescript SDK with version >= 1.43
For copy and paste convenience, you can find the similar code snippets to the ones shown in the screenshot above here:
  * Python (Async)
  * Python
  * TypeScript


```
import langsmith as ls# Copy this value from LangSmith UIdataset_id ="1c5e9c95-dfd4-4dc5-a4b8-df7ea921c913"asyncwith ls.AsyncClient()as client: examples =await client.similar_examples({"question":"knock knock"}, dataset_id=dataset_id, limit=1)print(examples[0].outputs)# {"output": "Few shots'll do the trick."}
```

```
from langsmith import Clientclient = Client()# Copy this value from LangSmith UIdataset_id ="1c5e9c95-dfd4-4dc5-a4b8-df7ea921c913"examples = client.similar_examples({"question":"knock knock"}, dataset_id=dataset_id, limit=1)print(examples[0].outputs)# {"output": "Few shots'll do the trick."}
```

```
import{ Client }from"langsmith";const client =newClient();// Copy this value from LangSmith UIconst dataset_id ="1c5e9c95-dfd4-4dc5-a4b8-df7ea921c913";const examples =await client.similarExamples({question:"knock knock"}, dataset_id,1);console.log(examples[0].outputs);// {output: "Few shots'll do the trick."}
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)[NextHow to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
  * [Pre-conditions](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection#pre-conditions)
  * [Index your dataset for few shot search](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection#index-your-dataset-for-few-shot-search)
  * [Test search quality in the few shot playground](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection#test-search-quality-in-the-few-shot-playground)
  * [Adding few shot search to your application](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection#adding-few-shot-search-to-your-application)
    * [Code snippets](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection#code-snippets)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to evaluate a langchain runnable


On this page
# How to evaluate a `langchain` runnable
Key concepts
  * `langchain`: [Python](https://python.langchain.com) and [JS/TS](https://js.langchain.com)
  * Runnable: [Python](https://python.langchain.com/docs/concepts/runnables/) and [JS/TS](https://js.langchain.com/docs/concepts/runnables/)


`langchain` [Runnable](https://python.langchain.com/docs/concepts/runnables/) objects (such as chat models, retrievers, chains, etc.) can be passed directly into `evaluate()` / `aevaluate()`.
## Setup[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable#setup "Direct link to Setup")
Let's define a simple chain to evaluate. First, install all the required packages:
  * Python
  * TypeScript


```
pip install -U langsmith langchain[openai]
```

```
yarn add langsmith @langchain/openai
```

Now define a chain:
  * Python
  * TypeScript


```
from langchain.chat_models import init_chat_modelfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserinstructions =("Please review the user query below and determine if it contains any form ""of toxic behavior, such as insults, threats, or highly negative comments. ""Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.")prompt = ChatPromptTemplate([("system", instructions),("user","{text}")],)llm = init_chat_model("gpt-4o")chain = prompt | llm | StrOutputParser()
```

```
import{ ChatOpenAI }from"@langchain/openai";import{ ChatPromptTemplate }from"@langchain/core/prompts";import{ StringOutputParser }from"@langchain/core/output_parsers";const prompt = ChatPromptTemplate.fromMessages([["system","Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."],["user","{text}"]]);const chatModel =newChatOpenAI();const outputParser =newStringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser);
```

## Evaluate[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable#evaluate "Direct link to Evaluate")
To evaluate our chain we can pass it directly to the `evaluate()` / `aevaluate()` method. Note that the input variables of the chain must match the keys of the example inputs. In this case, the example inputs should have the form `{"text": "..."}`.
  * Python
  * TypeScript


Requires `langsmith>=0.2.0`
```
from langsmith import aevaluate, Clientclient = Client()# Clone a dataset of texts with toxicity labels.# Each example input has a "text" key and each output has a "label" key.dataset = client.clone_public_dataset("https://smith.langchain.com/public/3d6831e6-1680-4c88-94df-618c8e01fc55/d")defcorrect(outputs:dict, reference_outputs:dict)->bool:# Since our chain outputs a string not a dict, this string# gets stored under the default "output" key in the outputs dict:  actual = outputs["output"]  expected = reference_outputs["label"]return actual == expectedresults =await aevaluate(  chain,  data=dataset,  evaluators=[correct],  experiment_prefix="gpt-4o, baseline",)
```

```
import{ evaluate }from"langsmith/evaluation";import{ Client }from"langsmith";const langsmith =newClient();const dataset =await client.clonePublicDataset("https://smith.langchain.com/public/3d6831e6-1680-4c88-94df-618c8e01fc55/d")awaitevaluate(chain,{ data: dataset.name, evaluators:[correct], experimentPrefix:"gpt-4o, baseline",});
```

The runnable is traced appropriately for each output.
![](https://docs.smith.langchain.com/assets/images/runnable_eval-4e36ece7ae7bfabf2ca97e6fa247de58.png)
## Related[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable#related "Direct link to Related")
  * [How to evaluate a `langgraph` graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)[NextHow to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
  * [Setup](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable#setup)
  * [Evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable#evaluate)
  * [Related](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable#related)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to evaluate a langgraph graph


On this page
# How to evaluate a `langgraph` graph
Key concepts
[langgraph](https://langchain-ai.github.io/langgraph/)
`langgraph` is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Evaluating `langgraph` graphs can be challenging because a single invocation can involve many LLM calls, and which LLM calls are made may depend on the outputs of preceding calls. In this guide we will focus on the mechanics of how to pass graphs and graph nodes to `evaluate()` / `aevaluate()`. For evaluation techniques and best practices when building agents head to the [langgraph docs](https://langchain-ai.github.io/langgraph/tutorials/#evaluation).
## End-to-end evaluations[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#end-to-end-evaluations "Direct link to End-to-end evaluations")
The most common type of evaluation is an end-to-end one, where we want to evaluate the final graph output for each example input.
### Define a graph[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#define-a-graph "Direct link to Define a graph")
Lets construct a simple ReACT agent to start:
  * Python


```
from typing import Annotated, Literal, TypedDictfrom langchain.chat_models import init_chat_modelfrom langchain_core.tools import toolfrom langgraph.graph import END, START, StateGraphfrom langgraph.prebuilt import ToolNodefrom langgraph.graph.message import add_messagesclassState(TypedDict):# Messages have the type "list". The 'add_messages' function # in the annotation defines how this state key should be updated # (in this case, it appends messages to the list, rather than overwriting them)messages: Annotated[list, add_messages]# Define the tools for the agent to use@tooldefsearch(query:str)->str:"""Call to surf the web."""# This is a placeholder, but don't tell the LLM that...if"sf"in query.lower()or"san francisco"in query.lower():return"It's 60 degrees and foggy."return"It's 90 degrees and sunny."tools =[search]tool_node = ToolNode(tools)model = init_chat_model("claude-3-5-sonnet-latest").bind_tools(tools)# Define the function that determines whether to continue or notdefshould_continue(state: State)-> Literal["tools", END]:  messages = state['messages']  last_message = messages[-1]# If the LLM makes a tool call, then we route to the "tools" nodeif last_message.tool_calls:return"tools"# Otherwise, we stop (reply to the user)return END# Define the function that calls the modeldefcall_model(state: State):  messages = state['messages']  response = model.invoke(messages)# We return a list, because this will get added to the existing listreturn{"messages":[response]}# Define a new graphworkflow = StateGraph(State)# Define the two nodes we will cycle betweenworkflow.add_node("agent", call_model)workflow.add_node("tools", tool_node)# Set the entrypoint as 'agent'# This means that this node is the first one calledworkflow.add_edge(START,"agent")# We now add a conditional edgeworkflow.add_conditional_edges(# First, we define the start node. We use 'agent'. # This means these are the edges taken after the 'agent' node is called."agent",# Next, we pass in the function that will determine which node is called next.  should_continue,)# We now add a normal edge from 'tools' to 'agent'.# This means that after 'tools' is called, 'agent' node is called next.workflow.add_edge("tools",'agent')# Finally, we compile it!# This compiles it into a LangChain Runnable,# meaning you can use it as you would any other runnable.# Note that we're (optionally) passing the memory when compiling the graphapp = workflow.compile()
```

### Create a dataset[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#create-a-dataset "Direct link to Create a dataset")
Let's create a simple dataset of questions and expected responses:
  * Python


```
from langsmith import Clientquestions =["what's the weather in sf","whats the weather in san fran","whats the weather in tangier"]answers =["It's 60 degrees and foggy.","It's 60 degrees and foggy.","It's 90 degrees and sunny.",]ls_client = Client()dataset = ls_client.create_dataset("weather agent",  inputs=[{"question": q}for q in questions],  outputs=[{"answers": a}for a in answers],)
```

### Create an evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#create-an-evaluator "Direct link to Create an evaluator")
And a simple evaluator:
  * Python


Requires `langsmith>=0.2.0`
```
judge_llm = init_chat_model("gpt-4o")asyncdefcorrect(outputs:dict, reference_outputs:dict)->bool:  instructions =("Given an actual answer and an expected answer, determine whether"" the actual answer contains all of the information in the"" expected answer. Respond with 'CORRECT' if the actual answer"" does contain all of the expected information and 'INCORRECT'"" otherwise. Do not include anything else in your response.")# Our graph outputs a State dictionary, which in this case means# we'll have a 'messages' key and the final message should# be our actual answer.  actual_answer = outputs["messages"][-1].content  expected_answer = reference_outputs["answer"]  user_msg =(f"ACTUAL ANSWER: {actual_answer}"f"\n\nEXPECTED ANSWER: {expected_answer}")  response =await judge_llm.ainvoke([{"role":"system","content": instructions},{"role":"user","content": user_msg}])return response.content.upper()=="CORRECT"
```

### Run evaluations[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#run-evaluations "Direct link to Run evaluations")
Now we can run our evaluations and explore the results. We'll just need to wrap our graph function so that it can take inputs in the format they're stored on our example:
Evaluating with async nodes
If all of your graph nodes are defined as sync functions then you can use `evaluate` or `aevaluate`. If any of you nodes are defined as async, you'll need to use `aevaluate`
  * Python


Requires `langsmith>=0.2.0`
```
from langsmith import aevaluatedefexample_to_state(inputs:dict)->dict:return{"messages":[{"role":"user","content": inputs['question']}]}# We use LCEL declarative syntax here.# Remember that langgraph graphs are also langchain runnables.target = example_to_state | appexperiment_results =await aevaluate(  target,  data="weather agent",  evaluators=[correct],  max_concurrency=4,# optional  experiment_prefix="claude-3.5-baseline",# optional)
```

## Evaluating intermediate steps[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#evaluating-intermediate-steps "Direct link to Evaluating intermediate steps")
Often it is valuable to evaluate not only the final output of an agent but also the intermediate steps it has taken. What's nice about `langgraph` is that the output of a graph is a state object that often already carries information about the intermediate steps taken. Usually we can evaluate whatever we're interested in just by looking at the messages in our state. For example, we can look at the messages to assert that the model invoked the 'search' tool upon as a first step.
  * Python


Requires `langsmith>=0.2.0`
```
defright_tool(outputs:dict)->bool:  tool_calls = outputs["messages"][1].tool_callsreturnbool(tool_calls and tool_calls[0]["name"]=="search")experiment_results =await aevaluate(  target,  data="weather agent",  evaluators=[correct, right_tool],  max_concurrency=4,# optional  experiment_prefix="claude-3.5-baseline",# optional)
```

If we need access to information about intermediate steps that isn't in state, we can look at the Run object. This contains the full traces for all node inputs and outputs:
Custom evaluators
See more about what arguments you can pass to custom evaluators in this [how-to guide](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator).
  * Python


```
from langsmith.schemas import Run, Exampledefright_tool_from_run(run: Run, example: Example)->dict:# Get documents and answer  first_model_run =next(run for run in root_run.child_runs if run.name =="agent")  tool_calls = first_model_run.outputs["messages"][-1].tool_calls  right_tool =bool(tool_calls and tool_calls[0]["name"]=="search")return{"key":"right_tool","value": right_tool}experiment_results =await aevaluate(  target,  data="weather agent",  evaluators=[correct, right_tool_from_run],  max_concurrency=4,# optional  experiment_prefix="claude-3.5-baseline",# optional)
```

## Running and evaluating individual nodes[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#running-and-evaluating-individual-nodes "Direct link to Running and evaluating individual nodes")
Sometimes you want to evaluate a single node directly to save time and costs. `langgraph` makes it easy to do this. In this case we can even continue using the evaluators we've been using.
  * Python


```
node_target = example_to_state | app.nodes["agent"]node_experiment_results =await aevaluate(  node_target,  data="weather agent",  evaluators=[right_tool_from_run],  max_concurrency=4,# optional  experiment_prefix="claude-3.5-model-node",# optional)
```

## Related[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#related "Direct link to Related")
  * [`langgraph` evaluation docs](https://langchain-ai.github.io/langgraph/tutorials/#evaluation)


## Reference code[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#reference-code "Direct link to Reference code")
Click to see a consolidated code snippet
  * Python


```
from typing import Annotated, Literal, TypedDictfrom langchain.chat_models import init_chat_modelfrom langchain_core.tools import toolfrom langgraph.graph import END, START, StateGraphfrom langgraph.prebuilt import ToolNodefrom langgraph.graph.message import add_messagesfrom langsmith import Client, aevaluate# Define a graphclassState(TypedDict):# Messages have the type "list". The 'add_messages' function # in the annotation defines how this state key should be updated # (in this case, it appends messages to the list, rather than overwriting them)  messages: Annotated[list, add_messages]# Define the tools for the agent to use@tooldefsearch(query:str)->str:"""Call to surf the web."""# This is a placeholder, but don't tell the LLM that...if"sf"in query.lower()or"san francisco"in query.lower():return"It's 60 degrees and foggy."return"It's 90 degrees and sunny."tools =[search]tool_node = ToolNode(tools)model = init_chat_model("claude-3-5-sonnet-latest").bind_tools(tools)# Define the function that determines whether to continue or notdefshould_continue(state: State)-> Literal["tools", END]:  messages = state['messages']  last_message = messages[-1]# If the LLM makes a tool call, then we route to the "tools" nodeif last_message.tool_calls:return"tools"# Otherwise, we stop (reply to the user)return END# Define the function that calls the modeldefcall_model(state: State):  messages = state['messages']  response = model.invoke(messages)# We return a list, because this will get added to the existing listreturn{"messages":[response]}# Define a new graphworkflow = StateGraph(State)# Define the two nodes we will cycle betweenworkflow.add_node("agent", call_model)workflow.add_node("tools", tool_node)# Set the entrypoint as 'agent'# This means that this node is the first one calledworkflow.add_edge(START,"agent")# We now add a conditional edgeworkflow.add_conditional_edges(# First, we define the start node. We use 'agent'. # This means these are the edges taken after the 'agent' node is called."agent",# Next, we pass in the function that will determine which node is called next.  should_continue,)# We now add a normal edge from 'tools' to 'agent'.# This means that after 'tools' is called, 'agent' node is called next.workflow.add_edge("tools",'agent')# Finally, we compile it!# This compiles it into a LangChain Runnable,# meaning you can use it as you would any other runnable.# Note that we're (optionally) passing the memory when compiling the graphapp = workflow.compile()questions =["what's the weather in sf","whats the weather in san fran","whats the weather in tangier"]answers =["It's 60 degrees and foggy.","It's 60 degrees and foggy.","It's 90 degrees and sunny.",]# Create a datasetls_client = Client()dataset = ls_client.create_dataset("weather agent",  inputs=[{"question": q}for q in questions],  outputs=[{"answers": a}for a in answers],)# Define evaluatorsasyncdefcorrect(outputs:dict, reference_outputs:dict)->bool:  instructions =("Given an actual answer and an expected answer, determine whether"" the actual answer contains all of the information in the"" expected answer. Respond with 'CORRECT' if the actual answer"" does contain all of the expected information and 'INCORRECT'"" otherwise. Do not include anything else in your response.")# Our graph outputs a State dictionary, which in this case means# we'll have a 'messages' key and the final message should# be our actual answer.  actual_answer = outputs["messages"][-1].content  expected_answer = reference_outputs["answer"]  user_msg =(f"ACTUAL ANSWER: {actual_answer}"f"\n\nEXPECTED ANSWER: {expected_answer}")  response =await judge_llm.ainvoke([{"role":"system","content": instructions},{"role":"user","content": user_msg}])return response.content.upper()=="CORRECT"defright_tool(outputs:dict)->bool:  tool_calls = outputs["messages"][1].tool_callsreturnbool(tool_calls and tool_calls[0]["name"]=="search")# Run evaluationexperiment_results =await aevaluate(  target,  data="weather agent",  evaluators=[correct, right_tool],  max_concurrency=4,# optional  experiment_prefix="claude-3.5-baseline",# optional)
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/langgraph%3E).
[PreviousHow to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)[NextHow to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
  * [End-to-end evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#end-to-end-evaluations)
    * [Define a graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#define-a-graph)
    * [Create a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#create-a-dataset)
    * [Create an evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#create-an-evaluator)
    * [Run evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#run-evaluations)
  * [Evaluating intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#evaluating-intermediate-steps)
  * [Running and evaluating individual nodes](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#running-and-evaluating-individual-nodes)
  * [Related](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#related)
  * [Reference code](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph#reference-code)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to define an LLM-as-a-judge evaluator


# How to define an LLM-as-a-judge evaluator
Key concepts
  * [LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/concepts#llm-as-judge)


LLM applications can be challenging to evaluate since they often generate conversational text with no single correct answer.
This guide shows you how to define an LLM-as-a-judge evaluator for [offline evaluation](https://docs.smith.langchain.com/evaluation/concepts#offline-evaluation) using either the LangSmith SDK or the UI. Note: To run evaluations in real-time on your production traces, refer to [setting up online evaluations](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#configure-llm-as-judge-evaluators).
  * SDK
  * UI


## Pre-built evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#pre-built-evaluators "Direct link to Pre-built evaluators")
Pre-built evaluators are a useful starting point for setting up evaluations. Refer to [pre-built evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators) for how to use pre-built evaluators with LangSmith.
## Create your own LLM-as-a-judge evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#create-your-own-llm-as-a-judge-evaluator "Direct link to Create your own LLM-as-a-judge evaluator")
For complete control of evaluator logic, create your own LLM-as-a-judge evaluator and run it using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python/reference) / [TypeScript](https://docs.smith.langchain.com/reference/js)).
  * Python


Requires `langsmith>=0.2.0`
```
from langsmith import evaluate, traceable, wrappers, Clientfrom openai import OpenAI# Assumes you've installed pydanticfrom pydantic import BaseModel# Optionally wrap the OpenAI client to trace all model calls.oai_client = wrappers.wrap_openai(OpenAI())defvalid_reasoning(inputs:dict, outputs:dict)->bool:"""Use an LLM to judge if the reasoning and the answer are consistent.""" instructions ="""\Given the following question, answer, and reasoning, determine if the reasoning \for the answer is logically valid and consistent with question and the answer.\"""classResponse(BaseModel):  reasoning_is_valid:bool msg =f"Question: {inputs['question']}\nAnswer: {outputs['answer']}\nReasoning: {outputs['reasoning']}" response = oai_client.beta.chat.completions.parse(  model="gpt-4o",  messages=[{"role":"system","content": instructions,},{"role":"user","content": msg}],  response_format=Response)return response.choices[0].message.parsed.reasoning_is_valid# Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.@traceabledefdummy_app(inputs:dict)->dict:return{"answer":"hmm i'm not sure","reasoning":"i didn't understand the question"}ls_client = Client()dataset = ls_client.create_dataset("big questions")examples =[{"inputs":{"question":"how will the universe end"}},{"inputs":{"question":"are we alone"}},]ls_client.create_examples(dataset_id=dataset.id, examples=examples)results = evaluate( dummy_app, data=dataset, evaluators=[valid_reasoning])
```

See [here](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator) for more on how to write a custom evaluator.
## Pre-built evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#pre-built-evaluators-1 "Direct link to Pre-built evaluators")
Pre-built evaluators are a useful starting point when setting up evaluations. The LangSmith UI supports the following pre-built evaluators:
  * **Hallucination** : Detect factually incorrect outputs. Requires a reference output.
  * **Correctness** : Check semantic similarity to a reference.
  * **Conciseness** : Evaluate whether an answer is a concise response to a question.
  * **Code checker** : Verify correctness of code answers.


You can configure these evaluators::
  * When running an evaluation using the [playground](https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-playground)
  * As part of a dataset to [automatically run experiments over a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
  * When running an [online evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#configure-llm-as-judge-evaluators)


## Customize your LLM-as-a-judge evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#customize-your-llm-as-a-judge-evaluator "Direct link to Customize your LLM-as-a-judge evaluator")
Add specific instructions for your LLM-as-a-judge evalutor prompt and configure which parts of the input/output/reference output should be passed to the evaluator.
![](https://docs.smith.langchain.com/assets/images/playground_evaluator-3758d5cbed9fcb836ca929b9d7d93a1e.gif)
### Select/create the evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#selectcreate-the-evaluator "Direct link to Select/create the evaluator")
  * In the playground or from a dataset: Select the **+Evaluator** button
  * From a tracing project: Select **Add rules** , configure your rule and select **Apply evaluator**


Select the **Create your own evaluator option**. Alternativley, you may start by selecting a pre-built evaluator and editing it.
### Configure the evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#configure-the-evaluator "Direct link to Configure the evaluator")
#### Prompt[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#prompt "Direct link to Prompt")
Create a new prompt, or choose an existing prompt from the [prompt hub](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui).
  * **Create your own prompt** : Create a custom prompt inline.
  * **Pull a prompt from the prompt hub** : Use the **Select a prompt** dropdown to select from an existing prompt. You can't edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses. To make changes, edit the prompt in the playground and commit the version, and then pull in your new prompt in the evaluator.


#### Model[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#model "Direct link to Model")
Select the desired model from the provided options.
#### Mapping variables[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#mapping-variables "Direct link to Mapping variables")
Use variable mapping to indicate the variables that are passed into your evaluator prompt from your run or example. To aid with variable mapping, an example (or run) is provided for reference. Click on the the variables in your prompt and use the dropdown to map them to the relevant parts of the input, output, or reference output.
To add prompt variables type the variable with double curly brackets `{{prompt_var}}` if using mustache formatting (the default) or single curly brackets `{prompt_var}` if using f-string formatting.
You may remove variables as needed. For example if you are evaluating a metric such as conciseness, you typically don't need a reference output so you may remove that variable.
#### Preview[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#preview "Direct link to Preview")
Previewing the prompt will show you of what the formatted prompt will look like using the reference run and dataset example shown on the right.
#### Improve your evaluator with few-shot examples[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#improve-your-evaluator-with-few-shot-examples "Direct link to Improve your evaluator with few-shot examples")
To better align the LLM-as-a-judge evaluator to human preferences, LangSmith allows you to collect [human corrections](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators#make-corrections) on evaluator scores. With this selection enabled, corrections are then inserted automatically as few-shot examples into your prompt.
Learn [how to set up few-shot examples and make corrections](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators).
#### Feedback configuration[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#feedback-configuration "Direct link to Feedback configuration")
Feedback configuration is the scoring criteria that your LLM-as-a-judge evaluator will use. Think of this as the rubric that your evaluator will grade based on. Scores will be added as [feedback](https://docs.smith.langchain.com/observability/concepts#feedback) to a run or example. Defining feedback for your evaluator:
  1. **Name the feedback key** : This is the name that will appear when viewing evaluation results. Names should be unique across experiments.
  2. **Add a description** : Describe what the feedback represents.
  3. **Choose a feedback type** :


  * **Boolean** : True/false feedback.
  * **Categorical** : Select from predefined categories.
  * **Continuous** : Numerical scoring within a specified range.


If you are using an existing prompt from the hub, you must add an output schema to the prompt before configuring an evaluator to use it. Each top-level key in the output schema will be treated as a separate piece of feedback.
### Save the evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge#save-the-evaluator "Direct link to Save the evaluator")
Once your are finished configuring, save your changes.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)[NextHow to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/local

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/local#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to run an evaluation locally (beta, Python only)


On this page
# How to run an evaluation locally (beta, Python only)
Beta
This feature is still in beta.
Sometimes it is helpful to run an evaluation locally without uploading any results to LangSmith. For example, if you're quickly iterating on a prompt and want to smoke test it on a few examples, or if you're validating that your target and evaluator functions are defined correctly, you may not want to record these evaluations.
You can do this by using the LangSmith Python SDK and passing `upload_results=False` to `evaluate()` / `aevaluate()`.
This will run you application and evaluators exactly as it always does and return the same output, but nothing will be recorded to LangSmith. This includes not just the experiment results but also the application and evaluator traces.
## Example[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/local#example "Direct link to Example")
Let's take a look at an example:
  * Python


Requires `langsmith>=0.2.0`. Example also uses `pandas`.
```
from langsmith import Client# 1. Create and/or select your datasetls_client = Client()dataset = ls_client.clone_public_dataset("https://smith.langchain.com/public/a63525f9-bdf2-4512-83e3-077dc9417f96/d")# 2. Define an evaluatordefis_concise(outputs:dict, reference_outputs:dict)->bool:returnlen(outputs["answer"])<(3*len(reference_outputs["answer"]))# 3. Define the interface to your appdefchatbot(inputs:dict)->dict:return{"answer": inputs["question"]+" is a good question. I don't know the answer."}# 4. Run an evaluationexperiment = ls_client.evaluate(  chatbot,  data=dataset,  evaluators=[is_concise],  experiment_prefix="my-first-experiment",# 'upload_results' is the relevant arg.  upload_results=False)# 5. Analyze results locallyresults =list(experiment)# Check if 'is_concise' returned False.failed =[r for r in results ifnot r["evaluation_results"]["results"][0].score]# Explore the failed inputs and outputs.for r in failed:print(r["example"].inputs)print(r["run"].outputs)# Explore the results as a Pandas DataFrame.# Must have 'pandas' installed.df = experiment.to_pandas()df[["inputs.question","outputs.answer","reference.answer","feedback.is_concise"]]
```

  * Python


```
{'question':'What is the largest mammal?'}{'answer':"What is the largest mammal? is a good question. I don't know the answer."}{'question':'What do mammals and birds have in common?'}{'answer':"What do mammals and birds have in common? is a good question. I don't know the answer."}
```

inputs.question| outputs.answer| reference.answer| feedback.is_concise  
---|---|---|---  
0| What is the largest mammal?| What is the largest mammal? is a good question. I don't know the answer.| The blue whale| False  
1| What do mammals and birds have in common?| What do mammals and birds have in common? is a good question. I don't know the answer.| They are both warm-blooded| False  
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)[NextHow to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
  * [Example](https://docs.smith.langchain.com/evaluation/how_to_guides/local#example)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * Creating and Managing Datasets in the UI


On this page
# Creating and Managing Datasets in the UI
[Datasets](https://docs.smith.langchain.com/evaluation/concepts#datasets) enable you to perform repeatable evaluations over time using consistent data. Datasets are made up of [examples](https://docs.smith.langchain.com/evaluation/concepts#examples), which store inputs, outputs, and optionally, reference outputs.
Recommended Reading
For more information on datasets, evaluations and examples, read the [concepts guide on evaluation and datasets](https://docs.smith.langchain.com/evaluation/concepts#datasets-and-examples).
This guide outlines the various methods for creating and editing datasets in LangSmith's UI.
## Create a dataset and add examples[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-a-dataset-and-add-examples "Direct link to Create a dataset and add examples")
### Manually from a tracing project[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#manually-from-a-tracing-project "Direct link to Manually from a tracing project")
A common pattern for constructing datasets is to convert notable traces from your application into dataset examples. This approach requires that you have [tracing to LangSmith configured](https://docs.smith.langchain.com/observability/how_to_guides#tracing-configuration).
tip
A powerful technique to build datasets is to filter-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset. For tips on how to filter traces, see the [filtering traces](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application) guide.
There are two ways to manually add data from a tracing projects to datasets.
  1. Multi-select runs from the runs table:


![Multi-select runs](https://docs.smith.langchain.com/assets/images/multiselect_add_to_dataset-032bf091161634acb71dc7060806d538.png)
  1. Navigate to the run details page and click `Add to -> Dataset` on the top right corner:


![Add to dataset](https://docs.smith.langchain.com/assets/images/add_to..dataset-a5ad64c85cfa95f376b45b3e4da50de6.png)
When you select a dataset from the run details page, a modal will pop up letting you know if any [transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations) were applied or if schema validation failed. For example, the screenshot below shows a dataset that is using transformations to optimize for collecting LLM runs.
![](https://docs.smith.langchain.com/assets/images/confirmation-6ccb70467ab3b9b6fa443542be3b2eed.png)
You can then optionally edit the run before adding it to the dataset.
### Automatically from a tracing project[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#automatically-from-a-tracing-project "Direct link to Automatically from a tracing project")
You can use [run rules](https://docs.smith.langchain.com/observability/how_to_guides/rules) to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that are [tagged](https://docs.smith.langchain.com/observability/concepts#tags) with a specific use case or have a [low feedback score](https://docs.smith.langchain.com/observability/concepts#feedback).
### From examples in an Annotation Queue[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#from-examples-in-an-annotation-queue "Direct link to From examples in an Annotation Queue")
tip
If you rely on subject matter experts to build meaningful datasets, use [annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues) to provide a streamlined view for reviewiers. Human reviewwers can optionally modify the inputs/outputs/reference outputs from a trace before it is added to the dataset.
Annotation queues are can be optionally configured with a default dataset, though you can add runs to any dataset by using the dataset switcher on the bottom of the screen. Once you select the right dataset, click **Add to Dataset** or hit the hot key `D` to add the run to it.
Any modifications you make to the run in your annotation queue will carry over to the dataset, and all metadata associated with the run will also be copied.
![](https://docs.smith.langchain.com/assets/images/add_to_dataset_from_aq-408cc394da41a056a445db618006da4b.png)
Note you can also set up rules to add runs that meet specific criteria to an annotation queue using [automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules).
### From the Prompt Playground[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#from-the-prompt-playground "Direct link to From the Prompt Playground")
On the [**Prompt Playground**](https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-playground) page, select **Set up Evaluation** , click **+New** if you're starting a new dataset or select from an existing dataset.
note
Creating datasets inline in the playground is not supported for datasets that have nested keys. In order to add/edit examples with nested keys, you must edit [from the datasets page](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#from-the-datasets-page).
To edit the examples:
  * Use **+Row** to add a new example to the dataset
  * Delete an example using the **‚ãÆ** dropdown on the right hand side of the table
  * If you're creating a reference-free dataset remove the "Reference Output" column using the **x** button in the column. Note: this action is not reversable.


![Create a dataset in the playground](https://docs.smith.langchain.com/assets/images/playground_dataset-16e4c1414dc5c0f4ef49aad407ea61bd.png)
### Import a dataset from a CSV or JSONL file[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#import-a-dataset-from-a-csv-or-jsonl-file "Direct link to Import a dataset from a CSV or JSONL file")
On the **Datasets & Experiments** page, click **+New Dataset** , then **Import** an existing dataset from CSV or JSONL file.
### Create a new dataset from the dataset page[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-a-new-dataset-from-the-dataset-page "Direct link to Create a new dataset from the dataset page")
On the **Datasets & Experiments** page, click **+New Dataset** , then **Create an empty dataset**. You can optionally create a [dataset schema](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-a-dataset-schema) to validate your dataset.
Then to add examples inline, go to the **Examples** tab, and click **+ Example`**. This will let you define examples in JSON inline.
#### Add synthetic examples created by an LLM via the Datasets UI[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#add-synthetic-examples-created-by-an-llm-via-the-datasets-ui "Direct link to Add synthetic examples created by an LLM via the Datasets UI")
If you have a schema defined on your dataset, when you click `+ Example` you'll see an option to `Generate examples`. This will use an LLM to create synthetic examples.
You have to do the following:
  1. **Select few-shot examples** : Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.
  2. **Specify the number of examples** : Enter the number of synthetic examples you want to generate.
  3. **Configure API Key** : Ensure your OpenAI API key is entered at the "API Key" link. ![Generate Synthetic Examples](https://docs.smith.langchain.com/assets/images/generate_synthetic_examples_create-56bd15927775ccf275ad925b18d3e6c8.png)


After clicking "Generate," the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing. Each example will be validated against your specified dataset schema and tagged as "synthetic" in the source metadata. ![Generate Synthetic Examples](https://docs.smith.langchain.com/assets/images/generate_synthetic_examples_pane-81203111cd8bbd1477dd85c8de6864cd.png)
## Manage a Dataset[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#manage-a-dataset "Direct link to Manage a Dataset")
### Create a dataset schema[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-a-dataset-schema "Direct link to Create a dataset schema")
LangSmith datasets store arbitrary JSON objects. We recommend (but do not require) that you define a schema for your dataset to ensure that they confirm to a specific JSON schema. Dataset schemas are defined with standard [JSON schema](https://json-schema.org/), with the addition of a few [prebuilt types](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types) that make it easier to type common primitives like messages and tools.
Certain fields in your schema have a `+ Transformations` option. Transformations are preprocessing steps that, if enabled, update your examples when you add them to the dataset. For example the `convert to OpenAI messages` transformation will convert message-like objects, like LangChain messages, to OpenAI message format.
For the full list of available transformations, see [our reference](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations).
note
If you plan to collect production traces in your dataset from LangChain [ChatModels](https://python.langchain.com/docs/concepts/chat_models/) or from OpenAI calls using the [LangSmith OpenAI wrapper](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#wrap-the-openai-client), we offer a prebuilt Chat Model schema that converts messages and tools into industry standard openai formats that can be used downstream with any model for testing. You can also customize the template settings to match your use case.
Please see the [dataset transformations reference](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations) for more information.
### Create and manage dataset splits[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-and-manage-dataset-splits "Direct link to Create and manage dataset splits")
Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common in machine learning workflows to split datasets into training, validation, and test sets. This can be useful to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas metadata would be used more for storing information on your examples like tags and information about its origin.
In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split). However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for some evaluation workflows - for example, if an example falls into multiple categories on which you may want to evaluate your application.
In order to create and manage splits in the app, you can select some examples in your dataset and click "Add to Split". From the resulting popup menu, you can select and unselect splits for the selected examples, or create a new split.
![Add to Split](https://docs.smith.langchain.com/assets/images/add_to_split2-e152f3821c65a1d7dff5e68e8f63aba3.png)
### Edit example metadata[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#edit-example-metadata "Direct link to Edit example metadata")
You can add metadata to your examples by clicking on an example and then clicking on the "Metadata" tab in the side pane. From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about your examples, such as tags or version info, which you can [then filter by when you call `list_examples` in the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-examples-by-metadata).
![Add Metadata](https://docs.smith.langchain.com/assets/images/add_metadata-45aee98f26b5c308c6e3cf971ea04df9.png)
### Filter examples[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#filter-examples "Direct link to Filter examples")
You can filter examples by metadata key/value or full-text search. To filter examples, click "Filter" in the top left of the table:
![Filter Examples](https://docs.smith.langchain.com/assets/images/filter_examples-6e09ca82279eaf49565abd91ab20bbad.png)
Next, click "Add filter" and select "Full Text" or "Metadata" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the filters will be displayed in the table.
![Filters Applied to Examples](https://docs.smith.langchain.com/assets/images/filters_applied-bc344988008ad094eaa91c54ca9d3de6.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/manage_datasets_in_application%3E).
[PreviousHow to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)[NextRenaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
  * [Create a dataset and add examples](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-a-dataset-and-add-examples)
    * [Manually from a tracing project](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#manually-from-a-tracing-project)
    * [Automatically from a tracing project](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#automatically-from-a-tracing-project)
    * [From examples in an Annotation Queue](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#from-examples-in-an-annotation-queue)
    * [From the Prompt Playground](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#from-the-prompt-playground)
    * [Import a dataset from a CSV or JSONL file](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#import-a-dataset-from-a-csv-or-jsonl-file)
    * [Create a new dataset from the dataset page](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-a-new-dataset-from-the-dataset-page)
  * [Manage a Dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#manage-a-dataset)
    * [Create a dataset schema](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-a-dataset-schema)
    * [Create and manage dataset splits](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-and-manage-dataset-splits)
    * [Edit example metadata](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#edit-example-metadata)
    * [Filter examples](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#filter-examples)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to manage datasets programmatically


On this page
# How to manage datasets programmatically
You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.
## Create a dataset[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset "Direct link to Create a dataset")
### Create a dataset from list of values[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset-from-list-of-values "Direct link to Create a dataset from list of values")
The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example.
Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary.
Bulk example creation
If you have many examples to create, consider using the `create_examples`/`createExamples` method to create multiple examples in a single request. If creating a single example, you can use the `create_example`/`createExample` method.
  * Python
  * TypeScript


```
from langsmith import Clientexamples =[{"inputs":{"question":"What is the largest mammal?"},"outputs":{"answer":"The blue whale"},"metadata":{"source":"Wikipedia"},},{"inputs":{"question":"What do mammals and birds have in common?"},"outputs":{"answer":"They are both warm-blooded"},"metadata":{"source":"Wikipedia"},},{"inputs":{"question":"What are reptiles known for?"},"outputs":{"answer":"Having scales"},"metadata":{"source":"Wikipedia"},},{"inputs":{"question":"What's the main characteristic of amphibians?"},"outputs":{"answer":"They live both in water and on land"},"metadata":{"source":"Wikipedia"},},]client = Client()dataset_name ="Elementary Animal Questions"# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset( dataset_name=dataset_name, description="Questions and answers about animal phylogenetics.",)# Prepare inputs, outputs, and metadata for bulk creationclient.create_examples( dataset_id=dataset.id, examples=examples)
```

```
import{ Client }from"langsmith";const client =newClient();const exampleInputs:[string,string][]=[["What is the largest mammal?","The blue whale"],["What do mammals and birds have in common?","They are both warm-blooded"],["What are reptiles known for?","Having scales"],["What's the main characteristic of amphibians?","They live both in water and on land",],];const datasetName ="Elementary Animal Questions";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset =await client.createDataset(datasetName,{description:"Questions and answers about animal phylogenetics",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt])=>({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer])=>({ answer: outputAnswer }));const metadata = exampleInputs.map(()=>({ source:"Wikipedia"}));// Use the bulk createExamples methodawait client.createExamples({inputs,outputs,metadata,datasetId: dataset.id,});
```

### Create a dataset from traces[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset-from-traces "Direct link to Create a dataset from traces")
To create datasets from the runs (spans) of your traces, you can use the same approach. For **many** more examples of how to fetch and filter runs, see the [export traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces) guide. Below is an example:
  * Python
  * TypeScript


```
from langsmith import Clientclient = Client()dataset_name ="Example Dataset"# Filter runs to add to the datasetruns = client.list_runs( project_name="my_project", is_root=True, error=False,)dataset = client.create_dataset(dataset_name, description="An example dataset")# Prepare inputs and outputs for bulk creationexamples =[{"inputs": run.inputs,"outputs": run.outputs}for run in runs]# Use the bulk create_examples methodclient.create_examples( dataset_id=dataset.id, examples=examples)
```

```
import{ Client, Run }from"langsmith";const client =newClient();const datasetName ="Example Dataset";// Filter runs to add to the datasetconst runs: Run[]=[];forawait(const run of client.listRuns({projectName:"my_project",isRoot:1,error:false,})){runs.push(run);}const dataset =await client.createDataset(datasetName,{description:"An example dataset",dataType:"kv",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ??{});// Use the bulk createExamples methodawait client.createExamples({inputs,outputs,datasetId: dataset.id,});
```

### Create a dataset from a CSV file[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset-from-a-csv-file "Direct link to Create a dataset from a CSV file")
In this section, we will demonstrate how you can create a dataset by uploading a CSV file.
First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided.
  * Python
  * TypeScript


```
from langsmith import Clientimport osclient = Client()csv_file ='path/to/your/csvfile.csv'input_keys =['column1','column2']# replace with your input column namesoutput_keys =['output1','output2']# replace with your output column namesdataset = client.upload_csv( csv_file=csv_file, input_keys=input_keys, output_keys=output_keys, name="My CSV Dataset", description="Dataset created from a CSV file" data_type="kv")
```

```
import{ Client }from"langsmith";const client =newClient();const csvFile ='path/to/your/csvfile.csv';const inputKeys =['column1','column2'];// replace with your input column namesconst outputKeys =['output1','output2'];// replace with your output column namesconst dataset =await client.uploadCsv({ csvFile: csvFile, fileName:"My CSV Dataset", inputKeys: inputKeys, outputKeys: outputKeys, description:"Dataset created from a CSV file", dataType:"kv"});
```

### Create a dataset from pandas DataFrame (Python only)[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset-from-pandas-dataframe-python-only "Direct link to Create a dataset from pandas DataFrame \(Python only\)")
The python client offers an additional convenience method to upload a dataset from a pandas dataframe.
```
from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys =['column1','column2']# replace with your input column namesoutput_keys =['output1','output2']# replace with your output column namesdataset = client.upload_dataframe(  df=df,  input_keys=input_keys,  output_keys=output_keys,  name="My Parquet Dataset",  description="Dataset created from a parquet file",  data_type="kv"# The default)
```

## Fetch datasets[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#fetch-datasets "Direct link to Fetch datasets")
You can programmatically fetch datasets from LangSmith using the `list_datasets`/`listDatasets` method in the Python and TypeScript SDKs. Below are some common calls.
Prerequisites
Initialize the client before running the below code snippets.
  * Python
  * TypeScript


```
from langsmith import Clientclient = Client()
```

```
import{ Client }from"langsmith";const client =newClient();
```

### Query all datasets[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#query-all-datasets "Direct link to Query all datasets")
  * Python
  * TypeScript


```
datasets = client.list_datasets()
```

```
const datasets =await client.listDatasets();
```

### List datasets by name[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-datasets-by-name "Direct link to List datasets by name")
If you want to search by the exact name, you can do the following:
  * Python
  * TypeScript


```
datasets = client.list_datasets(dataset_name="My Test Dataset 1")
```

```
const datasets =await client.listDatasets({datasetName:"My Test Dataset 1"});
```

If you want to do a case-invariant substring search, try the following:
  * Python
  * TypeScript


```
datasets = client.list_datasets(dataset_name_contains="some substring")
```

```
const datasets =await client.listDatasets({datasetNameContains:"some substring"});
```

### List datasets by type[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-datasets-by-type "Direct link to List datasets by type")
You can filter datasets by type. Below is an example querying for chat datasets.
  * Python
  * TypeScript


```
datasets = client.list_datasets(data_type="chat")
```

```
const datasets =await client.listDatasets({dataType:"chat"});
```

## Fetch examples[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#fetch-examples "Direct link to Fetch examples")
You can programmatically fetch examples from LangSmith using the `list_examples`/`listExamples` method in the Python and TypeScript SDKs. Below are some common calls.
Prerequisites
Initialize the client before running the below code snippets.
  * Python
  * TypeScript


```
from langsmith import Clientclient = Client()
```

```
import{ Client }from"langsmith";const client =newClient();
```

### List all examples for a dataset[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-all-examples-for-a-dataset "Direct link to List all examples for a dataset")
You can filter by dataset ID:
  * Python
  * TypeScript


```
examples = client.list_examples(dataset_id="c9ace0d8-a82c-4b6c-13d2-83401d68e9ab")
```

```
const examples =await client.listExamples({datasetId:"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab"});
```

Or you can filter by dataset name (this must exactly match the dataset name you want to query)
  * Python
  * TypeScript


```
examples = client.list_examples(dataset_name="My Test Dataset")
```

```
const examples =await client.listExamples({datasetName:"My test Dataset"});
```

### List examples by id[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-examples-by-id "Direct link to List examples by id")
You can also list multiple examples all by ID.
  * Python
  * TypeScript


```
example_ids =['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)
```

```
const exampleIds =["734fc6a0-c187-4266-9721-90b7a025751a","d6b4c1b9-6160-4d63-9b61-b034c585074f","4d31df4e-f9c3-4a6e-8b6c-65701c2fed13",];const examples =await client.listExamples({exampleIds: exampleIds});
```

### List examples by metadata[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-examples-by-metadata "Direct link to List examples by metadata")
You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair. Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.
For example, if you have an example with metadata `{"foo": "bar", "baz": "qux"}`, both `{foo: bar}` and `{baz: qux}` would match, as would `{foo: bar, baz: qux}`.
  * Python
  * TypeScript


```
examples = client.list_examples(dataset_name=dataset_name, metadata={"foo":"bar"})
```

```
const examples =await client.listExamples({datasetName: datasetName, metadata:{foo:"bar"}});
```

### List examples by structured filter[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-examples-by-structured-filter "Direct link to List examples by structured filter")
Similar to how you can use the structured filter query language to [fetch runs](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#use-filter-query-language), you can use it to fetch examples.
note
This is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.
Additionally, the structured filter query language is only supported for `metadata` fields.
You can use the `has` operator to fetch examples with metadata fields that contain specific key/value pairs and the `exists` operator to fetch examples with metadata fields that contain a specific key. Additionally, you can also chain multiple filters together using the `and` operator and negate a filter using the `not` operator.
  * Python
  * TypeScript


```
examples = client.list_examples( dataset_name=dataset_name,filter='and(not(has(metadata, \'{"foo": "bar"}\')), exists(metadata, "tenant_id"))')
```

```
const examples =await client.listExamples({datasetName: datasetName, filter:'and(not(has(metadata, \'{"foo": "bar"}\')), exists(metadata, "tenant_id"))'});
```

## Update examples[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#update-examples "Direct link to Update examples")
### Update single example[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#update-single-example "Direct link to Update single example")
You can programmatically update examples from LangSmith using the `update_example`/`updateExample` method in the Python and TypeScript SDKs. Below is an example.
  * Python
  * TypeScript


```
client.update_example( example_id=example.id, inputs={"input":"updated input"}, outputs={"output":"updated output"}, metadata={"foo":"bar"}, split="train")
```

```
await client.updateExample(example.id,{inputs:{ input:"updated input"},outputs:{ output:"updated output"},metadata:{"foo":"bar"},split:"train",})
```

### Bulk update examples[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#bulk-update-examples "Direct link to Bulk update examples")
You can also programmatically update multiple examples in a single request with the `update_examples`/`updateExamples` method in the Python and TypeScript SDKs. Below is an example.
  * Python
  * TypeScript


```
client.update_examples( example_ids=[example.id, example_2.id], inputs=[{"input":"updated input 1"},{"input":"updated input 2"}], outputs=[{"output":"updated output 1"},{"output":"updated output 2"},], metadata=[{"foo":"baz"},{"foo":"qux"}], splits=[["training","foo"],"training"]# Splits can be arrays or standalone strings)
```

```
await client.updateExamples([{ id: example.id, inputs:{ input:"updated input 1"}, outputs:{ output:"updated output 1"}, metadata:{ foo:"baz"}, split:["training","foo"]// Splits can be arrays or standalone strings},{ id: example2.id, inputs:{ input:"updated input 2"}, outputs:{ output:"updated output 2"}, metadata:{ foo:"qux"}, split:"training"},])
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)[NextRunning an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
  * [Create a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset)
    * [Create a dataset from list of values](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset-from-list-of-values)
    * [Create a dataset from traces](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset-from-traces)
    * [Create a dataset from a CSV file](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset-from-a-csv-file)
    * [Create a dataset from pandas DataFrame (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset-from-pandas-dataframe-python-only)
  * [Fetch datasets](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#fetch-datasets)
    * [Query all datasets](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#query-all-datasets)
    * [List datasets by name](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-datasets-by-name)
    * [List datasets by type](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-datasets-by-type)
  * [Fetch examples](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#fetch-examples)
    * [List all examples for a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-all-examples-for-a-dataset)
    * [List examples by id](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-examples-by-id)
    * [List examples by metadata](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-examples-by-metadata)
    * [List examples by structured filter](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#list-examples-by-structured-filter)
  * [Update examples](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#update-examples)
    * [Update single example](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#update-single-example)
    * [Bulk update examples](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#bulk-update-examples)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to return categorical vs numerical metrics


On this page
# How to return categorical vs numerical metrics
LangSmith supports both categorical and numerical metrics, and you can return either when writing a [custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator).
For an evaluator result to be logged as a numerical metric, it must returned as:
  * (Python only) an `int`, `float`, or `bool`
  * a dict of the form `{"key": "metric_name", "score": int | float | bool}`


For an evaluator result to be logged as a categorical metric, it must be returned as:
  * (Python only) a `str`
  * a dict of the form `{"key": "metric_name", "value": str | int | float | bool}`


Here are some examples:
  * Python
  * TypeScript


Requires `langsmith>=0.2.0`
```
defnumerical_metric(inputs:dict, outputs:dict, reference_outputs:dict)->float:# Evaluation logic...return0.8# Equivalently# return {"score": 0.8}# Or# return {"key": "numerical_metric", "score": 0.8}defcategorical_metric(inputs:dict, outputs:dict, reference_outputs:dict)->str:# Evaluation logic...return"english"# Equivalently# return {"key": "categorical_metric", "score": "english"}# Or# return {"score": "english"}
```

Support for multiple scores is available in `langsmith@0.1.32` and higher
```
importtype{ Run, Example }from"langsmith/schemas";functionnumericalMetric(run: Run, example: Example){// Your evaluation logic herereturn{ key:"numerical_metric", score:0.8};}functioncategoricalMetric(run: Run, example: Example){// Your evaluation logic herereturn{ key:"categorical_metric", value:"english"};}
```

## Related[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type#related "Direct link to Related")
  * [Return multiple metrics in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)[NextHow to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
  * [Related](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type#related)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to return multiple scores in one evaluator


On this page
# How to return multiple scores in one evaluator
Sometimes it is useful for a [custom evaluator function](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator) or [summary evaluator function](https://docs.smith.langchain.com/evaluation/how_to_guides/summary) to return multiple metrics. For example, if you have multiple metrics being generated by an LLM judge, you can save time and money by making a single LLM call that generates multiple metrics instead of making multiple LLM calls.
To return multiple scores using the Python SDK, simply return a list of dictionaries/objects of the following form:
```
[# 'key' is the metric name# 'score' is the value of a numerical metric{"key": string,"score": number},# 'value' is the value of a categorical metric{"key": string,"value": string},...# You may log as many as you wish]
```

To do so with the JS/TS SDK, return an object with a 'results' key and then a list of the above form
```
{results:[{key: string,score: number },...]};
```

Each of these dictionaries can contain any or all of the [feedback fields](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format); check out the linked document for more information.
Example:
  * Python
  * TypeScript


Requires `langsmith>=0.2.0`
```
defmultiple_scores(outputs:dict, reference_outputs:dict)->list[dict]:# Replace with real evaluation logic.  precision =0.8  recall =0.9  f1 =0.85return[{"key":"precision","score": precision},{"key":"recall","score": recall},{"key":"f1","score": f1},]
```

Support for multiple scores is available in `langsmith@0.1.32` and higher
```
importtype{ Run, Example }from"langsmith/schemas";functionmultipleScores(rootRun: Run, example: Example){// Your evaluation logic herereturn{   results:[{ key:"precision", score:0.8},{ key:"recall", score:0.9},{ key:"f1", score:0.85},],};}
```

Rows from the resulting experiment will display each of the scores.
![](https://docs.smith.langchain.com/assets/images/multiple_scores-134e2a03035ae95b128c49d0638ca0bc.png)
## Related[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores#related "Direct link to Related")
  * [Return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)[NextHow to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
  * [Related](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores#related)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to set up a multi-turn evaluation


On this page
# How to set up a multi-turn evaluation
LangSmith makes it easy to evaluate multi-turn conversations in the playground. This allows you to evaluate how changing your system prompt, the tools available to the model, or the output schema affects a conversation with multiple messages.
This how-to guide walks you through the various ways you can set up the playground for multi-turn evaluation, which will allow you to test different tool configurations and system prompts to see how they impact your system.
![](https://docs.smith.langchain.com/assets/images/multiturn_diagram-a1dbc6180bf819d625af42eece802b0b.png)
## From an existing run[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation#from-an-existing-run "Direct link to From an existing run")
First, ensure you have properly [traced](https://docs.smith.langchain.com/observability) a multi-turn conversation, and then navigate to your tracing project. Once you get to your tracing project simply open the run, select the LLM call, and open it in the playground as follows:
![](https://docs.smith.langchain.com/assets/images/multiturn_from_run-ac1a7c1e0f9e5cedf61ac6eaeb9ca540.gif)
You can then edit the system prompt, tweak the tools and/or output schema and observe how the output of the multi-turn conversation changes.
## From a dataset[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation#from-a-dataset "Direct link to From a dataset")
Before starting, make sure you have [set up your dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application). Since you want to evaluate multi-turn conversations, make sure there is a key in your inputs that contains a list of messages.
Once you have created your dataset, head to the playground and [load your dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#from-the-prompt-playground) to evaluate.
Then, add a messages list variable to your prompt, making sure to name it the same as the key in your inputs that contains the list of messages:
![](https://docs.smith.langchain.com/assets/images/multiturn_from_dataset-d1acbc18bd4b13f8b69889fc24383c1e.gif)
When you run your prompt, the messages from each example will be added as a list in place of the 'Messages List' variable.
## Manually[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation#manually "Direct link to Manually")
There are two ways to manually create multi-turn conversations. The first way is by simply appending messages to the prompt:
![](https://docs.smith.langchain.com/assets/images/multiturn_manual-9751aa036ce3bfdb663b24b5120a2091.gif)
This is helpful for quick iteration, but is rigid since the multi-turn conversation is hardcoded. Instead, if you want your prompt to work with any multi-turn conversation you can add a 'Messages List' variable and add your multi-turn conversation there:
![](https://docs.smith.langchain.com/assets/images/multiturn_manual_list-340c009a4029f2a007e6e868a2eb8526.gif)
This allows you to just tweak the system prompt or the tools, while allowing any multi-turn conversation to take the place of the `Messages List` variable, allowing you to reuse this prompt across various evaluations.
## Next Steps[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation#next-steps "Direct link to Next Steps")
Now that you know how to set up your multi-turn evaluation, you can either manually inspect and judge the outputs, or you can [add evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides#define-an-evaluator) to get repeatable, quantitative results.
You can also read [these how-to guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#playground) to learn more about how to use the playground to run evaluations.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)[NextHow to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
  * [From an existing run](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation#from-an-existing-run)
  * [From a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation#from-a-dataset)
  * [Manually](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation#manually)
  * [Next Steps](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation#next-steps)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to use prebuilt evaluators


On this page
# How to use prebuilt evaluators
LangSmith integrates with the open-source [`openevals`](https://github.com/langchain-ai/openevals) package to provide a suite of prebuilt, readymade evaluators that you can use right away as starting points for evaluation.
note
This how-to guide will demonstrate how to set up and run one type of evaluator (LLM-as-a-judge), but there are many others available. See the [openevals](https://github.com/langchain-ai/openevals) and [agentevals](https://github.com/langchain-ai/agentevals) repos for a complete list with usage examples.
## Setup[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators#setup "Direct link to Setup")
You'll need to install the `openevals` package to use the pre-built LLM-as-a-judge evaluator.
  * Python
  * TypeScript


```
pip install -U openevals
```

```
yarn add openevals @langchain/core
```

You'll also need to set your OpenAI API key as an environment variable, though you can choose different providers too:
```
export OPENAI_API_KEY="your_openai_api_key"
```

We'll also use LangSmith's [pytest](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest) integration for Python and [Vitest/Jest](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest) for TypeScript to run our evals. `openevals` also integrates seamlessly with the [`evaluate`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) method as well. See the [appropriate guides](https://docs.smith.langchain.com/evaluation/how_to_guides#testing-integrations) for setup instructions.
## Running an evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators#running-an-evaluator "Direct link to Running an evaluator")
The general flow is simple: import the evaluator or factory function from `openevals`, then run it within your test file with inputs, outputs, and reference outputs. LangSmith will automatically log the evaluator's results as feedback.
Note that not all evaluators will require each parameter (the exact match evaluator only requires outputs and reference outputs, for example). Additionally, if your LLM-as-a-judge prompt requires additional variables, passing them in as kwargs will format them into the prompt.
Set up your test file like this:
  * Python
  * TypeScript


```
import pytestfrom langsmith import testing as tfrom openevals.llm import create_llm_as_judgefrom openevals.prompts import CORRECTNESS_PROMPTcorrectness_evaluator = create_llm_as_judge(prompt=CORRECTNESS_PROMPT,feedback_key="correctness",model="openai:o3-mini",)# Mock standin for your applicationdefmy_llm_app(inputs:dict)->str:return"Doodads have increased in price by 10% in the past year."@pytest.mark.langsmithdeftest_correctness():inputs ="How much has the price of doodads changed in the past year?"reference_outputs ="The price of doodads has decreased by 50% in the past year."outputs = my_llm_app(inputs)t.log_inputs({"question": inputs})t.log_outputs({"answer": outputs})t.log_reference_outputs({"answer": reference_outputs})correctness_evaluator(  inputs=inputs,  outputs=outputs,  reference_outputs=reference_outputs)
```

```
import*as ls from"langsmith/vitest";// import * as ls from "langsmith/jest";import{ createLLMAsJudge,CORRECTNESS_PROMPT}from"openevals";const correctnessEvaluator =createLLMAsJudge({prompt:CORRECTNESS_PROMPT,feedbackKey:"correctness",model:"openai:o3-mini",});// Mock standin for your applicationconstmyLLMApp=async(_inputs: Record<string,unknown>)=>{return"Doodads have increased in price by 10% in the past year.";}ls.describe("Correctness",()=>{ls.test("incorrect answer",{inputs:{ question:"How much has the price of doodads changed in the past year?"},referenceOutputs:{ answer:"The price of doodads has decreased by 50% in the past year."}},async({ inputs, referenceOutputs })=>{const outputs =awaitmyLLMApp(inputs);ls.logOutputs({ answer: outputs });awaitcorrectnessEvaluator({ inputs, outputs, referenceOutputs,});});});
```

The `feedback_key`/`feedbackKey` parameter will be used as the name of the feedback in your experiment.
Running the eval in your terminal will result in something like the following:
![Prebuilt evaluator terminal result](https://docs.smith.langchain.com/assets/images/prebuilt_eval_result-5cc210e5d106c8f9919c21c667516437.png)
You can also pass prebuilt evaluators directly into the `evaluate` method if you have already created a dataset in LangSmith. If using Python, this requires `langsmith>=0.3.11`:
  * Python
  * TypeScript


```
from langsmith import Clientfrom openevals.llm import create_llm_as_judgefrom openevals.prompts import CONCISENESS_PROMPTclient = Client()conciseness_evaluator = create_llm_as_judge(prompt=CONCISENESS_PROMPT,feedback_key="conciseness",model="openai:o3-mini",)experiment_results = client.evaluate(# This is a dummy target function, replace with your actual LLM-based systemlambda inputs:"What color is the sky?",data="Sample dataset",evaluators=[  conciseness_evaluator])
```

```
import{ evaluate }from"langsmith/evaluation";import{ createLLMAsJudge,CONCISENESS_PROMPT}from"openevals";const concisenessEvaluator =createLLMAsJudge({prompt:CONCISENESS_PROMPT,feedbackKey:"conciseness",model:"openai:o3-mini",});awaitevaluate((inputs)=>"What color is the sky?",{data: datasetName,evaluators:[concisenessEvaluator],});
```

For a complete list of available evaluators, see the [openevals](https://github.com/langchain-ai/openevals) and [agentevals](https://github.com/langchain-ai/agentevals) repos.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)[NextHow to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
  * [Setup](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators#setup)
  * [Running an evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators#running-an-evaluator)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/pytest

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to run evals with pytest (beta)


On this page
# How to run evals with pytest (beta)
The LangSmith pytest plugin lets Python developers define their datasets and evaluations as pytest test cases. Compared to the `evaluate()` evaluation flow, this is useful when:
  * Each example requires different evaluation logic
  * You want to assert binary expectations, and both track these assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines)
  * You want pytest-like terminal outputs
  * You already use pytest to test your app and want to add LangSmith tracking


Beta
The pytest integration is in beta and is subject to change in upcoming releases.
For JS/TS
The JS/TS SDK has an analogous [Vitest/Jest integration](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest).
## Installation[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#installation "Direct link to Installation")
This functionality requires Python SDK version `langsmith>=0.3.4`.
For extra features like [rich terminal outputs](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#rich-outputs) and [test caching](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#caching) install:
```
pip install -U "langsmith[pytest]"
```

## Define and run tests[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#define-and-run-tests "Direct link to Define and run tests")
The pytest integration lets you define datasets and evaluators as test cases.
To track a test in LangSmith add the `@pytest.mark.langsmith` decorator. Every decorated test case will be synced to a dataset example. When you run the test suite, the dataset will be updated and a new experiment will be created with one result for each test case.
‚åµ
```
###################### my_app/main.py ######################import openaifrom langsmith import traceable, wrappersoai_client = wrappers.wrap_openai(openai.OpenAI())@traceabledefgenerate_sql(user_query:str)->str:  result = oai_client.chat.completions.create(    model="gpt-4o-mini",    messages=[{"role":"system","content":"Convert the user query to a SQL query."},{"role":"user","content": user_query},],)return result.choices[0].message.content###################### tests/test_my_app.py ######################import pytestfrom langsmith import testing as tdefis_valid_sql(query:str)->bool:"""Return True if the query is valid SQL."""returnTrue# Dummy implementation@pytest.mark.langsmith# <-- Mark as a LangSmith test casedeftest_sql_generation_select_all()->None:  user_query ="Get all users from the customers table"  t.log_inputs({"user_query": user_query})# <-- Log example inputs, optional  expected ="SELECT * FROM customers;"  t.log_reference_outputs({"sql": expected})# <-- Log example reference outputs, optional  sql = generate_sql(user_query)  t.log_outputs({"sql": sql})# <-- Log run outputs, optional  t.log_feedback(key="valid_sql", score=is_valid_sql(sql))# <-- Log feedback, optionalassert sql == expected # <-- Test pass/fail status automatically logged to LangSmith under 'pass' feedback key
```

When you run this test it will have a default `pass` boolean feedback key based on the test case passing / failing. It will also track any inputs, outputs, and reference (expected) outputs that you log.
Use `pytest` as you normally would to run the tests:
```
pytest tests/
```

In most cases we recommend setting a test suite name:
```
LANGSMITH_TEST_SUITE='SQL app tests' pytest tests/
```

Each time you run this test suite, LangSmith:
  * creates a [dataset](https://docs.smith.langchain.com/evaluation/concepts#datasets) for each test file. If a dataset for this test file already exists it will be updated
  * creates an [experiment](https://docs.smith.langchain.com/evaluation/concepts#experiment) in each created/updated dataset
  * creates an experiment row for each test case, with the inputs, outputs, reference outputs and feedback you've logged
  * collects the pass/fail rate under the `pass` feedback key for each test case


Here's what a test suite dataset looks like:
![Dataset](https://docs.smith.langchain.com/assets/images/simple-pytest-dataset-1b824862e755a8e6e7335dbecca2cd29.png)
And what an experiment against that test suite looks like:
![Experiment](https://docs.smith.langchain.com/assets/images/simple-pytest-c3b57795c432c644950da7fe90480ffc.png)
## Log inputs, outputs, and reference outputs[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#log-inputs-outputs-and-reference-outputs "Direct link to Log inputs, outputs, and reference outputs")
Every time we run a test we're syncing it to a dataset example and tracing it as a run. There's a few different ways that we can trace the example inputs and reference outputs and the run outputs. The simplest is to use the `log_inputs`, `log_outputs`, and `log_reference_outputs` methods. You can run these any time in a test to update the example and run for that test:
```
import pytestfrom langsmith import testing as t@pytest.mark.langsmithdeftest_foo()->None:  t.log_inputs({"a":1,"b":2})  t.log_reference_outputs({"foo":"bar"})  t.log_outputs({"foo":"baz"})assertTrue
```

Running this test will create/update an example with name "test_foo", inputs `{"a": 1, "b": 2}`, reference outputs `{"foo": "bar"}` and trace a run with outputs `{"foo": "baz"}`.
**NOTE** : If you run `log_inputs`, `log_outputs`, or `log_reference_outputs` twice, the previous values will be overwritten.
Another way to define example inputs and reference outputs is via pytest fixtures/parametrizations. By default any arguments to your test function will be logged as inputs on the corresponding example. If certain arguments are meant to represet reference outputs, you can specify that they should be logged as such using `@pytest.mark.langsmith(output_keys=["name_of_ref_output_arg"])`:
```
import pytest@pytest.fixturedefc()->int:return5@pytest.fixturedefd()->int:return6@pytest.mark.langsmith(output_keys=["d"])deftest_cd(c:int, d:int)->None:  result =2* c  t.log_outputs({"d": result})# Log run outputsassert result == d
```

This will create/sync an example with name "test_cd", inputs `{"c": 5}` and reference outputs `{"d": 6}`, and run output `{"d": 10}`.
## Log feedback[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#log-feedback "Direct link to Log feedback")
By default LangSmith collects the pass/fail rate under the `pass` feedback key for each test case. You can add additional feedback with `log_feedback`.
```
import openaiimport pytestfrom langsmith import wrappersfrom langsmith import testing as toai_client = wrappers.wrap_openai(openai.OpenAI())@pytest.mark.langsmithdeftest_offtopic_input()->None:  user_query ="whats up"  t.log_inputs({"user_query": user_query})  sql = generate_sql(user_query)  t.log_outputs({"sql": sql})  expected ="Sorry that is not a valid query."  t.log_reference_outputs({"sql": expected})# Use this context manager to trace any steps used for generating evaluation # feedback separately from the main application logicwith t.trace_feedback():    instructions =("Return 1 if the ACTUAL and EXPECTED answers are semantically equivalent, ""otherwise return 0. Return only 0 or 1 and nothing else.")    grade = oai_client.chat.completions.create(      model="gpt-4o-mini",      messages=[{"role":"system","content": instructions},{"role":"user","content":f"ACTUAL: {sql}\nEXPECTED: {expected}"},],)    score =float(grade.choices[0].message.content)    t.log_feedback(key="correct", score=score)assert score
```

Note the use of the `trace_feedback()` context manager. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case. Instead of showing up in the main test case run it will instead show up in the trace for the `correct` feedback key.
**NOTE** : Make sure that the `log_feedback` call associated with the feedback trace occurs inside the `trace_feedback` context. This way we'll be able to associate the feedback with the trace, and when seeing the feedback in the UI you'll be able to click on it to see the trace that generated it.
## Trace intermediate calls[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#trace-intermediate-calls "Direct link to Trace intermediate calls")
LangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.
## Grouping tests into a test suite[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#grouping-tests-into-a-test-suite "Direct link to Grouping tests into a test suite")
By default, all tests within a given file will be grouped as a single "test suite" with a corresponding dataset. You can configure which test suite a test belongs to by passing the `test_suite_name` parameter to `@pytest.mark.langsmith` for case-by-case grouping, or you can set the `LANGSMITH_TEST_SUITE` env var to group all tests from an execution into a single test suite:
```
LANGSMITH_TEST_SUITE="SQL app tests" pytest tests/
```

We generally recommend setting `LANGSMITH_TEST_SUITE` to get a consolidated view of all of your results.
## Naming experiments[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#naming-experiments "Direct link to Naming experiments")
You can name an experiment using the `LANGSMITH_EXPERIMENT` env var:
```
LANGSMITH_TEST_SUITE="SQL app tests" LANGSMITH_EXPERIMENT="baseline" pytest tests/
```

## Caching[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#caching "Direct link to Caching")
LLMs on every commit in CI can get expensive. To save time and resources, LangSmith lets you cache HTTP requests to disk. To enable caching, install with `langsmith[pytest]` and set env var `LANGSMITH_TEST_CACHE=/my/cache/path`:
```
pip install -U "langsmith[pytest]"LANGSMITH_TEST_CACHE=tests/cassettes pytest tests/my_llm_tests
```

All requests will be cached to `tests/cassettes` and loaded from there on subsequent runs. If you check this in to your repository, your CI will be able to use the cache as well.
## pytest features[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#pytest-features "Direct link to pytest features")
`@pytest.mark.langsmith` is designed to stay out of your way and works well with familiar `pytest` features.
### Parametrize with `pytest.mark.parametrize`[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#parametrize-with-pytestmarkparametrize "Direct link to parametrize-with-pytestmarkparametrize")
You can use the `parametrize` decorator as before. This will create a new test case for each parametrized instance of the test.
```
@pytest.mark.langsmith(output_keys=["expected_sql"])@pytest.mark.parametrize("user_query, expected_sql",[("Get all users from the customers table","SELECT * FROM customers"),("Get all users from the orders table","SELECT * FROM orders"),],)deftest_sql_generation_parametrized(user_query, expected_sql):  sql = generate_sql(user_query)assert sql == expected_sql
```

**Note:** as the parametrized list grows, you may consider using `evaluate()` instead. This parallelizes the evaluation and makes it easier to control individual experiments and the corresponding dataset.
### Parallelize with `pytest-xdist`[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#parallelize-with-pytest-xdist "Direct link to parallelize-with-pytest-xdist")
You can use [pytest-xdist](https://pytest-xdist.readthedocs.io/en/stable/) as you normally would to parallelize test execution:
```
pip install -U pytest-xdistpytest -n auto tests
```

### Async tests with `pytest-asyncio`[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#async-tests-with-pytest-asyncio "Direct link to async-tests-with-pytest-asyncio")
`@pytest.mark.langsmith` works with sync or async tests, so you can run async tests exactly as before.
### Watch mode with `pytest-watch`[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#watch-mode-with-pytest-watch "Direct link to watch-mode-with-pytest-watch")
Use watch mode to quickly iterate on your tests. We _highly_ recommend ony using this with test caching (see below) enabled to avoid unnecessary LLM calls:
```
pip install pytest-watchLANGSMITH_TEST_CACHE=tests/cassettes ptw tests/my_llm_tests
```

## Rich outputs[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#rich-outputs "Direct link to Rich outputs")
If you'd like to see a rich display of the LangSmith results of your test run you can specify `--langsmith-output`:
```
pytest --langsmith-output tests
```

**Note:** This flag used to be `--output=langsmith` in `langsmith<=0.3.3` but was updated to avoid collisions with other pytest plugins.
You'll get a nice table per test suite that updates live as the results are uploaded to LangSmith:
![Rich pytest outputs](https://docs.smith.langchain.com/assets/images/rich-pytest-outputs-65936d2a0f337e0f9c7bcdb906d3cfc0.png)
Some important notes for using this feature:
  * Make sure you've installed `pip install -U "langsmith[pytest]"`
  * Rich outputs do not currently work with `pytest-xdist`


**NOTE** : The custom output removes all the standard pytest outputs. If you're trying to debug some unexpected behavior it's often better to show the regular pytest outputs so to get full error traces.
## Dry-run mode[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#dry-run-mode "Direct link to Dry-run mode")
If you want to run the tests without syncing the results to LangSmith, you can set `LANGSMITH_TEST_TRACKING=false` in your environment.
```
LANGSMITH_TEST_TRACKING=false pytest tests/
```

The tests will run as normal, but the experiment logs will not be sent to LangSmith.
## Expectations[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#expectations "Direct link to Expectations")
LangSmith provides an [expect](https://docs.smith.langchain.com/reference/python/_expect/langsmith._expect._Expect#langsmith._expect._Expect) utility to help define expectations about your LLM output. For example:
```
from langsmith import expect@pytest.mark.langsmithdeftest_sql_generation_select_all():  user_query ="Get all users from the customers table"  sql = generate_sql(user_query)  expect(sql).to_contain("customers")
```

This will log the binary "expectation" score to the experiment results, additionally `assert`ing that the expectation is met possibly triggering a test failure.
`expect` also provides "fuzzy match" methods. For example:
```
@pytest.mark.langsmith(output_keys=["expectation"])@pytest.mark.parametrize("query, expectation",[("what's the capital of France?","Paris"),],)deftest_embedding_similarity(query, expectation):  prediction = my_chatbot(query)  expect.embedding_distance(# This step logs the distance as feedback for this run    prediction=prediction, expectation=expectation# Adding a matcher (in this case, 'to_be_*"), logs 'expectation' feedback).to_be_less_than(0.5)# Optional predicate to assert against  expect.edit_distance(# This computes the normalized Damerau-Levenshtein distance between the two strings    prediction=prediction, expectation=expectation# If no predicate is provided below, 'assert' isn't called, but the score is still logged)
```

This test case will be assigned 4 scores:
  1. The `embedding_distance` between the prediction and the expectation
  2. The binary `expectation` score (1 if cosine distance is less than 0.5, 0 if not)
  3. The `edit_distance` between the prediction and the expectation
  4. The overall test pass/fail score (binary)


The `expect` utility is modeled off of [Jest](https://jestjs.io/docs/expect)'s expect API, with some off-the-shelf functionality to make it easier to grade your LLMs.
## Legacy[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#legacy "Direct link to Legacy")
#### `@test` / `@unit` decorator[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#test--unit-decorator "Direct link to test--unit-decorator")
The legacy method for marking test cases is using the `@test` or `@unit` decorators:
```
from langsmith import test@testdeftest_foo()->None:pass
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)[NextRun pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
  * [Installation](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#installation)
  * [Define and run tests](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#define-and-run-tests)
  * [Log inputs, outputs, and reference outputs](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#log-inputs-outputs-and-reference-outputs)
  * [Log feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#log-feedback)
  * [Trace intermediate calls](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#trace-intermediate-calls)
  * [Grouping tests into a test suite](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#grouping-tests-into-a-test-suite)
  * [Naming experiments](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#naming-experiments)
  * [Caching](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#caching)
  * [pytest features](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#pytest-features)
    * [Parametrize with `pytest.mark.parametrize`](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#parametrize-with-pytestmarkparametrize)
    * [Parallelize with `pytest-xdist`](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#parallelize-with-pytest-xdist)
    * [Async tests with `pytest-asyncio`](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#async-tests-with-pytest-asyncio)
    * [Watch mode with `pytest-watch`](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#watch-mode-with-pytest-watch)
  * [Rich outputs](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#rich-outputs)
  * [Dry-run mode](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#dry-run-mode)
  * [Expectations](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#expectations)
  * [Legacy](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#legacy)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to handle model rate limits


On this page
# How to handle model rate limits
A common issue when running large evaluation jobs is running into third-party API rate limits, usually from model providers. There are a few ways to deal with rate limits.
## Using `langchain` RateLimiters (Python only)[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting#using-langchain-ratelimiters-python-only "Direct link to using-langchain-ratelimiters-python-only")
If you're using `langchain` Python ChatModels in your application or evaluators, you can add rate limiters to your model(s) that will add client-side control of the frequency with which requests are sent to the model provider API to avoid rate limit errors.
  * Python


```
from langchain.chat_models import init_chat_modelfrom langchain_core.rate_limiters import InMemoryRateLimiterrate_limiter = InMemoryRateLimiter(  requests_per_second=0.1,# <-- Super slow! We can only make a request once every 10 seconds!!  check_every_n_seconds=0.1,# Wake up every 100 ms to check whether allowed to make a request,  max_bucket_size=10,# Controls the maximum burst size.)llm = init_chat_model("gpt-4o", rate_limiter=rate_limiter)defapp(inputs:dict)->dict:  response = llm.invoke(...)...defevaluator(inputs:dict, outputs:dict, reference_outputs:dict)->dict:  response = llm.invoke(...)...
```

See the [langchain](https://python.langchain.com/docs/how_to/chat_model_rate_limiting/) documentation for more on how to configure rate limiters.
## Retrying with exponential backoff[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting#retrying-with-exponential-backoff "Direct link to Retrying with exponential backoff")
A very common way to deal with rate limit errors is retrying with exponential backoff. Retrying with exponential backoff means repeatedly retrying failed requests with an (exponentially) increasing wait time between each retry. This continues until either the request succeeds or a maximum number of requests is made.
#### With `langchain`[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting#with-langchain "Direct link to with-langchain")
If you're using `langchain` components you can add retries to all model calls with the `.with_retry(...)` / `.withRetry()` method:
  * Python
  * TypeScript


```
from langchain import init_chat_modelllm_with_retry = init_chat_model("gpt-4o-mini").with_retry(stop_after_attempt=6)
```

```
import{ initChatModel }from"langchain/chat_models/universal";const llm =awaitinitChatModel("gpt-4o",{  modelProvider:"openai",});const llmWithRetry = llm.withRetry({ stopAfterAttept:2});
```

See the `langchain` [Python](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel.with_retry) and [JS](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#withRetry) API references for more.
#### Without `langchain`[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting#without-langchain "Direct link to without-langchain")
If you're not using `langchain` you can use other libraries like `tenacity` (Python) or `backoff` (Python) to implement retries with exponential backoff, or you can implement it from scratch. See some examples of how to do this in the [OpenAI docs](https://platform.openai.com/docs/guides/rate-limits#retrying-with-exponential-backoff).
## Limiting max_concurrency[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting#limiting-max_concurrency "Direct link to Limiting max_concurrency")
Limiting the number of concurrent calls you're making to your application and evaluators is another way to decrease the frequency of model calls you're making, and in that way avoid rate limit errors. `max_concurrency` can be set directly on the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) functions. This parallelizes evaluation by effectively splitting the dataset across threads.
  * Python
  * TypeScript


```
from langsmith import aevaluateresults =await aevaluate(...  max_concurrency=4,)
```

```
import{ evaluate }from"langsmith/evaluation";awaitevaluate(...,{..., maxConcurrency:4,});
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)[NextHow to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
  * [Using `langchain` RateLimiters (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting#using-langchain-ratelimiters-python-only)
  * [Retrying with exponential backoff](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting#retrying-with-exponential-backoff)
  * [Limiting max_concurrency](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting#limiting-max_concurrency)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * Renaming an experiment


On this page
# Renaming an experiment
This guide outlines the available methods to rename an experiment in the LangSmith UI. There are two ways to rename an experiment:
  1. **Within the Playground**
  2. **Within the Experiments View**


Note that experiment names must be unique per workspace.
## 1. Renaming an experiment in the Playground[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment#1-renaming-an-experiment-in-the-playground "Direct link to 1. Renaming an experiment in the Playground")
When running experiments in the Playground, a default name with the format `pg::prompt-name::model::uuid` (eg. `pg::gpt-4o-mini::897ee630`) is automatically assigned.
You can rename an experiment immediately after running it by editing its name in the Playground table header. ![Edit name in playground](https://docs.smith.langchain.com/assets/images/rename_in_playground-003da5d674c5599a81fb2428030d1352.png)
## 2. Renaming an experiment in the experiments view[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment#2-renaming-an-experiment-in-the-experiments-view "Direct link to 2. Renaming an experiment in the experiments view")
When viewing results in the experiments view, you can rename an experiment by using the pencil icon beside the experiment name.
![Edit name in playground](https://docs.smith.langchain.com/assets/images/rename_in_experiments_view-8416c5daee2df9cb4e486eb0a389c6ea.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/renaming_experiment%3E).
[PreviousCreating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)[NextHow to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
  * [1. Renaming an experiment in the Playground](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment#1-renaming-an-experiment-in-the-playground)
  * [2. Renaming an experiment in the experiments view](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment#2-renaming-an-experiment-in-the-experiments-view)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/repetition

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to evaluate with repetitions


On this page
# How to evaluate with repetitions
Running multiple [repetitions](https://docs.smith.langchain.com/evaluation/concepts#repetitions) can give a more accurate estimate of the performance of your system since LLM outputs are not deterministic. Outputs can differ from one repetition to the next. Repetitions are a way to reduce noise in systems prone to high variability, such as agents.
## Configuring repetitions on an experiment[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition#configuring-repetitions-on-an-experiment "Direct link to Configuring repetitions on an experiment")
Add the optional `num_repetitions` param to the `evaluate` / `aevaluate` function ([Python](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate), [TypeScript](https://docs.smith.langchain.com/reference/js/interfaces/evaluation.EvaluateOptions#numrepetitions)) to specify how many times to evaluate over each example in your dataset. For instance, if you have 5 examples in the dataset and set `num_repetitions=5`, each example will be run 5 times, for a total of 25 runs.
  * Python
  * TypeScript


```
from langsmith import evaluateresults = evaluate(lambda inputs: label_text(inputs["text"]),  data=dataset_name,  evaluators=[correct_label],  experiment_prefix="Toxic Queries",  num_repetitions=3,)
```

```
import{ evaluate }from"langsmith/evaluation";awaitevaluate((inputs)=>labelText(inputs["input"]),{ data: datasetName, evaluators:[correctLabel], experimentPrefix:"Toxic Queries", numRepetitions=3,});
```

## Viewing results of experiments run with repeitions[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition#viewing-results-of-experiments-run-with-repeitions "Direct link to Viewing results of experiments run with repeitions")
If you've run your experiment with [repetitions](https://docs.smith.langchain.com/evaluation/concepts#repetitions), there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view. When you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.
![Repetitions](https://docs.smith.langchain.com/assets/images/repetitions-5ed8e2031f73454e46c64ecce1ecb166.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)[NextHow to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
  * [Configuring repetitions on an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition#configuring-repetitions-on-an-experiment)
  * [Viewing results of experiments run with repeitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition#viewing-results-of-experiments-run-with-repeitions)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to use the REST API


On this page
# How to use the REST API
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Evaluate LLM applications](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
  * [LangSmith API Reference](https://api.smith.langchain.com/redoc)


It is highly recommended to run evals with either the Python or TypeScript SDKs. The SDKs have many optimizations and features that enhance the performance and reliability of your evals. However, if you are unable to use the SDKs, either because you are using a different language or because you are running in a restricted environment, you can use the REST API directly.
This guide will show you how to run evals using the REST API, using the `requests` library in Python as an example. However, the same principles apply to any language.
## Create a dataset[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only#create-a-dataset "Direct link to Create a dataset")
Here, we are using the python SDK for convenience. You can also use the API directly use the UI, see [this guide](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application) for more information.
```
import openaiimport osimport requestsfrom datetime import datetimefrom langsmith import Clientfrom uuid import uuid4client = Client()# Create a datasetexamples =[{"inputs":{"text":"Shut up, idiot"},"outputs":{"label":"Toxic"},},{"inputs":{"text":"You're a wonderful person"},"outputs":{"label":"Not toxic"},},{"inputs":{"text":"This is the worst thing ever"},"outputs":{"label":"Toxic"},},{"inputs":{"text":"I had a great day today"},"outputs":{"label":"Not toxic"},},{"inputs":{"text":"Nobody likes you"},"outputs":{"label":"Toxic"},},{"inputs":{"text":"This is unacceptable. I want to speak to the manager."},"outputs":{"label":"Not toxic"},},]dataset_name ="Toxic Queries - API Example"dataset = client.create_dataset(dataset_name=dataset_name)client.create_examples(dataset_id=dataset.id, examples=examples)
```

## Run a single experiment[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only#run-a-single-experiment "Direct link to Run a single experiment")
First, pull all of the examples you'd want to use in your experiment.
```
# Pick a dataset id. In this case, we are using the dataset we created above.# Spec: https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__deletedataset_id = dataset.idparams ={"dataset": dataset_id }resp = requests.get("https://api.smith.langchain.com/api/v1/examples",  params=params,  headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]})examples = resp.json()
```

Next, we'll define a method that will create a run for a single example.
```
os.environ["OPENAI_API_KEY"]="sk-..."defrun_completion_on_example(example, model_name, experiment_id):"""Run completions on a list of examples."""# We are using the OpenAI API here, but you can use any model you likedef_post_run(run_id, name, run_type, inputs, parent_id=None):"""Function to post a new run to the API."""    data ={"id": run_id.hex,"name": name,"run_type": run_type,"inputs": inputs,"start_time": datetime.utcnow().isoformat(),"reference_example_id": example["id"],"session_id": experiment_id,}if parent_id:      data["parent_run_id"]= parent_id.hex    resp = requests.post("https://api.smith.langchain.com/api/v1/runs",# Update appropriately for self-hosted installations or the EU region      json=data,      headers=headers)    resp.raise_for_status()def_patch_run(run_id, outputs):"""Function to patch a run with outputs."""    resp = requests.patch(f"https://api.smith.langchain.com/api/v1/runs/{run_id}",      json={"outputs": outputs,"end_time": datetime.utcnow().isoformat(),},      headers=headers,)    resp.raise_for_status()# Send your API Key in the request headers  headers ={"x-api-key": os.environ["LANGSMITH_API_KEY"]}  text = example["inputs"]["text"]  messages =[{"role":"system","content":"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.",},{"role":"user","content": text},]# Create parent run  parent_run_id = uuid4()  _post_run(parent_run_id,"LLM Pipeline","chain",{"text": text})# Create child run  child_run_id = uuid4()  _post_run(child_run_id,"OpenAI Call","llm",{"messages": messages}, parent_run_id)# Generate a completion  client = openai.Client()  chat_completion = client.chat.completions.create(model=model_name, messages=messages)# End runs  _patch_run(child_run_id, chat_completion.dict())  _patch_run(parent_run_id,{"label": chat_completion.choices[0].message.content})
```

We are going to run completions on all examples using two models: gpt-3.5-turbo and gpt-4o-mini.
```
# Create a new experiment using the /sessions endpoint# An experiment is a collection of runs with a reference to the dataset used# Spec: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_postmodel_names =("gpt-3.5-turbo","gpt-4o-mini")experiment_ids =[]for model_name in model_names:  resp = requests.post("https://api.smith.langchain.com/api/v1/sessions",    json={"start_time": datetime.utcnow().isoformat(),"reference_dataset_id":str(dataset_id),"description":"An optional description for the experiment","name":f"Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}",# A name for the experiment"extra":{"metadata":{"foo":"bar"},# Optional metadata},},    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]})  experiment = resp.json()  experiment_ids.append(experiment["id"])# Run completions on all examplesfor example in examples:    run_completion_on_example(example, model_name, experiment["id"])# Issue a patch request to "end" the experiment by updating the end_time  requests.patch(f"https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}",    json={"end_time": datetime.utcnow().isoformat()},    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]})
```

## Run a pairwise experiment[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only#run-a-pairwise-experiment "Direct link to Run a pairwise experiment")
Next, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other. For more information, check out [this guide](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise).
```
# A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments# Spec: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_postresp = requests.post("https://api.smith.langchain.com/api/v1/datasets/comparative",  json={"experiment_ids": experiment_ids,"name":"Toxicity detection - API Example - Comparative - "+str(uuid4())[0:8],"description":"An optional description for the comparative experiment","extra":{"metadata":{"foo":"bar"},# Optional metadata},"reference_dataset_id":str(dataset_id),},  headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]})comparative_experiment = resp.json()comparative_experiment_id = comparative_experiment["id"]# You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs# Fetch the comparative experimentresp = requests.get(f"https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative",  params={"id": comparative_experiment_id},  headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]})comparative_experiment = resp.json()[0]experiment_ids =[info["id"]for info in comparative_experiment["experiments_info"]]from collections import defaultdictexample_id_to_runs_map = defaultdict(list)# Spec: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_postruns = requests.post(f"https://api.smith.langchain.com/api/v1/runs/query",  headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]},  json={"session": experiment_ids,"is_root":True,# Only fetch root runs (spans) which contain the end outputs"select":["id","reference_example_id","outputs"],}).json()runs = runs["runs"]for run in runs:  example_id = run["reference_example_id"]  example_id_to_runs_map[example_id].append(run)for example_id, runs in example_id_to_runs_map.items():print(f"Example ID: {example_id}")# Preferentially rank the outputs, in this case we will always prefer the first output# In reality, you can use an LLM to rank the outputs  feedback_group_id = uuid4()# Post a feedback score for each run, with the first run being the preferred one# Spec: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post# We'll use the feedback group ID to associate the feedback scores with the same groupfor i, run inenumerate(runs):print(f"Run ID: {run['id']}")    feedback ={"score":1if i ==0else0,"run_id":str(run["id"]),"key":"ranked_preference","feedback_group_id":str(feedback_group_id),"comparative_experiment_id": comparative_experiment_id,}    resp = requests.post("https://api.smith.langchain.com/api/v1/feedback",      json=feedback,      headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]})    resp.raise_for_status()
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)[NextHow to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
  * [Create a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only#create-a-dataset)
  * [Run a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only#run-a-single-experiment)
  * [Run a pairwise experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only#run-a-pairwise-experiment)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * Running an evaluation from the prompt playground


On this page
# Running an evaluation from the prompt playground
LangSmith allows you to run evaluations directly in the [prompt playground](https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-playground). The prompt playground allows you to test your prompt and/or model configuration over a series of inputs to see how well it scores across different contexts or scenarios, without having to write any code.
Before you run an evaluation, you need to have an [existing dataset](https://docs.smith.langchain.com/evaluation/concepts#datasets). Learn how to [create a dataset from the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#set-up-your-dataset).
If you prefer to run experiments in code, visit [run an evaluation using the SDK](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application).
![](https://docs.smith.langchain.com/assets/images/playground_experiment-82c9290c515102da892c5a5c261d4e2e.gif)
## Create an experiment in the prompt playground[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground#create-an-experiment-in-the-prompt-playground "Direct link to Create an experiment in the prompt playground")
  1. **Navigate to the playground** by clicking **Playground** in the sidebar.
  2. **Add a prompt** by selecting an existing saved a prompt or creating a new one.
  3. **Select a dataset** from the **Test over dataset** dropdown


  * Note that the keys in the dataset input must match the input variables of the prompt. For example, in the above video the selected dataset has inputs with the key "blog", which correctly match the input variable of the prompt.
  * There is a maximum of 15 input variables allowed in the prompt playground.


  1. **Start the experiment** by clicking on the **Start** or CMD+Enter. This will run the prompt over all the examples in the dataset and create an entry for the experiment in the dataset details page. We recommend committing the prompt to the prompt hub before starting the experiment so that it can be easily referenced later when reviewing your experiment.
  2. **View the full results** by clicking **View full experiment**. This will take you to the experiment details page where you can see the results of the experiment.


## Add evaluation scores to the experiment[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground#add-evaluation-scores-to-the-experiment "Direct link to Add evaluation scores to the experiment")
Evaluate your experiment over specific critera by adding evaluators. Add LLM-as-a-judge or custom code evaluators in the playground using the **+Evaluator** button.
To learn more about adding evaluators in via UI, visit [how to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge).
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)[NextSet up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
  * [Create an experiment in the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground#create-an-experiment-in-the-prompt-playground)
  * [Add evaluation scores to the experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground#add-evaluation-scores-to-the-experiment)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * Set up feedback criteria


On this page
# Set up feedback criteria
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Conceptual guide on tracing and feedback](https://docs.smith.langchain.com/observability/concepts)
  * [Reference guide on feedback data format](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)


Feedback criteria are represented in the application as feedback tags. For human feedback, you can set up new feedback criteria as continuous feedback or categorical feedback.
To set up a new feedback criteria, follow [this link](https://smith.langchain.com/settings/workspaces/feedbacks) to view all existing tags for your workspace, then click **New Tag**.
## Continuous feedback[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria#continuous-feedback "Direct link to Continuous feedback")
For continuous feedback, you can enter a feedback tag name, then select a minimum and maximum value. Every value, including floating-point numbers, within this range will be accepted as feedback scores.
![](https://docs.smith.langchain.com/assets/images/cont_feedback-6f21c75b4c0a296d3d7acc936a7c3776.png)
## Categorical feedback[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria#categorical-feedback "Direct link to Categorical feedback")
For categorical feedback, you can enter a feedback tag name, then add a list of categories, each category mapping to a score. When you provide feedback, you can select one of these categories as the feedback score. Both the category label and the score will be logged as feedback in `value` and `score` fields, respectively.
![](https://docs.smith.langchain.com/assets/images/cat_feedback-095c88b03d912ac15bff9c917f30790c.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousRunning an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)[NextAnnotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
  * [Continuous feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria#continuous-feedback)
  * [Categorical feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria#categorical-feedback)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * Dataset Sharing


On this page
# Dataset Sharing
## Share a Dataset Publicly[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset#share-a-dataset-publicly "Direct link to Share a Dataset Publicly")
caution
Sharing a dataset publicly will make the **dataset examples, experiments and associated runs and feedback on this dataset accessible to anyone with the link** , even if they don't have a LangSmith account. Make sure you're not sharing sensitive information.
This feature is only available in the cloud-hosted version of LangSmith.
From the **Dataset & Experiments** tab, select a dataset, click **‚ãÆ** (top right of the page), click **Share Dataset**. This will open a dialog where you can copy the link to the dataset.
![](https://docs.smith.langchain.com/assets/images/share_dataset-7535831552f940a6edef92a3ab5b68d5.gif)
**To "unshare" a dataset:**
  1. Click on **Unshare** by click on **Public** in the upper right hand corner of any publicly shared dataset, then **Unshare** in the dialog. ![](https://docs.smith.langchain.com/assets/images/unshare_dataset-67f8c2b34e85f694da903cac49ed35d8.png)
  2. Navigate to your organization's list of publicly shared datasets, by clicking on **Settings** -> **Shared URLs** or [this link](https://smith.langchain.com/settings/shared), then click on **Unshare** next to the dataset you want to unshare. ![](https://docs.smith.langchain.com/assets/images/unshare_trace_list-0acdafab84827c1c6a1d66a7a9ce61e3.png)


## Export a Dataset[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset#export-a-dataset "Direct link to Export a Dataset")
You can export your LangSmith dataset to a CSV, JSONL or [OpenAI's fine tuning format](https://platform.openai.com/docs/guides/fine-tuning#example-format) from the LangSmith UI.
From the **Dataset & Experiments** tab, select a dataset, click **‚ãÆ** (top right of the page), click **Download Dataset**.
![Export Dataset Button](https://docs.smith.langchain.com/assets/images/export-dataset-button-7de2422c2d39c50593dce1b10ce94a62.gif)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/share_dataset%3E).
[PreviousUse annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)[NextHow to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
  * [Share a Dataset Publicly](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset#share-a-dataset-publicly)
  * [Export a Dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset#export-a-dataset)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/summary

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/summary#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to define a summary evaluator


On this page
# How to define a summary evaluator
Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment. For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset. These are called `summary_evaluators`.
## Basic example[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/summary#basic-example "Direct link to Basic example")
Here, we'll compute the f1-score, which is a combination of precision and recall.
This sort of metric can only be computed over all of the examples in our experiment, so our evaluator takes in a list of outputs, and a list of reference_outputs.
  * Python
  * TypeScript


```
deff1_score_summary_evaluator(outputs:list[dict], reference_outputs:list[dict])->dict:  true_positives =0  false_positives =0  false_negatives =0for output_dict, reference_output_dict inzip(outputs, reference_outputs):    output = output_dict["class"]    reference_output = reference_output_dict["class"]if output =="Toxic"and reference_output =="Toxic":      true_positives +=1elif output =="Toxic"and reference_output =="Not toxic":      false_positives +=1elif output =="Not toxic"and reference_output =="Toxic":      false_negatives +=1if true_positives ==0:return{"key":"f1_score","score":0.0}  precision = true_positives /(true_positives + false_positives)  recall = true_positives /(true_positives + false_negatives)  f1_score =2*(precision * recall)/(precision + recall)return{"key":"f1_score","score": f1_score}
```

```
functionf1ScoreSummaryEvaluator({ outputs, referenceOutputs }:{ outputs: Record<string,any>[], referenceOutputs: Record<string,any>[]}){let truePositives =0;let falsePositives =0;let falseNegatives =0;for(let i =0; i < outputs.length; i++){const output = outputs[i]["class"];const referenceOutput = referenceOutputs[i]["class"];if(output ==="Toxic"&& referenceOutput ==="Toxic"){   truePositives +=1;}elseif(output ==="Toxic"&& referenceOutput ==="Not toxic"){   falsePositives +=1;}elseif(output ==="Not toxic"&& referenceOutput ==="Toxic"){   falseNegatives +=1;}}if(truePositives ===0){return{ key:"f1_score", score:0.0};}const precision = truePositives /(truePositives + falsePositives);const recall = truePositives /(truePositives + falseNegatives);const f1Score =2*(precision * recall)/(precision + recall);return{ key:"f1_score", score: f1Score };}
```

You can then pass this evaluator to the `evaluate` method as follows:
  * Python
  * TypeScript


```
from langsmith import Clientls_client = Client()dataset = ls_client.clone_public_dataset("https://smith.langchain.com/public/3d6831e6-1680-4c88-94df-618c8e01fc55/d")defbad_classifier(inputs:dict)->dict:return{"class":"Not toxic"}defcorrect(outputs:dict, reference_outputs:dict)->bool:"""Row-level correctness evaluator."""return outputs["class"]== reference_outputs["label"]results = ls_client.evaluate(  bad_classified,  data=dataset,  evaluators=[correct],  summary_evaluators=[pass_50],)
```

```
import{ Client }from"langsmith";import{ evaluate }from"langsmith/evaluation";importtype{ EvaluationResult }from"langsmith/evaluation";const client =newClient();const datasetName ="Toxic queries";const dataset =await client.clonePublicDataset("https://smith.langchain.com/public/3d6831e6-1680-4c88-94df-618c8e01fc55/d",{ datasetName: datasetName });functioncorrect({ outputs, referenceOutputs }:{ outputs: Record<string,any>, referenceOutputs?: Record<string,any>}): EvaluationResult {const score = outputs["class"]=== referenceOutputs?["label"];return{ key:"correct", score };}functionbadClassifier(inputs: Record<string,any>):{class:string}{return{class:"Not toxic"};}awaitevaluate(badClassifier,{ data: datasetName, evaluators:[correct], summaryEvaluators:[summaryEval], experimentPrefix:"Toxic Queries",});
```

In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key.
![](https://docs.smith.langchain.com/assets/images/summary_eval-20d1a3d5cd63a91009dc3854a14077e1.png)
## Summary evaluator args[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/summary#summary-evaluator-args "Direct link to Summary evaluator args")
Summary evaluator functions must have specific argument names. They can take any subset of the following arguments:
  * `inputs: list[dict]`: A list of the inputs corresponding to a single example in a dataset.
  * `outputs: list[dict]`: A list of the dict outputs produced by each experiment on the given inputs.
  * `reference_outputs/referenceOutputs: list[dict]`: A list of the reference outputs associated with the example, if available.
  * `runs: list[Run]`: A list of the full [Run](https://docs.smith.langchain.com/reference/data_formats/run_data_format) objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.
  * `examples: list[Example]`: All of the dataset [Example](https://docs.smith.langchain.com/reference/data_formats/example_data_format) objects, including the example inputs, outputs (if available), and metdata (if available).


## Summary evaluator output[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/summary#summary-evaluator-output "Direct link to Summary evaluator output")
Summary evaluators are expected to return one of the following types:
Python and JS/TS
  * `dict`: dicts of the form `{"score": ..., "name": ...}` allow you to pass a numeric or boolean score and metric name.


Currently Python only
  * `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)[NextHow to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
  * [Basic example](https://docs.smith.langchain.com/evaluation/how_to_guides/summary#basic-example)
  * [Summary evaluator args](https://docs.smith.langchain.com/evaluation/how_to_guides/summary#summary-evaluator-args)
  * [Summary evaluator output](https://docs.smith.langchain.com/evaluation/how_to_guides/summary#summary-evaluator-output)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to upload experiments run outside of LangSmith with the REST API


On this page
# How to upload experiments run outside of LangSmith with the REST API
Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our `/datasets/upload-experiment` endpoint.
This guide will show you how to upload evals using the REST API, using the `requests` library in Python as an example. However, the same principles apply to any language.
## Request body schema[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments#request-body-schema "Direct link to Request body schema")
Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within the experiment. Each object in the `results` represents a "row" in the experiment - a single dataset example, along with an associated run. Note that `dataset_id` and `dataset_name` refer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset in LangSmith (unless that dataset was created via this endpoint).
You may use the following schema to upload experiments to the `/datasets/upload-experiment` endpoint:
```
{"experiment_name":"string (required)","experiment_description":"string (optional)","experiment_start_time":"datetime (required)","experiment_end_time":"datetime (required)","dataset_id":"uuid (optional - an external dataset id, used to group experiments together)","dataset_name":"string (optional - must provide either dataset_id or dataset_name)","dataset_description":"string (optional)","experiment_metadata":{// Object (any shape - optional)"key":"value"},"summary_experiment_scores":[// List of summary feedback objects (optional){"key":"string (required)","score":"number (optional)","value":"string (optional)","comment":"string (optional)","feedback_source":{// Object (optional)"type":"string (required)"},"feedback_config":{// Object (optional)"type":"string enum: continuous, categorical, or freeform","min":"number (optional)","max":"number (optional)","categories":[// List of feedback category objects (optional)"value":"number (required)","label":"string (optional)"]},"created_at":"datetime (optional - defaults to now)","modified_at":"datetime (optional - defaults to now)","correction":"Object or string (optional)"}],"results":[// List of experiment row objects (required){"row_id":"uuid (required)","inputs":{// Object (required - any shape). This will"key":"val"// be the input to both the run and the dataset example.},"expected_outputs":{// Object (optional - any shape)."key":"val"// These will be the outputs of the dataset examples.},"actual_outputs":{// Object (optional - any shape)."key": "val    // These will be the outputs of the runs.},"evaluation_scores":[// List of feedback objects for the run (optional){"key":"string (required)","score":"number (optional)","value":"string (optional)","comment":"string (optional)","feedback_source":{// Object (optional)"type":"string (required)"},"feedback_config":{// Object (optional)"type":"string enum: continuous, categorical, or freeform","min":"number (optional)","max":"number (optional)","categories":[// List of feedback category objects (optional)"value":"number (required)","label":"string (optional)"]},"created_at":"datetime (optional - defaults to now)","modified_at":"datetime (optional - defaults to now)","correction":"Object or string (optional)"}],"start_time":"datetime (required)",// The start/end times for the runs will be used to"end_time":"datetime (required)",// calculate latency. They must all fall between the"run_name":"string (optional)",// start and end times for the experiment."error":"string (optional)","run_metadata":{// Object (any shape - optional)"key":"value"}}]}
```

The response JSON will be a dict with keys `experiment` and `dataset`, each of which is an object that contains relevant information about the experiment and dataset that was created.
## Considerations[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments#considerations "Direct link to Considerations")
You may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together under a single dataset, and you will be able to [use the comparison view to compare results between experiments](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results).
Ensure that the start and end times of your individual rows are all between the start and end time of your experiment.
You must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if you only provide a name.
You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets.
## Example request[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments#example-request "Direct link to Example request")
Below is an example of a simple call to the `/datasets/upload-experiment`. This is a basic example that just uses the most important fields as an illustration.
```
import osimport requestsbody ={"experiment_name":"My external experiment","experiment_description":"An experiment uploaded to LangSmith","dataset_name":"my-external-dataset","summary_experiment_scores":[{"key":"summary_accuracy","score":0.9,"comment":"Great job!"}],"results":[{"row_id":"<<uuid>>","inputs":{"input":"Hello, what is the weather in San Francisco today?"},"expected_outputs":{"output":"Sorry, I am unable to provide information about the current weather."},"actual_outputs":{"output":"The weather is partly cloudy with a high of 65."},"evaluation_scores":[{"key":"hallucination","score":1,"comment":"The chatbot made up the weather instead of identifying that ""they don't have enough info to answer the question. This is ""a hallucination."}],"start_time":"2024-08-03T00:12:39","end_time":"2024-08-03T00:12:41","run_name":"Chatbot"},{"row_id":"<<uuid>>","inputs":{"input":"Hello, what is the square root of 49?"},"expected_outputs":{"output":"The square root of 49 is 7."},"actual_outputs":{"output":"7."},"evaluation_scores":[{"key":"hallucination","score":0,"comment":"The chatbot correctly identified the answer. This is not a ""hallucination."}],"start_time":"2024-08-03T00:12:40","end_time":"2024-08-03T00:12:42","run_name":"Chatbot"}],"experiment_start_time":"2024-08-03T00:12:38","experiment_end_time":"2024-08-03T00:12:43"}resp = requests.post("https://api.smith.langchain.com/api/v1/datasets/upload-experiment",  json=body,  headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]})print(resp.json())
```

Below is the response received:
```
{"dataset":{"name":"my-external-dataset","description":null,"created_at":"2024-08-03T00:36:23.289730+00:00","data_type":"kv","inputs_schema_definition":null,"outputs_schema_definition":null,"externally_managed":true,"id":"<<uuid>>","tenant_id":"<<uuid>>","example_count":0,"session_count":0,"modified_at":"2024-08-03T00:36:23.289730+00:00","last_session_start_time":null},"experiment":{"start_time":"2024-08-03T00:12:38","end_time":"2024-08-03T00:12:43+00:00","extra":null,"name":"My external experiment","description":"An experiment uploaded to LangSmith","default_dataset_id":null,"reference_dataset_id":"<<uuid>>","trace_tier":"longlived","id":"<<uuid>>","run_count":null,"latency_p50":null,"latency_p99":null,"first_token_p50":null,"first_token_p99":null,"total_tokens":null,"prompt_tokens":null,"completion_tokens":null,"total_cost":null,"prompt_cost":null,"completion_cost":null,"tenant_id":"<<uuid>>","last_run_start_time":null,"last_run_start_time_live":null,"feedback_stats":null,"session_feedback_stats":null,"run_facets":null,"error_rate":null,"streaming_rate":null,"test_run_number":1}}
```

Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds. If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this information in the request body).
## View the experiment in the UI[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments#view-the-experiment-in-the-ui "Direct link to View the experiment in the UI")
Now, login to the UI and click on your newly-created dataset! You should see a single experiment: ![Uploaded experiments table](https://docs.smith.langchain.com/assets/images/uploaded_dataset-953e12d3f0d606e8784d20cf755a9495.png)
Your examples will have been uploaded: ![Uploaded examples](https://docs.smith.langchain.com/assets/images/uploaded_dataset_examples-30539abed0f9ad138fb26aae4b13f3b0.png)
Clicking on your experiment will bring you to the comparison view: ![Uploaded experiment comparison view](https://docs.smith.langchain.com/assets/images/uploaded_experiment-a55a0255bef43ccba484bee7497e5384.png)
As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousHow to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)[NextHow to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
  * [Request body schema](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments#request-body-schema)
  * [Considerations](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments#considerations)
  * [Example request](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments#example-request)
  * [View the experiment in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments#view-the-experiment-in-the-ui)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to use off-the-shelf evaluators (Python only)


On this page
# How to use off-the-shelf evaluators (Python only)
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [LangChain evaluator reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code. These evaluators are meant to be used more as a starting point for evaluation.
Prerequisites
Create a dataset and set up the LangSmith client in Python to follow along
```
from langsmith import Clientclient = Client() Create a datasetexamples =[{"inputs":{"input":"Ankush"},"outputs":{"expected":"Hello Ankush"},},{"inputs":{"input":"Harrison"},"outputs":{"expected":"Hello Harrison"},},]dataset_name ="Hello Set"dataset = client.create_dataset(dataset_name=dataset_name)client.create_examples(dataset_id=dataset.id, examples=examples)
```

## Use question and answer (correctness) evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#use-question-and-answer-correctness-evaluators "Direct link to Use question and answer \(correctness\) evaluators")
Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you! Three QA evaluators you can load are: `"qa"`, `"context_qa"`, `"cot_qa"`. Based on our meta-evals, we recommend using `"cot_qa"`, or Chain of Thought QA.
Here is a trivial example that uses a `"cot_qa"` evaluator to evaluate a simple pipeline that prefixes the input with "Hello":
```
from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator("cot_qa")client = Client()evaluate(lambdainput:"Hello "+input["input"],  data=dataset_name,  evaluators=[cot_qa_evaluator],)
```

## Use criteria evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#use-criteria-evaluators "Direct link to Use criteria evaluators")
If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the `"criteria"` evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules.
  * The `"criteria"` evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion


```
from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator("criteria",  config={"criteria":{"says_hello":"Does the submission say hello?",}})client = Client()evaluate(lambdainput:"Hello "+input["input"],  data=dataset_name,  evaluators=[    criteria_evaluator,],)
```

Supported Criteria
Default criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality. To specify custom criteria, write a mapping of a criterion name to its description, such as:
```
criterion ={"creativity":"Is this submission creative, imaginative, or novel?"}criteria_evaluator = LangChainStringEvaluator("labeled_criteria",  config={"criteria": criterion})
```

Interpreting the Score
Evaluation scores don't have an inherent "direction" (i.e., higher is not necessarily better). The direction of the score depends on the criteria being evaluated. For example, a score of 1 for "helpfulness" means that the prediction was deemed to be helpful by the model. However, a score of 1 for "maliciousness" means that the prediction contains malicious content, which, of course, is "bad".
## Use labeled criteria evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#use-labeled-criteria-evaluators "Direct link to Use labeled criteria evaluators")
If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the `"labeled_criteria"` or `"labeled_score_string"` evaluators.
  * The `"labeled_criteria"` evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference label
  * The `"labeled_score_string"` evaluator instructs an LLM to assess the prediction against a reference label on a specified scale


```
from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator("labeled_criteria",  config={"criteria":{"helpfulness":("Is this submission helpful to the user,"" taking into account the correct reference answer?")}})labeled_score_evaluator = LangChainStringEvaluator("labeled_score_string",  config={"criteria":{"accuracy":"How accurate is this prediction compared to the reference on a scale of 1-10?"},"normalize_by":10,})client = Client()evaluate(lambdainput:"Hello "+input["input"],  data=dataset_name,  evaluators=[    labeled_criteria_evaluator,    labeled_score_evaluator],)
```

## Use string or embedding distance metrics[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#use-string-or-embedding-distance-metrics "Direct link to Use string or embedding distance metrics")
To measure the similarity between a predicted string and a reference, you can use string distance metrics:
  * The `"string_distance"` evaluator computes a normalized string edit distance between the prediction and reference
  * The `"embedding_distance"` evaluator computes the distance between the text embeddings of the prediction and reference


```
 !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator("string_distance",  config={"distance":"levenshtein","normalize_score":True})embedding_distance_evaluator = LangChainStringEvaluator("embedding_distance",  config={# Defaults to OpenAI, but you can customize which embedding provider to use:# "embeddings": HuggingFaceEmbeddings(model="distilbert-base-uncased"),# Can also choose "euclidean", "chebyshev", "hamming", and "manhattan""distance_metric":"cosine",})evaluate(lambdainput:"Hello "+input["input"],  data=dataset_name,  evaluators=[    string_distance_evaluator,    embedding_distance_evaluator,],)
```

## Use a custom LLM in off-the-shelf evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#use-a-custom-llm-in-off-the-shelf-evaluators "Direct link to Use a custom LLM in off-the-shelf evaluators")
You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries.
```
from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model="gpt-4o-mini")cot_qa_evaluator = LangChainStringEvaluator("cot_qa", config={"llm": eval_llm})evaluate(lambdainput:"Hello "+input["input"],  data=dataset_name,  evaluators=[cot_qa_evaluator],)
```

## Handle multiple input or output fields[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#handle-multiple-input-or-output-fields "Direct link to Handle multiple input or output fields")
LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the `prepare_data` function to extract the relevant fields for evaluation. These map the keys `"prediction"`, `"reference"`, and `"input"` to the correct fields in the input and output dictionaries.
For the below example, we have a model that outputs two fields: `"greeting"` and `"foo"`. We want to evaluate the `"greeting"` field against the `"expected"` field in the output dictionary.
```
from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator("labeled_criteria",  config={"criteria":{"helpfulness":("Is this submission helpful to the user,"" taking into account the correct reference answer?")}},  prepare_data=lambda run, example:{"prediction": run.outputs["greeting"],"reference": example.outputs["expected"],"input": example.inputs["input"],})client = Client()evaluate(lambdainput:{"greeting":"Hello "+input["input"],"foo":"bar"},  data=dataset_name,  evaluators=[    labeled_criteria_evaluator],)
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old%3E).
[PreviousDataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)[NextHow to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
  * [Use question and answer (correctness) evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#use-question-and-answer-correctness-evaluators)
  * [Use criteria evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#use-criteria-evaluators)
  * [Use labeled criteria evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#use-labeled-criteria-evaluators)
  * [Use string or embedding distance metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#use-string-or-embedding-distance-metrics)
  * [Use a custom LLM in off-the-shelf evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#use-a-custom-llm-in-off-the-shelf-evaluators)
  * [Handle multiple input or output fields](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old#handle-multiple-input-or-output-fields)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to version a dataset


On this page
# How to version a dataset
In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created.
## Create a new version of a dataset[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets#create-a-new-version-of-a-dataset "Direct link to Create a new version of a dataset")
Any time you _add_ , _update_ , or _delete_ examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and to understand how your dataset has evolved.
By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the "Examples" tab, you can see the state of the dataset at that point in time.
![Version Datasets](https://docs.smith.langchain.com/assets/images/version_dataset-0e86bb335a1bc69a1afc084fec6fe7a3.png)
Note that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the "latest" version of the dataset. Also, by default the **latest version of the dataset is shown in the "Examples" tab** and experiments from **all versions are shown in the "Tests" tab**.
In the "Tests" tab, you can see the results of tests run on the dataset at different versions.
![Version Datasets](https://docs.smith.langchain.com/assets/images/version_dataset_tests-a970d7b56bef1e2b0f949d15cad5f44a.png)
## Tag a version[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets#tag-a-version "Direct link to Tag a version")
You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset's history.
For example, you might tag a version of your dataset as "prod" and use it to run tests against your LLM pipeline.
Tagging can be done in the UI by clicking on "+ Tag this version" in the "Examples" tab.
![Tagging Datasets](https://docs.smith.langchain.com/assets/images/tag_this_version-556429eb25b2c5d632742c406d7aeb12.png)
You can also tag versions of your dataset using the SDK. Here's an example of how to tag a version of a dataset using the python SDK:
```
from langsmith import Clientfrom datetime import datetimeclient = Client()initial_time = datetime(2024,1,1,0,0,0)# The timestamp of the version you want to tag# You can tag a specific dataset version with a semantic name, like "prod"client.update_dataset_tag(  dataset_name=toxic_dataset_name, as_of=initial_time, tag="prod")
```

To run an evaluation on a particular tagged version of a dataset, you can follow [this guide](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version).
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/version_datasets%3E).
[PreviousHow to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)[NextUse annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
  * [Create a new version of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets#create-a-new-version-of-a-dataset)
  * [Tag a version](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets#tag-a-version)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest

[Skip to main content](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * How to run evals with Vitest/Jest (beta)


On this page
# How to run evals with Vitest/Jest (beta)
LangSmith provides integrations with Vitest and Jest that allow JavaScript and TypeScript developers define their datasets and evaluate using familiar syntax.
![](https://docs.smith.langchain.com/assets/images/jest_vitest_reporter_output-5a16faa419913f0a8b0a40dd46ba9abb.png)
Compared to the `evaluate()` evaluation flow, this is useful when:
  * Each example requires different evaluation logic
  * You want to assert binary expectations, and both track these assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines)
  * You want to take advantage of mocks, watch mode, local results, or other features of the Vitest/Jest ecosystems


Installation
Requires JS/TS SDK version `langsmith>=0.3.1`.
Beta
The Vitest/Jest integrations are in beta and are subject to change in upcoming releases.
For Python
The Python SDK has an analogous [pytest integration](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest).
## Setup[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#setup "Direct link to Setup")
Set up the integrations as follows. Note that while you can add LangSmith evals alongside your other unit tests (as standard `*.test.ts` files) using your existing test config files, the below examples will also set up a separate test config file and command to run your evals. It will assume you end your test files with `.eval.ts`.
This ensures that the custom test reporter and other LangSmith touchpoints do not modify your existing test outputs.
### Vitest[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#vitest "Direct link to Vitest")
Install the required development dependencies if you have not already:
  * yarn
  * npm
  * pnpm


```
yarn add -D vitest dotenv
```

```
npm install -D vitest dotenv
```

```
pnpm add -D vitest dotenv
```

The examples below also require `openai` (and of course `langsmith`!) as a dependency:
  * yarn
  * npm
  * pnpm


```
yarn add langsmith openai
```

```
npm install langsmith openai
```

```
pnpm add langsmith openai
```

Then create a separate `ls.vitest.config.ts` file with the following base config:
```
import{ defineConfig }from"vitest/config";exportdefaultdefineConfig({ test:{  include:["**/*.eval.?(c|m)[jt]s"],  reporters:["langsmith/vitest/reporter"],  setupFiles:["dotenv/config"],},});
```

  * `include` ensures that only files ending with some variation of `eval.ts` in your project are run
  * `reporters` is responsible for nicely formatting your output as shown above
  * `setupFiles` runs `dotenv` to load environment variables before running your evals


Finally, add the following to the `scripts` field in your `package.json` to run Vitest with the config you just created:
```
{"name":"YOUR_PROJECT_NAME","scripts":{"eval":"vitest run --config ls.vitest.config.ts"},"dependencies":{  ...},"devDependencies":{  ...}}
```

Note that the above script disables Vitest's default watch mode for running evals since many evaluators may include longer running LLM calls.
### Jest[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#jest "Direct link to Jest")
Install the required development dependencies if you have not already:
  * yarn
  * npm
  * pnpm


```
yarn add -D jest dotenv
```

```
npm install -D jest dotenv
```

```
pnpm add -D jest dotenv
```

The examples below also require `openai` (and of course `langsmith`!) as a dependency:
  * yarn
  * npm
  * pnpm


```
yarn add langsmith openai
```

```
npm install langsmith openai
```

```
pnpm add langsmith openai
```

info
The setup instructions below are for basic JS files and CJS. To add support for TypeScript and ESM, see Jest's official docs or use [Vitest](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#vitest).
Then create a separate config file named `ls.jest.config.cjs`:
```
module.exports ={ testMatch:["**/*.eval.?(c|m)[jt]s"], reporters:["langsmith/jest/reporter"], setupFiles:["dotenv/config"],};
```

  * `testMatch` ensures that only files ending with some variation of `eval.js` in your project are run
  * `reporters` is responsible for nicely formatting your output as shown above
  * `setupFiles` runs `dotenv` to load environment variables before running your evals


Finally, add the following to the `scripts` field in your `package.json` to run Jest with the config you just created:
```
{"name":"YOUR_PROJECT_NAME","scripts":{"eval":"jest --config ls.jest.config.cjs"},"dependencies":{  ...},"devDependencies":{  ...}}
```

## Define and run evals[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#define-and-run-evals "Direct link to Define and run evals")
You can now define evals as tests using familiar Vitest/Jest syntax, with a few caveats:
  * You should import `describe` and `test` from the `langsmith/jest` or `langsmith/vitest` entrypoint
  * You must wrap your test cases in a `describe` block
  * When declaring tests, the signature is slightly different - there is an extra argument containing example inputs and expected outputs


Try it out by creating a file named `sql.eval.ts` (or `sql.eval.js` if you are using Jest without TypeScript) and pasting the below contents into it:
```
import*as ls from"langsmith/vitest";import{ expect }from"vitest";// import * as ls from "langsmith/jest";// import { expect } from "@jest/globals";import OpenAI from"openai";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers/openai";// Add "openai" as a dependency and set OPENAI_API_KEY as an environment variableconst tracedClient =wrapOpenAI(newOpenAI());const generateSql =traceable(async(userQuery:string)=>{const result =await tracedClient.chat.completions.create({   model:"gpt-4o-mini",   messages:[{     role:"system",     content:"Convert the user query to a SQL query. Do not wrap in any markdown tags.",},{     role:"user",     content: userQuery,},],});return result.choices[0].message.content;},{ name:"generate_sql"});ls.describe("generate sql demo",()=>{ ls.test("generates select all",{   inputs:{ userQuery:"Get all users from the customers table"},   referenceOutputs:{ sql:"SELECT * FROM customers;"},},async({ inputs, referenceOutputs })=>{const sql =awaitgenerateSql(inputs.userQuery);   ls.logOutputs({ sql });// <-- Log run outputs, optionalexpect(sql).toEqual(referenceOutputs?.sql);// <-- Assertion result logged under 'pass' feedback key});});
```

You can think of each `ls.test()` case as corresponding to a dataset example, and `ls.describe()` as defining a LangSmith dataset. If you have LangSmith [tracing environment variables](https://docs.smith.langchain.com/#3-set-up-your-environment) set when you run the test suite, the SDK does the following:
  * creates a [dataset](https://docs.smith.langchain.com/evaluation/concepts#datasets) with the same name as the name passed to `ls.describe()` in LangSmith if it does not exist
  * creates an example in the dataset for each input and expected output passed into a test case if a matching one does not already exist
  * creates a new [experiment](https://docs.smith.langchain.com/evaluation/concepts#experiment) with one result for each test case
  * collects the pass/fail rate under the `pass` feedback key for each test case


When you run this test it will have a default `pass` boolean feedback key based on the test case passing / failing. It will also track any outputs that you log with the `ls.logOutputs()` or return from the test function as "actual" result values from your app for the experiment.
Create a `.env` file with your `OPENAI_API_KEY` and LangSmith credentials if you don't already have one:
```
OPENAI_API_KEY="YOUR_KEY_HERE"LANGSMITH_API_KEY="YOUR_LANGSMITH_KEY"LANGSMITH_TRACING_V2="true"
```

Now use the `eval` script we set up in the previous step to run the test:
  * yarn
  * npm
  * pnpm


```
yarn run eval
```

```
npm run eval
```

```
pnpm run eval
```

And your declared test should run!
Once it finishes, if you've set your LangSmith environment variables, you should see a link directing you to an experiment created in LangSmith alongside the test results.
Here's what an experiment against that test suite looks like:
![Experiment](https://docs.smith.langchain.com/assets/images/simple-vitest-275ea954978c11001a6189088ee4a795.png)
## Trace feedback[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#trace-feedback "Direct link to Trace feedback")
By default LangSmith collects the pass/fail rate under the `pass` feedback key for each test case. You can add additional feedback with either `ls.logFeedback()` or `wrapEvaluator()`. To do so, try the following as your `sql.eval.ts` file (or `sql.eval.js` if you are using Jest without TypeScript):
```
import*as ls from"langsmith/vitest";// import * as ls from "langsmith/jest";import OpenAI from"openai";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers/openai";// Add "openai" as a dependency and set OPENAI_API_KEY as an environment variableconst tracedClient =wrapOpenAI(newOpenAI());const generateSql =traceable(async(userQuery:string)=>{const result =await tracedClient.chat.completions.create({   model:"gpt-4o-mini",   messages:[{     role:"system",     content:"Convert the user query to a SQL query. Do not wrap in any markdown tags.",},{     role:"user",     content: userQuery,},],});return result.choices[0].message.content ??"";},{ name:"generate_sql"});constmyEvaluator=async(params:{ outputs:{ sql:string}; referenceOutputs:{ sql:string};})=>{const{ outputs, referenceOutputs }= params;const instructions =["Return 1 if the ACTUAL and EXPECTED answers are semantically equivalent, ","otherwise return 0. Return only 0 or 1 and nothing else.",].join("\n");const grade =await tracedClient.chat.completions.create({  model:"gpt-4o-mini",  messages:[{    role:"system",    content: instructions,},{    role:"user",    content:`ACTUAL: ${outputs.sql}\nEXPECTED: ${referenceOutputs?.sql}`,},],});const score =parseInt(grade.choices[0].message.content ??"");return{ key:"correctness", score };};ls.describe("generate sql demo",()=>{ ls.test("generates select all",{   inputs:{ userQuery:"Get all users from the customers table"},   referenceOutputs:{ sql:"SELECT * FROM customers;"},},async({ inputs, referenceOutputs })=>{const sql =awaitgenerateSql(inputs.userQuery);   ls.logOutputs({ sql });const wrappedEvaluator = ls.wrapEvaluator(myEvaluator);// Will automatically log "correctness" as feedbackawaitwrappedEvaluator({    outputs:{ sql },    referenceOutputs,});// You can also manually log feedback with `ls.logFeedback()`   ls.logFeedback({    key:"harmfulness",    score:0.2,});}); ls.test("offtopic input",{   inputs:{ userQuery:"whats up"},   referenceOutputs:{ sql:"sorry that is not a valid query"},},async({ inputs, referenceOutputs })=>{const sql =awaitgenerateSql(inputs.userQuery);   ls.logOutputs({ sql });const wrappedEvaluator = ls.wrapEvaluator(myEvaluator);// Will automatically log "correctness" as feedbackawaitwrappedEvaluator({    outputs:{ sql },    referenceOutputs,});// You can also manually log feedback with `ls.logFeedback()`   ls.logFeedback({    key:"harmfulness",    score:0.2,});});});
```

Note the use of `ls.wrapEvaluator()` around the `myEvaluator` function. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case to avoid clutter, and conveniently creates feedback if the return value from the wrapped function matches `{ key: string; score: number | boolean }`. In this case, instead of showing up in the main test case run, the evaluator trace will instead show up in a trace associated with the `correctness` feedback key.
You can see the evaluator runs in LangSmith by clicking their corresponding feedback chips in the UI.
## Running multiple examples against a test case[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#running-multiple-examples-against-a-test-case "Direct link to Running multiple examples against a test case")
You can run the same test case over multiple examples and parameterize your tests using `ls.test.each()`. This is useful when you want to evaluate your app the same way against different inputs:
```
import*as ls from"langsmith/vitest";// import * as ls from "langsmith/jest";constDATASET=[{ inputs:{ userQuery:"whats up"}, referenceOutputs:{ sql:"sorry that is not a valid query"}},{ inputs:{ userQuery:"what color is the sky?"}, referenceOutputs:{ sql:"sorry that is not a valid query"}},{ inputs:{ userQuery:"how are you today?"}, referenceOutputs:{ sql:"sorry that is not a valid query"}}];ls.describe("generate sql demo",()=>{ ls.test.each(DATASET)("offtopic inputs",async({ inputs, referenceOutputs })=>{...},)});
```

If you have tracking enabled, each example in the local dataset will be synced to the one created in LangSmith.
## Log outputs[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#log-outputs "Direct link to Log outputs")
Every time we run a test we're syncing it to a dataset example and tracing it as a run. To trace final outputs for the run, you can use `ls.logOutputs()` like this:
```
import*as ls from"langsmith/vitest";// import * as ls from "langsmith/jest";ls.describe("generate sql demo",()=>{ ls.test("offtopic input",{   inputs:{ userQuery:"..."},   referenceOutputs:{ sql:"..."}},async({ inputs, referenceOutputs })=>{   ls.logOutputs({ sql:"SELECT * FROM users;"})},)});
```

The logged outputs will appear in your reporter summary and in LangSmith.
You can also directly return a value from your test function:
```
import*as ls from"langsmith/vitest";// import * as ls from "langsmith/jest";ls.describe("generate sql demo",()=>{ ls.test("offtopic input",{   inputs:{ userQuery:"..."},   referenceOutputs:{ sql:"..."}},async({ inputs, referenceOutputs })=>{return{ sql:"SELECT * FROM users;"}},);});
```

However keep in mind if you do this that if your test fails to complete due to a failed assertion or other error, your output will not appear.
## Trace intermediate calls[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#trace-intermediate-calls "Direct link to Trace intermediate calls")
LangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.
## Focusing or skipping tests[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#focusing-or-skipping-tests "Direct link to Focusing or skipping tests")
You can chain the Vitest/Jest `.skip` and `.only` methods on `ls.test()` and `ls.describe()`:
```
import*as ls from"langsmith/vitest";// import * as ls from "langsmith/jest";ls.describe("generate sql demo",()=>{ ls.test.skip("offtopic input",{   inputs:{ userQuery:"..."},   referenceOutputs:{ sql:"..."}},async({ inputs, referenceOutputs })=>{return{ sql:"SELECT * FROM users;"}},); ls.test.only("other",{   inputs:{ userQuery:"..."},   referenceOutputs:{ sql:"..."}},async({ inputs, referenceOutputs })=>{return{ sql:"SELECT * FROM users;"}},);});
```

## Configuring test suites[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#configuring-test-suites "Direct link to Configuring test suites")
You can configure test suites with values like metadata or a custom client by passing an extra argument to `ls.describe()` for the full suite or by passing a `config` field into `ls.test()` for individual tests:
```
ls.describe("test suite name",()=>{ ls.test("test name",{  inputs:{...},  referenceOutputs:{...},// Extra config for the test run  config:{ tags:[...], metadata:{...}}},{  name:"test name",  tags:["tag1","tag2"],  skip:true,  only:true,});},{ testSuiteName:"overridden value", metadata:{...},// Custom client client:newClient(),});
```

The test suite will also automatically extract environment variables from `process.env.ENVIRONMENT`, `process.env.NODE_ENV` and `process.env.LANGSMITH_ENVIRONMENT` and set them as metadata on created experiments. You can then filter experiments by metadata in LangSmith's UI.
See [the API refs](https://docs.smith.langchain.com/reference/js/functions/vitest.describe) for a full list of configuration options.
## Dry-run mode[‚Äã](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#dry-run-mode "Direct link to Dry-run mode")
If you want to run the tests without syncing the results to LangSmith, you can set omit your LangSmith tracing environment variables or set `LANGSMITH_TEST_TRACKING=false` in your environment.
The tests will run as normal, but the experiment logs will not be sent to LangSmith.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/how_to_guides/vitest_jest%3E).
[PreviousHow to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)[NextConceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Setup](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#setup)
    * [Vitest](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#vitest)
    * [Jest](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#jest)
  * [Define and run evals](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#define-and-run-evals)
  * [Trace feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#trace-feedback)
  * [Running multiple examples against a test case](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#running-multiple-examples-against-a-test-case)
  * [Log outputs](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#log-outputs)
  * [Trace intermediate calls](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#trace-intermediate-calls)
  * [Focusing or skipping tests](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#focusing-or-skipping-tests)
  * [Configuring test suites](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#configuring-test-suites)
  * [Dry-run mode](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest#dry-run-mode)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/tutorials

[Skip to main content](https://docs.smith.langchain.com/evaluation/tutorials#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/tutorials)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/tutorials)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/tutorials)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * Tutorials


# Evaluation tutorials
New to LangSmith or to LLM app development in general? Read this material to quickly get up and running.
  * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
  * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
  * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
  * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
  * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/tutorials%3E).
[PreviousQuick Start](https://docs.smith.langchain.com/evaluation)[NextEvaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/tutorials/agents

[Skip to main content](https://docs.smith.langchain.com/evaluation/tutorials/agents#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/tutorials/agents)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/tutorials/agents)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/tutorials/agents)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
  * Evaluate a complex agent


On this page
# Evaluate a complex agent
Key concepts
[Agent evaluation](https://docs.smith.langchain.com/evaluation/concepts#agents) | [Evaluators](https://docs.smith.langchain.com/evaluation/concepts#evaluators) | [LLM-as-judge evaluators](https://docs.smith.langchain.com/evaluation/concepts#llm-as-judge)
In this tutorial, we'll build a customer support bot that helps users navigate a digital music store. Then, we'll go through the three most effective types of evaluations to run on chat bots:
  * [Final response](https://docs.smith.langchain.com/evaluation/concepts#evaluating-an-agents-final-response): Evaluate the agent's final response.
  * [Trajectory](https://docs.smith.langchain.com/evaluation/concepts#evaluating-an-agents-trajectory): Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer.
  * [Single step](https://docs.smith.langchain.com/evaluation/concepts#evaluating-a-single-step-of-an-agent): Evaluate any agent step in isolation (e.g., whether it selects the appropriate first tool for a given ).


We'll build our agent using [LangGraph](https://github.com/langchain-ai/langgraph), but the techniques and LangSmith functionality shown here are framework-agnostic.
## Setup[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#setup "Direct link to Setup")
### Configure the environment[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#configure-the-environment "Direct link to Configure the environment")
Let's install the required dependencies:
```
pip install -U langgraph langchain[openai]
```

Let's set up environment variables for OpenAI and [LangSmith](https://smith.langchain.com):
‚åµ
```
import getpassimport osdef_set_env(var:str)->None:ifnot os.environ.get(var):    os.environ[var]= getpass.getpass(f"Set {var}: ")os.environ["LANGSMITH_TRACING"]="true"_set_env("LANGSMITH_API_KEY")_set_env("OPENAI_API_KEY")
```

### Download the database[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#download-the-database "Direct link to Download the database")
We will create a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will load the `chinook` database, which is a sample database that represents a digital media store. Find more information about the database [here](https://www.sqlitetutorial.net/sqlite-sample-database/).
For convenience, we have hosted the database in a public GCS bucket:
‚åµ
```
import requestsurl ="https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db"response = requests.get(url)if response.status_code ==200:# Open a local file in binary write modewithopen("chinook.db","wb")asfile:# Write the content of the response (the file) to the local filefile.write(response.content)print("File downloaded and saved as Chinook.db")else:print(f"Failed to download the file. Status code: {response.status_code}")
```

Here's a sample of the data in the db:
‚åµ
```
import sqlite3 ...
```

```
[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Ant√¥nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]
```

And here's the database schema (image from <https://github.com/lerocha/chinook-database>):
![Chinook DB](https://docs.smith.langchain.com/assets/images/chinook-diagram-1e507a47313c15ffd706a785175b2c14.png)
### Define the customer support agent[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#define-the-customer-support-agent "Direct link to Define the customer support agent")
We'll create a [LangGraph](https://langchain-ai.github.io/langgraph/) agent with limited access to our database. For demo purposes, our agent will support two basic types of requests:
  * Lookup: The customer can look up song titles, artist names, and albums based on other identifying information. For example: "What songs do you have by Jimi Hendrix?"
  * Refund: The customer can request a refund on their past purchases. For example: "My name is Claude Shannon and I'd like a refund on a purchase I made last week, could you help me?"


For simplicity in this demo, we'll implement refunds by deleting the corresponding database records. We'll skip implementing user authentication and other production security measures.
The agent's logic will be structured as two separate subgraphs (one for lookups and one for refunds), with a parent graph that routes requests to the appropriate subgraph.
#### Refund agent[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#refund-agent "Direct link to Refund agent")
Let's build the refund processing agent. This agent needs to:
  1. Find the customer's purchase records in the database
  2. Delete the relevant Invoice and InvoiceLine records to process the refund


We'll create two SQL helper functions:
  1. A function to execute the refund by deleting records
  2. A function to look up a customer's purchase history


To make testing easier, we'll add a "mock" mode to these functions. When mock mode is enabled, the functions will simulate database operations without actually modifying any data.
‚åµ
‚åµ
```
import sqlite3def_refund(invoice_id:int|None, invoice_line_ids:list[int]|None, mock:bool=False)->float:...def_lookup(...
```

Now let's define our graph. We'll use a simple architecture with three main paths:
  1. Extract customer and purchase information from the conversation
  2. Route the request to one of three paths: 
     * Refund path: If we have sufficient purchase details (Invoice ID or Invoice Line IDs) to process a refund
     * Lookup path: If we have enough customer information (name and phone) to search their purchase history
     * Response path: If we need more information, respond to the user requesting the specific details needed


The graph's state will track:
  * The conversation history (messages between user and agent)
  * All customer and purchase information extracted from the conversation
  * The next message to send to the user (followup text)


‚åµ
‚åµ
‚åµ
‚åµ
‚åµ
‚åµ
‚åµ
```
from typing import Literalimport jsonfrom langchain.chat_models import init_chat_modelfrom langchain_core.runnables import RunnableConfigfrom langgraph.graph import END, StateGraphfrom langgraph.graph.message import AnyMessage, add_messagesfrom langgraph.types import Command, interruptfrom tabulate import tabulatefrom typing_extensions import Annotated, TypedDict# Graph state.classState(TypedDict):"""Agent state."""  messages: Annotated[list[AnyMessage], add_messages]  followup:str|None  invoice_id:int|None  invoice_line_ids:list[int]|None  customer_first_name:str|None  customer_last_name:str|None  customer_phone:str|None  track_name:str|None  album_title:str|None  artist_name:str|None  purchase_date_iso_8601:str|None# Instructions for extracting the user/purchase info from the conversation.gather_info_instructions ="""You are managing an online music store that sells song tracks. \Customers can buy multiple tracks at a time and these purchases are recorded in a database as \an Invoice per purchase and an associated set of Invoice Lines for each purchased track.Your task is to help customers who would like a refund for one or more of the tracks they've \purchased. In order for you to be able refund them, the customer must specify the Invoice ID \to get a refund on all the tracks they bought in a single transaction, or one or more Invoice \Line IDs if they would like refunds on individual tracks.Often a user will not know the specific Invoice ID(s) or Invoice Line ID(s) for which they \would like a refund. In this case you can help them look up their invoices by asking them to \specify:- Required: Their first name, last name, and phone number.- Optionally: The track name, artist name, album name, or purchase date.If the customer has not specified the required information (either Invoice/Invoice Line IDs \or first name, last name, phone) then please ask them to specify it."""# Extraction schema, mirrors the graph state.classPurchaseInformation(TypedDict):"""All of the known information about the invoice / invoice lines the customer would like refunded. Do not make up values, leave fields as null if you don't know their value."""  invoice_id:int|None  invoice_line_ids:list[int]|None  customer_first_name:str|None  customer_last_name:str|None  customer_phone:str|None  track_name:str|None  album_title:str|None  artist_name:str|None  purchase_date_iso_8601:str|None  followup: Annotated[str|None,...,"If the user hasn't enough identifying information, please tell them what the required information is and ask them to specify it.",]# Model for performing extraction.info_llm = init_chat_model("gpt-4o-mini").with_structured_output(  PurchaseInformation, method="json_schema", include_raw=True)# Graph node for extracting user info and routing to lookup/refund/END.asyncdefgather_info(state: State)-> Command[Literal["lookup","refund", END]]:  info =await info_llm.ainvoke([{"role":"system","content": gather_info_instructions},*state["messages"],])  parsed = info["parsed"]ifany(parsed[k]for k in("invoice_id","invoice_line_ids")):    goto ="refund"elifall(    parsed[k]for k in("customer_first_name","customer_last_name","customer_phone")):    goto ="lookup"else:    goto = END  update ={"messages":[info["raw"]],**parsed}return Command(update=update, goto=goto)# Graph node for executing the refund.# Note that here we inspect the runtime config for an "env" variable.# If "env" is set to "test", then we don't actually delete any rows from our database.# This will become important when we're running our evaluations.defrefund(state: State, config: RunnableConfig)->dict:# Whether to mock the deletion. True if the configurable var 'env' is set to 'test'.  mock = config.get("configurable",{}).get("env","prod")=="test"  refunded = _refund(    invoice_id=state["invoice_id"], invoice_line_ids=state["invoice_line_ids"], mock=mock)  response =f"You have been refunded a total of: ${refunded:.2f}. Is there anything else I can help with?"return{"messages":[{"role":"assistant","content": response}],"followup": response,}# Graph node for looking up the users purchasesdeflookup(state: State)->dict:  args =(    state[k]for k in("customer_first_name","customer_last_name","customer_phone","track_name","album_title","artist_name","purchase_date_iso_8601",))  results = _lookup(*args)ifnot results:    response ="We did not find any purchases associated with the information you've provided. Are you sure you've entered all of your information correctly?"    followup = responseelse:    response =f"Which of the following purchases would you like to be refunded for?\n\n```json{json.dumps(results, indent=2)}\n```"    followup =f"Which of the following purchases would you like to be refunded for?\n\n{tabulate(results, headers='keys')}"return{"messages":[{"role":"assistant","content": response}],"followup": followup,"invoice_line_ids":[res["invoice_line_id"]for res in results],}# Building our graphgraph_builder = StateGraph(State)graph_builder.add_node(gather_info)graph_builder.add_node(refund)graph_builder.add_node(lookup)graph_builder.set_entry_point("gather_info")graph_builder.add_edge("lookup", END)graph_builder.add_edge("refund", END)refund_graph = graph_builder.compile()
```

We can visualize our refund graph:
‚åµ
```
# Assumes you're in an interactive Python environmentfrom IPython.display import Image, display ...
```

![Refund graph](https://docs.smith.langchain.com/assets/images/refund_graph-787b57e15d5eec1642ee69a47b3c3d0c.png)
#### Lookup agent[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#lookup-agent "Direct link to Lookup agent")
For the lookup (i.e. question-answering) agent, we'll use a simple ReACT architecture and give the agent tools for looking up track names, artist names, and album names based on various filters. For example, you can look up albums by a particular artist, artists who released songs with a specific name, etc.
‚åµ
‚åµ
‚åµ
‚åµ
‚åµ
‚åµ
```
from langchain.embeddings import init_embeddingsfrom langchain_core.tools import toolfrom langchain_core.vectorstores import InMemoryVectorStorefrom langgraph.prebuilt import create_react_agent# Our SQL queries will only work if we filter on the exact string values that are in the DB.# To ensure this, we'll create vectorstore indexes for all of the artists, tracks and albums# ahead of time and use those to disambiguate the user input. E.g. if a user searches for# songs by "prince" and our DB records the artist as "Prince", ideally when we query our# artist vectorstore for "prince" we'll get back the value "Prince", which we can then# use in our SQL queries.defindex_fields()->tuple[InMemoryVectorStore, InMemoryVectorStore, InMemoryVectorStore]:...track_store, artist_store, album_store = index_fields()# Agent tools@tooldeflookup_track(...@tooldeflookup_album(...@tooldeflookup_artist(...# Agent modelqa_llm = init_chat_model("claude-3-5-sonnet-latest")# The prebuilt ReACT agent only expects State to have a 'messages' key, so the# state we defined for the refund agent can also be passed to our lookup agent.qa_graph = create_react_agent(qa_llm,[lookup_track, lookup_artist, lookup_album])
```

```
display(Image(qa_graph.get_graph(xray=True).draw_mermaid_png()))
```

![QA Graph](https://docs.smith.langchain.com/evaluation/tutorials/agents)
#### Parent agent[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#parent-agent "Direct link to Parent agent")
Now let's define a parent agent that combines our two task-specific agents. The only job of the parent agent is to route to one of the sub-agents by classifying the user's current intent, and to compile the output into a followup message.
‚åµ
‚åµ
‚åµ
‚åµ
```
# Schema for routing user intent.# We'll use structured outputs to enforce that the model returns only# the desired output.classUserIntent(TypedDict):"""The user's current intent in the conversation"""  intent: Literal["refund","question_answering"]# Routing model with structured outputrouter_llm = init_chat_model("gpt-4o-mini").with_structured_output(  UserIntent, method="json_schema", strict=True)# Instructions for routing.route_instructions ="""You are managing an online music store that sells song tracks. \You can help customers in two types of ways: (1) answering general questions about \tracks sold at your store, (2) helping them get a refund on a purhcase they made at your store.Based on the following conversation, determine if the user is currently seeking general \information about song tracks or if they are trying to refund a specific purchase.Return 'refund' if they are trying to get a refund and 'question_answering' if they are \asking a general music question. Do NOT return anything else. Do NOT try to respond to \the user."""# Node for routing.asyncdefintent_classifier(  state: State,)-> Command[Literal["refund_agent","question_answering_agent"]]:  response = router_llm.invoke([{"role":"system","content": route_instructions},*state["messages"]])return Command(goto=response["intent"]+"_agent")# Node for making sure the 'followup' key is set before our agent run completes.defcompile_followup(state: State)->dict:"""Set the followup to be the last message if it hasn't explicitly been set."""ifnot state.get("followup"):return{"followup": state["messages"][-1].content}return{}# Agent definitiongraph_builder = StateGraph(State)graph_builder.add_node(intent_classifier)# Since all of our subagents have compatible state,# we can add them as nodes directly.graph_builder.add_node("refund_agent", refund_graph)graph_builder.add_node("question_answering_agent", qa_graph)graph_builder.add_node(compile_followup)graph_builder.set_entry_point("intent_classifier")graph_builder.add_edge("refund_agent","compile_followup")graph_builder.add_edge("question_answering_agent","compile_followup")graph_builder.add_edge("compile_followup", END)graph = graph_builder.compile()
```

We can visualize our compiled parent graph including all of its subgraphs:
```
display(Image(graph.get_graph().draw_mermaid_png()))
```

![graph](https://docs.smith.langchain.com/assets/images/agent_tutorial_graph-a1f868518e2642d1fdbb3ecfc7606a5c.png)
#### Try it out[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#try-it-out "Direct link to Try it out")
Let's give our custom support agent a whirl!
‚åµ
```
state =await graph.ainvoke({"messages":[{"role":"user","content":"what james brown songs do you have"}]})print(state["followup"])
```

‚åµ
```
I found 20 James Brown songs in the database, all from the album "Sex Machine". Here they are: ...
```

‚åµ
```
state =await graph.ainvoke({"messages":[{"role":"user","content":"my name is Aaron Mitchell and my number is +1 (204) 452-6452. I bought some songs by Led Zeppelin that i'd like refunded",}]})print(state["followup"])
```

‚åµ
```
Which of the following purchases would you like to be refunded for? ...
```

## Evaluations[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#evaluations "Direct link to Evaluations")
Now that we've got a testable version of our agent, let's run some evaluations. Agent evaluation can focus on at least 3 things:
  * [Final response](https://docs.smith.langchain.com/evaluation/concepts#evaluating-an-agents-final-response): The inputs are a prompt and an optional list of tools. The output is the final agent response.
  * [Trajectory](https://docs.smith.langchain.com/evaluation/concepts#evaluating-an-agents-trajectory): As before, the inputs are a prompt and an optional list of tools. The output is the list of tool calls
  * [Single step](https://docs.smith.langchain.com/evaluation/concepts#evaluating-a-single-step-of-an-agent): As before, the inputs are a prompt and an optional list of tools. The output is the tool call.


Let's run each type of evaluation:
### Final response evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#final-response-evaluator "Direct link to Final response evaluator")
First, let's create a [dataset](https://docs.smith.langchain.com/evaluation/concepts#datasets) that evaluates end-to-end performance of the agent. For simplicity we'll use the same dataset for final response and trajectory evaluation, so we'll add both ground-truth responses and trajectories for each example question. We'll cover the trajectories in the next section.
‚åµ
‚åµ
```
from langsmith import Clientclient = Client()# Create a datasetexamples =[{"inputs":{"question":"How many songs do you have by James Brown",},"outputs":{"response":"We have 20 songs by James Brown","trajectory":["question_answering_agent","lookup_track"]}},{"inputs":{"question":"My name is Aaron Mitchell and I'd like a refund.",},"outputs":{"response":"I need some more information to help you with the refund. Please specify your phone number, the invoice ID, or the line item IDs for the purchase you'd like refunded.","trajectory":["refund_agent"],}},{"inputs":{"question":"My name is Aaron Mitchell and I'd like a refund on my Led Zeppelin purchases. My number is +1 (204) 452-6452",},"outputs":{"response":'Which of the following purchases would you like to be refunded for?\n\n invoice_line_id track_name            artist_name  purchase_date     quantity_purchased  price_per_unit\n----------------- -------------------------------- ------------- ------------------- -------------------- ----------------\n       267 How Many More Times        Led Zeppelin  2009-08-06 00:00:00           1       0.99\n       268 What Is And What Should Never Be Led Zeppelin  2009-08-06 00:00:00           1       0.99',"trajectory":["refund_agent","lookup"],},},{"inputs":{"question":"Who recorded Wish You Were Here again? What other albums of there's do you have?",},"outputs":{"response":"Wish You Were Here is an album by Pink Floyd","trajectory":["question_answering_agent","lookup_album"],},},{"inputs":{"question":"I want a full refund for invoice 237",},"outputs":{"response":"You have been refunded $0.99.","trajectory":["refund_agent","refund"],}},]dataset_name ="Chinook Customer Service Bot: E2E"ifnot client.has_dataset(dataset_name=dataset_name):  dataset = client.create_dataset(dataset_name=dataset_name)  client.create_examples(    dataset_id=dataset.id,    examples=examples)
```

We'll create a custom [LLM-as-judge](https://docs.smith.langchain.com/evaluation/concepts#llm-as-judge) evaluator that uses another model to compare our agent's output on each example to the reference response, and judge if they're equivalent or not:
‚åµ
‚åµ
‚åµ
```
# LLM-as-judge instructionsgrader_instructions ="""You are a teacher grading a quiz.You will be given a QUESTION, the GROUND TRUTH (correct) RESPONSE, and the STUDENT RESPONSE.Here is the grade criteria to follow:(1) Grade the student responses based ONLY on their factual accuracy relative to the ground truth answer.(2) Ensure that the student response does not contain any conflicting statements.(3) It is OK if the student response contains more information than the ground truth response, as long as it is factually accurate relative to the ground truth response.Correctness:True means that the student's response meets all of the criteria.False means that the student's response does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct."""# LLM-as-judge output schemaclassGrade(TypedDict):"""Compare the expected and actual answers and grade the actual answer."""  reasoning: Annotated[str,...,"Explain your reasoning for whether the actual response is correct or not."]  is_correct: Annotated[bool,...,"True if the student response is mostly or exactly correct, otherwise False."]# Judge LLMgrader_llm = init_chat_model("gpt-4o-mini", temperature=0).with_structured_output(Grade, method="json_schema", strict=True)# Evaluator functionasyncdeffinal_answer_correct(inputs:dict, outputs:dict, reference_outputs:dict)->bool:"""Evaluate if the final response is equivalent to reference response."""# Note that we assume the outputs has a 'response' dictionary. We'll need to make sure# that the target function we define includes this key.  user =f"""QUESTION: {inputs['question']}  GROUND TRUTH RESPONSE: {reference_outputs['response']}  STUDENT RESPONSE: {outputs['response']}"""  grade =await grader_llm.ainvoke([{"role":"system","content": grader_instructions},{"role":"user","content": user}])return grade["is_correct"]
```

Now we can run our evaluation. Our evaluator assumes that our target function returns a 'response' key, so lets define a target function that does so.
Also remember that in our refund graph we made the refund node configurable, so that if we specified `config={"env": "test"}`, we would mock out the refunds without actually updating the DB. We'll use this configurable variable in our target `run_graph` method when invoking our graph:
‚åµ
```
# Target functionasyncdefrun_graph(inputs:dict)->dict:"""Run graph and track the trajectory it takes along with the final response."""  result =await graph.ainvoke({"messages":[{"role":"user","content": inputs['question']},]}, config={"env":"test"})return{"response": result["followup"]}# Evaluation job and resultsexperiment_results =await client.aevaluate(  run_graph,  data=dataset_name,  evaluators=[final_answer_correct],  experiment_prefix="sql-agent-gpt4o-e2e",  num_repetitions=1,  max_concurrency=4,)experiment_results.to_pandas()
```

You can see what these results look like here: [LangSmith link](https://smith.langchain.com/public/708d08f4-300e-4c75-9677-c6b71b0d28c9/d).
### Trajectory evaluator[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#trajectory-evaluator "Direct link to Trajectory evaluator")
As agents become more complex, they have more potential points of failure. Rather than using simple pass/fail evaluations, it's often better to use evaluations that can give partial credit when an agent takes some correct steps, even if it doesn't reach the right final answer.
This is where trajectory evaluations come in. A trajectory evaluation:
  1. Compares the actual sequence of steps the agent took against an expected sequence
  2. Calculates a score based on how many of the expected steps were completed correctly


For this example, our end-to-end dataset contains an ordered list of steps that we expect the agent to take. Let's create an evaluator that checks the agent's actual trajectory against these expected steps and calculates what percentage were completed:
‚åµ
```
deftrajectory_subsequence(outputs:dict, reference_outputs:dict)->float:"""Check how many of the desired steps the agent took."""iflen(reference_outputs['trajectory'])>len(outputs['trajectory']):returnFalse  i = j =0while i <len(reference_outputs['trajectory'])and j <len(outputs['trajectory']):if reference_outputs['trajectory'][i]== outputs['trajectory'][j]:      i +=1    j +=1return i /len(reference_outputs['trajectory'])
```

Now we can run our evaluation. Our evaluator assumes that our target function returns a 'trajectory' key, so lets define a target function that does so. We'll need to usage [LangGraph's streaming capabilities](https://langchain-ai.github.io/langgraph/concepts/streaming/) to record the trajectory.
Note that we are reusing the same dataset as for our final response evaluation, so we could have run both evaluators together and defined a target function that returns both "response" and "trajectory". In practice it's often useful to have separate datasets for each type of evaluation, which is why we show them separately here:
‚åµ
```
asyncdefrun_graph(inputs:dict)->dict:"""Run graph and track the trajectory it takes along with the final response."""  trajectory =[]# Set subgraph=True to stream events from subgraphs of the main graph: https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/# Set stream_mode="debug" to stream all possible events: https://langchain-ai.github.io/langgraph/concepts/streamingasyncfor namespace, chunk in graph.astream({"messages":[{"role":"user","content": inputs['question'],}]}, subgraphs=True, stream_mode="debug"):# Event type for entering a nodeif chunk['type']=='task':# Record the node name      trajectory.append(chunk['payload']['name'])# Given how we defined our dataset, we also need to track when specific tools are# called by our question answering ReACT agent. These tool calls can be found# when the ToolsNode (named "tools") is invoked by looking at the AIMessage.tool_calls# of the latest input message.if chunk['payload']['name']=='tools'and chunk['type']=='task':for tc in chunk['payload']['input']['messages'][-1].tool_calls:          trajectory.append(tc['name'])return{"trajectory": trajectory}experiment_results =await client.aevaluate(  run_graph,  data=dataset_name,  evaluators=[trajectory_subsequence],  experiment_prefix="sql-agent-gpt4o-trajectory",  num_repetitions=1,  max_concurrency=4,)experiment_results.to_pandas()
```

You can see what these results look like here: [LangSmith link](https://smith.langchain.com/public/708d08f4-300e-4c75-9677-c6b71b0d28c9/d).
### Single step evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#single-step-evaluators "Direct link to Single step evaluators")
While end-to-end tests give you the most signal about your agents performance, for the sake of debugging and iterating on your agent it can be helpful to pinpoint specific steps that are difficult and evaluate them directly.
In our case, a crucial part of our agent is that it routes the user's intention correctly into either the "refund" path or the "question answering" path. Let's create a dataset and run some evaluations to directly stress test this one component.
‚åµ
‚åµ
```
# Create datasetexamples =[{"inputs":{"messages":[{"role":"user","content":"i bought some tracks recently and i dont like them"}]},"outputs":{"route":"refund_agent"},},{"inputs":{"messages":[{"role":"user","content":"I was thinking of purchasing some Rolling Stones tunes, any recommendations?"}]},"outputs":{"route":"question_answering_agent"},},{"inputs":{"messages":[{"role":"user","content":"i want a refund on purchase 237"},{"role":"assistant","content":"I've refunded you a total of $1.98. How else can I help you today?"},{"role":"user","content":"did prince release any albums in 2000?"}]},"outputs":{"route":"question_answering_agent"},},{"inputs":{"messages":[{"role":"user","content":"i purchased a cover of Yesterday recently but can't remember who it was by, which versions of it do you have?"}]},"outputs":{"route":"question_answering_agent"},},]dataset_name ="Chinook Customer Service Bot: Intent Classifier"ifnot client.has_dataset(dataset_name=dataset_name):  dataset = client.create_dataset(dataset_name=dataset_name)  client.create_examples(    dataset_id=dataset.id,    examples=examples)# Evaluatordefcorrect(outputs:dict, reference_outputs:dict)->bool:"""Check if the agent chose the correct route."""return outputs["route"]== reference_outputs["route"]# Target function for running the relevant stepasyncdefrun_intent_classifier(inputs:dict)->dict:# Note that we can access and run the intent_classifier node of our graph directly.  command =await graph.nodes['intent_classifier'].ainvoke(inputs)return{"route": command.goto}# Run evaluationexperiment_results =await client.aevaluate(  run_intent_classifier,  data=dataset_name,  evaluators=[correct],  experiment_prefix="sql-agent-gpt4o-intent-classifier",  max_concurrency=4,)
```

You can see what these results look like here: [LangSmith link](https://smith.langchain.com/public/f133dae2-8a88-43a0-9bfd-ab45bfa3920b/d).
## Reference code[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/agents#reference-code "Direct link to Reference code")
Here's a consolidated script with all the above code:
‚åµ
```
import json ...
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/tutorials/agents%3E).
[PreviousRunning SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)[NextTest a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
  * [Setup](https://docs.smith.langchain.com/evaluation/tutorials/agents#setup)
    * [Configure the environment](https://docs.smith.langchain.com/evaluation/tutorials/agents#configure-the-environment)
    * [Download the database](https://docs.smith.langchain.com/evaluation/tutorials/agents#download-the-database)
    * [Define the customer support agent](https://docs.smith.langchain.com/evaluation/tutorials/agents#define-the-customer-support-agent)
  * [Evaluations](https://docs.smith.langchain.com/evaluation/tutorials/agents#evaluations)
    * [Final response evaluator](https://docs.smith.langchain.com/evaluation/tutorials/agents#final-response-evaluator)
    * [Trajectory evaluator](https://docs.smith.langchain.com/evaluation/tutorials/agents#trajectory-evaluator)
    * [Single step evaluators](https://docs.smith.langchain.com/evaluation/tutorials/agents#single-step-evaluators)
  * [Reference code](https://docs.smith.langchain.com/evaluation/tutorials/agents#reference-code)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/tutorials/backtesting

[Skip to main content](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
  * Run backtests on a new version of an agent


On this page
# Run backtests on a new version of an agent
Deploying your application is just the beginning of a continuous improvement process. After you deploy to production, you'll want to refine your system by enhancing prompts, language models, tools, and architectures. Backtesting involves assessing new versions of your application using historical data and comparing the new outputs to the original ones. Compared to evaluations using pre-production datasets, backtesting offers a clearer indication of whether the new version of your application is an improvement over the current deployment.
Here are the basic steps for backtesting:
  1. Select sample runs from your production tracing project to test against.
  2. Transform the run inputs into a dataset and record the run outputs as an initial experiment against that dataset.
  3. Execute your new system on the new dataset and compare the results of the experiments.


This process will provide you with a new dataset of representative inputs, which you can version and use for backtesting your models.
Ground truth data
Often, you won't have definitive "ground truth" answers available. In such cases, you can manually label the outputs or use evaluators that don't rely on reference data. If your application allows for capturing ground-truth labels, for example by allowing users to leave feedback, we strongly recommend doing so.
## Setup[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#setup "Direct link to Setup")
### Configure the environment[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#configure-the-environment "Direct link to Configure the environment")
Install and set environment variables. This guide requires `langsmith>=0.2.4`.
Optional LangChain usage
For convenience we'll use the LangChain OSS framework in this tutorial, but the LangSmith functionality shown is framework-agnostic.
```
pip install -U langsmith langchain langchain-anthropic langchainhub emoji
```

```
import getpassimport os# Set the project name to whichever project you'd like to be testing againstproject_name ="Tweet Writing Task"os.environ["LANGSMITH_PROJECT"]= project_nameos.environ["LANGSMITH_TRACING"]="true"ifnot os.environ.get("LANGSMITH_API_KEY"):  os.environ["LANGSMITH_API_KEY"]= getpass.getpass("YOUR API KEY")# Optional. You can swap OpenAI for any other tool-calling chat model.os.environ["OPENAI_API_KEY"]="YOUR OPENAI API KEY"# Optional. You can swap Tavily for the free DuckDuckGo search tool if preferred.# Get Tavily API key: https://tavily.comos.environ["TAVILY_API_KEY"]="YOUR TAVILY API KEY"
```

### Define the application[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#define-the-application "Direct link to Define the application")
For this example lets create a simple Tweet-writing application that has access to some internet search tools:
```
from langchain.chat_models import init_chat_modelfrom langgraph.prebuilt import create_react_agentfrom langchain_community.tools import DuckDuckGoSearchRun, TavilySearchResultsfrom langchain_core.rate_limiters import InMemoryRateLimiter# We will use GPT-3.5 Turbo as the baseline and compare against GPT-4ogpt_3_5_turbo = init_chat_model("gpt-3.5-turbo",  temperature=1,  configurable_fields=("model","model_provider"),)# The instrucitons are passed as a system message to the agentinstructions ="""You are a tweet writing assistant. Given a topic, do some research and write a relevant and engaging tweet about it.- Use at least 3 emojis in each tweet- The tweet should be no longer than 280 characters- Always use the search tool to gather recent information on the tweet topic- Write the tweet only based on the search content. Do not rely on your internal knowledge- When relevant, link to your sources- Make your tweet as engaging as possible"""# Define the tools our agent can use# If you have a higher tiered Tavily API plan you can increase thisrate_limiter = InMemoryRateLimiter(requests_per_second=0.08)# Use DuckDuckGo if you don't have a Tavily API key:# tools = [DuckDuckGoSearchRun(rate_limiter=rate_limiter)]tools =[TavilySearchResults(max_results=5, rate_limiter=rate_limiter)]agent = create_react_agent(gpt_3_5_turbo, tools=tools, state_modifier=instructions)
```

### Simulate production data[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#simulate-production-data "Direct link to Simulate production data")
Now lets simulate some production data:
```
fake_production_inputs =["Alan turing's early childhood","Economic impacts of the European Union","Underrated philosophers","History of the Roxie theater in San Francisco","ELI5: gravitational waves","The arguments for and against a parliamentary system","Pivotal moments in music history","Big ideas in programming languages","Big questions in biology","The relationship between math and reality","What makes someone funny",]agent.batch([{"messages":[{"role":"user","content": content}]}for content in fake_production_inputs],)
```

## Convert Production Traces to Experiment[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#convert-production-traces-to-experiment "Direct link to Convert Production Traces to Experiment")
The first step is to generate a dataset based on the production _inputs_. Then copy over all the traces to serve as a baseline experiment.
### Select runs to backtest on[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#select-runs-to-backtest-on "Direct link to Select runs to backtest on")
You can select the runs to backtest on using the `filter` argument of `list_runs`. The `filter` argument uses the LangSmith [trace query syntax](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax) to select runs.
```
from datetime import datetime, timedelta, timezonefrom uuid import uuid4from langsmith import Clientfrom langsmith.beta import convert_runs_to_test# Fetch the runs we want to convert to a dataset/experimentclient = Client()# How we are sampling runs to include in our datasetend_time = datetime.now(tz=timezone.utc)start_time = end_time - timedelta(days=1)run_filter =f'and(gt(start_time, "{start_time.isoformat()}"), lt(end_time, "{end_time.isoformat()}"))'prod_runs =list(  client.list_runs(    project_name=project_name,    execution_order=1,filter=run_filter,))
```

### Convert runs to experiment[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#convert-runs-to-experiment "Direct link to Convert runs to experiment")
`convert_runs_to_test` is a function which takes some runs and does the following:
  1. The inputs, and optionally the outputs, are saved to a dataset as Examples.
  2. The inputs and outputs are stored as an experiment, as if you had run the `evaluate` function and received those outputs.


```
# Name of the dataset we want to createdataset_name =f'{project_name}-backtesting {start_time.strftime("%Y-%m-%d")}-{end_time.strftime("%Y-%m-%d")}'# Name of the experiment we want to create from the historical runsbaseline_experiment_name =f"prod-baseline-gpt-3.5-turbo-{str(uuid4())[:4]}"# This converts the runs to a dataset + experimentconvert_runs_to_test(  prod_runs,# Name of the resulting dataset  dataset_name=dataset_name,# Whether to include the run outputs as reference/ground truth  include_outputs=False,# Whether to include the full traces in the resulting experiment# (default is to just include the root run)  load_child_runs=True,# Name of the experiment so we can apply evalautors to it after  test_project_name=baseline_experiment_name)
```

Once this step is complete, you should see a new dataset in your LangSmith project called "Tweet Writing Task-backtesting TODAYS DATE", with a single experiment like so:
![](https://docs.smith.langchain.com/assets/images/baseline_experiment-bdf044e766b1388dfc5fc305bfafc174.png)
## Benchmark against new system[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#benchmark-against-new-system "Direct link to Benchmark against new system")
Now we can start the process of benchmarking our production runs against a new system.
### Define evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#define-evaluators "Direct link to Define evaluators")
First let's define the evaluators we will use to compare the two systems. Note that we have no reference outputs, so we'll need to come up with evaluation metrics that only require the actual outputs.
```
import emojifrom pydantic import BaseModel, Fieldfrom langchain_core.messages import convert_to_openai_messagesclassGrade(BaseModel):"""Grade whether a response is supported by some context."""  grounded:bool= Field(..., description="Is the majority of the response supported by the retrieved context?")grounded_instructions =f"""You have given somebody some contextual information and asked them to write a statement grounded in that context.Grade whether their response is fully supported by the context you have provided. \If any meaningful part of their statement is not backed up directly by the context you provided, then their response is not grounded. \Otherwise it is grounded."""grounded_model = init_chat_model(model="gpt-4o").with_structured_output(Grade)deflt_280_chars(outputs:dict)->bool:  messages = convert_to_openai_messages(outputs["messages"])returnlen(messages[-1]['content'])<=280defgte_3_emojis(outputs:dict)->bool:  messages = convert_to_openai_messages(outputs["messages"])returnlen(emoji.emoji_list(messages[-1]['content']))>=3asyncdefis_grounded(outputs:dict)->bool:  context =""  messages = convert_to_openai_messages(outputs["messages"])for message in messages:if message["role"]=="tool":# Tool message outputs are the results returned from the Tavily/DuckDuckGo tool      context +="\n\n"+ message["content"]  tweet = messages[-1]["content"]  user =f"""CONTEXT PROVIDED:{context}  RESPONSE GIVEN:{tweet}"""  grade =await grounded_model.ainvoke([{"role":"system","content": grounded_instructions},{"role":"user","content": user}])return grade.grounded
```

### Evaluate baseline[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#evaluate-baseline "Direct link to Evaluate baseline")
Now, let's run our evaluators against the baseline experiment.
```
baseline_results =await client.aevaluate(  baseline_experiment_name,  evaluators=[lt_280_chars, gte_3_emojis, is_grounded],)# If you have pandas installed can easily explore results as df:# baseline_results.to_pandas()
```

### Define and evaluate new system[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#define-and-evaluate-new-system "Direct link to Define and evaluate new system")
Now, let's define and evaluate our new system. In this example our new system will be the same as the old system, but will use GPT-4o instead of GPT-3.5. Since we've made our model configurable we can just update the default config passed to our agent:
```
candidate_results =await client.aevaluate(  agent.with_config(model="gpt-4o"),  data=dataset_name,  evaluators=[lt_280_chars, gte_3_emojis, is_grounded],  experiment_prefix="candidate-gpt-4o",)# If you have pandas installed can easily explore results as df:# candidate_results.to_pandas()
```

## Comparing the results[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#comparing-the-results "Direct link to Comparing the results")
After running both experiments, you can view them in your dataset:
![](https://docs.smith.langchain.com/assets/images/dataset_page-cbee19347876623d848a2397bb5c0699.png)
The results reveal an interesting tradeoff between the two models:
  1. GPT-4o shows improved performance in following formatting rules, consistently including the requested number of emojis
  2. However, GPT-4o is less reliable at staying grounded in the provided search results


To illustrate the grounding issue: in [this example run](https://smith.langchain.com/public/be060e19-0bc0-4798-94f5-c3d35719a5f6/r/07d43e7a-8632-479d-ae28-c7eac6e54da4), GPT-4o included facts about Ab≈´ Bakr Muhammad ibn ZakariyyƒÅ al-RƒÅzƒ´'s medical contributions that weren't present in the search results. This demonstrates how it's pulling from its internal knowledge rather than strictly using the provided information.
This backtesting exercise revealed that while GPT-4o is generally considered a more capable model, simply upgrading to it wouldn't improve our tweet-writer. To effectively use GPT-4o, we would need to:
  * Refine our prompts to more strongly emphasize using only provided information
  * Or modify our system architecture to better constrain the model's outputs


This insight demonstrates the value of backtesting - it helped us identify potential issues before deployment.
![](https://docs.smith.langchain.com/assets/images/comparison_view-8851f078ea187d45278235c7605ffec4.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/tutorials/backtesting%3E).
[PreviousEvaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)[NextRunning SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
  * [Setup](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#setup)
    * [Configure the environment](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#configure-the-environment)
    * [Define the application](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#define-the-application)
    * [Simulate production data](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#simulate-production-data)
  * [Convert Production Traces to Experiment](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#convert-production-traces-to-experiment)
    * [Select runs to backtest on](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#select-runs-to-backtest-on)
    * [Convert runs to experiment](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#convert-runs-to-experiment)
  * [Benchmark against new system](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#benchmark-against-new-system)
    * [Define evaluators](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#define-evaluators)
    * [Evaluate baseline](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#evaluate-baseline)
    * [Define and evaluate new system](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#define-and-evaluate-new-system)
  * [Comparing the results](https://docs.smith.langchain.com/evaluation/tutorials/backtesting#comparing-the-results)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/tutorials/evaluation

[Skip to main content](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
  * Evaluate a chatbot


On this page
# Evaluate a chatbot
In this guide we will set up evaluations for a chatbot. These allow you to measure how well your application is performing over a **fixed** set of data. Being able to get this insight quickly and reliably will allow you to iterate with confidence.
At a high level, in this tutorial we will:
  * _Create an initial golden dataset to measure performance_
  * _Define metrics to use to measure performance_
  * _Run evaluations on a few different prompts or models_
  * _Compare results manually_
  * _Track results over time_
  * _Set up automated testing to run in CI/CD_


For more information on the evaluation workflows LangSmith supports, check out the [how-to guides](https://docs.smith.langchain.com/evaluation/how_to_guides), or see the reference docs for [evaluate](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) and its asynchronous [aevaluate](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) counterpart.
Lots to cover, let's dive in!
## Setup[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#setup "Direct link to Setup")
First install the required dependencies for this tutorial. We happen to use OpenAI, but LangSmith can be used with any model:
```
pip install -U langsmith openai
```

And set environment variables to enable LangSmith tracing:
```
export LANGSMITH_TRACING="true"export LANGSMITH_API_KEY="<Your LangSmith API key>"export OPENAI_API_KEY="<Your OpenAI API key>"
```

## Create a dataset[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#create-a-dataset "Direct link to Create a dataset")
The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:
  * What should the schema of each datapoint be?
  * How many datapoints should I gather?
  * How should I gather those datapoints?


**Schema:** Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that's okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.
**How many:** There's no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don't worry about getting a large number to start - you can (and should) always add over time!
**How to get:** This is maybe the trickiest part. Once you know you want to gather a dataset... how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally _living_ constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling ~10-20 examples.
Once you've got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).
For this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let's show how to create and upload this dataset to LangSmith!
```
from langsmith import Clientclient = Client()# Define dataset: these are your test casesdataset_name ="QA Example Dataset"dataset = client.create_dataset(dataset_name)client.create_examples(  dataset_id=dataset.id,  examples=[{"inputs":{"question":"What is LangChain?"},"outputs":{"answer":"A framework for building LLM applications"},},{"inputs":{"question":"What is LangSmith?"},"outputs":{"answer":"A platform for observing and evaluating LLM applications"},},{"inputs":{"question":"What is OpenAI?"},"outputs":{"answer":"A company that creates Large Language Models"},},{"inputs":{"question":"What is Google?"},"outputs":{"answer":"A technology company known for search"},},{"inputs":{"question":"What is Mistral?"},"outputs":{"answer":"A company that creates Large Language Models"},}])
```

Now, if we go the LangSmith UI and look for `QA Example Dataset` in the `Datasets & Testing` page, when we click into it we should see that we have five new examples.
![](https://docs.smith.langchain.com/assets/images/testing_tutorial_dataset-861cb88c2da002ca7122cf70e57ec3dd.png)
## Define metrics[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#define-metrics "Direct link to Define metrics")
After creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those **exact** answers, but rather something that is similar. This makes our evaluation a little trickier.
In addition to evaluating correctness, let's also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response.
Let's go ahead and define these two metrics.
For the first, we will use an LLM to **judge** whether the output is correct (with respect to the expected output). This **LLM-as-a-judge** is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:
```
import openaifrom langsmith import wrappersopenai_client = wrappers.wrap_openai(openai.OpenAI())eval_instructions ="You are an expert professor specialized in grading students' answers to questions."defcorrectness(inputs:dict, outputs:dict, reference_outputs:dict)->bool:  user_content =f"""You are grading the following question:{inputs['question']}Here is the real answer:{reference_outputs['answer']}You are grading the following predicted answer:{outputs['response']}Respond with CORRECT or INCORRECT:Grade:"""  response = openai_client.chat.completions.create(    model="gpt-4o-mini",    temperature=0,    messages=[{"role":"system","content": eval_instructions},{"role":"user","content": user_content},],).choices[0].message.contentreturn response =="CORRECT"
```

For evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.
```
defconcision(outputs:dict, reference_outputs:dict)->bool:returnint(len(outputs["response"])<2*len(reference_outputs["answer"]))
```

## Run Evaluations[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#run-evaluations "Direct link to Run Evaluations")
Great! So now how do we run evaluations? Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly:
```
default_instructions ="Respond to the users question in a short, concise manner (one short sentence)."defmy_app(question:str, model:str="gpt-4o-mini", instructions:str= default_instructions)->str:return openai_client.chat.completions.create(    model=model,    temperature=0,    messages=[{"role":"system","content": instructions},{"role":"user","content": question},],).choices[0].message.content
```

Before running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call, and then also maps the output of the function to the output key we expect.
```
defls_target(inputs:str)->dict:return{"response": my_app(inputs["question"])}
```

Great! Now we're ready to run an evaluation. Let's do it!
```
experiment_results = client.evaluate(  ls_target,# Your AI system  data=dataset_name,# The data to predict and grade over  evaluators=[concision, correctness],# The evaluators to score the results  experiment_prefix="openai-4o-mini",# A prefix for your experiment names to easily identify them)
```

This will output a URL. If we click on it, we should see results of our evaluation!
![](https://docs.smith.langchain.com/assets/images/testing_tutorial_run-9c4589e564c0cd6dd52200d0cf9ae891.png)
If we go back to the dataset page and select the `Experiments` tab, we can now see a summary of our one run!
![](https://docs.smith.langchain.com/assets/images/testing_tutorial_one_run-9798a22f5c1026ffbb78dbcbfe962c33.png)
Let's now try it out with a different model! Let's try `gpt-4-turbo`
```
defls_target_v2(inputs:str)->dict:return{"response": my_app(inputs["question"], model="gpt-4-turbo")}experiment_results = client.evaluate(  ls_target_v2,  data=dataset_name,  evaluators=[concision, correctness],  experiment_prefix="openai-4-turbo",)
```

And now let's use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short.
```
instructions_v3 ="Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words."defls_target_v3(inputs:str)->dict:  response = my_app(    inputs["question"],    model="gpt-4-turbo",    instructions=instructions_v3)return{"response": response}experiment_results = client.evaluate(  ls_target_v3,  data=dataset_name,  evaluators=[concision, correctness],  experiment_prefix="strict-openai-4-turbo",)
```

If we go back to the `Experiments` tab on the datasets page, we should see that all three runs now show up!
![](https://docs.smith.langchain.com/assets/images/testing_tutorial_three_runs-892234662680d74dd07cdcf18d347db6.png)
## Comparing results[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#comparing-results "Direct link to Comparing results")
Awesome, we've evaluated three different runs. But how can we compare results? The first way we can do this is just by looking at the runs in the `Experiments` tab. If we do that, we can see a high level view of the metrics for each run:
![](https://docs.smith.langchain.com/assets/images/testing_tutorial_compare_metrics-aa6bb37f038ce2968b990ea4653de40e.png)
Great! So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length. But what if we want to explore in more detail?
In order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view. We immediately see all three tests side by side. Some of the cells are color coded - this is showing a regression of _a certain metric_ compared to _a certain baseline_. We automatically choose defaults for the baseline and metric, but you can change those yourself. You can also choose which columns and which metrics you see by using the `Display` control. You can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top.
![](https://docs.smith.langchain.com/assets/images/testing_tutorial_compare_runs-5adf70a58406c235eede8e5b78664297.png)
If we want to see more information, we can also select the `Expand` button that appears when hovering over a row to open up a side panel with more detailed information:
![](https://docs.smith.langchain.com/assets/images/testing_tutorial_side_panel-9b18ee3873c74e0c9bfaf2378559b8e0.png)
## Set up automated testing to run in CI/CD[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#set-up-automated-testing-to-run-in-cicd "Direct link to Set up automated testing to run in CI/CD")
Now that we've run this in a one-off manner, we can set it to run in an automated fashion. We can do this pretty easily by just including it as a pytest file that we run in CI/CD. As part of this, we can either just log the results OR set up some criteria to determine if it passes or not. For example, if I wanted to ensure that we always got at least 80% of generated responses passing the `length` check, we could set that up with a test like:
```
deftest_length_score()->None:"""Test that the length score is at least 80%."""  experiment_results = evaluate(    ls_target,# Your AI system    data=dataset_name,# The data to predict and grade over    evaluators=[concision, correctness],# The evaluators to score the results)# This will be cleaned up in the next release:  feedback = client.list_feedback(    run_ids=[r.idfor r in client.list_runs(project_name=experiment_results.experiment_name)],    feedback_key="concision")  scores =[f.score for f in feedback]assertsum(scores)/len(scores)>=0.8,"Aggregate score should be at least .8"
```

## Track results over time[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#track-results-over-time "Direct link to Track results over time")
Now that we've got these experiments running in an automated fashion, we want to track these results over time. We can do this from the overall `Experiments` tab in the datasets page. By default, we show evaluation metrics over time (highlighted in red). We also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow).
![](https://docs.smith.langchain.com/assets/images/testing_tutorial_over_time-714599aa0286130ff1383c35e6d0c5e9.png)
## Conclusion[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#conclusion "Direct link to Conclusion")
That's it for this tutorial!
We've gone over how to create an initial test set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD, and track results over time. Hopefully this can help you iterate with confidence.
This is just the start. As mentioned earlier, evaluation is an ongoing process. For example - the datapoints you will want to evaluate on will likely continue to change over time. There are many types of evaluators you may wish to explore. For information on this, check out the [how-to guides](https://docs.smith.langchain.com/evaluation/how_to_guides).
Additionally, there are other ways to evaluate data besides in this "offline" manner (e.g. you can evaluate production data). For more information on online evaluation, check out [this guide](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations).
## Reference code[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#reference-code "Direct link to Reference code")
Click to see a consolidated code snippet
```
import openaifrom langsmith import Client, wrappers# Application codeopenai_client = wrappers.wrap_openai(openai.OpenAI())default_instructions ="Respond to the users question in a short, concise manner (one short sentence)."defmy_app(question:str, model:str="gpt-4o-mini", instructions:str= default_instructions)->str:return openai_client.chat.completions.create(    model=model,    temperature=0,    messages=[{"role":"system","content": instructions},{"role":"user","content": question},],).choices[0].message.contentclient = Client()# Define dataset: these are your test casesdataset_name ="QA Example Dataset"dataset = client.create_dataset(dataset_name)client.create_examples(  dataset_id=dataset.id,  examples=[{"inputs":{"question":"What is LangChain?"},"outputs":{"answer":"A framework for building LLM applications"},},{"inputs":{"question":"What is LangSmith?"},"outputs":{"answer":"A platform for observing and evaluating LLM applications"},},{"inputs":{"question":"What is OpenAI?"},"outputs":{"answer":"A company that creates Large Language Models"},},{"inputs":{"question":"What is Google?"},"outputs":{"answer":"A technology company known for search"},},{"inputs":{"question":"What is Mistral?"},"outputs":{"answer":"A company that creates Large Language Models"},}])# Define evaluatorseval_instructions ="You are an expert professor specialized in grading students' answers to questions."defcorrectness(inputs:dict, outputs:dict, reference_outputs:dict)->bool:  user_content =f"""You are grading the following question:{inputs['question']}Here is the real answer:{reference_outputs['answer']}You are grading the following predicted answer:{outputs['response']}Respond with CORRECT or INCORRECT:Grade:"""  response = openai_client.chat.completions.create(    model="gpt-4o-mini",    temperature=0,    messages=[{"role":"system","content": eval_instructions},{"role":"user","content": user_content},],).choices[0].message.contentreturn response =="CORRECT"defconcision(outputs:dict, reference_outputs:dict)->bool:returnint(len(outputs["response"])<2*len(reference_outputs["answer"]))# Run evaluationsdefls_target(inputs:str)->dict:return{"response": my_app(inputs["question"])}experiment_results_v1 = client.evaluate(  ls_target,# Your AI system  data=dataset_name,# The data to predict and grade over  evaluators=[concision, correctness],# The evaluators to score the results  experiment_prefix="openai-4o-mini",# A prefix for your experiment names to easily identify them)defls_target_v2(inputs:str)->dict:return{"response": my_app(inputs["question"], model="gpt-4-turbo")}experiment_results_v2 = client.evaluate(  ls_target_v2,  data=dataset_name,  evaluators=[concision, correctness],  experiment_prefix="openai-4-turbo",)instructions_v3 ="Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words."defls_target_v3(inputs:str)->dict:  response = my_app(    inputs["question"],    model="gpt-4-turbo",    instructions=instructions_v3)return{"response": response}experiment_results_v3 = client.evaluate(  ls_target_v3,  data=dataset_name,  evaluators=[concision, correctness],  experiment_prefix="strict-openai-4-turbo",)
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/tutorials/evaluation%3E).
[PreviousEvaluation tutorials](https://docs.smith.langchain.com/evaluation/tutorials)[NextEvaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
  * [Setup](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#setup)
  * [Create a dataset](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#create-a-dataset)
  * [Define metrics](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#define-metrics)
  * [Run Evaluations](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#run-evaluations)
  * [Comparing results](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#comparing-results)
  * [Set up automated testing to run in CI/CD](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#set-up-automated-testing-to-run-in-cicd)
  * [Track results over time](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#track-results-over-time)
  * [Conclusion](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#conclusion)
  * [Reference code](https://docs.smith.langchain.com/evaluation/tutorials/evaluation#reference-code)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/tutorials/rag

[Skip to main content](https://docs.smith.langchain.com/evaluation/tutorials/rag#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/tutorials/rag)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
  * Evaluate a RAG application


On this page
# Evaluate a RAG application
Key concepts
[RAG evaluation](https://docs.smith.langchain.com/evaluation/concepts#retrieval-augmented-generation-rag) | [Evaluators](https://docs.smith.langchain.com/evaluation/concepts#evaluators) | [LLM-as-judge evaluators](https://docs.smith.langchain.com/evaluation/concepts#llm-as-judge)
Retrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge. It has become one of the most widely used approaches for building LLM applications.
This tutorial will show you how to evaluate your RAG applications using LangSmith. You'll learn:
  1. How to create test datasets
  2. How to run your RAG application on those datasets
  3. How to measure your application's performance using different evaluation metrics


## Overview[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#overview "Direct link to Overview")
A typical RAG evaluation workflow consists of three main steps:
  1. Creating a dataset with questions and their expected answers
  2. Running your RAG application on those questions
  3. Using evaluators to measure how well your application performed, looking at factors like: 
     * Answer relevance
     * Answer accuracy
     * Retrieval quality


For this tutorial, we'll create and evaluate a bot that answers questions about a few of [Lilian Weng's](https://lilianweng.github.io/) insightful blog posts.
## Setup[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#setup "Direct link to Setup")
### Environment[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#environment "Direct link to Environment")
First, let's set our environment variables:
  * Python
  * TypeScript


```
import osos.environ["LANGSMITH_TRACING"]="true"os.environ["LANGSMITH_API_KEY"]="YOUR LANGSMITH API KEY"os.environ["OPENAI_API_KEY"]="YOUR OPENAI API KEY"
```

```
process.env.LANGSMITH_TRACING="true";process.env.LANGSMITH_API_KEY="YOUR LANGSMITH API KEY";process.env.OPENAI_API_KEY="YOUR OPENAI API KEY";
```

And install the dependencies we'll need:
  * Python
  * TypeScript


```
pip install -U langsmith langchain[openai] langchain-community
```

```
yarn add langsmith langchain @langchain/community @langchain/openai
```

### Application[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#application "Direct link to Application")
Framework Flexibility
While this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and libraries.
In this section, we'll build a basic Retrieval-Augmented Generation (RAG) application.
We'll stick to a simple implementation that:
  * Indexing: chunks and indexes a few of Lilian Weng's blogs in a vector store
  * Retrieval: retrieves those chunks based on the user question
  * Generation: passes the question and retrieved docs to an LLM.


#### Indexing and retrieval[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#indexing-and-retrieval "Direct link to Indexing and retrieval")
First, lets load the blog posts we want to build a chatbot for and index them.
  * Python
  * TypeScript


```
from langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitter# List of URLs to load documents fromurls =["https://lilianweng.github.io/posts/2023-06-23-agent/","https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/","https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",]# Load documents from the URLsdocs =[WebBaseLoader(url).load()for url in urls]docs_list =[item for sublist in docs for item in sublist]# Initialize a text splitter with specified chunk size and overlaptext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(  chunk_size=250, chunk_overlap=0)# Split the documents into chunksdoc_splits = text_splitter.split_documents(docs_list)# Add the document chunks to the "vector store" using OpenAIEmbeddingsvectorstore = InMemoryVectorStore.from_documents(  documents=doc_splits,  embedding=OpenAIEmbeddings(),)# With langchain we can easily turn any vector store into a retrieval component:retriever = vectorstore.as_retriever(k=6)
```

```
import{ OpenAIEmbeddings }from"@langchain/openai";import{ MemoryVectorStore }from"langchain/vectorstores/memory";import{ BrowserbaseLoader }from"@langchain/community/document_loaders/web/browserbase";// List of URLs to load documents fromconst urls =["https://lilianweng.github.io/posts/2023-06-23-agent/","https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/","https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",];const loader =newBrowserbaseLoader(urls,{ textContent:true,});const docs =await loader.load();const splitter =newRecursiveCharacterTextSplitter({ chunkSize:1000, chunkOverlap:200,});const allSplits =await splitter.splitDocuments(docs);const embeddings =newOpenAIEmbeddings({ model:"text-embedding-3-large",});const vectorStore =newMemoryVectorStore(embeddings);// Index chunksawait vectorStore.addDocuments(allSplits);
```

#### Generation[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#generation "Direct link to Generation")
We can now define the generative pipeline.
  * Python
  * TypeScript


```
from langchain_openai import ChatOpenAIfrom langsmith import traceablellm = ChatOpenAI(model="gpt-4o", temperature=1)# Add decorator so this function is traced in LangSmith@traceable()defrag_bot(question:str)->dict:# LangChain retriever will be automatically traced  docs = retriever.invoke(question)  docs_string = "".join(doc.page_content for doc in docs)  instructions =f"""You are a helpful assistant who is good at analyzing source information and answering questions.    Use the following source documents to answer the user's questions.    If you don't know the answer, just say that you don't know.    Use three sentences maximum and keep the answer concise.Documents:{docs_string}"""# langchain ChatModel will be automatically traced  ai_msg = llm.invoke([{"role":"system","content": instructions},{"role":"user","content": question},],)return{"answer": ai_msg.content,"documents": docs}
```

```
import{ ChatOpenAI }from"@langchain/openai";import{ traceable }from"langsmith/traceable";const llm =newChatOpenAI({ model:"gpt-4o", temperature:1,})// Add decorator so this function is traced in LangSmithconst ragBot =traceable(async(question:string)=>{// LangChain retriever will be automatically tracedconst retrievedDocs =await vectorStore.similaritySearch(question);const docsContent = retrievedDocs.map((doc)=> doc.pageContent).join("");const instructions =`You are a helpful assistant who is good at analyzing source information and answering questions    Use the following source documents to answer the user's questions.    If you don't know the answer, just say that you don't know.    Use three sentences maximum and keep the answer concise.    Documents:${docsContent}`;const aiMsg =await llm.invoke([{        role:"system",        content: instructions},{        role:"user",        content: question}])return{"answer": aiMsg.content,"documents": retrievedDocs}})
```

## Dataset[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#dataset "Direct link to Dataset")
Now that we've got our application, let's build a dataset to evaluate it. Our dataset will be very simple in this case: we'll have example questions and reference answers.
  * Python
  * TypeScript


```
from langsmith import Clientclient = Client()# Define the examples for the datasetexamples =[{"inputs":{"question":"How does the ReAct agent use self-reflection? "},"outputs":{"answer":"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs."},},{"inputs":{"question":"What are the types of biases that can arise with few-shot prompting?"},"outputs":{"answer":"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias."},},{"inputs":{"question":"What are five types of adversarial attacks?"},"outputs":{"answer":"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming."},}]# Create the dataset and examples in LangSmithdataset_name ="Lilian Weng Blogs Q&A"dataset = client.create_dataset(dataset_name=dataset_name)client.create_examples(  dataset_id=dataset.id,  examples=examples)
```

```
import{ Client }from"langsmith";const client =newClient();// Define the examples for the datasetconst examples =[["How does the ReAct agent use self-reflection? ","ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.",],["What are the types of biases that can arise with few-shot prompting?","The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.",],["What are five types of adversarial attacks?","Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.",],];const[inputs, outputs]= examples.reduce<[Array<{ input:string}>,Array<{ outputs:string}>]>(([inputs, outputs], item)=>[[...inputs,{ input: item[0]}],[...outputs,{ outputs: item[1]}],],[[],[]]);const datasetName ="Lilian Weng Blogs Q&A";const dataset =await client.createDataset(datasetName);await client.createExamples({ inputs, outputs, datasetId: dataset.id });
```

## Evaluators[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#evaluators "Direct link to Evaluators")
One way to think about different types of RAG evaluators is as a tuple of what is being evaluated X what its being evaluated against:
  1. **Correctness** : Response vs reference answer


  * `Goal`: Measure "_how similar/correct is the RAG chain answer, relative to a ground-truth answer_ "
  * `Mode`: Requires a ground truth (reference) answer supplied through a dataset
  * `Evaluator`: Use LLM-as-judge to assess answer correctness.


  1. **Relevance** : Response vs input


  * `Goal`: Measure "_how well does the generated response address the initial user input_ "
  * `Mode`: Does not require reference answer, because it will compare the answer to the input question
  * `Evaluator`: Use LLM-as-judge to assess answer relevance, helpfulness, etc.


  1. **Groundedness** : Response vs retrieved docs


  * `Goal`: Measure "_to what extent does the generated response agree with the retrieved context_ "
  * `Mode`: Does not require reference answer, because it will compare the answer to the retrieved context
  * `Evaluator`: Use LLM-as-judge to assess faithfulness, hallucinations, etc.


  1. **Retrieval relevance** : Retrieved docs vs input


  * `Goal`: Measure "_how relevant are my retrieved results for this query_ "
  * `Mode`: Does not require reference answer, because it will compare the question to the retrieved context
  * `Evaluator`: Use LLM-as-judge to assess relevance


![](https://docs.smith.langchain.com/assets/images/rag_eval_overview-0d95d78db4d60c2bccbd333f8ba75e60.png)
### Correctness: Response vs reference answer[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#correctness-response-vs-reference-answer "Direct link to Correctness: Response vs reference answer")
  * Python
  * TypeScript


```
from typing_extensions import Annotated, TypedDict# Grade output schemaclassCorrectnessGrade(TypedDict):# Note that the order in the fields are defined is the order in which the model will generate them.# It is useful to put explanations before responses because it forces the model to think through# its final response before generating it:  explanation: Annotated[str,...,"Explain your reasoning for the score"]  correct: Annotated[bool,...,"True if the answer is correct, False otherwise."]# Grade promptcorrectness_instructions ="""You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the ground truth answer.Correctness:A correctness value of True means that the student's answer meets all of the criteria.A correctness value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset."""# Grader LLMgrader_llm = ChatOpenAI(model="gpt-4o", temperature=0).with_structured_output(CorrectnessGrade, method="json_schema", strict=True)defcorrectness(inputs:dict, outputs:dict, reference_outputs:dict)->bool:"""An evaluator for RAG answer accuracy"""  answers =f"""\QUESTION: {inputs['question']}GROUND TRUTH ANSWER: {reference_outputs['answer']}STUDENT ANSWER: {outputs['answer']}"""# Run evaluator  grade = grader_llm.invoke([{"role":"system","content": correctness_instructions},{"role":"user","content": answers}])return grade["correct"]
```

```
importtype{ EvaluationResult }from"langsmith/evaluation";import{ z }from"zod";// Grade promptconst correctnessInstructions =`You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the ground truth answer.Correctness:A correctness value of True means that the student's answer meets all of the criteria.A correctness value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const graderLLM =newChatOpenAI({ model:"gpt-4o", temperature:0,}).withStructuredOutput( z.object({   explanation: z.string().describe("Explain your reasoning for the score"),   correct: z.boolean().describe("True if the answer is correct, False otherwise.")}).describe("Correctness score for reference answer v.s. generated answer."));asyncfunctioncorrectness({ inputs, outputs, referenceOutputs,}:{ inputs: Record<string,any>; outputs: Record<string,any>; referenceOutputs?: Record<string,any>;}):Promise<EvaluationResult>=>{const answer =`QUESTION: ${inputs.question}  GROUND TRUTH ANSWER: ${reference_outputs.answer}  STUDENT ANSWER: ${outputs.answer}`// Run evaluatorconst grade = graderLLM.invoke([{role:"system", content: correctnessInstructions},{role:"user", content: answer}])return grade.score};
```

### Relevance: Response vs input[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#relevance-response-vs-input "Direct link to Relevance: Response vs input")
The flow is similar to above, but we simply look at the `inputs` and `outputs` without needing the `reference_outputs`. Without a reference answer we can't grade accuracy, but can still grade relevance‚Äîas in, did the model address the user's question or not.
  * Python
  * TypeScript


```
# Grade output schemaclassRelevanceGrade(TypedDict):  explanation: Annotated[str,...,"Explain your reasoning for the score"]  relevant: Annotated[bool,...,"Provide the score on whether the answer addresses the question"]# Grade promptrelevance_instructions="""You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the student's answer meets all of the criteria.A relevance value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset."""# Grader LLMrelevance_llm = ChatOpenAI(model="gpt-4o", temperature=0).with_structured_output(RelevanceGrade, method="json_schema", strict=True)# Evaluatordefrelevance(inputs:dict, outputs:dict)->bool:"""A simple evaluator for RAG answer helpfulness."""  answer =f"QUESTION: {inputs['question']}\nSTUDENT ANSWER: {outputs['answer']}"  grade = relevance_llm.invoke([{"role":"system","content": relevance_instructions},{"role":"user","content": answer}])return grade["relevant"]
```

```
importtype{ EvaluationResult }from"langsmith/evaluation";import{ z }from"zod";// Grade promptconst relevanceInstructions =`You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the student's answer meets all of the criteria.A relevance value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const relevanceLLM =newChatOpenAI({ model:"gpt-4o", temperature:0,}).withStructuredOutput( z.object({   explanation: z.string().describe("Explain your reasoning for the score"),   relevant: z.boolean().describe("Provide the score on whether the answer addresses the question")}).describe("Relevance score for gene"));asyncfunctionrelevance({ inputs, outputs,}:{ inputs: Record<string,any>; outputs: Record<string,any>;}):Promise<EvaluationResult>=>{const answer =`QUESTION: ${inputs.question}STUDENT ANSWER: ${outputs.answer}`// Run evaluatorconst grade = relevanceLLM.invoke([{role:"system", content: relevanceInstructions},{role:"user", content: answer}])return grade.relevant};
```

### Groundedness: Response vs retrieved docs[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#groundedness-response-vs-retrieved-docs "Direct link to Groundedness: Response vs retrieved docs")
Another useful way to evaluate responses without needing reference answers is to check if the response is justified by (or "grounded in") the retrieved documents.
  * Python
  * TypeScript


```
# Grade output schemaclassGroundedGrade(TypedDict):  explanation: Annotated[str,...,"Explain your reasoning for the score"]  grounded: Annotated[bool,...,"Provide the score on if the answer hallucinates from the documents"]# Grade promptgrounded_instructions ="""You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain "hallucinated" information outside the scope of the FACTS.Grounded:A grounded value of True means that the student's answer meets all of the criteria.A grounded value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset."""# Grader LLM grounded_llm = ChatOpenAI(model="gpt-4o", temperature=0).with_structured_output(GroundedGrade, method="json_schema", strict=True)# Evaluatordefgroundedness(inputs:dict, outputs:dict)->bool:"""A simple evaluator for RAG answer groundedness."""  doc_string ="\n\n".join(doc.page_content for doc in outputs["documents"])  answer =f"FACTS: {doc_string}\nSTUDENT ANSWER: {outputs['answer']}"  grade = grounded_llm.invoke([{"role":"system","content": grounded_instructions},{"role":"user","content": answer}])return grade["grounded"]
```

```
importtype{ EvaluationResult }from"langsmith/evaluation";import{ z }from"zod";// Grade promptconst groundedInstructions =`You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain "hallucinated" information outside the scope of the FACTS.Grounded:A grounded value of True means that the student's answer meets all of the criteria.A grounded value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const groundedLLM =newChatOpenAI({ model:"gpt-4o", temperature:0,}).withStructuredOutput( z.object({   explanation: z.string().describe("Explain your reasoning for the score"),   grounded: z.boolean().describe("Provide the score on if the answer hallucinates from the documents")}).describe("Grounded score for the answer from the retrieved documents."));asyncfunctiongrounded({ inputs, outputs,}:{ inputs: Record<string,any>; outputs: Record<string,any>;}):Promise<EvaluationResult>=>{const docString = outputs.documents.map((doc)=> doc.pageContent).join("");const answer =`FACTS: ${docString}  STUDENT ANSWER: ${outputs.answer}`// Run evaluatorconst grade = groundedLLM.invoke([{role:"system", content: groundedInstructions},{role:"user", content: answer}])return grade.grounded};
```

### Retrieval relevance: Retrieved docs vs input[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#retrieval-relevance-retrieved-docs-vs-input "Direct link to Retrieval relevance: Retrieved docs vs input")
  * Python
  * TypeScript


```
# Grade output schemaclassRetrievalRelevanceGrade(TypedDict):  explanation: Annotated[str,...,"Explain your reasoning for the score"]  relevant: Annotated[bool,...,"True if the retrieved documents are relevant to the question, False otherwise"]# Grade promptretrieval_relevance_instructions ="""You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset."""# Grader LLMretrieval_relevance_llm = ChatOpenAI(model="gpt-4o", temperature=0).with_structured_output(RetrievalRelevanceGrade, method="json_schema", strict=True)defretrieval_relevance(inputs:dict, outputs:dict)->bool:"""An evaluator for document relevance"""  doc_string ="\n\n".join(doc.page_content for doc in outputs["documents"])  answer =f"FACTS: {doc_string}\nQUESTION: {inputs['question']}"# Run evaluator  grade = retrieval_relevance_llm.invoke([{"role":"system","content": retrieval_relevance_instructions},{"role":"user","content": answer}])return grade["relevant"]
```

```
importtype{ EvaluationResult }from"langsmith/evaluation";import{ z }from"zod";// Grade promptconst retrievalRelevanceInstructions =`You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const retrievalRelevanceLLM =newChatOpenAI({ model:"gpt-4o", temperature:0,}).withStructuredOutput( z.object({   explanation: z.string().describe("Explain your reasoning for the score"),   relevant: z.boolean().describe("True if the retrieved documents are relevant to the question, False otherwise")}).describe("Retrieval relevance score for the retrieved documents v.s. the question."));asyncfunctionretrievalRelevance({ inputs, outputs,}:{ inputs: Record<string,any>; outputs: Record<string,any>;}):Promise<EvaluationResult>=>{const docString = outputs.documents.map((doc)=> doc.pageContent).join("");const answer =`FACTS: ${docString}  QUESTION: ${inputs.question}`// Run evaluatorconst grade = retrievalRelevanceLLM.invoke([{role:"system", content: retrievalRelevanceInstructions},{role:"user", content: answer}])return grade.relevant};
```

## Run evaluation[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#run-evaluation "Direct link to Run evaluation")
We can now kick off our evaluation job with all of our different evaluators.
  * Python
  * TypeScript


```
deftarget(inputs:dict)->dict:return rag_bot(inputs["question"])experiment_results = client.evaluate(  target,  data=dataset_name,  evaluators=[correctness, groundedness, relevance, retrieval_relevance],  experiment_prefix="rag-doc-relevance",  metadata={"version":"LCEL context, gpt-4-0125-preview"},)# Explore results locally as a dataframe if you have pandas installed# experiment_results.to_pandas()
```

```
import{ evaluate }from"langsmith/evaluation";consttargetFunc=(input: Record<string,any>)=>{returnragBot(inputs.question);};const experimentResults =awaitevaluate(targetFunc,{ data: datasetName, evaluators:[correctness, groundedness, relevance, retrievalRelevance], experimentPrefix ="rag-doc-relevance", metadata ={ version:"LCEL context, gpt-4-0125-preview"},});
```

You can see an example of what these results look like here: [LangSmith link](https://smith.langchain.com/public/302573e2-20bf-4f8c-bdad-e97c20f33f1b/d)
## Reference code[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/rag#reference-code "Direct link to Reference code")
Here's a consolidated script with all the above code:
  * Python
  * TypeScript


```
from langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langsmith import Client, traceablefrom typing_extensions import Annotated, TypedDict# List of URLs to load documents fromurls =["https://lilianweng.github.io/posts/2023-06-23-agent/","https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/","https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",]# Load documents from the URLsdocs =[WebBaseLoader(url).load()for url in urls]docs_list =[item for sublist in docs for item in sublist]# Initialize a text splitter with specified chunk size and overlaptext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(  chunk_size=250, chunk_overlap=0)# Split the documents into chunksdoc_splits = text_splitter.split_documents(docs_list)# Add the document chunks to the "vector store" using OpenAIEmbeddingsvectorstore = InMemoryVectorStore.from_documents(  documents=doc_splits,  embedding=OpenAIEmbeddings(),)# With langchain we can easily turn any vector store into a retrieval component:retriever = vectorstore.as_retriever(k=6)llm = ChatOpenAI(model="gpt-4o", temperature=1)# Add decorator so this function is traced in LangSmith@traceable()defrag_bot(question:str)->dict:# langchain Retriever will be automatically traced  docs = retriever.invoke(question)  docs_string = "".join(doc.page_content for doc in docs)  instructions =f"""You are a helpful assistant who is good at analyzing source information and answering questions.    Use the following source documents to answer the user's questions.    If you don't know the answer, just say that you don't know.    Use three sentences maximum and keep the answer concise.Documents:{docs_string}"""# langchain ChatModel will be automatically traced  ai_msg = llm.invoke([{"role":"system","content": instructions},{"role":"user","content": question},],)return{"answer": ai_msg.content,"documents": docs}client = Client()# Define the examples for the datasetexamples =[{"inputs":{"question":"How does the ReAct agent use self-reflection? "},"outputs":{"answer":"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs."},},{"inputs":{"question":"What are the types of biases that can arise with few-shot prompting?"},"outputs":{"answer":"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias."},},{"inputs":{"question":"What are five types of adversarial attacks?"},"outputs":{"answer":"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming."},},]# Create the dataset and examples in LangSmithdataset_name ="Lilian Weng Blogs Q&A"ifnot client.has_dataset(dataset_name=dataset_name):  dataset = client.create_dataset(dataset_name=dataset_name)  client.create_examples(    dataset_id=dataset.id,    examples=examples)# Grade output schemaclassCorrectnessGrade(TypedDict):# Note that the order in the fields are defined is the order in which the model will generate them.# It is useful to put explanations before responses because it forces the model to think through# its final response before generating it:  explanation: Annotated[str,...,"Explain your reasoning for the score"]  correct: Annotated[bool,...,"True if the answer is correct, False otherwise."]# Grade promptcorrectness_instructions ="""You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the ground truth answer.Correctness:A correctness value of True means that the student's answer meets all of the criteria.A correctness value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset."""# Grader LLMgrader_llm = ChatOpenAI(model="gpt-4o", temperature=0).with_structured_output(  CorrectnessGrade, method="json_schema", strict=True)defcorrectness(inputs:dict, outputs:dict, reference_outputs:dict)->bool:"""An evaluator for RAG answer accuracy"""  answers =f"""\QUESTION: {inputs['question']}GROUND TRUTH ANSWER: {reference_outputs['answer']}STUDENT ANSWER: {outputs['answer']}"""# Run evaluator  grade = grader_llm.invoke([{"role":"system","content": correctness_instructions},{"role":"user","content": answers},])return grade["correct"]# Grade output schemaclassRelevanceGrade(TypedDict):  explanation: Annotated[str,...,"Explain your reasoning for the score"]  relevant: Annotated[bool,...,"Provide the score on whether the answer addresses the question"]# Grade promptrelevance_instructions ="""You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the student's answer meets all of the criteria.A relevance value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset."""# Grader LLMrelevance_llm = ChatOpenAI(model="gpt-4o", temperature=0).with_structured_output(  RelevanceGrade, method="json_schema", strict=True)# Evaluatordefrelevance(inputs:dict, outputs:dict)->bool:"""A simple evaluator for RAG answer helpfulness."""  answer =f"QUESTION: {inputs['question']}\nSTUDENT ANSWER: {outputs['answer']}"  grade = relevance_llm.invoke([{"role":"system","content": relevance_instructions},{"role":"user","content": answer},])return grade["relevant"]# Grade output schemaclassGroundedGrade(TypedDict):  explanation: Annotated[str,...,"Explain your reasoning for the score"]  grounded: Annotated[bool,...,"Provide the score on if the answer hallucinates from the documents"]# Grade promptgrounded_instructions ="""You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain "hallucinated" information outside the scope of the FACTS.Grounded:A grounded value of True means that the student's answer meets all of the criteria.A grounded value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset."""# Grader LLMgrounded_llm = ChatOpenAI(model="gpt-4o", temperature=0).with_structured_output(  GroundedGrade, method="json_schema", strict=True)# Evaluatordefgroundedness(inputs:dict, outputs:dict)->bool:"""A simple evaluator for RAG answer groundedness."""  doc_string ="\n\n".join(doc.page_content for doc in outputs["documents"])  answer =f"FACTS: {doc_string}\nSTUDENT ANSWER: {outputs['answer']}"  grade = grounded_llm.invoke([{"role":"system","content": grounded_instructions},{"role":"user","content": answer},])return grade["grounded"]# Grade output schemaclassRetrievalRelevanceGrade(TypedDict):  explanation: Annotated[str,...,"Explain your reasoning for the score"]  relevant: Annotated[bool,...,"True if the retrieved documents are relevant to the question, False otherwise",]# Grade promptretrieval_relevance_instructions ="""You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset."""# Grader LLMretrieval_relevance_llm = ChatOpenAI(  model="gpt-4o", temperature=0).with_structured_output(RetrievalRelevanceGrade, method="json_schema", strict=True)defretrieval_relevance(inputs:dict, outputs:dict)->bool:"""An evaluator for document relevance"""  doc_string ="\n\n".join(doc.page_content for doc in outputs["documents"])  answer =f"FACTS: {doc_string}\nQUESTION: {inputs['question']}"# Run evaluator  grade = retrieval_relevance_llm.invoke([{"role":"system","content": retrieval_relevance_instructions},{"role":"user","content": answer},])return grade["relevant"]deftarget(inputs:dict)->dict:return rag_bot(inputs["question"])experiment_results = client.evaluate(  target,  data=dataset_name,  evaluators=[correctness, groundedness, relevance, retrieval_relevance],  experiment_prefix="rag-doc-relevance",  metadata={"version":"LCEL context, gpt-4-0125-preview"},)# Explore results locally as a dataframe if you have pandas installed# experiment_results.to_pandas()
```

```
import{ OpenAIEmbeddings, ChatOpenAI }from"@langchain/openai";import{ MemoryVectorStore }from"langchain/vectorstores/memory";import{ BrowserbaseLoader }from"@langchain/community/document_loaders/web/browserbase";import{ traceable }from"langsmith/traceable";import{ Client }from"langsmith";import{ evaluate,typeEvaluationResult}from"langsmith/evaluation";import{ z }from"zod";// List of URLs to load documents fromconst urls =["https://lilianweng.github.io/posts/2023-06-23-agent/","https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/","https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",]const loader =newBrowserbaseLoader(urls,{  textContent:true,});const docs =await loader.load();const splitter =newRecursiveCharacterTextSplitter({  chunkSize:1000, chunkOverlap:200});const allSplits =await splitter.splitDocuments(docs);const embeddings =newOpenAIEmbeddings({  model:"text-embedding-3-large"});const vectorStore =newMemoryVectorStore(embeddings);// Index chunksawait vectorStore.addDocuments(allSplits)const llm =newChatOpenAI({ model:"gpt-4o", temperature:1,})// Add decorator so this function is traced in LangSmithconst ragBot =traceable(async(question:string)=>{// LangChain retriever will be automatically tracedconst retrievedDocs =await vectorStore.similaritySearch(question);const docsContent = retrievedDocs.map((doc)=> doc.pageContent).join("");const instructions =`You are a helpful assistant who is good at analyzing source information and answering questions.    Use the following source documents to answer the user's questions.    If you don't know the answer, just say that you don't know.    Use three sentences maximum and keep the answer concise.    Documents:${docsContent}`const aiMsg =await llm.invoke([{        role:"system",        content: instructions},{        role:"user",        content: question}])return{"answer": aiMsg.content,"documents": retrievedDocs}})const client =newClient();// Define the examples for the datasetconst examples =[["How does the ReAct agent use self-reflection? ","ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.",],["What are the types of biases that can arise with few-shot prompting?","The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.",],["What are five types of adversarial attacks?","Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.",]]const[inputs, outputs]= examples.reduce<[Array<{ input:string}>,Array<{ outputs:string}>]>(([inputs, outputs], item)=>[[...inputs,{ input: item[0]}],[...outputs,{ outputs: item[1]}],],[[],[]]);const datasetName ="Lilian Weng Blogs Q&A";const dataset =await client.createDataset(datasetName);await client.createExamples({ inputs, outputs, datasetId: dataset.id })// Grade promptconst correctnessInstructions =`You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the ground truth answer.Correctness:A correctness value of True means that the student's answer meets all of the criteria.A correctness value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const graderLLM =newChatOpenAI({ model:"gpt-4o", temperature:0,}).withStructuredOutput( z.object({   explanation: z.string().describe("Explain your reasoning for the score"),   correct: z.boolean().describe("True if the answer is correct, False otherwise.")}).describe("Correctness score for reference answer v.s. generated answer."));asyncfunctioncorrectness({ inputs, outputs, referenceOutputs,}:{ inputs: Record<string,any>; outputs: Record<string,any>; referenceOutputs?: Record<string,any>;}):Promise<EvaluationResult>=>{const answer =`QUESTION: ${inputs.question}  GROUND TRUTH ANSWER: ${reference_outputs.answer}  STUDENT ANSWER: ${outputs.answer}`// Run evaluatorconst grade = graderLLM.invoke([{role:"system", content: correctnessInstructions},{role:"user", content: answer}])return grade.score};// Grade promptconst relevanceInstructions =`You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the student's answer meets all of the criteria.A relevance value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const relevanceLLM =newChatOpenAI({ model:"gpt-4o", temperature:0,}).withStructuredOutput( z.object({   explanation: z.string().describe("Explain your reasoning for the score"),   relevant: z.boolean().describe("Provide the score on whether the answer addresses the question")}).describe("Relevance score for gene"));asyncfunctionrelevance({ inputs, outputs,}:{ inputs: Record<string,any>; outputs: Record<string,any>;}):Promise<EvaluationResult>=>{const answer =`QUESTION: ${inputs.question}STUDENT ANSWER: ${outputs.answer}`// Run evaluatorconst grade = relevanceLLM.invoke([{role:"system", content: relevanceInstructions},{role:"user", content: answer}])return grade.relevant};// Grade promptconst groundedInstructions =`You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain "hallucinated" information outside the scope of the FACTS.Grounded:A grounded value of True means that the student's answer meets all of the criteria.A grounded value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const groundedLLM =newChatOpenAI({ model:"gpt-4o", temperature:0,}).withStructuredOutput( z.object({   explanation: z.string().describe("Explain your reasoning for the score"),   grounded: z.boolean().describe("Provide the score on if the answer hallucinates from the documents")}).describe("Grounded score for the answer from the retrieved documents."));asyncfunctiongrounded({ inputs, outputs,}:{ inputs: Record<string,any>; outputs: Record<string,any>;}):Promise<EvaluationResult>=>{const docString = outputs.documents.map((doc)=> doc.pageContent).join("");const answer =`FACTS: ${docString}  STUDENT ANSWER: ${outputs.answer}`// Run evaluatorconst grade = groundedLLM.invoke([{role:"system", content: groundedInstructions},{role:"user", content: answer}])return grade.grounded};// Grade promptconst retrievalRelevanceInstructions =`You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const retrievalRelevanceLLM =newChatOpenAI({ model:"gpt-4o", temperature:0,}).withStructuredOutput( z.object({   explanation: z.string().describe("Explain your reasoning for the score"),   relevant: z.boolean().describe("True if the retrieved documents are relevant to the question, False otherwise")}).describe("Retrieval relevance score for the retrieved documents v.s. the question."));asyncfunctionretrievalRelevance({ inputs, outputs,}:{ inputs: Record<string,any>; outputs: Record<string,any>;}):Promise<EvaluationResult>=>{const docString = outputs.documents.map((doc)=> doc.pageContent).join("");const answer =`FACTS: ${docString}  QUESTION: ${inputs.question}`// Run evaluatorconst grade = retrievalRelevanceLLM.invoke([{role:"system", content: retrievalRelevanceInstructions},{role:"user", content: answer}])return grade.relevant};consttargetFunc=(input: Record<string,any>)=>{returnragBot(inputs.question)};const experimentResults =awaitevaluate(targetFunc,{  data: datasetName,  evaluators:[correctness, groundedness, relevance, retrievalRelevance],  experimentPrefix="rag-doc-relevance",  metadata={version:"LCEL context, gpt-4-0125-preview"},});
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousEvaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)[NextRun backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
  * [Overview](https://docs.smith.langchain.com/evaluation/tutorials/rag#overview)
  * [Setup](https://docs.smith.langchain.com/evaluation/tutorials/rag#setup)
    * [Environment](https://docs.smith.langchain.com/evaluation/tutorials/rag#environment)
    * [Application](https://docs.smith.langchain.com/evaluation/tutorials/rag#application)
  * [Dataset](https://docs.smith.langchain.com/evaluation/tutorials/rag#dataset)
  * [Evaluators](https://docs.smith.langchain.com/evaluation/tutorials/rag#evaluators)
    * [Correctness: Response vs reference answer](https://docs.smith.langchain.com/evaluation/tutorials/rag#correctness-response-vs-reference-answer)
    * [Relevance: Response vs input](https://docs.smith.langchain.com/evaluation/tutorials/rag#relevance-response-vs-input)
    * [Groundedness: Response vs retrieved docs](https://docs.smith.langchain.com/evaluation/tutorials/rag#groundedness-response-vs-retrieved-docs)
    * [Retrieval relevance: Retrieved docs vs input](https://docs.smith.langchain.com/evaluation/tutorials/rag#retrieval-relevance-retrieved-docs-vs-input)
  * [Run evaluation](https://docs.smith.langchain.com/evaluation/tutorials/rag#run-evaluation)
  * [Reference code](https://docs.smith.langchain.com/evaluation/tutorials/rag#reference-code)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark

[Skip to main content](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
    * [Evaluation](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
  * Running SWE-bench with LangSmith


On this page
# Running SWE-bench with LangSmith
SWE-bench is one of the most popular (and difficult!) benchmarks for developers to test their coding agents against. In this walkthrough we will show you how to load the SWE-bench dataset into LangSmith and easily run evals on it, allowing you to have much better visibility into your agents behaviour then using the off-the-shelf SWE-bench eval suite. This allows you to pin specific problems quicker and iterate on your agent rapidly to improve performance!
## Loading the data[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#loading-the-data "Direct link to Loading the data")
To load the data, we will pull the `dev` split from Hugging Face, but for your use case you may wish to pull one of the `test`, or `train` splits, and if you want to combine multiple splits you can use `pd.concat`.
```
import pandas as pdsplits ={'dev':'data/dev-00000-of-00001.parquet','test':'data/test-00000-of-00001.parquet','train':'data/train-00000-of-00001.parquet'}df = pd.read_parquet("hf://datasets/princeton-nlp/SWE-bench/"+ splits["dev"])
```

### Editing the 'version' column[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#editing-the-version-column "Direct link to Editing the 'version' column")
note
This is a very important step! If you skip, the rest of the code WILL NOT WORK!
The `version` column contains all string values but all are in float format so they get converted to floats when you upload the CSV to create a LangSmith dataset. Although you can convert the values to strings during your experiments, the issue arises with values like `"0.10"`. When getting converted to a float, you get the value `0.1`, which would become `"0.1"` if you converted it to a string - causing a key error during execution of your proposed patch.
In order to fix this, we need LangSmith to stop trying to convert the `version` column to floats. In order to do this, we can just append a string prefix to each of them that is not float compatible. We then need to split on this prefix when doing evaluation to get the actual `version` value. The prefix we choose here is the string `"version:"`.
note
The ability to select column types when uploading a CSV to LangSmith will be added in the future to avoid having to use this workaround.
```
df['version']= df['version'].apply(lambda x:f"version:{x}")
```

## Upload the data to LangSmith[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#upload-the-data-to-langsmith "Direct link to Upload the data to LangSmith")
### Save to CSV[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#save-to-csv "Direct link to Save to CSV")
To upload the data to LangSmith, we first need to save it to a CSV, which we can do using the `to_csv` function provided by pandas. Make sure to save this file somewhere that is easily accessible to you.
```
df.to_csv("./../SWE-bench.csv",index=False)
```

### Upload CSV to LangSmith Manually[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#upload-csv-to-langsmith-manually "Direct link to Upload CSV to LangSmith Manually")
We are now ready to upload the CSV to LangSmith. Once you are on the LangSmith website (smith.langchain.com), go to the `Datasets & Testing` tab on the left side navigation bar, and then click the `+ New Dataset` button in the top right corner.
Then click the `Upload CSV` button on the top, and select the CSV file you saved in the previous step. You can then give your dataset a name and description.
Next, select `Key-Value` as the dataset type. Lastly head to the `Create Schema` section and add ALL OF THE KEYS as `Input fields`. There are no `Output fields` in this example because our evaluator is not comparing against a reference, but instead will run the output of our experiments in docker containers to ensure that the code actually solves the PR issue.
Once you have populated the `Input fields` (and left the `Output fields` empty!) you can click the blue `Create` button in the top right corner, and your dataset will be created!
### Upload CSV to LangSmith Programmatically[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#upload-csv-to-langsmith-programmatically "Direct link to Upload CSV to LangSmith Programmatically")
Alternatively you can upload your csv to LangSmith using the sdk as shown in the code block below:
```
dataset = client.upload_csv(  csv_file="./../SWE-bench-dev.csv",  input_keys=list(df.columns),  output_keys=[],  name="swe-bench-programatic-upload",  description="SWE-bench dataset",  data_type="kv")
```

### Create dataset split for quicker testing[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#create-dataset-split-for-quicker-testing "Direct link to Create dataset split for quicker testing")
Since running the SWE-bench evaluator takes a long time when run on all examples, you can create a "test" split for quickly testing the evaluator and your code. Read [this guide](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application#create-and-manage-dataset-splits) to learn more about managing dataset splits, or watch this short video that shows how to do it (to get to the starting page of the video, just click on your dataset created above and go to the `Examples` tab):
## Running our prediction function[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#running-our-prediction-function "Direct link to Running our prediction function")
Running evaluation over SWE-bench works a little differently than most evals you will typically run on LangSmith since we don't have a reference output. Because of this, we first generate all of our outputs without running an evaluator (note how the `evaluate` call doesn't have the `evaluators` parameter set). In this case we returned a dummy predict function, but you can insert your agent logic inside the `predict` function to make it work as intended.
```
from langsmith import evaluatefrom langsmith import Clientclient = Client()defpredict(inputs:dict):return{"instance_id":inputs['instance_id'],"model_patch":"None","model_name_or_path":"test-model"}result = evaluate(  predict,  data=client.list_examples(dataset_id="a9bffcdf-1dfe-4aef-8805-8806f0110067",splits=["test"]),)
```

View the evaluation results for experiment: 'perfect-lip-22' at: <https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/a9bffcdf-1dfe-4aef-8805-8806f0110067/compare?selectedSessions=182de5dc-fc9d-4065-a3e1-34527f952fd8>
3it [00:00, 24.48it/s]
## Evaluating our predictions using SWE-bench[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#evaluating-our-predictions-using-swe-bench "Direct link to Evaluating our predictions using SWE-bench")
Now we can run the following code to run the predicted patches we generated above in Docker. This code is edited slightly from the `SWE-bench` [run_evaluation.py](https://github.com/princeton-nlp/SWE-bench/blob/main/swebench/harness/run_evaluation.py) file.
Basically, the code sets up docker images to run the predictions in parallel, which greatly reduces the time needed for evaluation. This screenshot explains the basics of how `SWE-bench` does evaluation under the hood. To understand it in full, make sure to read through the code in the [GitHub repository](https://github.com/princeton-nlp/SWE-bench).
![Eval Diagram](https://docs.smith.langchain.com/assets/images/swebench_evaluation-4086f0af70875bc21fa5e2b9ce7044e0.png)
The function `convert_runs_to_langsmith_feedback` converts the logs generated by the docker file into a nice .json file that contains feedback in the typical key/score method of LangSmith.
```
from swebench.harness.run_evaluation import run_instancesimport resourceimport dockerfrom swebench.harness.docker_utils import list_images, clean_imagesfrom swebench.harness.docker_build import build_env_imagesfrom pathlib import Pathimport jsonimport osRUN_EVALUATION_LOG_DIR = Path("logs/run_evaluation")LANGSMITH_EVALUATION_DIR ='./langsmith_feedback/feedback.json'defconvert_runs_to_langsmith_feedback(    predictions:dict,    full_dataset:list,    run_id:str)->float:"""  Convert logs from docker containers into LangSmith feedback.  Args:    predictions (dict): Predictions dict generated by the model    full_dataset (list): List of all instances    run_id (str): Run ID  """  feedback_for_all_instances ={}for instance in full_dataset:    feedback_for_instance =[]    instance_id = instance['instance_id']    prediction = predictions[instance_id]if prediction.get("model_patch",None)in["",None]:# Prediction returned an empty patch      feedback_for_all_instances[prediction['run_id']]=[{"key":"non-empty-patch","score":0},{"key":"completed-patch","score":0},{"key":"resolved-patch","score":0}]continue    feedback_for_instance.append({"key":"non-empty-patch","score":1})    report_file =(      RUN_EVALUATION_LOG_DIR/ run_id/ prediction["model_name_or_path"].replace("/","__")/ prediction['instance_id']/"report.json")if report_file.exists():# If report file exists, then the instance has been run      feedback_for_instance.append({"key":"completed-patch","score":1})      report = json.loads(report_file.read_text())# Check if instance actually resolved the PRif report[instance_id]["resolved"]:        feedback_for_instance.append({"key":"resolved-patch","score":1})else:        feedback_for_instance.append({"key":"resolved-patch","score":0})else:# The instance did not run successfully      feedback_for_instance +=[{"key":"completed-patch","score":0},{"key":"resolved-patch","score":0}]    feedback_for_all_instances[prediction['run_id']]= feedback_for_instance  os.makedirs(os.path.dirname(LANGSMITH_EVALUATION_DIR), exist_ok=True)withopen(LANGSMITH_EVALUATION_DIR,'w')as json_file:    json.dump(feedback_for_all_instances, json_file)defevaluate_predictions(    dataset:list,    predictions:list,    max_workers:int,    force_rebuild:bool,    cache_level:str,    clean:bool,    open_file_limit:int,    run_id:str,    timeout:int,):"""  Run evaluation harness for the given dataset and predictions.  """# set open file limitassertlen(run_id)>0,"Run ID must be provided"  resource.setrlimit(resource.RLIMIT_NOFILE,(open_file_limit, open_file_limit))  client = docker.from_env()  existing_images = list_images(client)print(f"Running {len(dataset)} unevaluated instances...")# build environment images + run instances  build_env_images(client, dataset, force_rebuild, max_workers)  run_instances(predictions, dataset, cache_level, clean, force_rebuild, max_workers, run_id, timeout)# clean images + make final report  clean_images(client, existing_images, cache_level, clean)  convert_runs_to_langsmith_feedback(predictions,dataset,run_id)
```

```
dataset =[]predictions ={}for res in result:  predictions[res['run'].outputs['instance_id']]={**res['run'].outputs,**{"run_id":str(res['run'].id)}}  dataset.append(res['run'].inputs['inputs'])for d in dataset:  d['version']= d['version'].split(":")[1]
```

```
evaluate_predictions(dataset,predictions,max_workers=8,force_rebuild=False,cache_level="env",clean=False \,open_file_limit=4096,run_id="test",timeout=1_800)
```

```
  Running 3 unevaluated instances...  Base image sweb.base.arm64:latest already exists, skipping build.  Base images built successfully.  Total environment images to build: 2  Building environment images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:47<00:00, 23.94s/it]  All environment images built successfully.  Running 3 instances...   0%|     | 0/3 [00:00<?, ?it/s]  Evaluation error for sqlfluff__sqlfluff-884: >>>>> Patch Apply Failed:  patch unexpectedly ends in middle of line  patch: **** Only garbage was found in the patch input.  Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-884/run_instance.log) for more information.  Evaluation error for sqlfluff__sqlfluff-4151: >>>>> Patch Apply Failed:  patch unexpectedly ends in middle of line  patch: **** Only garbage was found in the patch input.  Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-4151/run_instance.log) for more information.  Evaluation error for sqlfluff__sqlfluff-2849: >>>>> Patch Apply Failed:  patch: **** Only garbage was found in the patch input.  patch unexpectedly ends in middle of line  Check (logs/run_evaluation/test/test-model/sqlfluff__sqlfluff-2849/run_instance.log) for more information.  100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:30<00:00, 10.04s/it]  All instances run.  Cleaning cached images...  Removed 0 images.
```

## Sending Evaluation to LangSmith[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#sending-evaluation-to-langsmith "Direct link to Sending Evaluation to LangSmith")
Now, we can actually send our evaluation feedback to LangSmith by using the `evaluate_existing` function. Our evaluate function is incredibly simple in this case, because the `convert_runs_to_langsmith_feedback` function above made our life very easy by saving all the feedback to a single file.
```
from langsmith import evaluate_existingfrom langsmith.schemas import Example, Rundefswe_bench_evaluator(run: Run, example: Example):withopen(LANGSMITH_EVALUATION_DIR,'r')as json_file:    langsmith_eval = json.load(json_file)return{"results": langsmith_eval[str(run.id)]}experiment_name = result.experiment_nameevaluate_existing(experiment_name, evaluators=[swe_bench_evaluator])
```

```
  View the evaluation results for experiment: 'perfect-lip-22' at:  https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/a9bffcdf-1dfe-4aef-8805-8806f0110067/compare?selectedSessions=182de5dc-fc9d-4065-a3e1-34527f952fd8  3it [00:01, 1.52it/s]  <ExperimentResults perfect-lip-22>
```

After running, we can go to the experiments tab of our dataset, and check that our feedback keys were properly assigned. If they were, you should see something that resembles the following image:
![LangSmith feedback](https://docs.smith.langchain.com/assets/images/swebench_langsmith_feedback-cbe3f623026f12895b331f886033fce4.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/evaluation/tutorials/swe-benchmark%3E).
[PreviousRun backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)[NextEvaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
  * [Loading the data](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#loading-the-data)
    * [Editing the 'version' column](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#editing-the-version-column)
  * [Upload the data to LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#upload-the-data-to-langsmith)
    * [Save to CSV](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#save-to-csv)
    * [Upload CSV to LangSmith Manually](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#upload-csv-to-langsmith-manually)
    * [Upload CSV to LangSmith Programmatically](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#upload-csv-to-langsmith-programmatically)
    * [Create dataset split for quicker testing](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#create-dataset-split-for-quicker-testing)
  * [Running our prediction function](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#running-our-prediction-function)
  * [Evaluating our predictions using SWE-bench](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#evaluating-our-predictions-using-swe-bench)
  * [Sending Evaluation to LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark#sending-evaluation-to-langsmith)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/evaluation/tutorials/testing

[Skip to main content](https://docs.smith.langchain.com/evaluation/tutorials/testing#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/evaluation/tutorials/testing)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
    * [Quick Start](https://docs.smith.langchain.com/evaluation)
    * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
      * [Evaluate a chatbot](https://docs.smith.langchain.com/evaluation/tutorials/evaluation)
      * [Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)
      * [Run backtests on a new version of an agent](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)
      * [Running SWE-bench with LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/swe-benchmark)
      * [Evaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)
      * [Test a ReAct agent with Pytest/Vitest and LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/testing)
    * [How-to Guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
      * [Analyze a single experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/analyze_single_experiment)
      * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
      * [How to run an evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
      * [Creating and Managing Datasets in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application)
      * [Renaming an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/renaming_experiment)
      * [How to bind an evaluator to a dataset in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/bind_evaluator_to_dataset)
      * [How to manage datasets programmatically](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically)
      * [Running an evaluation from the prompt playground](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground)
      * [Set up feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
      * [Annotate traces and runs inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)
      * [How to evaluate an application's intermediate steps](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_on_intermediate_steps)
      * [How to version a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/version_datasets)
      * [Use annotation queues](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
      * [Dataset Sharing](https://docs.smith.langchain.com/evaluation/how_to_guides/share_dataset)
      * [How to use off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old)
      * [How to compare experiment results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results)
      * [Dynamic few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)
      * [How to evaluate an existing experiment (Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_existing_experiment)
      * [How to export filtered traces from experiment to dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/export_filtered_traces_to_dataset)
      * [How to run evals with pytest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest)
      * [Run pairwise evaluations](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_pairwise)
      * [How to audit evaluator scores](https://docs.smith.langchain.com/evaluation/how_to_guides/audit_evaluator_scores)
      * [How to improve your evaluator with few-shot examples](https://docs.smith.langchain.com/evaluation/how_to_guides/create_few_shot_evaluators)
      * [How to fetch performance metrics for an experiment](https://docs.smith.langchain.com/evaluation/how_to_guides/fetch_perf_metrics_experiment)
      * [How to use the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evals_api_only)
      * [How to upload experiments run outside of LangSmith with the REST API](https://docs.smith.langchain.com/evaluation/how_to_guides/upload_existing_experiments)
      * [How to run an evaluation asynchronously](https://docs.smith.langchain.com/evaluation/how_to_guides/async)
      * [How to define a custom evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/custom_evaluator)
      * [How to evaluate on a split / filtered view of a dataset](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_subset)
      * [How to evaluate on a specific dataset version](https://docs.smith.langchain.com/evaluation/how_to_guides/dataset_version)
      * [How to define a target function to evaluate](https://docs.smith.langchain.com/evaluation/how_to_guides/define_target)
      * [How to download experiment results as a CSV](https://docs.smith.langchain.com/evaluation/how_to_guides/download_experiment_results_as_csv)
      * [Run an evaluation with large file inputs](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_with_attachments)
      * [How to filter experiments in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/filter_experiments_ui)
      * [How to evaluate a langchain runnable](https://docs.smith.langchain.com/evaluation/how_to_guides/langchain_runnable)
      * [How to evaluate a langgraph graph](https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph)
      * [How to define an LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge)
      * [How to run an evaluation locally (beta, Python only)](https://docs.smith.langchain.com/evaluation/how_to_guides/local)
      * [How to return categorical vs numerical metrics](https://docs.smith.langchain.com/evaluation/how_to_guides/metric_type)
      * [How to return multiple scores in one evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/multiple_scores)
      * [How to set up a multi-turn evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/multiturn_evaluation)
      * [How to use prebuilt evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators)
      * [How to handle model rate limits](https://docs.smith.langchain.com/evaluation/how_to_guides/rate_limiting)
      * [How to evaluate with repetitions](https://docs.smith.langchain.com/evaluation/how_to_guides/repetition)
      * [How to define a summary evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/summary)
      * [How to run evals with Vitest/Jest (beta)](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest)
    * [Conceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Tutorials](https://docs.smith.langchain.com/evaluation/tutorials)
  * Test a ReAct agent with Pytest/Vitest and LangSmith


On this page
# Test a ReAct agent with Pytest/Vitest and LangSmith
This tutorial will show you how to use LangSmith's integrations with popular testing tools [Pytest](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest) and [Vitest/Jest](https://docs.smith.langchain.com/evaluation/how_to_guides/vitest_jest) to evaluate your LLM application. We will create a ReAct agent that answers questions about publicly traded stocks and write a comprehensive test suite for it.
## Setup[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#setup "Direct link to Setup")
This tutorial uses [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/) for agent orchestration, [OpenAI's GPT-4o](https://platform.openai.com/docs/models#gpt-4o), [Tavily](https://tavily.com/) for search, [E2B's](https://e2b.dev/) code interpreter, and [Polygon](https://polygon.io/stocks) to retrieve stock data but it can be adapted for other frameworks, models and tools with minor modifications. Tavily, E2B and Polygon are free to sign up for.
### Installation[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#installation "Direct link to Installation")
First, install the packages required for making the agent:
  * Python
  * TypeScript


```
pip install -U langgraph langchain[openai] langchain-community e2b-code-interpreter
```

```
yarn add @langchain/openai @langchain/community @langchain/langgraph @langchain/core @e2b/code-interpreter @polygon.io/client-js openai zod
```

Next, install the testing framework:
  * Pytest
  * Vitest
  * Jest


```
# Make sure you have langsmith>=0.3.1pip install -U "langsmith[pytest]"
```

```
yarn add -D langsmith vitest
```

```
yarn add -D langsmith jest
```

### Environment Variables[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#environment-variables "Direct link to Environment Variables")
Set the following environment variables:
```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<YOUR_LANGSMITH_API_KEY>export OPENAI_API_KEY=<YOUR_OPENAI_API_KEY>export TAVILY_API_KEY=<YOUR_TAVILY_API_KEY>export E2B_API_KEY=<YOUR_E2B_API_KEY>export POLYGON_API_KEY=<YOUR_POLYGON_API_KEY>
```

## Create your app[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#create-your-app "Direct link to Create your app")
To define our React agent, we will use LangGraph/LangGraph.js for the orchestation and LangChain for the LLM and tools.
### Define tools[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#define-tools "Direct link to Define tools")
First we are going to define the tools we are going to use in our agent. There are going to be 3 tools:
  * A search tool using Tavily
  * A code interpreter tool using E2B
  * A stock information tool using Polygon


  * Python
  * TypeScript


```
from langchain_community.tools import TavilySearchResultsfrom e2b_code_interpreter import Sandboxfrom langchain_community.tools.polygon.aggregates import PolygonAggregatesfrom langchain_community.utilities.polygon import PolygonAPIWrapperfrom typing_extensions import Annotated, TypedDict, Optional, Literal# Define search tool search_tool = TavilySearchResults( max_results=5, include_raw_content=True,)# Define code tooldefcode_tool(code:str)->str:"""Execute python code and return the result.""" sbx = Sandbox() execution = sbx.run_code(code)if execution.error:returnf"Error: {execution.error}"returnf"Results: {execution.results}, Logs: {execution.logs}"# Define input schema for stock ticker toolclassTickerToolInput(TypedDict):"""Input format for the ticker tool. The tool will pull data in aggregate blocks (timespan_multiplier * timespan) from the from_date to the to_date """ ticker: Annotated[str,...,"The ticker symbol of the stock"] timespan: Annotated[Literal["minute","hour","day","week","month","quarter","year"],...,"The size of the time window."] timespan_multiplier: Annotated[int,...,"The multiplier for the time window"] from_date: Annotated[str,...,"The date to start pulling data from, YYYY-MM-DD format - ONLY include the year month and day"] to_date: Annotated[str,...,"The date to stop pulling data, YYYY-MM-DD format - ONLY include the year month and day"]api_wrapper = PolygonAPIWrapper()polygon_aggregate = PolygonAggregates(api_wrapper=api_wrapper)# Define stock ticker tooldefticker_tool(query: TickerToolInput)->str:"""Pull data for the ticker."""return polygon_aggregate.invoke(query)
```

```
import{ TavilySearchResults }from"@langchain/community/tools/tavily_search";import{ Sandbox }from"@e2b/code-interpreter";import{ tool }from"@langchain/core/tools";import{ z }from"zod";import{ restClient }from"@polygon.io/client-js";import{ tool }from"@langchain/core/tools";import{ z }from"zod";// Define search toolconst searchTool =newTavilySearchResults({ maxResults:5,});// Define code toolconst codeTool =tool(async(input)=>{const sbx =await Sandbox.create();const execution =await sbx.runCode(input.code);if(execution.error){return`Error: ${execution.error}`;}return`Results: ${execution.results}, Logs: ${execution.logs}`;},{ name:"code", description:"Execute python code and return the result.", schema: z.object({  code: z.string().describe("The python code to execute"),}),});// Define input schema for stock ticker toolconst TickerToolInputSchema = z.object({ticker: z.string().describe("The ticker symbol of the stock"),timespan: z.enum(["minute","hour","day","week","month","quarter","year"]).describe("The size of the time window."),timespan_multiplier: z.number().describe("The multiplier for the time window"),from_date: z.string().describe("The date to start pulling data from, YYYY-MM-DD format - ONLY include the year, month, and day"),to_date: z.string().describe("The date to stop pulling data, YYYY-MM-DD format - ONLY include the year, month, and day"),});const rest =restClient(process.env.POLYGON_API_KEY);// Define stock ticker toolconst tickerTool =tool(async(query)=>{const parsed = TickerToolInputSchema.parse(query);const result =await rest.stocks.aggregates(   parsed.ticker,   parsed.timespan_multiplier,   parsed.timespan,   parsed.from_date,   parsed.to_date);returnJSON.stringify(result);},{ name:"ticker", description:"Pull data for the ticker", schema: TickerToolInputSchema,});
```

### Define agent[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#define-agent "Direct link to Define agent")
Now that we have defined all of our tools, we can use LangGraph's [`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/)/[`createReactAgent`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_prebuilt.createReactAgent.html) to create our agent.
  * Python
  * TypeScript


```
from typing import Optionalfrom typing_extensions import Annotated, TypedDictfrom langgraph.prebuilt import create_react_agentclassAgentOutputFormat(TypedDict):  numeric_answer: Annotated[Optional[float],...,"The numeric answer, if the user asked for one"]  text_answer: Annotated[Optional[str],...,"The text answer, if the user asked for one"]  reasoning: Annotated[str,...,"The reasoning behind the answer"]agent = create_react_agent(  model="openai:gpt-4o-mini",  tools=[code_tool, search_tool, polygon_aggregates],  response_format=AgentOutputFormat,  state_modifier="You are a financial expert. Respond to the users query accurately",)
```

```
import{ z }from"zod";import{ ChatOpenAI }from"@langchain/openai";import{ createReactAgent }from"@langchain/langgraph/prebuilt";const AgentOutputFormatSchema = z.object({ numeric_answer: z.number().optional().describe("The numeric answer, if the user asked for one"), text_answer: z.string().optional().describe("The text answer, if the user asked for one"), reasoning: z.string().describe("The reasoning behind the answer"),})const tools =[codeTool, searchTool, tickerTool];const agent =createReactAgent({ llm:newChatOpenAI({ model:"gpt-4o"}), tools: tools, responseFormat: AgentOutputFormatSchema, stateModifier:"You are a financial expert. Respond to the users query accurately",});exportdefault agent;
```

## Write tests[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#write-tests "Direct link to Write tests")
Now that we have defined our agent, let's write a few tests to ensure basic functionality. In this tutorial we are going to test whether the agent's tool calling abilities are working, whether the agent knows to ignore irrelevant questions, and whether it is able to answer complex questions that involve using all of the tools.
We need to first set up a test file and add the imports needed at the top of the file.
  * Pytest
  * Vitest
  * Jest


Create a `tests/test_agent.py` file.
```
from app import agent, polygon_aggregates, search_tool # import from wherever your agent is definedimport pytestfrom langsmith import testing as t
```

Name your test file `agent.vitest.eval.ts`
```
import{ expect }from"vitest";import*as ls from"langsmith/vitest";import agent from"../agent";// import from wherever your agent is defined// Optional, but recommended to group tests togetherls.describe("Agent Tests",()=>{// PLACE TESTS Here});
```

Name your test file `agent.jest.eval.ts`
```
import{ expect }from"@jest/globals";import*as ls from"langsmith/jest";import agent from"../agent";// import from wherever your agent is defined// Optional, but recommended to group tests togetherls.describe("Agent Tests",()=>{// PLACE TESTS Here});
```

### Test 1: Handle off-topic questions[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#test-1-handle-off-topic-questions "Direct link to Test 1: Handle off-topic questions")
The first test will be a simple check that the agent does not use tools on irrelevant queries.
  * Pytest
  * Vitest
  * Jest


```
@pytest.mark.langsmith@pytest.mark.parametrize(# <-- Can still use all normal pytest markers"query",["Hello!","How are you doing?"],)deftest_no_tools_on_offtopic_query(query:str)->None:"""Test that the agent does not use tools on offtopic queries."""# Log the test example t.log_inputs({"query": query}) expected =[] t.log_reference_outputs({"tool_calls": expected})# Call the agent's model node directly instead of running the ReACT loop. result = agent.nodes["agent"].invoke({"messages":[{"role":"user","content": query}]}) actual = result["messages"][0].tool_calls t.log_outputs({"tool_calls": actual})# Check that no tool calls were made.assert actual == expected
```

```
ls.test.each([{ inputs:{ query:"Hello!"}, expected:{ numMessages:2}},{ inputs:{ query:"How are you doing?"}, expected:{ numMessages:2}},])("should not use tools on offtopic query: %s",async({ inputs:{ query }, expected:{ numMessages }})=>{const result =await agent.invoke({ messages:[{ role:"user", content: query }]});  ls.logOutputs(result);// Check that the flow was HUMAN -> AI FINAL RESPONSE (no tools called)expect(result.messages).toHaveLength(numMessages);});
```

```
ls.test.each([{ inputs:{ query:"Hello!"}, expected:{ numMessages:2}},{ inputs:{ query:"How are you doing?"}, expected:{ numMessages:2}},])("should not use tools on offtopic query: %s",async({ inputs:{ query }, expected:{ numMessages }})=>{const result =await agent.invoke({ messages:[{ role:"user", content: query }]});  ls.logOutputs(result);// Check that the flow was HUMAN -> AI FINAL RESPONSE (no tools called)expect(result.messages).toHaveLength(numMessages);});
```

### Test 2: Simple tool calling[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#test-2-simple-tool-calling "Direct link to Test 2: Simple tool calling")
For tool calling, we are going to verify that the agent calls the correct tool with the correct parameters.
  * Pytest
  * Vitest
  * Jest


```
@pytest.mark.langsmithdeftest_searches_for_correct_ticker()->None:"""Test that the model looks up the correct ticker on simple query."""# Log the test example query ="What is the price of Apple?" t.log_inputs({"query": query}) expected ="AAPL" t.log_reference_outputs({"ticker": expected})# Call the agent's model node directly instead of running the full ReACT loop. result = agent.nodes["agent"].invoke({"messages":[{"role":"user","content": query}]}) tool_calls = result["messages"][0].tool_callsif tool_calls[0]["name"]== polygon_aggregates.name:   actual = tool_calls[0]["args"]["ticker"]else:   actual =None t.log_outputs({"ticker": actual})# Check that the right ticker was queriedassert actual == expected
```

```
ls.test("should search for correct ticker",{  inputs:{ query:"What is the price of Apple?"},  expected:{ numMessages:4},},async({ inputs:{ query }, expected:{ numMessages }})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});  ls.logOutputs(result);// The agent should have made a single tool call to the ticker toolconst toolCalls =(result.messages[1]as AIMessage).tool_calls ||[];const tickerQuery =JSON.parse(toolCalls[0].function.arguments).query.ticker;// Check that the right ticker was queriedexpect(tickerQuery).toBe("AAPL");// Check that the flow was HUMAN -> AI -> TOOL -> AI FINAL RESPONSEexpect(result.messages).toHaveLength(numMessages);});
```

```
ls.test("should search for correct ticker",{  inputs:{ query:"What is the price of Apple?"},  expected:{ numMessages:4},},async({ inputs:{ query }, expected:{ numMessages }})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});  ls.logOutputs(result);// The agent should have made a single tool call to the ticker toolconst toolCalls =(result.messages[1]as AIMessage).tool_calls ||[];const tickerQuery =JSON.parse(toolCalls[0].function.arguments).query.ticker;// Check that the right ticker was queriedexpect(tickerQuery).toBe("AAPL");// Check that the flow was HUMAN -> AI -> TOOL -> AI FINAL RESPONSEexpect(result.messages).toHaveLength(numMessages);});
```

### Test 3: Complex tool calling[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#test-3-complex-tool-calling "Direct link to Test 3: Complex tool calling")
Some tool calls are easier to test than others. With the ticker lookup, we can assert that the correct ticker is searched. With the coding tool, the inputs and outputs of the tool are much less constrained, and there are lots of ways to get to the right answer. In this case, it's simpler to test that the tool is used correctly by running the full agent and asserting that it both calls the coding tool and that it ends up with the right answer.
  * Pytest
  * Vitest
  * Jest


```
@pytest.mark.langsmithdeftest_executes_code_when_needed()->None: query =("In the past year Facebook stock went up by 66.76%, ""Apple by 25.24%, Google by 37.11%, Amazon by 47.52%, ""Netflix by 78.31%. Whats the avg return in the past ""year of the FAANG stocks, expressed as a percentage?") t.log_inputs({"query": query}) expected =50.988 t.log_reference_outputs({"response": expected})# Test that the agent executes code when needed result = agent.invoke({"messages":[{"role":"user","content": query}]}) t.log_outputs({"result": result["structured_response"].get("numeric_answer")})# Grab all the tool calls made by the LLM tool_calls =[   tc["name"]for msg in result["messages"]for tc ingetattr(msg,"tool_calls",[])]# This will log the number of steps taken by the agent, which is useful for# determining how efficiently the agent gets to an answer. t.log_feedback(key="num_steps", score=len(result["messages"])-1)# Assert that the code tool was usedassert"code_tool"in tool_calls# Assert that a numeric answer was provided:assert result["structured_response"].get("numeric_answer")isnotNone# Assert that the answer is correctassertabs(result["structured_response"]["numeric_answer"]- expected)<=0.01
```

```
ls.test("should execute code when needed",{  inputs:{ query:"What was the average return rate for FAANG stock in 2024?"},  expected:{ answer:53},},async({ inputs:{ query }, expected:{ answer }})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});  ls.logOutputs(result);// Grab all the tool calls made by the LLMconst toolCalls = result.messages.filter(m =>(m as AIMessage).tool_calls).flatMap(m =>(m as AIMessage).tool_calls?.map(tc => tc.name));// This will log the number of steps taken by the LLM, which we can track over time to measure performance  ls.logFeedback({   key:"num_steps",   score: result.messages.length -1,// The first message is the user message});// Assert that the tool calls include the "code_tool" functionexpect(toolCalls).toContain("code_tool");// Assert that the answer is within 1 of the expected answerexpect(Math.abs(result.structured_response.numeric_answer - answer)).toBeLessThanOrEqual(1);});
```

```
ls.test("should execute code when needed",{  inputs:{ query:"What was the average return rate for FAANG stock in 2024?"},  expected:{ answer:53},},async({ inputs:{ query }, expected:{ answer }})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});  ls.logOutputs(result);// Grab all the tool calls made by the LLMconst toolCalls = result.messages.filter(m =>(m as AIMessage).tool_calls).flatMap(m =>(m as AIMessage).tool_calls?.map(tc => tc.name));// This will log the number of steps taken by the LLM, which we can track over time to measure performance  ls.logFeedback({   key:"num_steps",   score: result.messages.length -1,// The first message is the user message});// Assert that the tool calls include the "code_tool" functionexpect(toolCalls).toContain("code_tool");// Assert that the answer is within 1 of the expected answerexpect(Math.abs(result.structured_response.numeric_answer - answer)).toBeLessThanOrEqual(1);});
```

### Test 4: LLM-as-a-judge[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#test-4-llm-as-a-judge "Direct link to Test 4: LLM-as-a-judge")
We are going to ensure that the agent's answer is grounded in the search results by running an LLM-as-a-judge evaluation. In order to trace the LLM as a judge call separately from our agent, we will use the LangSmith provided `trace_feedback` context manager in Python and `wrapEvaluator` function in JS/TS.
  * Pytest
  * Vitest
  * Jest


```
from typing_extensions import Annotated, TypedDictfrom langchain.chat_models import init_chat_modelclassGrade(TypedDict):"""Evaluate the groundedness of an answer in source documents.""" score: Annotated[bool,...,"Return True if the answer is fully grounded in the source documents, otherwise False.",]judge_llm = init_chat_model("gpt-4o").with_structured_output(Grade)@pytest.mark.langsmithdeftest_grounded_in_source_info()->None:"""Test that response is grounded in the tool outputs.""" query ="How did Nvidia stock do in 2024 according to analysts?" t.log_inputs({"query": query}) result = agent.invoke({"messages":[{"role":"user","content": query}]})# Grab all the search calls made by the LLM search_results ="\n\n".join(   msg.contentfor msg in result["messages"]if msg.type=="tool"and msg.name == search_tool.name) t.log_outputs({"response": result["structured_response"].get("text_answer"),"search_results": search_results,})# Trace the feedback LLM run separately from the agent run.with t.trace_feedback():# Instructions for the LLM judge   instructions =("Grade the following ANSWER. ""The ANSWER should be fully grounded in (i.e. supported by) the source DOCUMENTS. ""Return True if the ANSWER is fully grounded in the DOCUMENTS. ""Return False if the ANSWER is not grounded in the DOCUMENTS.")   answer_and_docs =(f"ANSWER: {result['structured_response'].get('text_answer','')}\n"f"DOCUMENTS:\n{search_results}")# Run the judge LLM   grade = judge_llm.invoke([{"role":"system","content": instructions},{"role":"user","content": answer_and_docs},])   t.log_feedback(key="groundedness", score=grade["score"])assert grade['score']
```

```
// THIS CODE GOES OUTSIDE THE TEST - IT IS JUST A HELPER FUNCTIONconst judgeLLM =newChatOpenAI({ model:"gpt-4o"});constgroundedEvaluator=async(params:{ answer:string; referenceDocuments:string,})=>{// Instructions for the LLM judgeconst instructions =["Return 1 if the ANSWER is grounded in the DOCUMENTS","Return 0 if the ANSWER is not grounded in the DOCUMENTS",].join("\n");// Run the judge LLMconst grade =await judgeLLM.invoke([{ role:"system", content: instructions },{ role:"user", content:`ANSWER: ${params.answer}\nDOCUMENTS: ${params.referenceDocuments}`},]);const score =parseInt(grade.content.toString());return{ key:"groundedness", score };};// THIS CODE GOES INSIDE THE TESTls.test("grounded in the source",{  inputs:{ query:"How did Nvidia stock do in 2024 according to analysts?"},},async({ inputs:{ query }})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});const wrappedEvaluator = ls.wrapEvaluator(groundedEvaluator);awaitwrappedEvaluator({   answer: result.structuredResponse.text_answer ??"",   referenceDocuments: result.structuredResponse.reasoning,})  ls.logOutputs(result);});
```

```
// THIS CODE GOES OUTSIDE THE TEST - IT IS JUST A HELPER FUNCTIONconst judgeLLM =newChatOpenAI({ model:"gpt-4o"});constgroundedEvaluator=async(params:{ answer:string; referenceDocuments:string,})=>{// Instructions for the LLM judgeconst instructions =["Return 1 if the ANSWER is grounded in the DOCUMENTS","Return 0 if the ANSWER is not grounded in the DOCUMENTS",].join("\n");// Run the judge LLMconst grade =await judgeLLM.invoke([{ role:"system", content: instructions },{ role:"user", content:`ANSWER: ${params.answer}\nDOCUMENTS: ${params.referenceDocuments}`},]);const score =parseInt(grade.content.toString());return{ key:"groundedness", score };};// THIS CODE GOES INSIDE THE TESTls.test("grounded in the source",{  inputs:{ query:"How did Nvidia stock do in 2024 according to analysts?"},},async({ inputs:{ query }})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});const wrappedEvaluator = ls.wrapEvaluator(groundedEvaluator);awaitwrappedEvaluator({   answer: result.structuredResponse.text_answer ??"",   referenceDocuments: result.structuredResponse.reasoning,})  ls.logOutputs(result);});
```

## Run tests[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#run-tests "Direct link to Run tests")
Once you have setup your config files (if you are using Vitest or Jest), you can run your tests using the following commands:
Config files for Vitest/Jest
  * Vitest
  * Jest


ls.vitest.config.ts
```
import{ defineConfig }from"vitest/config";exportdefaultdefineConfig({ test:{  include:["**/*.eval.?(c|m)[jt]s"],  reporters:["langsmith/vitest/reporter"],  setupFiles:["dotenv/config"],},});
```

ls.jest.config.ts
```
require('dotenv').config();module.exports ={ preset:'ts-jest', testEnvironment:'node', testMatch:['<rootDir>/tests/jest/**/*.jest.eval.ts'], testPathIgnorePatterns:['<rootDir>/tests/vitest/.*.vitest.eval.ts$'], reporters:["langsmith/jest/reporter"],};
```

  * Pytest
  * Vitest
  * Jest


```
pytest --langsmith-output tests
```

```
yarn vitest --config ls.vitest.config.ts
```

```
yarn jest --config ls.jest.config.ts
```

## Reference code[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#reference-code "Direct link to Reference code")
Remember to also add the config files for [Vitest](https://docs.smith.langchain.com/evaluation/tutorials/testing#config-file-for-vitest) and [Jest](https://docs.smith.langchain.com/evaluation/tutorials/testing#config-file-for-jest) to your project.
### Agent[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#agent "Direct link to Agent")
Agent code
  * Pytest
  * TypeScript


```
from typing import Optionalfrom e2b_code_interpreter import Sandboxfrom langchain_community.tools import PolygonAggregates, TavilySearchResultsfrom langchain_community.utilities.polygon import PolygonAPIWrapperfrom langgraph.prebuilt import create_react_agentfrom typing_extensions import Annotated, TypedDictsearch_tool = TavilySearchResults( max_results=5, include_raw_content=True,)defcode_tool(code:str)->str:"""Execute python code and return the result.""" sbx = Sandbox() execution = sbx.run_code(code)if execution.error:returnf"Error: {execution.error}"returnf"Results: {execution.results}, Logs: {execution.logs}"polygon_aggregates = PolygonAggregates(api_wrapper=PolygonAPIWrapper())classAgentOutputFormat(TypedDict): numeric_answer: Annotated[   Optional[float],...,"The numeric answer, if the user asked for one"] text_answer: Annotated[   Optional[str],...,"The text answer, if the user asked for one"] reasoning: Annotated[str,...,"The reasoning behind the answer"]agent = create_react_agent( model="openai:gpt-4o-mini", tools=[code_tool, search_tool, polygon_aggregates], response_format=AgentOutputFormat, state_modifier="You are a financial expert. Respond to the users query accurately",)
```

```
import{ ChatOpenAI }from"@langchain/openai";import{ createReactAgent }from"@langchain/langgraph/prebuilt";import{ TavilySearchResults }from"@langchain/community/tools/tavily_search";import{ Sandbox }from'@e2b/code-interpreter'import{ restClient }from'@polygon.io/client-js';import{ tool }from"@langchain/core/tools";import{ z }from"zod";const codeTool =tool(async(input)=>{const sbx =await Sandbox.create();const execution =await sbx.runCode(input.code);if(execution.error){return`Error: ${execution.error}`;}return`Results: ${execution.results}, Logs: ${execution.logs}`;},{ name:"code", description:"Execute python code and return the result.", schema: z.object({  code: z.string().describe("The python code to execute"),}),});const TickerToolInputSchema = z.object({ticker: z.string().describe("The ticker symbol of the stock"),timespan: z.enum(["minute","hour","day","week","month","quarter","year"]).describe("The size of the time window."),timespan_multiplier: z.number().describe("The multiplier for the time window"),from_date: z.string().describe("The date to start pulling data from, YYYY-MM-DD format - ONLY include the year, month, and day"),to_date: z.string().describe("The date to stop pulling data, YYYY-MM-DD format - ONLY include the year, month, and day"),});const rest =restClient(process.env.POLYGON_API_KEY);const tickerTool =tool(async(query)=>{const parsed = TickerToolInputSchema.parse(query);const result =await rest.stocks.aggregates(  parsed.ticker,  parsed.timespan_multiplier,  parsed.timespan,  parsed.from_date,  parsed.to_date);returnJSON.stringify(result);},{ name:"ticker", description:"Pull data for the ticker", schema: TickerToolInputSchema,});const searchTool =newTavilySearchResults({maxResults:5,});const AgentOutputFormatSchema = z.object({numeric_answer: z.number().optional().describe("The numeric answer, if the user asked for one"),text_answer: z.string().optional().describe("The text answer, if the user asked for one"),reasoning: z.string().describe("The reasoning behind the answer"),})const tools =[codeTool, searchTool, tickerTool];const agent =createReactAgent({llm:newChatOpenAI({ model:"gpt-4o"}),tools: tools,responseFormat: AgentOutputFormatSchema,stateModifier:"You are a financial expert. Respond to the users query accurately",});exportdefault agent;
```

### Tests[‚Äã](https://docs.smith.langchain.com/evaluation/tutorials/testing#tests "Direct link to Tests")
Test code
  * Pytest
  * Vitest
  * Jest


```
# from app import agent, polygon_aggregates, search_tool # import from wherever your agent is definedimport pytestfrom langchain.chat_models import init_chat_modelfrom langsmith import testing as tfrom typing_extensions import Annotated, TypedDict@pytest.mark.langsmith@pytest.mark.parametrize(# <-- Can still use all normal pytest markers"query",["Hello!","How are you doing?"],)deftest_no_tools_on_offtopic_query(query:str)->None:"""Test that the agent does not use tools on offtopic queries."""# Log the test example t.log_inputs({"query": query}) expected =[] t.log_reference_outputs({"tool_calls": expected})# Call the agent's model node directly instead of running the ReACT loop. result = agent.nodes["agent"].invoke({"messages":[{"role":"user","content": query}]}) actual = result["messages"][0].tool_calls t.log_outputs({"tool_calls": actual})# Check that no tool calls were made.assert actual == expected@pytest.mark.langsmithdeftest_searches_for_correct_ticker()->None:"""Test that the model looks up the correct ticker on simple query."""# Log the test example query ="What is the price of Apple?" t.log_inputs({"query": query}) expected ="AAPL" t.log_reference_outputs({"ticker": expected})# Call the agent's model node directly instead of running the full ReACT loop. result = agent.nodes["agent"].invoke({"messages":[{"role":"user","content": query}]}) tool_calls = result["messages"][0].tool_callsif tool_calls[0]["name"]== polygon_aggregates.name:   actual = tool_calls[0]["args"]["ticker"]else:   actual =None t.log_outputs({"ticker": actual})# Check that the right ticker was queriedassert actual == expected@pytest.mark.langsmithdeftest_executes_code_when_needed()->None: query =("In the past year Facebook stock went up by 66.76%, ""Apple by 25.24%, Google by 37.11%, Amazon by 47.52%, ""Netflix by 78.31%. Whats the avg return in the past ""year of the FAANG stocks, expressed as a percentage?") t.log_inputs({"query": query}) expected =50.988 t.log_reference_outputs({"response": expected})# Test that the agent executes code when needed result = agent.invoke({"messages":[{"role":"user","content": query}]}) t.log_outputs({"result": result["structured_response"].get("numeric_answer")})# Grab all the tool calls made by the LLM tool_calls =[   tc["name"]for msg in result["messages"]for tc ingetattr(msg,"tool_calls",[])]# This will log the number of steps taken by the agent, which is useful for# determining how efficiently the agent gets to an answer. t.log_feedback(key="num_steps", value=len(result["messages"])-1)# Assert that the code tool was usedassert"code_tool"in tool_calls# Assert that a numeric answer was provided:assert result["structured_response"].get("numeric_answer")isnotNone# Assert that the answer is correctassertabs(result["structured_response"]["numeric_answer"]- expected)<=0.01classGrade(TypedDict):"""Evaluate the groundedness of an answer in source documents.""" score: Annotated[bool,...,"Return True if the answer is fully grounded in the source documents, otherwise False.",]judge_llm = init_chat_model("gpt-4o").with_structured_output(Grade)@pytest.mark.langsmithdeftest_grounded_in_source_info()->None:"""Test that response is grounded in the tool outputs.""" query ="How did Nvidia stock do in 2024 according to analysts?" t.log_inputs({"query": query}) result = agent.invoke({"messages":[{"role":"user","content": query}]})# Grab all the search calls made by the LLM search_results ="\n\n".join(   msg.contentfor msg in result["messages"]if msg.type=="tool"and msg.name == search_tool.name) t.log_outputs({"response": result["structured_response"].get("text_answer"),"search_results": search_results,})# Trace the feedback LLM run separately from the agent run.with t.trace_feedback():# Instructions for the LLM judge   instructions =("Grade the following ANSWER. ""The ANSWER should be fully grounded in (i.e. supported by) the source DOCUMENTS. ""Return True if the ANSWER is fully grounded in the DOCUMENTS. ""Return False if the ANSWER is not grounded in the DOCUMENTS.")   answer_and_docs =(f"ANSWER: {result['structured_response'].get('text_answer','')}\n"f"DOCUMENTS:\n{search_results}")# Run the judge LLM   grade = judge_llm.invoke([{"role":"system","content": instructions},{"role":"user","content": answer_and_docs},])   t.log_feedback(key="groundedness", score=grade["score"])assert grade["score"]
```

```
import{ expect }from"vitest";import*as ls from"langsmith/vitest";import agent from"../agent";import{ AIMessage, ToolMessage }from"@langchain/core/messages";import{ ChatOpenAI }from"@langchain/openai";const judgeLLM =newChatOpenAI({ model:"gpt-4o"});constgroundedEvaluator=async(params:{ answer:string; referenceDocuments:string,})=>{const instructions =["Return 1 if the ANSWER is grounded in the DOCUMENTS","Return 0 if the ANSWER is not grounded in the DOCUMENTS",].join("\n");const grade =await judgeLLM.invoke([{ role:"system", content: instructions },{ role:"user", content:`ANSWER: ${params.answer}\nDOCUMENTS: ${params.referenceDocuments}`},]);const score =parseInt(grade.content.toString());return{ key:"groundedness", score };}ls.describe("Agent Tests",()=>{ls.test.each([{ inputs:{ query:"Hello!"}, referenceOutputs:{ numMessages:2}},{ inputs:{ query:"How are you doing?"}, referenceOutputs:{ numMessages:2}},])("should not use tools on offtopic query: %s",async({ inputs:{ query }, referenceOutputs:{ numMessages }})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});  ls.logOutputs(result);expect(result.messages).toHaveLength(numMessages);});ls.test("should search for correct ticker",{  inputs:{ query:"What is the price of Apple?"},  referenceOutputs:{ numMessages:4},},async({ inputs:{ query }, referenceOutputs:{ numMessages }})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});const toolCalls =(result.messages[1]as AIMessage).tool_calls ||[];const tickerQuery = toolCalls[0].args.ticker;  ls.logOutputs(result);expect(tickerQuery).toBe("AAPL");expect(result.messages).toHaveLength(numMessages);});ls.test("should execute code when needed",{  inputs:{ query:"What was the average return rate for FAANG stock in 2024?"},  referenceOutputs:{ answer:53},},async({ inputs:{ query }, referenceOutputs:{ answer }})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});const toolCalls = result.messages.filter(m =>(m as AIMessage).tool_calls).flatMap(m =>(m as AIMessage).tool_calls?.map(tc => tc.name));  ls.logFeedback({   key:"num_steps",   score: result.messages.length -1,});  ls.logOutputs(result);expect(toolCalls).toContain("code_tool");expect(Math.abs((result.structuredResponse.numeric_answer ??0- answer)- answer)).toBeLessThanOrEqual(1);});ls.test("grounded in the source",{  inputs:{ query:"How did Nvidia stock do in 2024?"},  referenceOutputs:{},},async({ inputs:{ query }, referenceOutputs:{}})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});const referenceDocuments = result.messages.filter((m): m is ToolMessage => m.name?.includes('tavily_search_results_json')??false).map(m => m.content).join('\n');const wrappedEvaluator = ls.wrapEvaluator(groundedEvaluator);awaitwrappedEvaluator({   answer: result.structuredResponse.text_answer ??"",   referenceDocuments: referenceDocuments,})  ls.logOutputs(result);});});
```

```
import{ expect }from"@jest/globals";import*as ls from"langsmith/jest";import agent from"../agent";import{ AIMessage }from"@langchain/core/messages";import{ ChatOpenAI }from"@langchain/openai";const judgeLLM =newChatOpenAI({ model:"gpt-4o"});constgroundedEvaluator=async(params:{ answer:string; referenceDocuments:string,})=>{const instructions =["Return 1 if the ANSWER is grounded in the DOCUMENTS","Return 0 if the ANSWER is not grounded in the DOCUMENTS",].join("\n");const grade =await judgeLLM.invoke([{ role:"system", content: instructions },{ role:"user", content:`ANSWER: ${params.answer}\nDOCUMENTS: ${params.referenceDocuments}`},]);const score =parseInt(grade.content.toString());return{ key:"groundedness", score };}ls.describe("Agent Tests",()=>{ls.test.each([{ inputs:{ query:"Hello!"}, referenceOutputs:{ numMessages:2}},{ inputs:{ query:"How are you doing?"}, referenceOutputs:{ numMessages:2}},])("should not use tools on offtopic query: %s",async({ inputs:{ query }, referenceOutputs:{ numMessages }})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});  ls.logOutputs(result);expect(result.messages).toHaveLength(numMessages);});ls.test("should search for correct ticker",{  inputs:{ query:"What is the price of Apple?"},  referenceOutputs:{ numMessages:4},},async({ inputs:{ query }, referenceOutputs:{ numMessages }})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});const toolCalls =(result.messages[1]as AIMessage).tool_calls ||[];const tickerQuery = toolCalls[0].args.ticker;  ls.logOutputs(result);expect(tickerQuery).toBe("AAPL");expect(result.messages).toHaveLength(numMessages);});ls.test("should execute code when needed",{  inputs:{ query:"What was the average return rate for FAANG stock in 2024?"},  referenceOutputs:{ answer:53},},async({ inputs:{ query }, referenceOutputs:{ answer }})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});const toolCalls = result.messages.filter(m =>(m as AIMessage).tool_calls).flatMap(m =>(m as AIMessage).tool_calls?.map(tc => tc.name));  ls.logFeedback({   key:"num_steps",   score: result.messages.length -1,});  ls.logOutputs(result);expect(toolCalls).toContain("code_tool");expect(Math.abs(result.structuredResponse.numeric_answer ??0- answer)).toBeLessThanOrEqual(1);});ls.test("grounded in the source",{  inputs:{ query:"How did Nvidia stock do in 2024 according to analysts?"},  referenceOutputs:{},},async({ inputs:{ query }, referenceOutputs:{}})=>{const result =await agent.invoke({   messages:[{ role:"user", content: query }],});const wrappedEvaluator = ls.wrapEvaluator(groundedEvaluator);awaitwrappedEvaluator({   answer: result.structuredResponse.text_answer ??"",   referenceDocuments: result.structuredResponse.reasoning,})  ls.logOutputs(result);});});
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousEvaluate a complex agent](https://docs.smith.langchain.com/evaluation/tutorials/agents)[NextEvaluation how-to guides](https://docs.smith.langchain.com/evaluation/how_to_guides)
  * [Setup](https://docs.smith.langchain.com/evaluation/tutorials/testing#setup)
    * [Installation](https://docs.smith.langchain.com/evaluation/tutorials/testing#installation)
    * [Environment Variables](https://docs.smith.langchain.com/evaluation/tutorials/testing#environment-variables)
  * [Create your app](https://docs.smith.langchain.com/evaluation/tutorials/testing#create-your-app)
    * [Define tools](https://docs.smith.langchain.com/evaluation/tutorials/testing#define-tools)
    * [Define agent](https://docs.smith.langchain.com/evaluation/tutorials/testing#define-agent)
  * [Write tests](https://docs.smith.langchain.com/evaluation/tutorials/testing#write-tests)
    * [Test 1: Handle off-topic questions](https://docs.smith.langchain.com/evaluation/tutorials/testing#test-1-handle-off-topic-questions)
    * [Test 2: Simple tool calling](https://docs.smith.langchain.com/evaluation/tutorials/testing#test-2-simple-tool-calling)
    * [Test 3: Complex tool calling](https://docs.smith.langchain.com/evaluation/tutorials/testing#test-3-complex-tool-calling)
    * [Test 4: LLM-as-a-judge](https://docs.smith.langchain.com/evaluation/tutorials/testing#test-4-llm-as-a-judge)
  * [Run tests](https://docs.smith.langchain.com/evaluation/tutorials/testing#run-tests)
  * [Reference code](https://docs.smith.langchain.com/evaluation/tutorials/testing#reference-code)
    * [Agent](https://docs.smith.langchain.com/evaluation/tutorials/testing#agent)
    * [Tests](https://docs.smith.langchain.com/evaluation/tutorials/testing#tests)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/langgraph_cloud

[Skip to main content](https://docs.smith.langchain.com/langgraph_cloud#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/langgraph_cloud)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/langgraph_cloud)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/langgraph_cloud)
    * [Evaluation](https://docs.smith.langchain.com/langgraph_cloud)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/langgraph_cloud)


  * [](https://docs.smith.langchain.com/)
  * Deployment (LangGraph Platform)


# LangGraph Platform
LangGraph Platform is a managed service for deploying and hosting LangGraph applications. Deploying applications with LangGraph Cloud shortens the time-to-market for developers. With one click, deploy a production-ready API with built-in persistence for your LangGraph application. LangGraph Cloud APIs are horizontally scalable and deployed with durable storage.
LangGraph Platform is seamlessly integrated with LangSmith and is accessible from the **Deployments** section in the left-hand navigation bar.
See the official [LangGraph Platform documentation](https://langchain-ai.github.io/langgraph/cloud/) for more details.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/langgraph_cloud%3E).
[PreviousConceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)[NextTutorials](https://docs.smith.langchain.com/administration/tutorials)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability

[Skip to main content](https://docs.smith.langchain.com/observability#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * Observability


On this page
# Observability Quick Start
This tutorial will get you up and running with our observability SDK by showing you how to trace your application to LangSmith.
If you're already familiar with the observability SDK, or are interested in tracing more than just LLM calls you can skip to the [next steps section](https://docs.smith.langchain.com/observability#next-steps), or check out the [how-to guides](https://docs.smith.langchain.com/observability/how_to_guides).
Trace LangChain or LangGraph Applications
If you are using [LangChain](https://python.langchain.com/docs/introduction/) or [LangGraph](https://langchain-ai.github.io/langgraph/), which both integrate seamlessly with LangSmith, you can get started by reading the guides for tracing with [LangChain](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain) or tracing with [LangGraph](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph).
## 1. Install Dependencies[‚Äã](https://docs.smith.langchain.com/observability#1-install-dependencies "Direct link to 1. Install Dependencies")
  * Python
  * TypeScript


```
pip install -U langsmith openai
```

```
yarn add langsmith openai
```

## 2. Create an API key[‚Äã](https://docs.smith.langchain.com/observability#2-create-an-api-key "Direct link to 2. Create an API key")
To create an API key head to the [LangSmith settings page](https://smith.langchain.com/settings). Then click **Create API Key.**
## 3. Set up your environment[‚Äã](https://docs.smith.langchain.com/observability#3-set-up-your-environment "Direct link to 3. Set up your environment")
  * Shell


```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY="<your-langsmith-api-key>"# The example uses OpenAI, but it's not necessary if your code uses another LLM providerexport OPENAI_API_KEY="<your-openai-api-key>"
```

## 4. Define your application[‚Äã](https://docs.smith.langchain.com/observability#4-define-your-application "Direct link to 4. Define your application")
We will instrument a simple [RAG](https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-retrieval-augmented-generation-rag) application for this tutorial, but feel free to use your own code if you'd like - just make sure it has an LLM call!
Application Code
  * Python
  * TypeScript


```
from openai import OpenAIopenai_client = OpenAI()# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdefretriever(query:str):  results =["Harrison worked at Kensho"]return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdefrag(question):  docs = retriever(question)  system_message ="""Answer the users question using only the provided information below:  {docs}""".format(docs="\n".join(docs))return openai_client.chat.completions.create(    messages=[{"role":"system","content": system_message},{"role":"user","content": question},],    model="gpt-4o-mini",)
```

```
import{ OpenAI }from"openai";const openAIClient =newOpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasyncfunctionretriever(query:string){return["This is a document"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasyncfunctionrag(question:string){const docs =awaitretriever(question);const systemMessage ="Answer the users question using only the provided information below:\n\n"+  docs.join("\n");returnawait openAIClient.chat.completions.create({  messages:[{ role:"system", content: systemMessage },{ role:"user", content: question },],  model:"gpt-4o-mini",});}
```

## 5. Trace OpenAI calls[‚Äã](https://docs.smith.langchain.com/observability#5-trace-openai-calls "Direct link to 5. Trace OpenAI calls")
The first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the [`wrap_openai`](https://docs.smith.langchain.com/reference/python/wrappers/langsmith.wrappers._openai.wrap_openai_) (Python) or [`wrapOpenAI`](https://docs.smith.langchain.com/reference/js/functions/wrappers_openai.wrapOpenAI) (TypeScript) wrappers. All you have to do is modify your code to use the wrapped client instead of using the `OpenAI` client directly.
  * Python
  * TypeScript


```
from openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdefretriever(query:str):  results =["Harrison worked at Kensho"]return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdefrag(question):  docs = retriever(question)  system_message ="""Answer the users question using only the provided information below:  {docs}""".format(docs="\n".join(docs))return openai_client.chat.completions.create(    messages=[{"role":"system","content": system_message},{"role":"user","content": question},],    model="gpt-4o-mini",)
```

```
import{ OpenAI }from"openai";import{ wrapOpenAI }from"langsmith/wrappers";const openAIClient =wrapOpenAI(newOpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasyncfunctionretriever(query:string){return["This is a document"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasyncfunctionrag(question:string){const docs =awaitretriever(question);const systemMessage ="Answer the users question using only the provided information below:\n\n"+  docs.join("\n");returnawait openAIClient.chat.completions.create({  messages:[{ role:"system", content: systemMessage },{ role:"user", content: question },],  model:"gpt-4o-mini",});}
```

Now when you call your application as follows:
```
rag("where did harrison work")
```

This will produce a trace of just the OpenAI call in LangSmith's default tracing project. It should look something like [this](https://smith.langchain.com/public/e7b7d256-10fe-4d49-a8d5-36ca8e5af0d2/r).
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_openai-667b87c0df7e5bd45538c165314d7e22.png)
## 6. Trace entire application[‚Äã](https://docs.smith.langchain.com/observability#6-trace-entire-application "Direct link to 6. Trace entire application")
You can also use the [`traceable`] decorator ([Python](https://docs.smith.langchain.com/reference/python/run_helpers/langsmith.run_helpers.traceable) or [TypeScript](https://langsmith-docs-bdk0fivr6-langchain.vercel.app/reference/js/functions/traceable.traceable)) to trace your entire application instead of just the LLM calls.
  * Python
  * TypeScript


```
from openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())defretriever(query:str):  results =["Harrison worked at Kensho"]return results@traceabledefrag(question):  docs = retriever(question)  system_message ="""Answer the users question using only the provided information below:  {docs}""".format(docs="\n".join(docs))return openai_client.chat.completions.create(    messages=[{"role":"system","content": system_message},{"role":"user","content": question},],    model="gpt-4o-mini",)
```

```
import{ OpenAI }from"openai";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers";const openAIClient =wrapOpenAI(newOpenAI());asyncfunctionretriever(query:string){return["This is a document"];}const rag =traceable(asyncfunctionrag(question:string){const docs =awaitretriever(question);const systemMessage ="Answer the users question using only the provided information below:\n\n"+  docs.join("\n");returnawait openAIClient.chat.completions.create({  messages:[{ role:"system", content: systemMessage },{ role:"user", content: question },],  model:"gpt-4o-mini",});});
```

Now if you call your application as follows:
```
rag("where did harrison work")
```

This will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like [this](https://smith.langchain.com/public/2174f4e9-48ab-4f9e-a8c4-470372d976f1/r)
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_chain-5023f6584725ddccf4052f7fc050977c.png)
## Next steps[‚Äã](https://docs.smith.langchain.com/observability#next-steps "Direct link to Next steps")
Congratulations! If you've made it this far, you're well on your way to being an expert in observability with LangSmith. Here are some topics you might want to explore next:
  * [Trace multiturn conversations](https://docs.smith.langchain.com/observability/how_to_guides/threads)
  * [Send traces to a specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
  * [Filter traces in a project](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)


Or you can visit the [how-to guides page](https://docs.smith.langchain.com/observability/how_to_guides) to find out about all the things you can do with LangSmith observability.
If you prefer a video tutorial, check out the [Tracing Basics video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousGet Started](https://docs.smith.langchain.com/)[NextQuick Start](https://docs.smith.langchain.com/observability)
  * [1. Install Dependencies](https://docs.smith.langchain.com/observability#1-install-dependencies)
  * [2. Create an API key](https://docs.smith.langchain.com/observability#2-create-an-api-key)
  * [3. Set up your environment](https://docs.smith.langchain.com/observability#3-set-up-your-environment)
  * [4. Define your application](https://docs.smith.langchain.com/observability#4-define-your-application)
  * [5. Trace OpenAI calls](https://docs.smith.langchain.com/observability#5-trace-openai-calls)
  * [6. Trace entire application](https://docs.smith.langchain.com/observability#6-trace-entire-application)
  * [Next steps](https://docs.smith.langchain.com/observability#next-steps)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/concepts

[Skip to main content](https://docs.smith.langchain.com/observability/concepts#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/concepts)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/concepts)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/concepts)
    * [Evaluation](https://docs.smith.langchain.com/observability/concepts)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/concepts)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * Conceptual Guide


On this page
# Concepts
This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A `Trace` is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a `Run`. A `Project` is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer.
![LangSmith Primitives](https://docs.smith.langchain.com/assets/images/primitives-708e671bad3ba4cd65e2eaaa3d64d40b.png)
Primitive datatypes in LangSmith
## Runs[‚Äã](https://docs.smith.langchain.com/observability/concepts#runs "Direct link to Runs")
A `Run` is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with [OpenTelemetry](https://opentelemetry.io/), you can think of a run as a span.
To learn more about how runs are stored in the application, see [this reference guide](https://docs.smith.langchain.com/reference/data_formats/run_data_format).
![Run](https://docs.smith.langchain.com/assets/images/run-4d754fd31fb6f3a2b1bf054a2a8b0b42.png)
## Traces[‚Äã](https://docs.smith.langchain.com/observability/concepts#traces "Direct link to Traces")
A `Trace` is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with [OpenTelemetry](https://opentelemetry.io/), you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID. ![Trace](https://docs.smith.langchain.com/assets/images/trace-8d4b4c1fde7abcddaad4fa8a56f2e27b.png)
## Projects[‚Äã](https://docs.smith.langchain.com/observability/concepts#projects "Direct link to Projects")
A `Project` is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces. ![Project](https://docs.smith.langchain.com/assets/images/project-9fc0692079f84a1df9fdabb89add8652.png)
## Feedback[‚Äã](https://docs.smith.langchain.com/observability/concepts#feedback "Direct link to Feedback")
`Feedback` allows you to score an individual run based on certain criteria. Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID. Feedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization.
Collecting feedback on runs can be done in a number of ways:
  1. [Sent up along with a trace](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback) from the LLM application
  2. Generated by a user in the app [inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline) or in an [annotation queue](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
  3. Generated by an automatic evaluator during [offline evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
  4. Generated by an [online evaluator](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)


To learn more about how feedback is stored in the application, see [this reference guide](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format).
![Feedback](https://docs.smith.langchain.com/assets/images/feedback-0f2b3437b61a53482968180e68181d82.png)
## Tags[‚Äã](https://docs.smith.langchain.com/observability/concepts#tags "Direct link to Tags")
`Tags` are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. [Learn how to attach tags to your traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags) ![Tags](https://docs.smith.langchain.com/assets/images/tags-1829bc06f85f1fdab97d551d6ff5d040.png)
## Metadata[‚Äã](https://docs.smith.langchain.com/observability/concepts#metadata "Direct link to Metadata")
`Metadata` is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. [Learn how to add metadata to your traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags) ![Metadata](https://docs.smith.langchain.com/assets/images/metadata-1108d056aeaff94b835d18a355e124f5.png)
## Data storage and retention[‚Äã](https://docs.smith.langchain.com/observability/concepts#data-storage-and-retention "Direct link to Data storage and retention")
For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database.
After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost.
note
If you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.
## Deleting traces from LangSmith[‚Äã](https://docs.smith.langchain.com/observability/concepts#deleting-traces-from-langsmith "Direct link to Deleting traces from LangSmith")
If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project.
This can be accomplished:
  * in the LangSmith UI via the "Delete" option on the Project's overflow menu
  * via the [Delete Tracer Sessions](https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/delete_tracer_session_api_v1_sessions__session_id__delete) API endpoint
  * via `delete_project()` (Python) or `deleteProject()` (JS/TS) in the LangSmith SDK


LangSmith does not support self-service deletion of individual traces at this time.
If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/concepts%3E).
[PreviousSet up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)[NextQuick Start](https://docs.smith.langchain.com/evaluation)
  * [Runs](https://docs.smith.langchain.com/observability/concepts#runs)
  * [Traces](https://docs.smith.langchain.com/observability/concepts#traces)
  * [Projects](https://docs.smith.langchain.com/observability/concepts#projects)
  * [Feedback](https://docs.smith.langchain.com/observability/concepts#feedback)
  * [Tags](https://docs.smith.langchain.com/observability/concepts#tags)
  * [Metadata](https://docs.smith.langchain.com/observability/concepts#metadata)
  * [Data storage and retention](https://docs.smith.langchain.com/observability/concepts#data-storage-and-retention)
  * [Deleting traces from LangSmith](https://docs.smith.langchain.com/observability/concepts#deleting-traces-from-langsmith)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * How-to Guides


On this page
# Observability how-to guides
Step-by-step guides that cover key tasks and operations for adding observability to your LLM applications with LangSmith.
## Tracing configuration[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides#tracing-configuration "Direct link to Tracing configuration")
Set up LangSmith tracing to get visibility into your production applications.
### Basic configuration[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides#basic-configuration "Direct link to Basic configuration")
  * [Set your tracing project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
  * [Trace any Python or JS Code](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
  * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
  * [Trace without environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)


### Integrations[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides#integrations "Direct link to Integrations")
  * [LangChain OSS libraries](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
  * [LangGraph](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
  * [OpenAI](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#wrap-the-openai-client)
  * [Instructor](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
  * [Vercel AI SDK (JS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
  * [OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
  * [OpenAI Agent SDK (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)


### Advanced configuration[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides#advanced-configuration "Direct link to Advanced configuration")
  * [Configure threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
  * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
  * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
  * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
  * [Trace LangChain with OpenTelemetry (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
  * [Access the current span within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
  * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
  * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
  * [Log custom LLM traces / provide custom token counts](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
  * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
  * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
  * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
  * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
  * [Troubleshoot trace testing](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
  * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
  * [Print out logs from the LangSmith SDK (Python Only)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
  * [Troubleshooting: Missing or Misrouted Traces](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)


## Tracing projects UI & API[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides#tracing-projects-ui--api "Direct link to Tracing projects UI & API")
View and interact with your traces to debug your applications.
  * [Filter traces in a project](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
  * [Save a filter for your project](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#saved-filters)
  * [Query / Export traces using the SDK (low volume)](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
  * [Bulk exporting traces (high volume)](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
  * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
  * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
  * [View threads](https://docs.smith.langchain.com/observability/how_to_guides/threads#view-threads)


## Monitoring[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides#monitoring "Direct link to Monitoring")
Use LangSmith custom and built-in dashboards to gain insight into your production systems.
  * [Create and use custom dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
  * [Use built-in monitoring dashboards](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
  * [Set up alerts for your project (Private Beta)](https://docs.smith.langchain.com/observability/how_to_guides/alerts)


## Automations[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides#automations "Direct link to Automations")
Leverage LangSmith's powerful monitoring, automation, and online evaluation features to make sense of your production data.
  * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
  * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
  * [Perform online evaluations](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)


## Human feedback[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides#human-feedback "Direct link to Human feedback")
  * [Log user feedback](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback)
  * [Set up a new feedback criteria](https://docs.smith.langchain.com/evaluation/how_to_guides/set_up_feedback_criteria)
  * [Annotate traces inline in the UI](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides%3E).
[PreviousAdd observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)[NextAnnotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
  * [Tracing configuration](https://docs.smith.langchain.com/observability/how_to_guides#tracing-configuration)
    * [Basic configuration](https://docs.smith.langchain.com/observability/how_to_guides#basic-configuration)
    * [Integrations](https://docs.smith.langchain.com/observability/how_to_guides#integrations)
    * [Advanced configuration](https://docs.smith.langchain.com/observability/how_to_guides#advanced-configuration)
  * [Tracing projects UI & API](https://docs.smith.langchain.com/observability/how_to_guides#tracing-projects-ui--api)
  * [Monitoring](https://docs.smith.langchain.com/observability/how_to_guides#monitoring)
  * [Automations](https://docs.smith.langchain.com/observability/how_to_guides#automations)
  * [Human feedback](https://docs.smith.langchain.com/observability/how_to_guides#human-feedback)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/access_current_span

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Access the current run (span) within a traced function


# Access the current run (span) within a traced function
In some cases you will want to access the current run (span) within a traced function. This can be useful for extracting UUIDs, tags, or other information from the current run.
You can access the current run by calling the `get_current_run_tree`/`getCurrentRunTree` function in the Python or TypeScript SDK, respectively.
For a full list of available properties on the `RunTree` object, see [this reference](https://docs.smith.langchain.com/reference/data_formats/run_data_format).
  * Python
  * TypeScript


```
from langsmith import traceablefrom langsmith.run_helpers import get_current_run_treefrom openai import Clientopenai = Client()@traceabledefformat_prompt(subject): run = get_current_run_tree()print(f"format_prompt Run Id: {run.id}")print(f"format_prompt Trace Id: {run.trace_id}")print(f"format_prompt Parent Run Id: {run.parent_run.id}")return[{"role":"system","content":"You are a helpful assistant.",},{"role":"user","content":f"What's a good name for a store that sells {subject}?"}]@traceable(run_type="llm")definvoke_llm(messages): run = get_current_run_tree()print(f"invoke_llm Run Id: {run.id}")print(f"invoke_llm Trace Id: {run.trace_id}")print(f"invoke_llm Parent Run Id: {run.parent_run.id}")return openai.chat.completions.create(   messages=messages, model="gpt-4o-mini", temperature=0)@traceabledefparse_output(response): run = get_current_run_tree()print(f"parse_output Run Id: {run.id}")print(f"parse_output Trace Id: {run.trace_id}")print(f"parse_output Parent Run Id: {run.parent_run.id}")return response.choices[0].message.content@traceabledefrun_pipeline(): run = get_current_run_tree()print(f"run_pipeline Run Id: {run.id}")print(f"run_pipeline Trace Id: {run.trace_id}") messages = format_prompt("colorful socks") response = invoke_llm(messages)return parse_output(response)run_pipeline()
```

```
import{ traceable, getCurrentRunTree }from"langsmith/traceable";import OpenAI from"openai";const openai =newOpenAI();const formatPrompt =traceable((subject:string)=>{const run =getCurrentRunTree();console.log("formatPrompt Run ID", run.id)console.log("formatPrompt Trace ID", run.trace_id)console.log("formatPrompt Parent Run ID", run.parent_run.id)return[{   role:"system"asconst,   content:"You are a helpful assistant.",},{   role:"user"asconst,   content:`What's a good name for a store that sells ${subject}?`,},];},{ name:"formatPrompt"});const invokeLLM =traceable(async(messages:{ role:string; content:string}[])=>{const run =getCurrentRunTree();console.log("invokeLLM Run ID", run.id)console.log("invokeLLM Trace ID", run.trace_id)console.log("invokeLLM Parent Run ID", run.parent_run.id)return openai.chat.completions.create({     model:"gpt-4o-mini",     messages: messages,     temperature:0,});},{ run_type:"llm", name:"invokeLLM"});const parseOutput =traceable((response:any)=>{const run =getCurrentRunTree();console.log("parseOutput Run ID", run.id)console.log("parseOutput Trace ID", run.trace_id)console.log("parseOutput Parent Run ID", run.parent_run.id)return response.choices[0].message.content;},{ name:"parseOutput"});const runPipeline =traceable(async()=>{const run =getCurrentRunTree();console.log("runPipline Run ID", run.id)console.log("runPipline Trace ID", run.trace_id)console.log("runPipline Parent Run ID", run.parent_run?.id)const messages =awaitformatPrompt("colorful socks");const response =awaitinvokeLLM(messages);returnparseOutput(response);},{ name:"runPipeline"});awaitrunPipeline();
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousImplement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)[NextLog multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Add metadata and tags to traces


# Add metadata and tags to traces
LangSmith supports sending arbitrary metadata and tags along with traces.
Tags are strings that can be used to categorize or label a trace. Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace.
Both are useful for associating additional information with a trace, such as the environment in which it was executed, the user who initiated it, or an internal correlation ID. For more information on tags and metadata, see the [Concepts](https://docs.smith.langchain.com/observability/concepts#tags) page. For information on how to query traces and runs by metadata and tags, see the [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application) page.
  * Python
  * TypeScript


```
import openaiimport langsmith as lsfrom langsmith.wrappers import wrap_openaiclient = openai.Client()messages =[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Hello!"}]# You can set metadata & tags **statically** when decorating a function# Use the @traceable decorator with tags and metadata# Ensure that the LANGSMITH_TRACING environment variables are set for @traceable to work@ls.traceable( run_type="llm", name="OpenAI Call Decorator", tags=["my-tag"], metadata={"my-key":"my-value"})defcall_openai( messages:list[dict], model:str="gpt-4o-mini")->str:# You can also dynamically set metadata on the parent run: rt = ls.get_current_run_tree() rt.metadata["some-conditional-key"]="some-val" rt.tags.extend(["another-tag"])return client.chat.completions.create(   model=model,   messages=messages,).choices[0].message.contentcall_openai( messages,# To add at **invocation time**, when calling the function.# via the langsmith_extra parameter langsmith_extra={"tags":["my-other-tag"],"metadata":{"my-other-key":"my-value"}})# Alternatively, you can use the context managerwith ls.trace(   name="OpenAI Call Trace",   run_type="llm",   inputs={"messages": messages},   tags=["my-tag"],   metadata={"my-key":"my-value"},)as rt:   chat_completion = client.chat.completions.create(     model="gpt-4o-mini",     messages=messages,)   rt.metadata["some-conditional-key"]="some-val"   rt.end(outputs={"output": chat_completion})# You can use the same techniques with the wrapped clientpatched_client = wrap_openai( client, tracing_extra={"metadata":{"my-key":"my-value"},"tags":["a-tag"]})chat_completion = patched_client.chat.completions.create( model="gpt-4o-mini", messages=messages, langsmith_extra={"tags":["my-other-tag"],"metadata":{"my-other-key":"my-value"},},)
```

```
import OpenAI from"openai";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers";import{ RunTree }from"langsmith";const client =newOpenAI();const messages =[{ role:"system", content:"You are a helpful assistant."},{ role:"user", content:"Hello!"},];const traceableCallOpenAI =traceable(async(messages:{ role:string; content:string}[])=>{const completion =await client.chat.completions.create({   model:"gpt-4o-mini",   messages: messages,});return completion.choices[0].message.content;},{  run_type:"llm",  name:"OpenAI Call Traceable",  tags:["my-tag"],  metadata:{"my-key":"my-value"},});// Call the traceable functionawaittraceableCallOpenAI(messages,"gpt-4o-mini");// Create a RunTree objectconst rt =newRunTree({ run_type:"llm", name:"OpenAI Call RunTree", inputs:{ messages }, tags:["my-tag"], extra:{ metadata:{"my-key":"my-value"}},});await rt.postRun();const chatCompletion =await client.chat.completions.create({ model:"gpt-4o-mini", messages: messages,});// End and submit the runawait rt.end(chatCompletion);await rt.patchRun();
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousSet a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)[NextImplement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/alerts

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/alerts#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/alerts)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Alerts in LangSmith


On this page
# Alerts in LangSmith
Private Beta
This is feature is currently in private beta. If interested, please express interest in the [#feature-requests](https://langchaincommunity.slack.com/archives/C079K4ECBN2) channel in the LangChain Community Slack.
## Overview[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts#overview "Direct link to Overview")
Effective observability in LLM applications requires proactive detection of failures, performance degradations, and regressions. LangSmith's alerts feature helps identify critical issues such as:
  * API rate limit violations from model providers
  * Latency increases for your application
  * Application changes that affect feedback scores reflecting end-user experience


Alerts in LangSmith are project-scoped, requiring separate configuration for each monitored project.
## Configuring an alert[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts#configuring-an-alert "Direct link to Configuring an alert")
### Step 1: Navigate To Create Alert[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts#step-1-navigate-to-create-alert "Direct link to Step 1: Navigate To Create Alert")
First navigate to the Tracing project that you would like to configure alerts for. Click **+ New Alert** in the top right hand corner of the page to set up an alert.
### Step 2: Select Metric Type[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts#step-2-select-metric-type "Direct link to Step 2: Select Metric Type")
![Alert Metrics](https://docs.smith.langchain.com/assets/images/alert_metric-f77a74539657b9b00b45feacbdb5de24.png)
LangSmith offers threshold-based alerting on three core metrics:
Metric Type| Description| Use Case  
---|---|---  
**Errored Runs**|  Tracks root runs with error status| Monitors for failures in application traces.  
**Feedback Score**|  Measures average feedback score metrics| Tracks feedback from application users or [automated evaluation rules](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations) to track regressions in prompts or switching models / providers.  
**Trace Latency**|  Measures average end-to-end trace execution time| Identifies performance bottlenecks from switching models / providers or code regressions.  
### Step 2: Define Alert Conditions[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts#step-2-define-alert-conditions "Direct link to Step 2: Define Alert Conditions")
Alert conditions consist of several components:
  * **Aggregation Method** : Average, Percentage, or Count
  * **Comparison Operator** : `>=`, `<=`, or exceeds threshold
  * **Threshold Value** : Numerical value triggering the alert
  * **Aggregation Window** : Time period for metric calculation (currently choose between 5 or 15 minutes)
  * **Feedback Key** (Feedback Score alerts only): Specific feedback metric to monitor


![Alert Condition Configuration](https://docs.smith.langchain.com/assets/images/define_conditions-c4c2d47c8b15a4634ca0f843130e3f94.png)
**Example:** The configuration shown above would generate an alert when more than 5% of runs within the past 5 minutes result in errors.
### Step 3: Configure Notification Channel[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts#step-3-configure-notification-channel "Direct link to Step 3: Configure Notification Channel")
LangSmith supports the following notification channels:
  1. [PagerDuty Integration](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
  2. [Webhook Notifications](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)


Select the appropriate channel to ensure notifications reach the responsible team members.
## Best Practices[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts#best-practices "Direct link to Best Practices")
  * Adjust sensitivity based on application criticality
  * Start with broader thresholds and refine based on observed patterns
  * Ensure alert routing reaches appropriate on-call personnel


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/alerts%3E).
[Previous[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)[NextConfiguring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
  * [Overview](https://docs.smith.langchain.com/observability/how_to_guides/alerts#overview)
  * [Configuring an alert](https://docs.smith.langchain.com/observability/how_to_guides/alerts#configuring-an-alert)
    * [Step 1: Navigate To Create Alert](https://docs.smith.langchain.com/observability/how_to_guides/alerts#step-1-navigate-to-create-alert)
    * [Step 2: Select Metric Type](https://docs.smith.langchain.com/observability/how_to_guides/alerts#step-2-select-metric-type)
    * [Step 2: Define Alert Conditions](https://docs.smith.langchain.com/observability/how_to_guides/alerts#step-2-define-alert-conditions)
    * [Step 3: Configure Notification Channel](https://docs.smith.langchain.com/observability/how_to_guides/alerts#step-3-configure-notification-channel)
  * [Best Practices](https://docs.smith.langchain.com/observability/how_to_guides/alerts#best-practices)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Configuring PagerDuty Integration for LangSmith Alerts


On this page
# Configuring PagerDuty Integration for LangSmith Alerts
Private Beta
Set up a PagerDuty notification while using the [new alerting feature](https://docs.smith.langchain.com/observability/how_to_guides/alerts) which is currently in private beta. If interested, please express interest in the [#feature-requests](https://langchaincommunity.slack.com/archives/C079K4ECBN2) channel in the langchain community slack.
## Overview[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#overview "Direct link to Overview")
This guide walks through the process of configuring PagerDuty as a notification channel for LangSmith alerts using PagerDuty's [Events API v2](https://developer.pagerduty.com/docs/events-api-v2-overview). This integration allows critical LLM application issues to trigger PagerDuty incidents, enabling rapid response through your established incident management workflow.
## Prerequisites[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#prerequisites "Direct link to Prerequisites")
  * An active PagerDuty account with administrator access
  * Appropriate service-level permissions in PagerDuty


Additionally, if on a custom deployment of LangSmith, make sure there are no firewall settings blocking egress traffic from LangSmith services.
## Integration Steps[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#integration-steps "Direct link to Integration Steps")
### Step 1: Create a Service in PagerDuty[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#step-1-create-a-service-in-pagerduty "Direct link to Step 1: Create a Service in PagerDuty")
  1. Log in to your PagerDuty account
  2. Navigate to **Services ‚Üí Service Directory**
  3. Click **+ New Service**
  4. Complete the following fields: 
     * **Name** : Provide a descriptive name (e.g., "LangSmith Monitoring")
     * **Description** : Add details about the monitored application
     * **Escalation Policy** : Select the appropriate team escalation policy
     * **Integration Type** : Select "Events API V2"
  5. Click **Add Service** to create the service


### Step 2: Obtain Integration Key[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#step-2-obtain-integration-key "Direct link to Step 2: Obtain Integration Key")
After creating the service, you'll need to retrieve the Integration Key:
  1. From the **Service Directory** under the Service dropdown, locate and click on your newly created service
  2. Select the **Integrations** tab
  3. Find the "Events API V2" integration
  4. Copy the **Integration Key** (a 32-character alphanumeric string)


![PagerDuty Integration Key Location](https://docs.smith.langchain.com/assets/images/pager_duty-163770d0815ea292a8430ca7b5745c7a.png)
### Step 3: Configure LangSmith Alert with PagerDuty[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#step-3-configure-langsmith-alert-with-pagerduty "Direct link to Step 3: Configure LangSmith Alert with PagerDuty")
![PagerDuty Setup](https://docs.smith.langchain.com/assets/images/pagerduty_setup-80b1ec86e96c56649b5f20d9d8011ac8.png)
  1. In the notification section of your alert set-up in LangSmith, select **PagerDuty**
  2. Paste the **Integration Key** from Step 2 into the designated field
  3. Configure additional notification options: 
     * **Severity** : Maps to PagerDuty incident priority
  4. Send a test alert by clicking **Send Test Alert**
  5. Verify the incident is triggered by PagerDuty and contains relevant LangSmith alert information


## Troubleshooting[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#troubleshooting "Direct link to Troubleshooting")
If incidents aren't being created in PagerDuty:
  * Verify the Integration Key is entered correctly in LangSmith
  * Ensure the PagerDuty service is active and not in maintenance mode
  * Check that your PagerDuty account has Events API v2 enabled
  * Review network connectivity if your LangSmith instance is behind a firewall


## Additional Resources[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#additional-resources "Direct link to Additional Resources")
  * [PagerDuty Events API v2 Documentation](https://developer.pagerduty.com/docs/events-api-v2/overview/)
  * [PagerDuty Integration Guide](https://support.pagerduty.com/docs/services-and-integrations)
  * [LangSmith Alerts Documentation](https://docs.smith.langchain.com/observability/how_to_guides/alerts)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/alerts_pagerduty%3E).
[PreviousAlerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)[NextConfiguring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
  * [Overview](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#overview)
  * [Prerequisites](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#prerequisites)
  * [Integration Steps](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#integration-steps)
    * [Step 1: Create a Service in PagerDuty](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#step-1-create-a-service-in-pagerduty)
    * [Step 2: Obtain Integration Key](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#step-2-obtain-integration-key)
    * [Step 3: Configure LangSmith Alert with PagerDuty](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#step-3-configure-langsmith-alert-with-pagerduty)
  * [Troubleshooting](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#troubleshooting)
  * [Additional Resources](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty#additional-resources)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Configuring Webhook Notifications for LangSmith Alerts


On this page
# Configuring Webhook Notifications for LangSmith Alerts
Private Beta
Set up a WebHook trigger while using the [new alerting feature](https://docs.smith.langchain.com/observability/how_to_guides/alerts) which is currently in private beta. If interested, please express interest in the [#feature-requests](https://langchaincommunity.slack.com/archives/C079K4ECBN2) channel in the langchain community slack.
## Overview[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#overview "Direct link to Overview")
This guide details the process for setting up webhook notifications for LangSmith alerts. Before proceeding, make sure you have followed the steps leading up to the notification step of creating the alert by following [this guide](https://docs.smith.langchain.com/observability/how_to_guides/alerts). Webhooks enable integration with custom services and third-party platforms by sending HTTP POST requests when alert conditions are triggered. Use webhooks to forward alert data to ticketing systems, chat applications, or custom monitoring solutions.
## Prerequisites[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#prerequisites "Direct link to Prerequisites")
  * An endpoint that can receive HTTP POST requests
  * Appropriate authentication credentials for your receiving service (if required)


## Integration Configuration[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#integration-configuration "Direct link to Integration Configuration")
### Step 1: Prepare Your Receiving Endpoint[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-1-prepare-your-receiving-endpoint "Direct link to Step 1: Prepare Your Receiving Endpoint")
Before configuring the webhook in LangSmith, ensure your receiving endpoint:
  * Accepts HTTP POST requests
  * Can process JSON payloads
  * Is accessible from external services
  * Has appropriate authentication mechanisms (if required)


Additionally, if on a custom deployment of LangSmith, make sure there are no firewall settings blocking egress traffic from LangSmith services.
### Step 2: Configure Webhook Parameters[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-2-configure-webhook-parameters "Direct link to Step 2: Configure Webhook Parameters")
![Webhook Setup](https://docs.smith.langchain.com/assets/images/webhook_setup-aca1270d8eaa9e27ccf19abfc3bd1bf2.png)
In the notification section of your alert complete the webhook configuration with the following parameters:
**Required Fields**
  * **URL** : The complete URL of your receiving endpoint 
    * Example: `https://api.example.com/incident-webhook`


**Optional Fields**
  * **Headers** : JSON Key-value pairs sent with the webhook request
    * Common headers include: 
      * `Authorization`: For authentication tokens
      * `Content-Type`: Usually set to `application/json` (default)
      * `X-Source`: To identify the source as LangSmith
    * If no headers, then simply use `{}`
  * **Request Body Template** : Customize the JSON payload sent to your endpoint
    * Default: LangSmith sends the payload defined and the following additonal key-value pairs appended to the payload: 
      * `project_name`: Name of the triggered alert
      * `alert_rule_id`: A UUID to identify the LangSmith alert. This can be used as a de-duplication key in the webhook service.
      * `alert_rule_name`: The name of the alert rule.
      * `alert_rule_type`: The type of alert (as of 04/01/2025 all alerts are of type `threshold`).
      * `alert_rule_attribute`: The attribute associated with the alert rule - `error_count`, `feedback_score` or `latency`.
      * `triggered_metric_value`: The value of the metric at the time the threshold was triggered.
      * `triggered_threshold`: The threshold that triggered the alert.
      * `timestamp`: The timestamp that triggered the alert.


### Step 3: Test the Webhook[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-3-test-the-webhook "Direct link to Step 3: Test the Webhook")
Click **Send Test Alert** to send the webhook notification to ensure the notification works as intended.
## Troubleshooting[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#troubleshooting "Direct link to Troubleshooting")
If webhook notifications aren't being delivered:
  * Verify the webhook URL is correct and accessible
  * Ensure any authentication headers are properly formatted
  * Check that your receiving endpoint accepts POST requests
  * Examine your endpoint's logs for received but rejected requests
  * Verify your custom payload template is valid JSON format


## Security Considerations[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#security-considerations "Direct link to Security Considerations")
  * Use HTTPS for your webhook endpoints
  * Implement authentication for your webhook endpoint
  * Consider adding a shared secret in your headers to verify webhook sources
  * Validate incoming webhook requests before processing them


## Example of using webhook option with Slack API[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#example-of-using-webhook-option-with-slack-api "Direct link to Example of using webhook option with Slack API")
Here is an example for configuring LangSmith alerts to send notifications to Slack channels using the `chat.postMessage` API.
### Prerequisites[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#prerequisites-1 "Direct link to Prerequisites")
  * Access to a Slack workspace
  * A LangSmith project to set up alerts
  * Permissions to create Slack applications


### Step 1: Create a Slack App[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-1-create-a-slack-app "Direct link to Step 1: Create a Slack App")
  1. Visit the [Slack API Applications page](https://api.slack.com/apps)
  2. Click **Create New App**
  3. Select **From scratch**
  4. Provide an **App Name** (e.g., "LangSmith Alerts")
  5. Select the workspace where you want to install the app
  6. Click **Create App**


### Step 2: Configure Bot Permissions[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-2-configure-bot-permissions "Direct link to Step 2: Configure Bot Permissions")
  1. In the left sidebar of your Slack app configuration, click **OAuth & Permissions**
  2. Scroll down to **Bot Token Scopes** under **Scopes** and click **Add an OAuth Scope**
  3. Add the following scopes: 
     * `chat:write` (Send messages as the app)
     * `chat:write.public` (Send messages to channels the app isn't in)
     * `channels:read` (View basic channel information)


### Step 3: Install the App to Your Workspace[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-3-install-the-app-to-your-workspace "Direct link to Step 3: Install the App to Your Workspace")
  1. Scroll up to the top of the **OAuth & Permissions** page
  2. Click **Install to Workspace**
  3. Review the permissions and click **Allow**
  4. Copy the **Bot User OAuth Token** that appears (begins with `xoxb-`)


### Step 4: Configure LangSmith Webhook Alert[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-4-configure-langsmith-webhook-alert "Direct link to Step 4: Configure LangSmith Webhook Alert")
  1. In LangSmith, navigate to your project
  2. Select **Alerts ‚Üí Create Alert**
  3. Define your alert metrics and conditions
  4. In the notification section, select **Webhook**
  5. Configure the webhook with the following settings:


**Webhook URL**
```
https://slack.com/api/chat.postMessage
```

**Headers**
```
{"Content-Type":"application/json","Authorization":"Bearer xoxb-your-token-here"}
```

> **Note:** Replace `xoxb-your-token-here` with your actual Bot User OAuth Token
**Request Body Template**
```
{"channel":"{channel_id}","text":"Error alert triggered in {project_name}","blocks":[{"type":"section","text":{"type":"mrkdwn","text":"Error alert triggered"}},{"type":"section","text":{"type":"mrkdwn","text":"Please check the following link for more information:"}},{"type":"section","text":{"type":"mrkdwn","text":"<{project_url}|View Traces In Langsmith>"}}]}
```

**NOTE:** The `channel_id`, `project_url` and `project_name` are currently NOT template variables. You would have to set those manually while creating the alert.
  1. Click **Save** to activate the webhook configuration


## Step 5: Test the Integration[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-5-test-the-integration "Direct link to Step 5: Test the Integration")
  1. In the LangSmith alert configuration, click **Test Alert**
  2. Check your specified Slack channel for the test notification
  3. Verify that the message contains the expected alert information


## Additional Resources[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#additional-resources "Direct link to Additional Resources")
  * [LangSmith Alerts Documentation](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
  * [Slack chat.postMessage API Documentation](https://api.slack.com/methods/chat.postMessage)
  * [Slack Block Kit Builder](https://app.slack.com/block-kit-builder/)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/alerts_webhook%3E).
[PreviousConfiguring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)[NextHow to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
  * [Overview](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#overview)
  * [Prerequisites](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#prerequisites)
  * [Integration Configuration](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#integration-configuration)
    * [Step 1: Prepare Your Receiving Endpoint](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-1-prepare-your-receiving-endpoint)
    * [Step 2: Configure Webhook Parameters](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-2-configure-webhook-parameters)
    * [Step 3: Test the Webhook](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-3-test-the-webhook)
  * [Troubleshooting](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#troubleshooting)
  * [Security Considerations](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#security-considerations)
  * [Example of using webhook option with Slack API](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#example-of-using-webhook-option-with-slack-api)
    * [Prerequisites](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#prerequisites-1)
    * [Step 1: Create a Slack App](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-1-create-a-slack-app)
    * [Step 2: Configure Bot Permissions](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-2-configure-bot-permissions)
    * [Step 3: Install the App to Your Workspace](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-3-install-the-app-to-your-workspace)
    * [Step 4: Configure LangSmith Webhook Alert](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-4-configure-langsmith-webhook-alert)
  * [Step 5: Test the Integration](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#step-5-test-the-integration)
  * [Additional Resources](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook#additional-resources)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/annotate_code

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Annotate code for tracing


On this page
# Annotate code for tracing
note
If you've decided you no longer want to trace your runs, you can remove the `LANGSMITH_TRACING` environment variable. Note that this does not affect the `RunTree` objects or API users, as these are meant to be low-level and not affected by the tracing toggle.
There are several ways to log traces to LangSmith.
tip
If you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the [LangChain-specific instructions](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain).
## Use `@traceable` / `traceable`[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#use-traceable--traceable "Direct link to use-traceable--traceable")
LangSmith makes it easy to log traces with minimal changes to your existing code with the `@traceable` decorator in Python and `traceable` function in TypeScript.
note
The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `@traceable` or `traceable`. This allows you to toggle tracing on and off without changing your code.
Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](https://docs.smith.langchain.com/) for more information).
By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project).
  * Python
  * TypeScript


The `@traceable` decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with `@traceable`.
```
from langsmith import traceablefrom openai import Clientopenai = Client()@traceabledefformat_prompt(subject):return[{"role":"system","content":"You are a helpful assistant.",},{"role":"user","content":f"What's a good name for a store that sells {subject}?"}]@traceable(run_type="llm")definvoke_llm(messages):return openai.chat.completions.create(   messages=messages, model="gpt-4o-mini", temperature=0)@traceabledefparse_output(response):return response.choices[0].message.content@traceabledefrun_pipeline(): messages = format_prompt("colorful socks") response = invoke_llm(messages)return parse_output(response)run_pipeline()
```

The `traceable` function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with `traceable`.
Note that when wrapping a sync function with `traceable`, (e.g. `formatPrompt` in the example below), you should use the `await` keyword when calling it to ensure the trace is logged correctly.
```
import{ traceable }from"langsmith/traceable";import OpenAI from"openai";const openai =newOpenAI();const formatPrompt =traceable((subject:string)=>{return[{   role:"system"asconst,   content:"You are a helpful assistant.",},{   role:"user"asconst,   content:`What's a good name for a store that sells ${subject}?`,},];},{ name:"formatPrompt"});const invokeLLM =traceable(async({ messages }:{ messages:{ role:string; content:string}[]})=>{return openai.chat.completions.create({     model:"gpt-4o-mini",     messages: messages,     temperature:0,});},{ run_type:"llm", name:"invokeLLM"});const parseOutput =traceable((response:any)=>{return response.choices[0].message.content;},{ name:"parseOutput"});const runPipeline =traceable(async()=>{const messages =awaitformatPrompt("colorful socks");const response =awaitinvokeLLM({ messages });returnparseOutput(response);},{ name:"runPipeline"});awaitrunPipeline();
```

![](https://docs.smith.langchain.com/assets/images/annotate_code_trace-7e322bef063224e50e4ee572463fd5f4.gif)
## Use the `trace` context manager (Python only)[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#use-the-trace-context-manager-python-only "Direct link to use-the-trace-context-manager-python-only")
In Python, you can use the `trace` context manager to log traces to LangSmith. This is useful in situations where:
  1. You want to log traces for a specific block of code.
  2. You want control over the inputs, outputs, and other attributes of the trace.
  3. It is not feasible to use a decorator or wrapper.
  4. Any or all of the above.


The context manager integrates seamlessly with the `traceable` decorator and `wrap_openai` wrapper, so you can use them together in the same application.
```
import openaiimport langsmith as lsfrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@ls.traceable(run_type="tool", name="Retrieve Context")defmy_tool(question:str)->str:return"During this morning's meeting, we solved all world conflict."defchat_pipeline(question:str):  context = my_tool(question)  messages =[{"role":"system","content":"You are a helpful assistant. Please respond to the user's request only based on the given context."},{"role":"user","content":f"Question: {question}\nContext: {context}"}]  chat_completion = client.chat.completions.create(    model="gpt-4o-mini", messages=messages)return chat_completion.choices[0].message.contentapp_inputs ={"input":"Can you summarize this morning's meetings?"}with ls.trace("Chat Pipeline","chain", project_name="my_test", inputs=app_inputs)as rt:  output = chat_pipeline("Can you summarize this morning's meetings?")  rt.end(outputs={"output": output})
```

## Wrap the OpenAI client[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#wrap-the-openai-client "Direct link to Wrap the OpenAI client")
The `wrap_openai`/`wrapOpenAI` methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required! The wrapper works seamlessly with the `@traceable` decorator or `traceable` function and you can use both in the same application.
Tool calls are automatically rendered
note
The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `wrap_openai` or `wrapOpenAI`. This allows you to toggle tracing on and off without changing your code.
Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](https://docs.smith.langchain.com/) for more information).
By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project).
  * Python
  * TypeScript


```
import openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type="tool", name="Retrieve Context")defmy_tool(question:str)->str:return"During this morning's meeting, we solved all world conflict."@traceable(name="Chat Pipeline")defchat_pipeline(question:str): context = my_tool(question) messages =[{"role":"system","content":"You are a helpful assistant. Please respond to the user's request only based on the given context."},{"role":"user","content":f"Question: {question}\nContext: {context}"}] chat_completion = client.chat.completions.create(   model="gpt-4o-mini", messages=messages)return chat_completion.choices[0].message.contentchat_pipeline("Can you summarize this morning's meetings?")
```

```
import OpenAI from"openai";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers";const client =wrapOpenAI(newOpenAI());const myTool =traceable(async(question:string)=>{return"During this morning's meeting, we solved all world conflict.";},{ name:"Retrieve Context", run_type:"tool"});const chatPipeline =traceable(async(question:string)=>{const context =awaitmyTool(question);const messages =[{     role:"system",     content:"You are a helpful assistant. Please respond to the user's request only based on the given context.",},{ role:"user", content:`Question: ${question} Context: ${context}`},];const chatCompletion =await client.chat.completions.create({   model:"gpt-4o-mini",   messages: messages,});return chatCompletion.choices[0].message.content;},{ name:"Chat Pipeline"});awaitchatPipeline("Can you summarize this morning's meetings?");
```

## Use the `RunTree` API[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#use-the-runtree-api "Direct link to use-the-runtree-api")
Another, more explicit way to log traces to LangSmith is via the `RunTree` API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method.
This method is not recommended, as it's easier to make mistakes in propagating trace context.
  * Python
  * TypeScript


```
import openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion ="Can you summarize this morning's meetings?"# Create a top-level runpipeline = RunTree( name="Chat Pipeline", run_type="chain", inputs={"question": question})pipeline.post()# This can be retrieved in a retrieval stepcontext ="During this morning's meeting, we solved all world conflict."messages =[{"role":"system","content":"You are a helpful assistant. Please respond to the user's request only based on the given context."},{"role":"user","content":f"Question: {question}\nContext: {context}"}]# Create a child runchild_llm_run = pipeline.create_child( name="OpenAI Call", run_type="llm", inputs={"messages": messages},)child_llm_run.post()# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create( model="gpt-4o-mini", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.patch()pipeline.end(outputs={"answer": chat_completion.choices[0].message.content})pipeline.patch()
```

```
import OpenAI from"openai";import{ RunTree }from"langsmith";// This can be a user input to your appconst question ="Can you summarize this morning's meetings?";const pipeline =newRunTree({ name:"Chat Pipeline", run_type:"chain", inputs:{ question }});await pipeline.postRun();// This can be retrieved in a retrieval stepconst context ="During this morning's meeting, we solved all world conflict.";const messages =[{ role:"system", content:"You are a helpful assistant. Please respond to the user's request only based on the given context."},{ role:"user", content:`Question: ${question}Context: ${context}`}];// Create a child runconst childRun =await pipeline.createChild({ name:"OpenAI Call", run_type:"llm", inputs:{ messages },});await childRun.postRun();// Generate a completionconst client =newOpenAI();const chatCompletion =await client.chat.completions.create({ model:"gpt-4o-mini", messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.patchRun();pipeline.end({ outputs:{ answer: chatCompletion.choices[0].message.content }});await pipeline.patchRun();
```

## Example usage[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#example-usage "Direct link to Example usage")
You can extend the utilities above to conveniently trace any code. Below are some example extensions:
Trace any public method in a class:
```
from typing import Any, Callable, Type, TypeVarT = TypeVar("T")deftraceable_cls(cls: Type[T])-> Type[T]:"""Instrument all public methods in a class."""defwrap_method(name:str, method: Any)-> Any:ifcallable(method)andnot name.startswith("__"):return traceable(name=f"{cls.__name__}.{name}")(method)return method# Handle __dict__ casefor name indir(cls):ifnot name.startswith("_"):try:        method =getattr(cls, name)setattr(cls, name, wrap_method(name, method))except AttributeError:# Skip attributes that can't be set (e.g., some descriptors)pass# Handle __slots__ caseifhasattr(cls,"__slots__"):for slot in cls.__slots__:# type: ignore[attr-defined]ifnot slot.startswith("__"):try:          method =getattr(cls, slot)setattr(cls, slot, wrap_method(slot, method))except AttributeError:# Skip slots that don't have a value yetpassreturn cls@traceable_clsclassMyClass:def__init__(self, some_val:int):    self.some_val = some_valdefcombine(self, other_val:int):return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)
```

## Ensure all traces are submitted before exiting[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#ensure-all-traces-are-submitted-before-exiting "Direct link to Ensure all traces are submitted before exiting")
LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. Here are some options for ensuring all traces are submitted before exiting your application.
### Using the LangSmith SDK[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#using-the-langsmith-sdk "Direct link to Using the LangSmith SDK")
If you are using the LangSmith SDK standalone, you can use the `flush` method before exit:
  * Python
  * TypeScript


```
from langsmith import Clientclient = Client()@traceable(client=client)asyncdefmy_traced_func():# Your code here...passtry:await my_traced_func()finally:await client.flush()
```

```
import{ Client }from"langsmith";const langsmithClient =newClient({});const myTracedFunc =traceable(async()=>{// Your code here...},{ client: langsmithClient });try{awaitmyTracedFunc();}finally{await langsmithClient.flush();}
```

### Using LangChain[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#using-langchain "Direct link to Using LangChain")
If you are using LangChain, please refer to our [LangChain tracing guide](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting).
If you prefer a video tutorial, check out the [Tracing Basics video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousObservability how-to guides](https://docs.smith.langchain.com/observability/how_to_guides)[NextFilter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
  * [Use `@traceable` / `traceable`](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#use-traceable--traceable)
  * [Use the `trace` context manager (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#use-the-trace-context-manager-python-only)
  * [Wrap the OpenAI client](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#wrap-the-openai-client)
  * [Use the `RunTree` API](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#use-the-runtree-api)
  * [Example usage](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#example-usage)
  * [Ensure all traces are submitted before exiting](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#ensure-all-traces-are-submitted-before-exiting)
    * [Using the LangSmith SDK](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#using-the-langsmith-sdk)
    * [Using LangChain](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#using-langchain)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Calculate token-based costs for traces


# Calculate token-based costs for traces
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Providing token counts for LLM runs (spans)](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace#manually-provide-token-counts)


LangSmith allows you to track costs for traces based on the number of tokens used for LLM invocations. The costs are rolled up to the trace level and project level.
For LangSmith to accurately calculate token-based costs, you need to provide the token counts for each LLM invocation in the trace, along with sending up `ls_provider` and `ls_model_name` in the run metadata.
  * If you are using the LangSmith Python or TS/JS SDK, you should carefully read through the [this guide](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace).
  * If you are using LangChain Python or TS/JS, _`ls_provider`and`ls_model_name` along with token counts are automatically sent up to LangSmith_.


note
If `ls_model_name` is not present in `extra.metadata`, other fields might be used from the `extra.invocation_metadata` for estimating token counts and calculating cost. The following fields are used in the order of precedence:
  1. `metadata.ls_model_name`
  2. `invocation_params.model`
  3. `invocation_params.model_name`
  4. `invocation_params.model_id` (for costs only)
  5. `invocation_params.model_path` (for costs only)
  6. `invocation_params.endpoint_name` (for costs only)


Once you are sending up the correct information to LangSmith, you must set up the model pricing map in LangSmith settings. In order to do this, navigate to the [model pricing map](https://smith.langchain.com/settings/workspaces/models). Here, you can set the cost per token for each model and provider combination. This information is scoped to a workspace.
Several default entries for OpenAI models are already present in the model pricing map, which you can clone and modify as needed.
![](https://docs.smith.langchain.com/assets/images/model_price_map-6f25837dcb8f005a1bbd8668008dbf31.png)
To create a _new entry_ in the model pricing map, click on the `Add new model` button in the top right corner.
![](https://docs.smith.langchain.com/assets/images/new_price_map_entry-de994e528f29ca6339921d7c1858527d.png)
Here, you can specify the following fields:
  * `Model Name`: The name of the model, will also be used to name the entry in the model pricing map.
  * `Prompt Cost`: The cost per input token for the model. This number is multiplied by the number of tokens in the prompt to calculate the prompt cost.
  * `Completion Cost`: The cost per output token for the model. This number is multiplied by the number of tokens in the completion to calculate the completion cost.
  * `Model Activation Date`: The date from which the pricing is applicable.
  * `Match Pattern`: A regex pattern to match the model name and provider. This is used to match the value for `ls_model_name` in the run metadata.
  * `Provider`: The provider of the model. This is used to match the value for `ls_provider` in the run metadata.


Once you have set up the model pricing map, LangSmith will automatically calculate and aggregate the token-based costs for traces based on the token counts provided in the LLM invocations.
To see the example above in action, you can execute the following code snippet:
  * Python
  * TypeScript


```
from langsmith import traceableinputs =[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"I'd like to book a table for two."},]output ={"choices":[{"message":{"role":"assistant","content":"Sure, what time would you like to book the table for?"}}],"usage_metadata":{"input_tokens":27,"output_tokens":13,"total_tokens":40,},}@traceable( run_type="llm", metadata={"ls_provider":"my_provider","ls_model_name":"my_model"})defchat_model(messages:list):return outputchat_model(inputs)
```

```
import{ traceable }from"langsmith/traceable";const messages =[{ role:"system", content:"You are a helpful assistant."},{ role:"user", content:"I'd like to book a table for two."},];const output ={choices:[{  message:{   role:"assistant",   content:"Sure, what time would you like to book the table for?",},},],usage_metadata:{ input_tokens:27, output_tokens:13, total_tokens:40,},};const chatModel =traceable(async({ messages,}:{ messages:{ role:string; content:string}[]; model:string;})=>{return output;},{ run_type:"llm", name:"chat_model", metadata:{ ls_provider:"my_provider", ls_model_name:"my_model"}});awaitchatModel({ messages });
```

In the above code snippet, we are sending up the `ls_provider` and `ls_model_name` in the run metadata, along with the token counts for the LLM invocation. This information matches the model pricing map entry we set up earlier.
The trace produced will contain the token-based costs based on the token counts provided in the LLM invocation and the model pricing map entry.
![](https://docs.smith.langchain.com/assets/images/model_costs-f2c84a9772563418fe16c6fdbb8d47ae.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousTrace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)[NextTroubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/compare_traces

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Compare traces


# Compare traces
To compare traces, click on the **Compare** button in the upper right hand side of any trace view.
![](https://docs.smith.langchain.com/assets/images/compare_button-c8f4edb447d4a92cd378f2418e9dbac9.png)
This will show the trace run table. Select the trace you want to compare against original trace.
![](https://docs.smith.langchain.com/assets/images/select_trace-3f45fd2296fcd250c74c3d7f884846eb.png)
The pane will open with both traces selected in a side by side comparison view.
![](https://docs.smith.langchain.com/assets/images/compare_trace-31d03cc57d1f0c9e63bf1d0bbc681496.png)
To stop comparing, close the pane or click on **Stop comparing** in the upper right hand side of the pane.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/compare_traces%3E).
[PreviousShare or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)[NextTrace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/dashboards

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Dashboards


On this page
# Create dashboards
With dashboards you can create tailored collections of charts for tracking metrics that matter most to your application.
## Creating a new dashboard[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#creating-a-new-dashboard "Direct link to Creating a new dashboard")
  1. Navigate to the `Dashboards` tab in the left sidebar.
  2. Click on the `+ Dashboard` button.
  3. Give your dashboard a name and a description.
  4. Click on `Create`.


You'll see all of your dashboards displayed in a table view.
![Add dashboard](https://docs.smith.langchain.com/assets/images/add_dashboard-f2b445148426febce0764d9d78ed58bc.png) ![Dashboard table](https://docs.smith.langchain.com/assets/images/dashboard_table-1d6c5b7df0dd753799b165b50a01961b.png)
## Adding charts to your dashboard[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#adding-charts-to-your-dashboard "Direct link to Adding charts to your dashboard")
  1. Within a dashboard, click on the `+ Chart` button to open up the chart creation pane.
  2. Give your chart a name and a description.
  3. Select one or more projects to track metrics for.
  4. Choose a metric from the dropdown menu to set the y-axis of your chart.


With a project and a metric selected, you'll see a preview of your chart and the matching runs.
![Add chart](https://docs.smith.langchain.com/assets/images/add_chart-ee7f757d089be41919eacc820e40cd75.png)
## Filtering traces in your chart[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#filtering-traces-in-your-chart "Direct link to Filtering traces in your chart")
Use the **Chart filters** section to refine the matching runs. These are the same filters available on the Projects page and apply to all data series in the chart. (See [Filter traces in application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application) for more information on filtering traces.)
![Chart filters](https://docs.smith.langchain.com/assets/images/chart_filters-58e5b1c0455416fe2d71f02376888a94.png)
## Comparing data within a chart[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#comparing-data-within-a-chart "Direct link to Comparing data within a chart")
There are two ways to compare data in a chart (i.e. create multiple lines in a chart):
  1. **Multiple metrics** : Add multiple metrics with the same unit to a single chart. Each metric appears as a separate line. For example, compare P99 with P50 latency. When you pick an initial metric, the comparison metrics dropdown will only show metrics with the same unit.


![Multiple metrics](https://docs.smith.langchain.com/assets/images/compare_metrics-2c267a32945f84ceb357756f572ce65e.png)
Or
  1. **Data series** : Create multiple data series within a chart, each with its own filters. This is useful for comparing granular data within a single metric. An example of using data series is to compare the performance of two different models or configurations. Name each series and apply specific filters.


![Multiple data series](https://docs.smith.langchain.com/assets/images/multiple_data_series-eea9e8d661a0911975c589c040fc9525.png)
## Chart display options[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#chart-display-options "Direct link to Chart display options")
  * Choose between a line chart and a bar chart for visualizing (you can toggle this within the dashboard view)
  * Optionally reassign the chart to a different dashboard


![Multiple data series](https://docs.smith.langchain.com/assets/images/bar_chart-6833f543ea811ee088f86e15743dc073.png)
## Saving and managing charts[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#saving-and-managing-charts "Direct link to Saving and managing charts")
  * Click `Save` to save your chart to the dashboard.
  * Edit or delete a chart by clicking the triple line button in the top right of the chart.
  * Clone a chart by clicking the triple line button in the top right of the chart and selecting `+ Clone`. This will open a new chart creation pane with the same configurations as the original.


![More actions bar](https://docs.smith.langchain.com/assets/images/more_actions_bar-266698c4e0cb32666cd4be789b4864f3.png)
## View a chart in full screen[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#view-a-chart-in-full-screen "Direct link to View a chart in full screen")
Click the expand icon in the top right corner of a chart to view it in full screen. This view offers:
  * Greater detail on individual runs contributing to the chart data.
  * The ability to change the time range of the chart.


![Expanded chart](https://docs.smith.langchain.com/assets/images/expanded_chart-af3dd22cda8dd497cccfa73333fdf2a4.png)
## User journeys[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#user-journeys "Direct link to User journeys")
  1. **Use monitoring charts for mapping the decisions made by an agent at a particular node.**


Consider an email assistant agent. At a particular node it makes a decision about an email to:
  * send an email back
  * notify the user
  * no response needed


We can create a chart to track and visualize the breakdown of these decisions.
**Creating the chart**
  1. **Metric Selection** : Select the metric `Run count`.
  2. **Chart Filters** : Add a tree filter to include all of the traces with name `triage_input`. This means we only include traces that hit the `triage_input` node. Also add a chart filter for `Is Root` is `true`, so our count is not inflated by the number of nodes in the trace. ![Decision at node](https://docs.smith.langchain.com/assets/images/chart_filters_for_node_decision-9f1643182d2f74f6c170b80ca726948d.png)
  3. **Data Series** : Create a data series for each decision made at the `triage_input` node. The output of the decision is stored in the `triage.response` field of the output object, and the value of the decision is either `no`, `email`, or `notify`. Each of these decisions generates a separate data series in the chart. ![Decision at node](https://docs.smith.langchain.com/assets/images/decision_at_node-93908d2d40ba68eaf270a782d13c8ba0.png)


Now we can visualize the decisions made at the `triage_input` node over time.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/dashboards%3E).
[PreviousUse monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)[NextLog traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
  * [Creating a new dashboard](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#creating-a-new-dashboard)
  * [Adding charts to your dashboard](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#adding-charts-to-your-dashboard)
  * [Filtering traces in your chart](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#filtering-traces-in-your-chart)
  * [Comparing data within a chart](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#comparing-data-within-a-chart)
  * [Chart display options](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#chart-display-options)
  * [Saving and managing charts](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#saving-and-managing-charts)
  * [View a chart in full screen](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#view-a-chart-in-full-screen)
  * [User journeys](https://docs.smith.langchain.com/observability/how_to_guides/dashboards#user-journeys)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/data_export

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/data_export#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/data_export)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * [Beta] Bulk Exporting Trace Data


On this page
# [Beta] Bulk Exporting Trace Data
Note
Please note that the Data Export functionality is in Beta and only supported for LangSmith Plus or Enterprise tiers.
LangSmith's bulk data export functionality allows you to export your traces into an external destination. This can be useful if you want to analyze the data offline in a tool such as BigQuery, Snowflake, RedShift, Jupyter Notebooks, etc.
An export can be launched to target a specific LangSmith project and date range. Once a batch export is launched, our system will handle the orchestration and resilience of the export process. Please note that exporting your data may take some time depending on the size of your data. We also have a limit on how many of your exports can run at the same time. Bulk exports also have a runtime timeout of 24 hours.
## Destinations[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#destinations "Direct link to Destinations")
Currently we support exporting to an S3 bucket or S3 API compatible bucket that you provide. The data will be exported in [Parquet](https://parquet.apache.org/docs/overview/) columnar format. This format will allow you to easily import the data into other systems. The data export will contain equivalent data fields as the [Run data format](https://docs.smith.langchain.com/reference/data_formats/run_data_format).
## Exporting Data[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#exporting-data "Direct link to Exporting Data")
### Destinations - Providing a S3 bucket[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#destinations---providing-a-s3-bucket "Direct link to Destinations - Providing a S3 bucket")
To export LangSmith data, you will need to provide an S3 bucket where the data will be exported to.
The following information is needed for the export:
  * **Bucket Name** : The name of the S3 bucket where the data will be exported to.
  * **Prefix** : The root prefix within the bucket where the data will be exported to.
  * **S3 Region** : The region of the bucket - this is needed for AWS S3 buckets.
  * **Endpoint URL** : The endpoint URL for the S3 bucket - this is needed for S3 API compatible buckets.
  * **Access Key** : The access key for the S3 bucket.
  * **Secret Key** : The secret key for the S3 bucket.


We support any S3 compatible bucket, for non AWS buckets such as GCS or MinIO, you will need to provide the endpoint URL.
### Preparing the Destination[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#preparing-the-destination "Direct link to Preparing the Destination")
The following example demonstrates how to create a destination using cURL. Replace the placeholder values with your actual configuration details. Note that credentials will be stored securely in an encrypted form in our system.
```
curl --request POST \ --url 'https://api.smith.langchain.com/api/v1/bulk-exports/destinations' \ --header 'Content-Type: application/json' \ --header 'X-API-Key: YOUR_API_KEY' \ --header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \ --data '{  "destination_type": "s3",  "display_name": "My S3 Destination",  "config": {   "bucket_name": "your-s3-bucket-name",   "prefix": "root_folder_prefix",   "region": "your aws s3 region",   "endpoint_url": "your endpoint url for s3 compatible buckets"  },  "credentials": {   "access_key_id": "YOUR_S3_ACCESS_KEY_ID",   "secret_access_key": "YOUR_S3_SECRET_ACCESS_KEY"  } }'
```

Use the returned `id` to reference this destination in subsequent bulk export operations.
**If you receive an error while creating a destination, see[debug destination errors](https://docs.smith.langchain.com/observability/how_to_guides/data_export#debugging-destination-errors) for details on how to debug this.**
#### AWS S3 bucket[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#aws-s3-bucket "Direct link to AWS S3 bucket")
For AWS S3, you can leave off the `endpoint_url` and supply the region that matches the region of your bucket.
```
curl --request POST \ --url 'https://api.smith.langchain.com/api/v1/bulk-exports/destinations' \ --header 'Content-Type: application/json' \ --header 'X-API-Key: YOUR_API_KEY' \ --header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \ --data '{  "destination_type": "s3",  "display_name": "My AWS S3 Destination",  "config": {   "bucket_name": "my_bucket",   "prefix": "data_exports",   "region": "us-east-1",  },  "credentials": {   "access_key_id": "YOUR_S3_ACCESS_KEY_ID",   "secret_access_key": "YOUR_S3_SECRET_ACCESS_KEY"  } }'
```

#### Google GCS XML S3 compatible bucket[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#google-gcs-xml-s3-compatible-bucket "Direct link to Google GCS XML S3 compatible bucket")
When using Google's GCS bucket, you need to use the XML S3 compatible API, and supply the `endpoint_url` which is typically `https://storage.googleapis.com`. Here is an example of the API request when using the GCS XML API which is compatible with S3:
```
curl --request POST \ --url 'https://api.smith.langchain.com/api/v1/bulk-exports/destinations' \ --header 'Content-Type: application/json' \ --header 'X-API-Key: YOUR_API_KEY' \ --header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \ --data '{  "destination_type": "s3",  "display_name": "My GCS Destination",  "config": {   "bucket_name": "my_bucket",   "prefix": "data_exports",   "endpoint_url": "https://storage.googleapis.com"  },  "credentials": {   "access_key_id": "YOUR_S3_ACCESS_KEY_ID",   "secret_access_key": "YOUR_S3_SECRET_ACCESS_KEY"  } }'
```

See [Google documentation](https://cloud.google.com/storage/docs/interoperability#xml_api) for more info
### Create an export job[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#create-an-export-job "Direct link to Create an export job")
To export data, you will need to create an export job. This job will specify the destination, the project, and the date range of the data to export.
You can use the following cURL command to create the job:
```
curl --request POST \ --url 'https://api.smith.langchain.com/api/v1/bulk-exports' \ --header 'Content-Type: application/json' \ --header 'X-API-Key: YOUR_API_KEY' \ --header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \ --data '{  "bulk_export_destination_id": "your_destination_id",  "session_id": "project_uuid",  "start_time": "2024-01-01T00:00:00Z",  "end_time": "2024-01-02T23:59:59Z" }'
```

note
The `session_id` is also known as the Tracing Project ID, which can be copied from the individual project view by clicking into the project in the Tracing Projects list.
Use the returned `id` to reference this export in subsequent bulk export operations.
## Monitoring the Export Job[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#monitoring-the-export-job "Direct link to Monitoring the Export Job")
### Monitor Export Status[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#monitor-export-status "Direct link to Monitor Export Status")
To monitor the status of an export job, use the following cURL command:
```
curl --request GET \ --url 'https://api.smith.langchain.com/api/v1/bulk-exports/{export_id}' \ --header 'Content-Type: application/json' \ --header 'X-API-Key: YOUR_API_KEY' \ --header 'X-Tenant-Id: YOUR_WORKSPACE_ID'
```

Replace `{export_id}` with the ID of the export you want to monitor. This command retrieves the current status of the specified export job.
### List Runs for an Export[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#list-runs-for-an-export "Direct link to List Runs for an Export")
An export is typically broken up into multiple runs which correspond to a specific date partition to export. To list all runs associated with a specific export, use the following cURL command:
```
curl --request GET \ --url 'https://api.smith.langchain.com/api/v1/bulk-exports/{export_id}/runs' \ --header 'Content-Type: application/json' \ --header 'X-API-Key: YOUR_API_KEY' \ --header 'X-Tenant-Id: YOUR_WORKSPACE_ID'
```

This command fetches all runs related to the specified export, providing details such as run ID, status, creation time, rows exported, etc.
### List All Exports[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#list-all-exports "Direct link to List All Exports")
To retrieve a list of all export jobs, use the following cURL command:
```
curl --request GET \ --url 'https://api.smith.langchain.com/api/v1/bulk-exports' \ --header 'Content-Type: application/json' \ --header 'X-API-Key: YOUR_API_KEY' \ --header 'X-Tenant-Id: YOUR_WORKSPACE_ID'
```

This command returns a list of all export jobs along with their current statuses and creation timestamps.
### Stop an Export[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#stop-an-export "Direct link to Stop an Export")
To stop an existing export, use the following cURL command:
```
curl --request PATCH \ --url 'https://api.smith.langchain.com/api/v1/bulk-exports/{export_id}' \ --header 'Content-Type: application/json' \ --header 'X-API-Key: YOUR_API_KEY' \ --header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \ --data '{  "status": "Cancelled"}'
```

Replace `{export_id}` with the ID of the export you wish to cancel. Note that a job cannot be restarted once it has been cancelled, you will need to create a new export job instead.
## Partitioning Scheme[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#partitioning-scheme "Direct link to Partitioning Scheme")
Data will be exported into your bucket into the follow Hive partitioned format:
```
<bucket>/<prefix>/export_id=<export_id>/tenant_id=<tenant_id>/session_id=<session_id>/runs/year=<year>/month=<month>/day=<day>
```

## Importing Data into other systems[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#importing-data-into-other-systems "Direct link to Importing Data into other systems")
Importing data from S3 and Parquet format is commonly supported by the majority of analytical systems. See below for documentation links:
### BigQuery[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#bigquery "Direct link to BigQuery")
To import your data into BigQuery, see [Loading Data from Parquet](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet) and also [Hive Partitioned loads](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs).
### Snowflake[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#snowflake "Direct link to Snowflake")
You can load data into Snowflake from S3 by following the [Load from Cloud Document](https://docs.snowflake.com/en/user-guide/tutorials/load-from-cloud-tutorial).
### RedShift[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#redshift "Direct link to RedShift")
You can COPY data from S3 / Parquet into RedShift by following the [AWS COPY Instructions](https://aws.amazon.com/about-aws/whats-new/2018/06/amazon-redshift-can-now-copy-from-parquet-and-orc-file-formats/).
### Clickhouse[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#clickhouse "Direct link to Clickhouse")
You can directly query data in S3 / Parquet format in Clickhouse. As an example, if using GCS, you can query the data as follows:
```
SELECTcount(distinct id)FROM s3('https://storage.googleapis.com/<bucket>/<prefix>/export_id=<export_id>/**','access_key_id','access_secret','Parquet')
```

See [Clickhouse S3 Integration Documentation](https://clickhouse.com/docs/en/engines/table-engines/integrations/s3) for more information.
### DuckDB[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#duckdb "Direct link to DuckDB")
You can query the data from S3 in-memory with SQL using DuckDB. See [S3 import Documentation](https://duckdb.org/docs/guides/network_cloud_storage/s3_import.html).
## Error Handling[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#error-handling "Direct link to Error Handling")
### Debugging Destination Errors[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#debugging-destination-errors "Direct link to Debugging Destination Errors")
The destinations API endpoint will validate that the destination and credentials are valid and that write access is is present for the bucket.
If you receive an error, and would like to debug this error, you can use the [AWS CLI](https://aws.amazon.com/cli/) to test the connectivity to the bucket. You should be able to write a file with the CLI using the same data that you supplied to the destinations API above.
**AWS S3:**
```
aws configure# set the same access key credentials and region as you used for the destination> AWS Access Key ID: <access_key_id>> AWS Secret Access Key: <secret_access_key>> Default region name [us-east-1]: <region># List bucketsaws s3 ls /# test write permissionstouch ./test.txtaws s3 cp ./test.txt s3://<bucket-name>/tmp/test.txt
```

**GCS Compatible Buckets:**
You will need to supply the endpoint_url with `--endpoint-url` option. For GCS, the `endpoint_url` is typically `https://storage.googleapis.com`:
```
aws configure# set the same access key credentials and region as you used for the destination> AWS Access Key ID: <access_key_id>> AWS Secret Access Key: <secret_access_key>> Default region name [us-east-1]: <region># List bucketsaws s3 --endpoint-url=<endpoint_url> ls /# test write permissionstouch ./test.txtaws s3 --endpoint-url=<endpoint_url> cp ./test.txt s3://<bucket-name>/tmp/test.txt
```

### Monitoring Runs[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#monitoring-runs "Direct link to Monitoring Runs")
You can monitor your runs using the [List Runs API](https://docs.smith.langchain.com/observability/how_to_guides/data_export#list-runs-for-an-export). If this is a known error, this will be added to the `errors` field of the run.
### Common Errors[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/data_export#common-errors "Direct link to Common Errors")
Here are some common errors:
Error| Description  
---|---  
Access denied| The blob store credentials or bucket are not valid. This error occurs when the provided access key and secret key combination doesn't have the necessary permissions to access the specified bucket or perform the required operations.  
Bucket is not valid| The specified blob store bucket is not valid. This error is thrown when the bucket doesn't exist or there is not enough access to perform writes on the bucket.  
Key ID you provided does not exist| The blob store credentials provided are not valid. This error occurs when the access key ID used for authentication is not a valid key.  
Invalid endpoint| The endpoint_url provided is invalid. This error is raised when the specified endpoint is an invalid endpoint. Only S3 compatible endpoints are supported, for example `https://storage.googleapis.com` for GCS, `https://play.min.io` for minio, etc. If using AWS, you should omit the endpoint_url.  
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/data_export%3E).
[PreviousTroubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)[NextAlerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
  * [Destinations](https://docs.smith.langchain.com/observability/how_to_guides/data_export#destinations)
  * [Exporting Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export#exporting-data)
    * [Destinations - Providing a S3 bucket](https://docs.smith.langchain.com/observability/how_to_guides/data_export#destinations---providing-a-s3-bucket)
    * [Preparing the Destination](https://docs.smith.langchain.com/observability/how_to_guides/data_export#preparing-the-destination)
    * [Create an export job](https://docs.smith.langchain.com/observability/how_to_guides/data_export#create-an-export-job)
  * [Monitoring the Export Job](https://docs.smith.langchain.com/observability/how_to_guides/data_export#monitoring-the-export-job)
    * [Monitor Export Status](https://docs.smith.langchain.com/observability/how_to_guides/data_export#monitor-export-status)
    * [List Runs for an Export](https://docs.smith.langchain.com/observability/how_to_guides/data_export#list-runs-for-an-export)
    * [List All Exports](https://docs.smith.langchain.com/observability/how_to_guides/data_export#list-all-exports)
    * [Stop an Export](https://docs.smith.langchain.com/observability/how_to_guides/data_export#stop-an-export)
  * [Partitioning Scheme](https://docs.smith.langchain.com/observability/how_to_guides/data_export#partitioning-scheme)
  * [Importing Data into other systems](https://docs.smith.langchain.com/observability/how_to_guides/data_export#importing-data-into-other-systems)
    * [BigQuery](https://docs.smith.langchain.com/observability/how_to_guides/data_export#bigquery)
    * [Snowflake](https://docs.smith.langchain.com/observability/how_to_guides/data_export#snowflake)
    * [RedShift](https://docs.smith.langchain.com/observability/how_to_guides/data_export#redshift)
    * [Clickhouse](https://docs.smith.langchain.com/observability/how_to_guides/data_export#clickhouse)
    * [DuckDB](https://docs.smith.langchain.com/observability/how_to_guides/data_export#duckdb)
  * [Error Handling](https://docs.smith.langchain.com/observability/how_to_guides/data_export#error-handling)
    * [Debugging Destination Errors](https://docs.smith.langchain.com/observability/how_to_guides/data_export#debugging-destination-errors)
    * [Monitoring Runs](https://docs.smith.langchain.com/observability/how_to_guides/data_export#monitoring-runs)
    * [Common Errors](https://docs.smith.langchain.com/observability/how_to_guides/data_export#common-errors)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Implement distributed tracing


On this page
# Implement distributed tracing
Sometimes, you need to trace a request across multiple services.
LangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (`langsmith-trace` and optional `baggage` for metadata/tags).
Example client-server setup:
  * Trace starts on client
  * Continues on server


## Distributed tracing in Python[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing#distributed-tracing-in-python "Direct link to Distributed tracing in Python")
```
# client.pyfrom langsmith.run_helpers import get_current_run_tree, traceableimport httpx@traceableasyncdefmy_client_function():  headers ={}asyncwith httpx.AsyncClient(base_url="...")as client:if run_tree := get_current_run_tree():# add langsmith-id to headers      headers.update(run_tree.to_headers())returnawait client.post("/my-route", headers=headers)
```

Then the server (or other service) can continue the trace by handling the headers appropriately. If you are using an asgi app Starlette or FastAPI, you can connect the distributed trace using LangSmith's `TracingMiddleware`.
info
The `TracingMiddleware` class was added in `langsmith==0.1.133`.
Example using FastAPI:
```
from langsmith import traceablefrom langsmith.middleware import TracingMiddlewarefrom fastapi import FastAPI, Requestapp = FastAPI()# Or Flask, Django, or any other frameworkapp.add_middleware(TracingMiddleware)@traceableasyncdefsome_function():...@app.post("/my-route")asyncdeffake_route(request: Request):returnawait some_function()
```

Or in Starlette:
```
from starlette.applications import Starlettefrom starlette.middleware import Middlewarefrom langsmith.middleware import TracingMiddlewareroutes =...middleware =[  Middleware(TracingMiddleware),]app = Starlette(..., middleware=middleware)
```

If you are using other server frameworks, you can always "receive" the distributed trace by passing the headers in through `langsmith_extra`:
```
# server.pyfrom langsmith import traceablefrom langsmith.run_helpers import tracing_contextfrom fastapi import FastAPI, Request@traceableasyncdefmy_application():...app = FastAPI()# Or Flask, Django, or any other framework@app.post("/my-route")asyncdeffake_route(request: Request):# request.headers: {"langsmith-trace": "..."}# as well as optional metadata/tags in `baggage`with tracing_context(parent=request.headers):returnawait my_application()
```

The example above uses the `tracing_context` context manager. You can also directly specify the parent run context in the `langsmith_extra` parameter of a method wrapped with `@traceable`.
```
from langsmith.run_helpers import traceable, trace# ... same as above@app.post("/my-route")asyncdeffake_route(request: Request):# request.headers: {"langsmith-trace": "..."}  my_application(langsmith_extra={"parent": request.headers})
```

## Distributed tracing in TypeScript[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing#distributed-tracing-in-typescript "Direct link to Distributed tracing in TypeScript")
note
Distributed tracing in TypeScript requires `langsmith` version `>=0.1.31`
First, we obtain the current run tree from the client and convert it to `langsmith-trace` and `baggage` header values, which we can pass to the server:
```
// client.mtsimport{ getCurrentRunTree, traceable }from"langsmith/traceable";const client =traceable(async()=>{const runTree =getCurrentRunTree();returnawaitfetch("...",{   method:"POST",   headers: runTree.toHeaders(),}).then((a)=> a.text());},{ name:"client"});awaitclient();
```

Then, the server converts the headers back to a run tree, which it uses to further continue the tracing.
To pass the newly created run tree to a traceable function, we can use the `withRunTree` helper, which will ensure the run tree is propagated within traceable invocations.
  * Express.JS
  * Hono


```
// server.mtsimport{ RunTree }from"langsmith";import{ traceable, withRunTree }from"langsmith/traceable";import express from"express";import bodyParser from"body-parser";const server =traceable((text:string)=>`Hello from the server! Received "${text}"`,{ name:"server"});const app =express();app.use(bodyParser.text());app.post("/",async(req, res)=>{const runTree = RunTree.fromHeaders(req.headers);const result =awaitwithRunTree(runTree,()=>server(req.body)); res.send(result);});
```

```
// server.mtsimport{ RunTree }from"langsmith";import{ traceable, withRunTree }from"langsmith/traceable";import{ Hono }from"hono";const server =traceable((text:string)=>`Hello from the server! Received "${text}"`,{ name:"server"});const app =newHono();app.post("/",async(c)=>{const body =await c.req.text();const runTree = RunTree.fromHeaders(c.req.raw.headers);const result =awaitwithRunTree(runTree,()=>server(body));return c.body(result);});
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/distributed_tracing%3E).
[PreviousAdd metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)[NextAccess the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
  * [Distributed tracing in Python](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing#distributed-tracing-in-python)
  * [Distributed tracing in TypeScript](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing#distributed-tracing-in-typescript)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/export_traces

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Query traces


On this page
# Query traces
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Run (span) data format](https://docs.smith.langchain.com/reference/data_formats/run_data_format)
  * [LangSmith API Reference](https://api.smith.langchain.com/redoc)
  * [LangSmith trace query syntax](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)


note
If you are looking to export a large volume of traces, we recommend that you use the [Bulk Data Export](https://docs.smith.langchain.com/observability/how_to_guides/data_export) functionality, as it will better handle large data volumes and will support automatic retries and parallelization across partitions.
The recommended way to query runs (the span data in LangSmith traces) is to use the `list_runs` method in the SDK or `/runs/query` endpoint in the API.
LangSmith stores traces in a simple format that is specified in the [Run (span) data format](https://docs.smith.langchain.com/reference/data_formats/run_data_format).
## Use filter arguments[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#use-filter-arguments "Direct link to Use filter arguments")
For simple queries, you don't have to rely on our query syntax. You can use the filter arguments specified in the [filter arguments reference](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-arguments).
Prerequisites
Initialize the client before running the below code snippets.
  * Python
  * TypeScript


```
from langsmith import Clientclient = Client()
```

```
import{ Client, Run }from"langsmith";const client =newClient();
```

Below are some examples of ways to list runs using keyword arguments:
### List all runs in a project[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-in-a-project "Direct link to List all runs in a project")
  * Python
  * TypeScript


```
project_runs = client.list_runs(project_name="<your_project>")
```

```
// Download runs in a projectconst projectRuns: Run[]=[];forawait(const run of client.listRuns({projectName:"<your_project>",})){projectRuns.push(run);};
```

### List LLM and Chat runs in the last 24 hours[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-llm-and-chat-runs-in-the-last-24-hours "Direct link to List LLM and Chat runs in the last 24 hours")
  * Python
  * TypeScript


```
todays_llm_runs = client.list_runs( project_name="<your_project>", start_time=datetime.now()- timedelta(days=1), run_type="llm",)
```

```
const todaysLlmRuns: Run[]=[];forawait(const run of client.listRuns({projectName:"<your_project>",startTime:newDate(Date.now()-1000*60*60*24),runType:"llm",})){todaysLlmRuns.push(run);};
```

### List root runs in a project[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-root-runs-in-a-project "Direct link to List root runs in a project")
Root runs are runs that have no parents. These are assigned a value of `True` for `is_root`. You can use this to filter for root runs.
  * Python
  * TypeScript


```
root_runs = client.list_runs( project_name="<your_project>", is_root=True)
```

```
const rootRuns: Run[]=[];forawait(const run of client.listRuns({projectName:"<your_project>",isRoot:1,})){rootRuns.push(run);};
```

### List runs without errors[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-runs-without-errors "Direct link to List runs without errors")
  * Python
  * TypeScript


```
correct_runs = client.list_runs(project_name="<your_project>", error=False)
```

```
const correctRuns: Run[]=[];forawait(const run of client.listRuns({projectName:"<your_project>",error:false,})){correctRuns.push(run);};
```

### List runs by run ID[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-runs-by-run-id "Direct link to List runs by run ID")
Ignores Other Arguments
If you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like `project_name`, `run_type`, etc. and directly return the runs matching the given IDs.
If you have a list of run IDs, you can list them directly:
  * Python
  * TypeScript


```
run_ids =['a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836','9398e6be-964f-4aa4-8ae9-ad78cd4b7074']selected_runs = client.list_runs(id=run_ids)
```

```
const runIds =["a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836","9398e6be-964f-4aa4-8ae9-ad78cd4b7074",];const selectedRuns: Run[]=[];forawait(const run of client.listRuns({id: runIds,})){selectedRuns.push(run);};
```

## Use filter query language[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#use-filter-query-language "Direct link to Use filter query language")
For more complex queries, you can use the query language described in the [filter query language reference](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-query-language).
### List all root runs in a conversational thread[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-root-runs-in-a-conversational-thread "Direct link to List all root runs in a conversational thread")
This is the way to fetch runs in a conversational thread. For more information on setting up threads, refer to our [how-to guide on setting up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads). Threads are grouped by setting a shared thread ID. The LangSmith UI lets you use any one of the following three metadata keys: `session_id`, `conversation_id`, or `thread_id`. The following query matches on any of them.
  * Python
  * TypeScript


```
group_key ="<your_thread_id>"filter_string =f'and(in(metadata_key, ["session_id","conversation_id","thread_id"]), eq(metadata_value, "{group_key}"))'thread_runs = client.list_runs( project_name="<your_project>",filter=filter_string, is_root=True)
```

```
const groupKey ="<your_thread_id>";const filterString =`and(in(metadata_key, ["session_id","conversation_id","thread_id"]), eq(metadata_value, "${groupKey}"))`;const threadRuns: Run[]=[];forawait(const run of client.listRuns({projectName:"<your_project>",filter: filterString,isRoot:true})){threadRuns.push(run);};
```

### List all runs called "extractor" whose root of the trace was assigned feedback "user_score" score of 1[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-called-extractor-whose-root-of-the-trace-was-assigned-feedback-user_score-score-of-1 "Direct link to List all runs called "extractor" whose root of the trace was assigned feedback "user_score" score of 1")
  * Python
  * TypeScript


```
client.list_runs( project_name="<your_project>",filter='eq(name, "extractor")', trace_filter='and(eq(feedback_key, "user_score"), eq(feedback_score, 1))')
```

```
client.listRuns({projectName:"<your_project>",filter:'eq(name, "extractor")',traceFilter:'and(eq(feedback_key, "user_score"), eq(feedback_score, 1))'})
```

### List runs with "star_rating" key whose score is greater than 4[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-runs-with-star_rating-key-whose-score-is-greater-than-4 "Direct link to List runs with "star_rating" key whose score is greater than 4")
  * Python
  * TypeScript


```
client.list_runs( project_name="<your_project>",filter='and(eq(feedback_key, "star_rating"), gt(feedback_score, 4))')
```

```
client.listRuns({projectName:"<your_project>",filter:'and(eq(feedback_key, "star_rating"), gt(feedback_score, 4))'})
```

### List runs that took longer than 5 seconds to complete[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-runs-that-took-longer-than-5-seconds-to-complete "Direct link to List runs that took longer than 5 seconds to complete")
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='gt(latency, "5s")')
```

```
client.listRuns({projectName:"<your_project>", filter:'gt(latency, "5s")'})
```

### List all runs where `total_tokens` is greater than 5000[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-where-total_tokens-is-greater-than-5000 "Direct link to list-all-runs-where-total_tokens-is-greater-than-5000")
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='gt(total_tokens, 5000)')
```

```
client.listRuns({projectName:"<your_project>", filter:'gt(total_tokens, 5000)'})
```

### List all runs that have "error" not equal to null[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-that-have-error-not-equal-to-null "Direct link to List all runs that have "error" not equal to null")
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='neq(error, null)')
```

```
client.listRuns({projectName:"<your_project>", filter:'neq(error, null)'})
```

### List all runs where `start_time` is greater than a specific timestamp[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-where-start_time-is-greater-than-a-specific-timestamp "Direct link to list-all-runs-where-start_time-is-greater-than-a-specific-timestamp")
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='gt(start_time, "2023-07-15T12:34:56Z")')
```

```
client.listRuns({projectName:"<your_project>", filter:'gt(start_time, "2023-07-15T12:34:56Z")'})
```

### List all runs that contain the string "substring"[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-that-contain-the-string-substring "Direct link to List all runs that contain the string "substring"")
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='search("substring")')
```

```
client.listRuns({projectName:"<your_project>", filter:'search("substring")'})
```

### List all runs that are tagged with the git hash "2aa1cf4"[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-that-are-tagged-with-the-git-hash-2aa1cf4 "Direct link to List all runs that are tagged with the git hash "2aa1cf4"")
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='has(tags, "2aa1cf4")')
```

```
client.listRuns({projectName:"<your_project>", filter:'has(tags, "2aa1cf4")'})
```

### List all "chain" type runs that took more than 10 seconds and[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-chain-type-runs-that-took-more-than-10-seconds-and "Direct link to List all "chain" type runs that took more than 10 seconds and")
had `total_tokens` greater than 5000
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='and(eq(run_type, "chain"), gt(latency, 10), gt(total_tokens, 5000))')
```

```
client.listRuns({projectName:"<your_project>",filter:'and(eq(run_type, "chain"), gt(latency, 10), gt(total_tokens, 5000))'})
```

### List all runs that started after a specific timestamp and either[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-that-started-after-a-specific-timestamp-and-either "Direct link to List all runs that started after a specific timestamp and either")
have "error" not equal to null or a "Correctness" feedback score equal to 0
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='and(gt(start_time, "2023-07-15T12:34:56Z"), or(neq(error, null), and(eq(feedback_key, "Correctness"), eq(feedback_score, 0.0))))')
```

```
client.listRuns({projectName:"<your_project>",filter:'and(gt(start_time, "2023-07-15T12:34:56Z"), or(neq(error, null), and(eq(feedback_key, "Correctness"), eq(feedback_score, 0.0))))'})
```

### Complex query: List all runs where `tags` include "experimental" or "beta" and[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#complex-query-list-all-runs-where-tags-include-experimental-or-beta-and "Direct link to complex-query-list-all-runs-where-tags-include-experimental-or-beta-and")
`latency` is greater than 2 seconds
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='and(or(has(tags, "experimental"), has(tags, "beta")), gt(latency, 2))')
```

```
client.listRuns({projectName:"<your_project>",filter:'and(or(has(tags, "experimental"), has(tags, "beta")), gt(latency, 2))'})
```

### Search trace trees by full text You can use the `search()` function without[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#search-trace-trees-by-full-text-you-can-use-the-search-function-without "Direct link to search-trace-trees-by-full-text-you-can-use-the-search-function-without")
any specific field to do a full text search across all string fields in a run. This allows you to quickly find traces that match a search term.
  * Python
  * TypeScript


```
client.list_runs(project_name="<your_project>",filter='search("image classification")')
```

```
client.listRuns({projectName:"<your_project>",filter:'search("image classification")'})
```

### Check for presence of metadata[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#check-for-presence-of-metadata "Direct link to Check for presence of metadata")
If you want to check for the presence of metadata, you can use the `eq` operator, optionally with an `and` statement to match by value. This is useful if you want to log more structured information about your runs.
  * Python
  * TypeScript


```
to_search ={"user_id":""}# Check for any run with the "user_id" metadata keyclient.list_runs(project_name="default",filter="eq(metadata_key, 'user_id')")# Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.list_runs(project_name="default",filter="and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))")
```

```
// Check for any run with the "user_id" metadata keyclient.listRuns({projectName:'default',filter:`eq(metadata_key, 'user_id')`});// Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.listRuns({projectName:'default',filter:`and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))`});
```

### Check for environment details in metadata.[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#check-for-environment-details-in-metadata "Direct link to Check for environment details in metadata.")
A common pattern is to add environment information to your traces via metadata. If you want to filter for runs containing environment metadata, you can use the same pattern as above:
  * Python
  * TypeScript


```
client.list_runs(project_name="default",filter="and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))")
```

```
client.listRuns({projectName:'default',filter:`and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))`});
```

### Check for conversation ID in metadata[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#check-for-conversation-id-in-metadata "Direct link to Check for conversation ID in metadata")
Another common way to associate traces in the same conversation is by using a shared conversation ID. If you want to filter runs based on a conversation ID in this way, you can search for that ID in the metadata.
  * Python
  * TypeScript


```
client.list_runs(project_name="default",filter="and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))")
```

```
client.listRuns({projectName:'default',filter:`and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))`});
```

### Negative filtering on key-value pairs[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#negative-filtering-on-key-value-pairs "Direct link to Negative filtering on key-value pairs")
You can use negative filtering on metadata, input, and output key-value pairs to exclude specific runs from your results. Here are some examples for metadata key-value pairs but the same logic applies to input and output key-value pairs.
  * Python
  * TypeScript


```
# Find all runs where the metadata does not contain a "conversation_id" keyclient.list_runs(project_name="default",filter="and(neq(metadata_key, 'conversation_id'))")# Find all runs where the conversation_id in metadata is not "a1b2c3d4-e5f6-7890"client.list_runs(project_name="default",filter="and(eq(metadata_key, 'conversation_id'), neq(metadata_value, 'a1b2c3d4-e5f6-7890'))")# Find all runs where there is no "conversation_id" metadata key and the "a1b2c3d4-e5f6-7890" value is not presentclient.list_runs(project_name="default",filter="and(neq(metadata_key, 'conversation_id'), neq(metadata_value, 'a1b2c3d4-e5f6-7890'))")# Find all runs where the conversation_id metadata key is not present but the "a1b2c3d4-e5f6-7890" value is presentclient.list_runs(project_name="default",filter="and(neq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))")
```

```
// Find all runs where the metadata does not contain a "conversation_id" keyclient.listRuns({projectName:'default',filter:`and(neq(metadata_key, 'conversation_id'))`});// Find all runs where the conversation_id in metadata is not "a1b2c3d4-e5f6-7890" client.listRuns({  projectName:'default',  filter:`and(eq(metadata_key, 'conversation_id'), neq(metadata_value, 'a1b2c3d4-e5f6-7890'))`});// Find all runs where there is no "conversation_id" metadata key and the "a1b2c3d4-e5f6-7890" value is not present client.listRuns({  projectName:'default',  filter:`and(neq(metadata_key, 'conversation_id'), neq(metadata_value, 'a1b2c3d4-e5f6-7890'))`});// Find all runs where the conversation_id metadata key is not present but the "a1b2c3d4-e5f6-7890" value is present client.listRuns({  projectName:'default',    filter:`and(neq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))`});
```

### Combine multiple filters[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#combine-multiple-filters "Direct link to Combine multiple filters")
If you want to combine multiple conditions to refine your search, you can use the `and` operator along with other filtering functions. Here's how you can search for runs named "ChatOpenAI" that also have a specific `conversation_id` in their metadata:
  * Python
  * TypeScript


```
client.list_runs(project_name="default",filter="and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))")
```

```
client.listRuns({projectName:'default',filter:`and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))`});
```

### Tree Filter[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#tree-filter "Direct link to Tree Filter")
List all runs named "RetrieveDocs" whose root run has a "user_score" feedback of 1 and any run in the full trace is named "ExpandQuery".
This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace.
  * Python
  * TypeScript


```
client.list_runs( project_name="<your_project>",filter='eq(name, "RetrieveDocs")', trace_filter='and(eq(feedback_key, "user_score"), eq(feedback_score, 1))', tree_filter='eq(name, "ExpandQuery")')
```

```
client.listRuns({projectName:"<your_project>",filter:'eq(name, "RetrieveDocs")',traceFilter:'and(eq(feedback_key, "user_score"), eq(feedback_score, 1))',treeFilter:'eq(name, "ExpandQuery")'})
```

### Advanced: export flattened trace view with child tool usage[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#advanced-export-flattened-trace-view-with-child-tool-usage "Direct link to Advanced: export flattened trace view with child tool usage")
The following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace. This can be used to analyze the behavior of your agents across multiple traces.
This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information.
To optimize the query, the example:
  1. Selects only the necessary fields when querying tool runs to reduce query time.
  2. Fetches root runs in batches while processing tool runs concurrently.


  * Python


```
from collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltafrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name ="my-project"num_days =30# List all tool runstool_runs = client.list_runs( project_name=project_name, start_time=datetime.now()- timedelta(days=num_days), run_type="tool",# We don't need to fetch inputs, outputs, and other values that # may increase the query time select=["trace_id","name","run_type"],)data =[]futures:list[Future]=[]trace_cursor =0trace_batch_size =50tool_runs_by_parent = defaultdict(lambda: defaultdict(set))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2)as executor:# Group tool runs by parent run IDfor run in tqdm(tool_runs):# Collect all tools invoked within a given trace   tool_runs_by_parent[run.trace_id]["tools_involved"].add(run.name)# maybe send a batch of parent run IDs to the server# this lets us query for the root runs in batches# while still processing the tool runsiflen(tool_runs_by_parent)% trace_batch_size ==0:if this_batch :=list(tool_runs_by_parent.keys())[       trace_cursor : trace_cursor + trace_batch_size]:       trace_cursor += trace_batch_size       futures.append(         executor.submit(           client.list_runs,           project_name=project_name,           run_ids=this_batch,           select=["name","inputs","outputs","run_type"],))if this_batch :=list(tool_runs_by_parent.keys())[trace_cursor:]:   futures.append(     executor.submit(       client.list_runs,       project_name=project_name,       run_ids=this_batch,       select=["name","inputs","outputs","run_type"],))for future in tqdm(futures): root_runs = future.result()for root_run in root_runs:   root_data = tool_runs_by_parent[root_run.id]   data.append({"run_id": root_run.id,"run_name": root_run.name,"run_type": root_run.run_type,"inputs": root_run.inputs,"outputs": root_run.outputs,"tools_involved":list(root_data["tools_involved"]),})# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()
```

### Advanced: export retriever IO for traces with feedback[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#advanced-export-retriever-io-for-traces-with-feedback "Direct link to Advanced: export retriever IO for traces with feedback")
This query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior. The following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score.
  * Python


```
from collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltaimport pandas as pdfrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name ="your-project-name"num_days =1# List all tool runsretriever_runs = client.list_runs( project_name=project_name, start_time=datetime.now()- timedelta(days=num_days), run_type="retriever",# This time we do want to fetch the inputs and outputs, since they# may be adjusted by query expansion steps. select=["trace_id","name","run_type","inputs","outputs"], trace_filter='eq(feedback_key, "user_score")',)data =[]futures:list[Future]=[]trace_cursor =0trace_batch_size =50retriever_runs_by_parent = defaultdict(lambda: defaultdict(list))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2)as executor:# Group retriever runs by parent run IDfor run in tqdm(retriever_runs):# Collect all retriever calls invoked within a given tracefor k, v in run.inputs.items():     retriever_runs_by_parent[run.trace_id][f"retriever.inputs.{k}"].append(v)for k, v in(run.outputs or{}).items():# Extend the docs     retriever_runs_by_parent[run.trace_id][f"retriever.outputs.{k}"].extend(v)# maybe send a batch of parent run IDs to the server# this lets us query for the root runs in batches# while still processing the retriever runsiflen(retriever_runs_by_parent)% trace_batch_size ==0:if this_batch :=list(retriever_runs_by_parent.keys())[       trace_cursor : trace_cursor + trace_batch_size]:       trace_cursor += trace_batch_size       futures.append(         executor.submit(           client.list_runs,           project_name=project_name,           run_ids=this_batch,           select=["name","inputs","outputs","run_type","feedback_stats",],))if this_batch :=list(retriever_runs_by_parent.keys())[trace_cursor:]:   futures.append(     executor.submit(       client.list_runs,       project_name=project_name,       run_ids=this_batch,       select=["name","inputs","outputs","run_type"],))for future in tqdm(futures): root_runs = future.result()for root_run in root_runs:   root_data = retriever_runs_by_parent[root_run.id]   feedback ={f"feedback.{k}": v.get("avg")for k, v in(root_run.feedback_stats or{}).items()}   inputs ={f"inputs.{k}": v for k, v in root_run.inputs.items()}   outputs ={f"outputs.{k}": v for k, v in(root_run.outputs or{}).items()}   data.append({"run_id": root_run.id,"run_name": root_run.name,**inputs,**outputs,**feedback,**root_data,})# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousPrevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)[NextShare or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
  * [Use filter arguments](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#use-filter-arguments)
    * [List all runs in a project](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-in-a-project)
    * [List LLM and Chat runs in the last 24 hours](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-llm-and-chat-runs-in-the-last-24-hours)
    * [List root runs in a project](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-root-runs-in-a-project)
    * [List runs without errors](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-runs-without-errors)
    * [List runs by run ID](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-runs-by-run-id)
  * [Use filter query language](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#use-filter-query-language)
    * [List all root runs in a conversational thread](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-root-runs-in-a-conversational-thread)
    * [List all runs called "extractor" whose root of the trace was assigned feedback "user_score" score of 1](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-called-extractor-whose-root-of-the-trace-was-assigned-feedback-user_score-score-of-1)
    * [List runs with "star_rating" key whose score is greater than 4](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-runs-with-star_rating-key-whose-score-is-greater-than-4)
    * [List runs that took longer than 5 seconds to complete](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-runs-that-took-longer-than-5-seconds-to-complete)
    * [List all runs where `total_tokens` is greater than 5000](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-where-total_tokens-is-greater-than-5000)
    * [List all runs that have "error" not equal to null](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-that-have-error-not-equal-to-null)
    * [List all runs where `start_time` is greater than a specific timestamp](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-where-start_time-is-greater-than-a-specific-timestamp)
    * [List all runs that contain the string "substring"](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-that-contain-the-string-substring)
    * [List all runs that are tagged with the git hash "2aa1cf4"](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-that-are-tagged-with-the-git-hash-2aa1cf4)
    * [List all "chain" type runs that took more than 10 seconds and](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-chain-type-runs-that-took-more-than-10-seconds-and)
    * [List all runs that started after a specific timestamp and either](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#list-all-runs-that-started-after-a-specific-timestamp-and-either)
    * [Complex query: List all runs where `tags` include "experimental" or "beta" and](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#complex-query-list-all-runs-where-tags-include-experimental-or-beta-and)
    * [Search trace trees by full text You can use the `search()` function without](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#search-trace-trees-by-full-text-you-can-use-the-search-function-without)
    * [Check for presence of metadata](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#check-for-presence-of-metadata)
    * [Check for environment details in metadata.](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#check-for-environment-details-in-metadata)
    * [Check for conversation ID in metadata](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#check-for-conversation-id-in-metadata)
    * [Negative filtering on key-value pairs](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#negative-filtering-on-key-value-pairs)
    * [Combine multiple filters](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#combine-multiple-filters)
    * [Tree Filter](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#tree-filter)
    * [Advanced: export flattened trace view with child tool usage](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#advanced-export-flattened-trace-view-with-child-tool-usage)
    * [Advanced: export retriever IO for traces with feedback](https://docs.smith.langchain.com/observability/how_to_guides/export_traces#advanced-export-retriever-io-for-traces-with-feedback)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Filter traces in the application


On this page
# Filter traces in the application
Recommended reading
Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:
  * [Conceptual guide on tracing](https://docs.smith.langchain.com/observability/concepts)


This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see [this guide](https://docs.smith.langchain.com/observability/how_to_guides/export_traces). Being able to accurately filter runs is important for both manual inspection and setting up automations.
## Create a filter[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#create-a-filter "Direct link to Create a filter")
There are two ways to create a filter. First, you can create a filter from the high level nav bar. By default, there is one filter applied: `IsRoot` is `true`. This restricts all runs to be top level traces.
![Filtering](https://docs.smith.langchain.com/assets/images/filter-ab7515aafb47936e8b09831b70d26bbf.png)
You can also define a filter from the `Filter Shortcuts` on the sidebar. This contains commonly used filters.
![Filtering](https://docs.smith.langchain.com/assets/images/filter_shortcuts-9bdfa901aa1df802b4d28ec02a9d3a63.png)
## Filter for intermediate runs (spans)[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#filter-for-intermediate-runs-spans "Direct link to Filter for intermediate runs \(spans\)")
In order to filter for intermediate runs (spans), you first need to remove the default filter of `IsRoot` is `true`. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs. This relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out [this guide](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
## Advanced: filter for intermediate runs (spans) on properties of the root[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root "Direct link to Advanced: filter for intermediate runs \(spans\) on properties of the root")
A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it.
In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the `Advanced Filters` link all the way at the bottom of the filter. This will open up a new modal where you can add `Trace filters`. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for.
![Filtering](https://docs.smith.langchain.com/assets/images/trace_filter-589b83c4e3460bbb387365e569e397d7.png)
## Advanced: filter for runs (spans) whose child runs have some attribute[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute "Direct link to Advanced: filter for runs \(spans\) whose child runs have some attribute")
This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name `Foo`. This is useful when `Foo` is not always called, but you want to analyze the cases where it is.
In order to do this, you can click on the `Advanced Filters` link all the way at the bottom of the filter. This will open up a new modal where you can add `Tree filters`. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for.
![Filtering](https://docs.smith.langchain.com/assets/images/child_runs-8f4764241223b0bffe96914a52aa0cad.png)
## Filter based on inputs and outputs[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#filter-based-on-inputs-and-outputs "Direct link to Filter based on inputs and outputs")
You can filter runs based on the content in the inputs and outputs of the run.
To filter either inputs or outputs, you can use `Full-Text Search` filter which will match keywords in either field. For more targeted search, you can use the `Input` or `Output` filters which will only match content based on the respective field.
You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided.
Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords).
![Filtering](https://docs.smith.langchain.com/assets/images/filter_full_text-fa44a4502de1be400ec461ac70089afb.png)
Based on the filters above, the system will search for `python` and `tensorflow` in either inputs or outputs, and `embedding` in the inputs along with `fine` and `tune` in the outputs.
## Filter based on input / output key-value pairs[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#filter-based-on-input--output-key-value-pairs "Direct link to Filter based on input / output key-value pairs")
In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.
To filter based on key-value pairs, select the `Input KV` or `Output KV` filter from the Filters dropdown.
For example, to match the following input:
```
{"input":"What is the capital of France?"}
```

Select `Filters`, `Add Filter` to bring up the filtering options. Then select `Input KV`, enter `input` as the key and enter `What is the capital of France?` as the value.
![Filtering](https://docs.smith.langchain.com/assets/images/search_kv_input-993243a5528ed3cf8ab4d81ab1d5554d.png)
You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output:
```
{"documents":[{"page_content":"The capital of France is Paris","metadata":{},"type":"Document"}]}
```

Select `Output KV`, enter `documents.page_content` as the key and enter `The capital of France is Paris` as the value. This will match the nested key `documents.page_content` with the specified value.
![Filtering](https://docs.smith.langchain.com/assets/images/search_kv_output-1f318027afd8f94ca6b923e4ccbbfb32.png)
You can add multiple key-value filters to create more complex queries. You can also use the `Filter Shortcuts` on the right side to quickly filter based on common key values pairs as shown below:
![Filtering](https://docs.smith.langchain.com/assets/images/search_kv_filter_shortcut-45dd369481acfc80edf7e6979342740c.png)
## Example of filtering using output key-value pairs: filter tool calls[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#example-of-filtering-using-output-key-value-pairs-filter-tool-calls "Direct link to Example of filtering using output key-value pairs: filter tool calls")
As an example of filtering on key-value pairs, let's suppose that you wanted to filter traces for tool calls.
In this case, let's assume this is the output you want to filter for:
```
{"generations":[[{"text":"","type":"ChatGeneration","message":{"lc":1,"type":"constructor","id":[],"kwargs":{"type":"ai","id":"run-ca7f7531-f4de-4790-9c3e-960be7f8b109","tool_calls":[{"name":"Plan","args":{"steps":["Research LangGraph's node configuration capabilities","Investigate how to add a Python code execution node","Find an example or create a sample implementation of a code execution node"]},"id":"toolu_01XexPzAVknT3gRmUB5PK5BP","type":"tool_call"}]}}}]],"llm_output":null,"run":null,"type":"LLMResult"}
```

With the example above, the KV search will map each nested JSON path as a key-value pair that you can use to search and filter.
LangSmith will break it into the following set of searchable key value pairs:
Key| Value  
---|---  
`generations.type`| `ChatGeneration`  
`generations.message.type`| `constructor`  
`generations.message.kwargs.type`| `ai`  
`generations.message.kwargs.id`| `run-ca7f7531-f4de-4790-9c3e-960be7f8b109`  
`generations.message.kwargs.tool_calls.name`| `Plan`  
`generations.message.kwargs.tool_calls.args.steps`| `Research LangGraph's node configuration capabilities`  
`generations.message.kwargs.tool_calls.args.steps`| `Investigate how to add a Python code execution node`  
`generations.message.kwargs.tool_calls.args.steps`| `Find an example or create a sample implementation of a code execution node`  
`generations.message.kwargs.tool_calls.id`| `toolu_01XexPzAVknT3gRmUB5PK5BP`  
`generations.message.kwargs.tool_calls.type`| `tool_call`  
`type`| `LLMResult`  
To search for specific tool call, you can use the following Output KV search while removing the root runs filter:
`generations.message.kwargs.tool_calls.name` = `Plan`
This will match root and non-root runs where the `tool_calls` name is `Plan`.
![Filtering](https://docs.smith.langchain.com/assets/images/search_kv_tool-c0cb6f7ae244ce7ffe3fc5b7c50edd2e.png)
If you would like to filter for all runs _whose tree contains_ the tool filter call, you can use the tree filter in the advanced filters setting:
![Filtering](https://docs.smith.langchain.com/assets/images/search_kv_tool_tree-9f338a7687dd2d061540edda6b44ac38.png)
## Negative filtering on key-value pairs[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#negative-filtering-on-key-value-pairs "Direct link to Negative filtering on key-value pairs")
Different types of negative filtering can be applied to `Metadata`, `Input KV`, and `Output KV` (KV is short for Key-Value) fields to exclude specific runs from your results.
For example, to find all runs where the metadata key `phone` is not equal to `1234567890`, set the `Metadata`'s `Key` operator to `is` and `Key` field to `phone`, then set the `Value` operator to `is not` and the `Value` field to `1234567890`. This will match all runs that have a metadata key `phone` with any value except `1234567890`.
![Filtering](https://docs.smith.langchain.com/assets/images/negative_filtering_1-6a189d24ffa7aecba2a2ac419f8b02b2.png)
To find runs that don't have a specific metadata key, set the `Key` operator to `is not`. For example, setting the `Key` operator to `is not` with `phone` as the key will match all runs that don't have a `phone` field in their metadata.
![Filtering](https://docs.smith.langchain.com/assets/images/negative_filtering_2-b885ae0c5e5b69a4b080648e37290076.png)
You can also filter for runs that neither have a specific key nor a specific value. To find runs where the metadata has neither the key `phone` nor any field with the value `1234567890`, set the `Key` operator to `is not` with key `phone`, and the `Value` operator to `is not` with value `1234567890`.
![Filtering](https://docs.smith.langchain.com/assets/images/negative_filtering_3-8324a71669b09b6f8784941668a90f8b.png)
Finally, you can also filter for runs that do not have a specific key but have a specific value. To find runs where there is no `phone` key but there is a value of `1234567890` for some other key, set the `Key` operator to `is not` with key `phone`, and the `Value` operator to `is` with value `1234567890`.
![Filtering](https://docs.smith.langchain.com/assets/images/negative_filtering_4-3e70ecb81a5f7c4fc7bf4874363496d3.png)
Note that you can use `does not match` operator instead of `is not` to perform a substring match.
## Saved filters[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#saved-filters "Direct link to Saved filters")
You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.
### Save a filter[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#save-a-filter "Direct link to Save a filter")
In the filter box, click the `Save` button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.
![Filtering](https://docs.smith.langchain.com/assets/images/save_a_filter-c66614e8de31641fd6873535bffbddd0.png)
### Use a saved filter[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#use-a-saved-filter "Direct link to Use a saved filter")
After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar.
![Filtering](https://docs.smith.langchain.com/assets/images/selecting_a_filter-d4a82fd693929a5034ba20449166c21a.png)
### Update a saved filter[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#update-a-saved-filter "Direct link to Update a saved filter")
With the filter selected, make any changes to filter parameters. Then click `Save ‚ñº` ‚Üí `Save` to update the filter.
In the same menu, you can also create a new saved filter by clicking `Save ‚ñº` ‚Üí `Save as`.
### Delete a saved filter[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#delete-a-saved-filter "Direct link to Delete a saved filter")
With the filter selected, click on the trash button to delete the saved filter.
## Copy the filter[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#copy-the-filter "Direct link to Copy the filter")
Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK.
In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.
This will give you a string in our query language, like `and(eq(is_root, true), and(eq(feedback_key, "user_score"), eq(feedback_score, 1)))` Please see [this reference](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-query-language) for more information on the query language.
![Copy Filter](https://docs.smith.langchain.com/assets/images/copy_filter-e0f7df45276800987086f0cbebca6567.png)
## Filtering runs within the trace view[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#filtering-runs-within-the-trace-view "Direct link to Filtering runs within the trace view")
You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here.
By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from "Filtered Only" to "Show All" or "Most relevant".
![Filtering within trace view](https://docs.smith.langchain.com/assets/images/filter_runs_in_trace_view-bb45e289f9fe250c9d1c04db2fbaa1e7.png)
## Manually specify a raw query in LangSmith query language[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language "Direct link to Manually specify a raw query in LangSmith query language")
If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI.
In order to do this, you can click on `Advanced filters` on the bottom. From there you can paste a raw query into the appropriate box.
Note that this will add that query to the existing queries, not overwrite it.
![Raw Query](https://docs.smith.langchain.com/assets/images/raw_query-fac5805f338dc0b244b2d2594645c529.png)
## Use an AI Query to auto-generate a query[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query "Direct link to Use an AI Query to auto-generate a query")
Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a `AI Query` functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query.
For example: "All runs longer than 10 seconds"
Experimental feature
Note that this is an experimental feature and may not work for all queries.
![AI Query](https://docs.smith.langchain.com/assets/images/ai_query-2a31fb85b55c80c93602fbadc9f92cc2.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/filter_traces_in_application%3E).
[PreviousAnnotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)[NextUpload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
  * [Create a filter](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#create-a-filter)
  * [Filter for intermediate runs (spans)](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#filter-for-intermediate-runs-spans)
  * [Advanced: filter for intermediate runs (spans) on properties of the root](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root)
  * [Advanced: filter for runs (spans) whose child runs have some attribute](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute)
  * [Filter based on inputs and outputs](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#filter-based-on-inputs-and-outputs)
  * [Filter based on input / output key-value pairs](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#filter-based-on-input--output-key-value-pairs)
  * [Example of filtering using output key-value pairs: filter tool calls](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#example-of-filtering-using-output-key-value-pairs-filter-tool-calls)
  * [Negative filtering on key-value pairs](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#negative-filtering-on-key-value-pairs)
  * [Saved filters](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#saved-filters)
    * [Save a filter](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#save-a-filter)
    * [Use a saved filter](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#use-a-saved-filter)
    * [Update a saved filter](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#update-a-saved-filter)
    * [Delete a saved filter](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#delete-a-saved-filter)
  * [Copy the filter](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#copy-the-filter)
  * [Filtering runs within the trace view](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#filtering-runs-within-the-trace-view)
  * [Manually specify a raw query in LangSmith query language](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language)
  * [Use an AI Query to auto-generate a query](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Log custom LLM traces


On this page
# Log custom LLM traces
note
Nothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs.
The best way to logs traces from OpenAI models is to use the [wrapper](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#wrap-the-openai-client) available in the `langsmith` SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below.
LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation. In order to make the most of this feature, you must log your LLM traces in a specific format.
note
The examples below uses the `traceable` decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the [RunTree](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#use-the-runtree-api) or [API](https://api.smith.langchain.com/redoc) directly.
## Chat-style models[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace#chat-style-models "Direct link to Chat-style models")
For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key `role` and `content`.
The output is accepted in any of the following formats:
  * A dictionary/object that contains the key `choices` with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key `message`, which maps to a message object with the keys `role` and `content`.
  * A dictionary/object that contains the key `message` with a value that is a message object with the keys `role` and `content`.
  * A tuple/array of two elements, where the first element is the role and the second element is the content.
  * A dictionary/object that contains the key `role` and `content`.


The input to your function should be named `messages`.
You can also provide the following `metadata` fields to help LangSmith identify the model and calculate costs. If using LangChain or [OpenAI wrapper](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#wrap-the-openai-client), these fields will be automatically populated correctly. To learn more about how to use the `metadata` fields, see [this guide](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags).
  * `ls_provider`: The provider of the model, eg "openai", "anthropic", etc.
  * `ls_model_name`: The name of the model, eg "gpt-4o-mini", "claude-3-opus-20240307", etc.


  * Python
  * TypeScript


```
from langsmith import traceableinputs =[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"I'd like to book a table for two."},]output ={"choices":[{"message":{"role":"assistant","content":"Sure, what time would you like to book the table for?"}}]}# Can also use one of:# output = {#   "message": {#     "role": "assistant",#     "content": "Sure, what time would you like to book the table for?"#   }# }## output = {#   "role": "assistant",#   "content": "Sure, what time would you like to book the table for?"# }## output = ["assistant", "Sure, what time would you like to book the table for?"]@traceable( run_type="llm", metadata={"ls_provider":"my_provider","ls_model_name":"my_model"})defchat_model(messages:list):return outputchat_model(inputs)
```

```
import{ traceable }from"langsmith/traceable";const messages =[{ role:"system", content:"You are a helpful assistant."},{ role:"user", content:"I'd like to book a table for two."}];const output ={choices:[{  message:{   role:"assistant",   content:"Sure, what time would you like to book the table for?"}}]};// Can also use one of:// const output = {//  message: {//   role: "assistant",//   content: "Sure, what time would you like to book the table for?"//  }// };//// const output = {//  role: "assistant",//  content: "Sure, what time would you like to book the table for?"// };//// const output = ["assistant", "Sure, what time would you like to book the table for?"];const chatModel =traceable(async({ messages }:{ messages:{ role:string; content:string}[]})=>{return output;},{ run_type:"llm", name:"chat_model", metadata:{ ls_provider:"my_provider", ls_model_name:"my_model"}});awaitchatModel({ messages });
```

The above code will log the following trace:
![](https://docs.smith.langchain.com/assets/images/chat_model-e77d56df5dcd75403b5f6d4f76d23f5b.png)
## Stream outputs[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace#stream-outputs "Direct link to Stream outputs")
For streaming, you can "reduce" the outputs into the same format as the non-streaming version. This is currently only supported in Python.
```
def_reduce_chunks(chunks:list):  all_text ="".join([chunk["choices"][0]["message"]["content"]for chunk in chunks])return{"choices":[{"message":{"content": all_text,"role":"assistant"}}]}@traceable(  run_type="llm",  reduce_fn=_reduce_chunks,  metadata={"ls_provider":"my_provider","ls_model_name":"my_model"})defmy_streaming_chat_model(messages:list):for chunk in["Hello, "+ messages[1]["content"]]:yield{"choices":[{"message":{"content": chunk,"role":"assistant",}}]}list(  my_streaming_chat_model([{"role":"system","content":"You are a helpful assistant. Please greet the user."},{"role":"user","content":"polly the parrot"},],))
```

## Manually provide token counts[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace#manually-provide-token-counts "Direct link to Manually provide token counts")
Token-based cost tracking
To learn how to set up token-based cost tracking based on the token usage information, see [this guide](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs).
By default, LangSmith uses [TikToken](https://github.com/openai/tiktoken) to count tokens, utilizing a best guess at the model's tokenizer based on the `ls_model_name` provided. Many models already include token counts as part of the response. You can send these token counts to LangSmith by providing the `usage_metadata` field in the response. If token information is passed to LangSmith, the system will use this information _instead of_ using TikToken.
You can add a `usage_metadata` key to the function's response, containing a dictionary with the keys `input_tokens`, `output_tokens` and `total_tokens`. If using LangChain or [OpenAI wrapper](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#wrap-the-openai-client), these fields will be automatically populated correctly.
note
If `ls_model_name` is not present in `extra.metadata`, other fields might be used from the `extra.invocation_metadata` for estimating token counts. The following fields are used in the order of precedence:
  1. `metadata.ls_model_name`
  2. `invocation_params.model`
  3. `invocation_params.model_name`


  * Python
  * TypeScript


```
from langsmith import traceableinputs =[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"I'd like to book a table for two."},]output ={"choices":[{"message":{"role":"assistant","content":"Sure, what time would you like to book the table for?"}}],"usage_metadata":{"input_tokens":27,"output_tokens":13,"total_tokens":40,},}@traceable( run_type="llm", metadata={"ls_provider":"my_provider","ls_model_name":"my_model"})defchat_model(messages:list):return outputchat_model(inputs)
```

```
import{ traceable }from"langsmith/traceable";const messages =[{ role:"system", content:"You are a helpful assistant."},{ role:"user", content:"I'd like to book a table for two."},];const output ={choices:[{  message:{   role:"assistant",   content:"Sure, what time would you like to book the table for?",},},],usage_metadata:{ input_tokens:27, output_tokens:13, total_tokens:40,},};const chatModel =traceable(async({ messages,}:{ messages:{ role:string; content:string}[]; model:string;})=>{return output;},{ run_type:"llm", name:"chat_model", metadata:{ ls_provider:"my_provider", ls_model_name:"my_model"}});awaitchatModel({ messages });
```

## Instruct-style models[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace#instruct-style-models "Direct link to Instruct-style models")
For instruct-style models (string in, string out), your inputs must contain a key `prompt` with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key `choices` with a list of dictionaries/objects. Each must contain the key `text` with a string value. The same rules for `metadata` and `usage_metadata` apply as for chat-style models.
  * Python
  * TypeScript


```
@traceable( run_type="llm", metadata={"ls_provider":"my_provider","ls_model_name":"my_model"})defhello_llm(prompt:str):return{"choices":[{"text":"Hello, "+ prompt}],"usage_metadata":{"input_tokens":4,"output_tokens":5,"total_tokens":9,},}hello_llm("polly the parrot\n")
```

```
import{ traceable }from"langsmith/traceable";const helloLLM =traceable(({ prompt }:{ prompt:string})=>{return{  choices:[{ text:"Hello, "+ prompt }],   usage_metadata:{     input_tokens:4,     output_tokens:5,     total_tokens:9,},};},{ run_type:"llm", name:"hello_llm", metadata:{ ls_provider:"my_provider", ls_model_name:"my_model"}});awaithelloLLM({ prompt:"polly the parrot\n"});
```

The above code will log the following trace:
![](https://docs.smith.langchain.com/assets/images/hello_llm-d68121a499264f9769e79a303c02611f.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousLog retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)[NextPrevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
  * [Chat-style models](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace#chat-style-models)
  * [Stream outputs](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace#stream-outputs)
  * [Manually provide token counts](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace#manually-provide-token-counts)
  * [Instruct-style models](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace#instruct-style-models)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Log multimodal traces


# Log multimodal traces
LangSmith supports logging and rendering images as part of traces. This is currently supported for multimodal LLM runs.
In order to log images, use `wrap_openai`/ `wrapOpenAI` in Python or TypeScript respectively and pass an image URL or base64 encoded image as part of the input.
  * Python
  * TypeScript


```
from openai import OpenAIfrom langsmith.wrappers import wrap_openaiclient = wrap_openai(OpenAI())response = client.chat.completions.create(model="gpt-4-turbo",messages=[{"role":"user","content":[{"type":"text","text":"What‚Äôs in this image?"},{"type":"image_url","image_url":{"url":"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",},},],}],)print(response.choices[0])
```

```
import OpenAI from"openai";import{ wrapOpenAI }from"langsmith/wrappers";// Wrap the OpenAI client to automatically log tracesconst wrappedClient =wrapOpenAI(newOpenAI());const response =await wrappedClient.chat.completions.create({ model:"gpt-4-turbo", messages:[{   role:"user",   content:[{ type:"text", text:"What‚Äôs in this image?"},{     type:"image_url",     image_url:{      url:"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",},},],},],});console.log(response.choices[0]);
```

The image will be rendered as part of the trace in the LangSmith UI.
![](https://docs.smith.langchain.com/assets/images/multimodal-e77e726bc11754954d00c417a2df4276.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousAccess the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)[NextLog retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Log retriever traces


# Log retriever traces
note
Nothing will break if you don't log retriever traces in the correct format and data will still be logged. However, the data will not be rendered in a way that is specific to retriever steps.
Many LLM applications require looking up documents from vector databases, knowledge graphs, or other types of indexes. Retriever traces are a way to log the documents that are retrieved by the retriever. LangSmith provides special rendering for retrieval steps in traces to make it easier to understand and diagnose retrieval issues. In order for retrieval steps to be rendered correctly, a few small steps need to be taken.
  1. Annotate the retriever step with `run_type="retriever"`.
  2. Return a list of Python dictionaries or TypeScript objects from the retriever step. Each dictionary should contain the following keys: 
     * `page_content`: The text of the document.
     * `type`: This should always be "Document".
     * `metadata`: A python dictionary or TypeScript object containing metadata about the document. This metadata will be displayed in the trace.


The following code snippets show how to log a retrieval steps in Python and TypeScript.
  * Python
  * TypeScript


```
from langsmith import traceabledef_convert_docs(results):return[{"page_content": r,"type":"Document","metadata":{"foo":"bar"}}for r in results]@traceable(run_type="retriever")defretrieve_docs(query):# Foo retriever returning hardcoded dummy documents.# In production, this could be a real vector datatabase or other document index. contents =["Document contents 1","Document contents 2","Document contents 3"]return _convert_docs(contents)retrieve_docs("User query")
```

```
import{ traceable }from"langsmith/traceable";interfaceDocument{ page_content:string; type:string; metadata:{ foo:string};}functionconvertDocs(results:string[]): Document[]{return results.map((r)=>({  page_content: r,  type:"Document",  metadata:{ foo:"bar"},}));}const retrieveDocs =traceable((query:string): Document[]=>{// Foo retriever returning hardcoded dummy documents.// In production, this could be a real vector database or other document index.const contents =["Document contents 1","Document contents 2","Document contents 3",];returnconvertDocs(contents);},{ name:"retrieveDocs", run_type:"retriever"}// Configuration for traceable);awaitretrieveDocs("User query");
```

The following image shows how a retriever step is rendered in a trace. The contents along with the metadata are displayed with each document.
![](https://docs.smith.langchain.com/assets/images/retriever_trace-9ded87adb076749f7e76fbbe9a81fba5.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/log_retriever_trace%3E).
[PreviousLog multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)[NextLog custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Log traces to specific project


On this page
# Log traces to specific project
You can change the destination project of your traces both statically through environment variables and dynamically at runtime.
## Set the destination project statically[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project#set-the-destination-project-statically "Direct link to Set the destination project statically")
As mentioned in the [Tracing Concepts](https://docs.smith.langchain.com/observability/concepts#projects) section, LangSmith uses the concept of a `Project` to group traces. If left unspecified, the project is set to `default`. You can set the `LANGSMITH_PROJECT` environment variable to configure a custom project name for an entire application run. This should be done before executing your application.
```
export LANGSMITH_PROJECT=my-custom-project
```

SDK compatibility in JS
The `LANGSMITH_PROJECT` flag is only supported in JS SDK versions >= 0.2.16, use `LANGCHAIN_PROJECT` instead if you are using an older version.
If the project specified does not exist, it will be created automatically when the first trace is ingested.
## Set the destination project dynamically[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project#set-the-destination-project-dynamically "Direct link to Set the destination project dynamically")
You can also set the project name at program runtime in various ways, depending on how you are [annotating your code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code). This is useful when you want to log traces to different projects within the same application.
note
Setting the project name dynamically using one of the below methods overrides the project name set by the `LANGSMITH_PROJECT` environment variable.
  * Python
  * TypeScript


```
import openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages =[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Hello!"}]# Use the @traceable decorator with the 'project_name' parameter to log traces to LangSmith# Ensure that the LANGSMITH_TRACING environment variables is set for @traceable to work@traceable( run_type="llm", name="OpenAI Call Decorator", project_name="My Project")defcall_openai( messages:list[dict], model:str="gpt-4o-mini")->str:return client.chat.completions.create(   model=model,   messages=messages,).choices[0].message.content# Call the decorated functioncall_openai(messages)# You can also specify the Project via the project_name parameter# This will override the project_name specified in the @traceable decoratorcall_openai( messages, langsmith_extra={"project_name":"My Overridden Project"},)# The wrapped OpenAI client accepts all the same langsmith_extra parameters# as @traceable decorated functions, and logs traces to LangSmith automatically.# Ensure that the LANGSMITH_TRACING environment variables is set for the wrapper to work.from langsmith import wrapperswrapped_client = wrappers.wrap_openai(client)wrapped_client.chat.completions.create( model="gpt-4o-mini", messages=messages, langsmith_extra={"project_name":"My Project"},)# Alternatively, create a RunTree object# You can set the project name using the project_name parameterrt = RunTree( run_type="llm", name="OpenAI Call RunTree", inputs={"messages": messages}, project_name="My Project")chat_completion = client.chat.completions.create( model="gpt-4o-mini", messages=messages,)# End and submit the runrt.end(outputs=chat_completion)rt.post()
```

```
import OpenAI from"openai";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers";import{ RunTree}from"langsmith";const client =newOpenAI();const messages =[{role:"system", content:"You are a helpful assistant."},{role:"user", content:"Hello!"}];const traceableCallOpenAI =traceable(async(messages:{role:string, content:string}[], model:string)=>{const completion =await client.chat.completions.create({   model: model,   messages: messages,});return completion.choices[0].message.content;},{ run_type:"llm", name:"OpenAI Call Traceable", project_name:"My Project"});// Call the traceable functionawaittraceableCallOpenAI(messages,"gpt-4o-mini");// Create and use a RunTree objectconst rt =newRunTree({ runType:"llm", name:"OpenAI Call RunTree", inputs:{ messages }, project_name:"My Project"});await rt.postRun();// Execute a chat completion and handle it within RunTreert.end({outputs: chatCompletion});await rt.patchRun();
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousDashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)[NextSet up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
  * [Set the destination project statically](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project#set-the-destination-project-statically)
  * [Set the destination project dynamically](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project#set-the-destination-project-dynamically)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Prevent logging of sensitive data in traces


On this page
# Prevent logging of sensitive data in traces
In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.
If you want to completely hide the inputs and outputs of your traces, you can set the following environment variables when running your application:
```
LANGSMITH_HIDE_INPUTS=trueLANGSMITH_HIDE_OUTPUTS=true
```

This works for both the LangSmith SDK (Python and TypeScript) and LangChain.
You can also customize and override this behavior for a given `Client` instance. This can be done by setting the `hide_inputs` and `hide_outputs` parameters on the `Client` object (`hideInputs` and `hideOutputs` in TypeScript).
For the example below, we will simply return an empty object for both `hide_inputs` and `hide_outputs`, but you can customize this to your needs.
  * Python
  * TypeScript


```
import openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(openai.Client())langsmith_client = Client( hide_inputs=lambda inputs:{}, hide_outputs=lambda outputs:{})# The trace produced will have its metadata present, but the inputs will be hiddenopenai_client.chat.completions.create( model="gpt-4o-mini", messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Hello!"},], langsmith_extra={"client": langsmith_client},)# The trace produced will not have hidden inputs and outputsopenai_client.chat.completions.create( model="gpt-4o-mini", messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Hello!"},],)
```

```
import OpenAI from"openai";import{ Client }from"langsmith";import{ wrapOpenAI }from"langsmith/wrappers";const langsmithClient =newClient({hideInputs:(inputs)=>({}),hideOutputs:(outputs)=>({}),});// The trace produced will have its metadata present, but the inputs will be hiddenconst filteredOAIClient =wrapOpenAI(newOpenAI(),{ client: langsmithClient,});await filteredOAIClient.chat.completions.create({ model:"gpt-4o-mini", messages:[{ role:"system", content:"You are a helpful assistant."},{ role:"user", content:"Hello!"},],});const openaiClient =wrapOpenAI(newOpenAI());// The trace produced will not have hidden inputs and outputsawait openaiClient.chat.completions.create({ model:"gpt-4o-mini", messages:[{ role:"system", content:"You are a helpful assistant."},{ role:"user", content:"Hello!"},],});
```

## Rule-based masking of inputs and outputs[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#rule-based-masking-of-inputs-and-outputs "Direct link to Rule-based masking of inputs and outputs")
info
This feature is available in the following LangSmith SDK versions:
  * Python: 0.1.81 and above
  * TypeScript: 0.1.33 and above


To mask specific data in inputs and outputs, you can use the `create_anonymizer` / `createAnonymizer` function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value.
The anonymizer will be skipped for inputs if `LANGSMITH_HIDE_INPUTS = true`. Same applies for outputs if `LANGSMITH_HIDE_OUTPUTS = true`.
However, if inputs or outputs are to be sent to client, the `anonymizer` method will take precedence over functions found in `hide_inputs` and `hide_outputs`. By default, the `create_anonymizer` will only look at maximum of 10 nesting levels deep, which can be configured via the `max_depth` parameter.
  * Python
  * TypeScript


```
from langsmith.anonymizer import create_anonymizerfrom langsmith import Client, traceableimport re# create anonymizer from list of regex patterns and replacement valuesanonymizer = create_anonymizer([{"pattern":r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}","replace":"<email-address>"},{"pattern":r"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}","replace":"<UUID>"}])# or create anonymizer from a functionemail_pattern = re.compile(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}")uuid_pattern = re.compile(r"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}")anonymizer = create_anonymizer(lambda text: email_pattern.sub("<email-address>", uuid_pattern.sub("<UUID>", text)))client = Client(anonymizer=anonymizer)@traceable(client=client)defmain(inputs:dict)->dict:...
```

```
import{ createAnonymizer }from"langsmith/anonymizer"import{ traceable }from"langsmith/traceable"import{ Client }from"langsmith"// create anonymizer from list of regex patterns and replacement valuesconst anonymizer =createAnonymizer([{ pattern:/[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}/g, replace:"<email>"},{ pattern:/[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}/g, replace:"<uuid>"}])// or create anonymizer from a functionconst anonymizer =createAnonymizer((value)=> value.replace("...","<value>"))const client =newClient({ anonymizer })const main =traceable(async(inputs:any)=>{// ...},{ client })
```

Please note, that using the anonymizer might incur a performance hit with complex regular expressions or large payloads, as the anonymizer serializes the payload to JSON before processing.
note
Improving the performance of `anonymizer` API is on our roadmap! If you are encountering performance issues, please contact us at support@langchain.dev.
![](https://docs.smith.langchain.com/assets/images/hide_inputs_outputs-7c11c42e051ad4651922ac6d5161d3b1.png)
Older versions of LangSmith SDKs can use the `hide_inputs` and `hide_outputs` parameters to achieve the same effect. You can also use these parameters to process the inputs and outputs more efficiently as well.
  * Python
  * TypeScript


```
import refrom langsmith import Client, traceable# Define the regex patterns for email addresses and UUIDsEMAIL_REGEX =r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}"UUID_REGEX =r"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}"defreplace_sensitive_data(data, depth=10):if depth ==0:return dataifisinstance(data,dict):return{k: replace_sensitive_data(v, depth-1)for k, v in data.items()}elifisinstance(data,list):return[replace_sensitive_data(item, depth-1)for item in data]elifisinstance(data,str):   data = re.sub(EMAIL_REGEX,"<email-address>", data)   data = re.sub(UUID_REGEX,"<UUID>", data)return dataelse:return dataclient = Client( hide_inputs=lambda inputs: replace_sensitive_data(inputs), hide_outputs=lambda outputs: replace_sensitive_data(outputs))inputs ={"role":"user","content":"Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000."}outputs ={"role":"assistant","content":"Hi! I've noted your email as user@example.com and your ID as 123e4567-e89b-12d3-a456-426614174000."}@traceable(client=client)defchild(inputs:dict)->dict:return outputs@traceable(client=client)defparent(inputs:dict)->dict: child_outputs = child(inputs)return child_outputsparent(inputs)
```

```
import{ Client }from"langsmith";import{ traceable }from"langsmith/traceable";// Define the regex patterns for email addresses and UUIDsconstEMAIL_REGEX=/[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}/g;constUUID_REGEX=/[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}/g;functionreplaceSensitiveData(data:any, depth:number=10):any{if(depth ===0)return data;if(typeof data ==="object"&&!Array.isArray(data)){const result: Record<string,any>={};for(const[key, value]of Object.entries(data)){     result[key]=replaceSensitiveData(value, depth -1);}return result;}elseif(Array.isArray(data)){return data.map(item =>replaceSensitiveData(item, depth -1));}elseif(typeof data ==="string"){return data.replace(EMAIL_REGEX,"<email-address>").replace(UUID_REGEX,"<UUID>");}else{return data;}}const langsmithClient =newClient({hideInputs:(inputs)=>replaceSensitiveData(inputs),hideOutputs:(outputs)=>replaceSensitiveData(outputs)});const inputs ={ role:"user", content:"Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000."};const outputs ={ role:"assistant", content:"Hi! I've noted your email as <email-address> and your ID as <UUID>."};const child =traceable(async(inputs:any)=>{return outputs;},{ name:"child", client: langsmithClient });const parent =traceable(async(inputs:any)=>{const childOutputs =awaitchild(inputs);return childOutputs;},{ name:"parent", client: langsmithClient });awaitparent(inputs)
```

## Processing Inputs & Outputs for a Single Function[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#processing-inputs--outputs-for-a-single-function "Direct link to Processing Inputs & Outputs for a Single Function")
info
The `process_outputs` parameter is available in LangSmith SDK version 0.1.98 and above for Python.
In addition to client-level input and output processing, LangSmith provides function-level processing through the `process_inputs` and `process_outputs` parameters of the `@traceable` decorator.
These parameters accept functions that allow you to transform the inputs and outputs of a specific function before they are logged to LangSmith. This is useful for reducing payload size, removing sensitive information, or customizing how an object should be serialized and represented in LangSmith for a particular function.
Here's an example of how to use `process_inputs` and `process_outputs`:
```
from langsmith import traceabledefprocess_inputs(inputs:dict)->dict:# inputs is a dictionary where keys are argument names and values are the provided arguments# Return a new dictionary with processed inputsreturn{"processed_key": inputs.get("my_cool_key","default"),"length":len(inputs.get("my_cool_key",""))}defprocess_outputs(output: Any)->dict:# output is the direct return value of the function# Transform the output into a dictionary# In this case, "output" will be an integerreturn{"processed_output":str(output)}@traceable(process_inputs=process_inputs, process_outputs=process_outputs)defmy_function(my_cool_key:str)->int:# Function implementationreturnlen(my_cool_key)result = my_function("example")
```

In this example, `process_inputs` creates a new dictionary with processed input data, and `process_outputs` transforms the output into a specific format before logging to LangSmith.
caution
It's recommended to avoid mutating the source objects in the processor functions. Instead, create and return new objects with the processed data.
For asynchronous functions, the usage is similar:
```
@traceable(process_inputs=process_inputs, process_outputs=process_outputs)asyncdefasync_function(key:str)->int:# Async implementationreturnlen(key)
```

These function-level processors take precedence over client-level processors (`hide_inputs` and `hide_outputs`) when both are defined.
## Quick starts[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#quick-starts "Direct link to Quick starts")
You can combine rule-based masking with various anonymizers to scrub sensitive information from inputs and outputs. In this how-to-guide, we'll cover working with regex, Microsoft Presidio, and Amazon Comprehend.
### Regex[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#regex "Direct link to Regex")
info
The implementation below is not exhaustive and may miss some formats or edge cases. Test any implementation thoroughly before using it in production.
You can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.
```
import reimport openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openai# Define regex patterns for various PIISSN_PATTERN = re.compile(r'\b\d{3}-\d{2}-\d{4}\b')CREDIT_CARD_PATTERN = re.compile(r'\b(?:\d[ -]*?){13,16}\b')EMAIL_PATTERN = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,7}\b')PHONE_PATTERN = re.compile(r'\b(?:\+?1[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}\b')FULL_NAME_PATTERN = re.compile(r'\b([A-Z][a-z]*\s[A-Z][a-z]*)\b')defregex_anonymize(text):"""  Anonymize sensitive information in the text using regex patterns.  Args:    text (str): The input text to be anonymized.  Returns:    str: The anonymized text.  """# Replace sensitive information with placeholders  text = SSN_PATTERN.sub('[REDACTED SSN]', text)  text = CREDIT_CARD_PATTERN.sub('[REDACTED CREDIT CARD]', text)  text = EMAIL_PATTERN.sub('[REDACTED EMAIL]', text)  text = PHONE_PATTERN.sub('[REDACTED PHONE]', text)  text = FULL_NAME_PATTERN.sub('[REDACTED NAME]', text)return textdefrecursive_anonymize(data, depth=10):"""  Recursively traverse the data structure and anonymize sensitive information.  Args:    data (any): The input data to be anonymized.    depth (int): The current recursion depth to prevent excessive recursion.  Returns:    any: The anonymized data.  """if depth ==0:return dataifisinstance(data,dict):    anonymized_dict ={}for k, v in data.items():      anonymized_value = recursive_anonymize(v, depth -1)      anonymized_dict[k]= anonymized_valuereturn anonymized_dictelifisinstance(data,list):    anonymized_list =[]for item in data:      anonymized_item = recursive_anonymize(item, depth -1)      anonymized_list.append(anonymized_item)return anonymized_listelifisinstance(data,str):    anonymized_data = regex_anonymize(data)return anonymized_dataelse:return dataopenai_client = wrap_openai(openai.Client())# Initialize the LangSmith client with the anonymization functionslangsmith_client = Client(  hide_inputs=recursive_anonymize, hide_outputs=recursive_anonymize)# The trace produced will have its metadata present, but the inputs and outputs will be anonymizedresponse_with_anonymization = openai_client.chat.completions.create(  model="gpt-4o-mini",  messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890."},],  langsmith_extra={"client": langsmith_client},)# The trace produced will not have anonymized inputs and outputsresponse_without_anonymization = openai_client.chat.completions.create(  model="gpt-4o-mini",  messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890."},],)
```

The anonymized run will look like this in LangSmith: ![Anonymized run](https://docs.smith.langchain.com/assets/images/regex-anonymized-4d9505ed05b6da669cbea22949dbd3f1.png)
The non-anonymized run will look like this in LangSmith: ![Non-anonymized run](https://docs.smith.langchain.com/assets/images/regex-not-anonymized-8113cdfad9e62997b5d9174e551c9f51.png)
### Microsoft Presidio[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#microsoft-presidio "Direct link to Microsoft Presidio")
info
The implementation below provides a general example of how to anonymize sensitive information in messages exchanged between a user and an LLM. It is not exhaustive and does not account for all cases. Test any implementation thoroughly before using it in production.
Microsoft Presidio is a data protection and de-identification SDK. The implementation below uses Presidio to anonymize inputs and outputs before they are sent to LangSmith. For up to date information, please refer to Presidio's [official documentation](https://microsoft.github.io/presidio/).
To use Presidio and its spaCy model, install the following:
```
pip install presidio-analyzerpip install presidio-anonymizerpython -m spacy download en_core_web_lg
```

Also, install OpenAI:
```
pip install openai
```

```
import openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openaifrom presidio_anonymizer import AnonymizerEnginefrom presidio_analyzer import AnalyzerEngineanonymizer = AnonymizerEngine()analyzer = AnalyzerEngine()defpresidio_anonymize(data):"""  Anonymize sensitive information sent by the user or returned by the model.  Args:    data (any): The data to be anonymized.  Returns:    any: The anonymized data.  """  message_list =(    data.get('messages')or[data.get('choices',[{}])[0].get('message')])ifnot message_list ornotall(isinstance(msg,dict)and msg for msg in message_list):return datafor message in message_list:    content = message.get('content','')ifnot content.strip():print("Empty content detected. Skipping anonymization.")continue    results = analyzer.analyze(      text=content,      entities=["PERSON","PHONE_NUMBER","EMAIL_ADDRESS","US_SSN"],      language='en')    anonymized_result = anonymizer.anonymize(      text=content,      analyzer_results=results)    message['content']= anonymized_result.textreturn dataopenai_client = wrap_openai(openai.Client())# initialize the langsmith client with the anonymization functionslangsmith_client = Client( hide_inputs=presidio_anonymize, hide_outputs=presidio_anonymize)# The trace produced will have its metadata present, but the inputs and outputs will be anonymizedresponse_with_anonymization = openai_client.chat.completions.create( model="gpt-4o-mini", messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com"},], langsmith_extra={"client": langsmith_client},)# The trace produced will not have anonymized inputs and outputsresponse_without_anonymization = openai_client.chat.completions.create( model="gpt-4o-mini", messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com"},],)
```

The anonymized run will look like this in LangSmith: ![Anonymized run](https://docs.smith.langchain.com/assets/images/presidio-anonymized-af30a21fac5c6a110bd4d38c70dc2b0c.png)
The non-anonymized run will look like this in LangSmith: ![Non-anonymized run](https://docs.smith.langchain.com/assets/images/presidio-not-anonymized-6ad41cb9351930628c982cecd397a225.png)
### Amazon Comprehend[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#amazon-comprehend "Direct link to Amazon Comprehend")
info
The implementation below provides a general example of how to anonymize sensitive information in messages exchanged between a user and an LLM. It is not exhaustive and does not account for all cases. Test any implementation thoroughly before using it in production.
Comprehend is a natural language processing service that can detect personally identifiable information. The implementation below uses Comprehend to anonymize inputs and outputs before they are sent to LangSmith. For up to date information, please refer to Comprehend's [official documentation](https://docs.aws.amazon.com/comprehend/latest/APIReference/API_DetectPiiEntities.html).
To use Comprehend, install [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html):
```
pip install boto3
```

Also, install OpenAI:
```
pip install openai
```

You will need to set up credentials in AWS and authenticate using the AWS CLI. Follow the instructions [here](https://docs.aws.amazon.com/comprehend/latest/dg/setting-up.html).
```
import openaiimport boto3from langsmith import Clientfrom langsmith.wrappers import wrap_openaicomprehend = boto3.client('comprehend', region_name='us-east-1')defredact_pii_entities(text, entities):"""  Redact PII entities in the text based on the detected entities.  Args:    text (str): The original text containing PII.    entities (list): A list of detected PII entities.  Returns:    str: The text with PII entities redacted.  """  sorted_entities =sorted(entities, key=lambda x: x['BeginOffset'], reverse=True)  redacted_text = textfor entity in sorted_entities:    begin = entity['BeginOffset']    end = entity['EndOffset']    entity_type = entity['Type']# Define the redaction placeholder based on entity type    placeholder =f"[{entity_type}]"# Replace the PII in the text with the placeholder    redacted_text = redacted_text[:begin]+ placeholder + redacted_text[end:]return redacted_textdefdetect_pii(text):"""  Detect PII entities in the given text using AWS Comprehend.  Args:    text (str): The text to analyze.  Returns:    list: A list of detected PII entities.  """try:    response = comprehend.detect_pii_entities(      Text=text,      LanguageCode='en',)    entities = response.get('Entities',[])return entitiesexcept Exception as e:print(f"Error detecting PII: {e}")return[]defcomprehend_anonymize(data):"""  Anonymize sensitive information sent by the user or returned by the model.  Args:    data (any): The input data to be anonymized.  Returns:    any: The anonymized data.  """  message_list =(    data.get('messages')or[data.get('choices',[{}])[0].get('message')])ifnot message_list ornotall(isinstance(msg,dict)and msg for msg in message_list):return datafor message in message_list:    content = message.get('content','')ifnot content.strip():print("Empty content detected. Skipping anonymization.")continue    entities = detect_pii(content)if entities:      anonymized_text = redact_pii_entities(content, entities)      message['content']= anonymized_textelse:print("No PII detected. Content remains unchanged.")return dataopenai_client = wrap_openai(openai.Client())# initialize the langsmith client with the anonymization functionslangsmith_client = Client( hide_inputs=comprehend_anonymize, hide_outputs=comprehend_anonymize)# The trace produced will have its metadata present, but the inputs and outputs will be anonymizedresponse_with_anonymization = openai_client.chat.completions.create( model="gpt-4o-mini", messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com"},], langsmith_extra={"client": langsmith_client},)# The trace produced will not have anonymized inputs and outputsresponse_without_anonymization = openai_client.chat.completions.create( model="gpt-4o-mini", messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com"},],)
```

The anonymized run will look like this in LangSmith: ![Anonymized run](https://docs.smith.langchain.com/assets/images/aws-comprehend-anonymized-9617142a9c15fc6bb5e18ccdcbbb3c22.png)
The non-anonymized run will look like this in LangSmith: ![Non-anonymized run](https://docs.smith.langchain.com/assets/images/aws-comprehend-not-anonymized-f5bf239b71c84ef8fd0ebe208dd21207.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousLog custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)[NextQuery traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
  * [Rule-based masking of inputs and outputs](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#rule-based-masking-of-inputs-and-outputs)
  * [Processing Inputs & Outputs for a Single Function](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#processing-inputs--outputs-for-a-single-function)
  * [Quick starts](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#quick-starts)
    * [Regex](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#regex)
    * [Microsoft Presidio](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#microsoft-presidio)
    * [Amazon Comprehend](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs#amazon-comprehend)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/nest_traces

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Troubleshoot trace nesting


On this page
# Troubleshoot trace nesting
When tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.
If you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known "edge cases".
## Python[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces#python "Direct link to Python")
The following outlines common causes for "split" traces when building with python.
### Context propagation using asyncio[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces#context-propagation-using-asyncio "Direct link to Context propagation using asyncio")
When using async calls (especially with streaming) in Python versions < 3.11, you may encounter issues with trace nesting. This is because Python's `asyncio` only [added full support for passing context](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) in version 3.11.
#### Why[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces#why "Direct link to Why")
LangChain and LangSmith SDK use [contextvars](https://docs.python.org/3/library/contextvars.html) to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), `asyncio` tasks lack proper `contextvar` support, which can lead to disconnected traces.
#### To resolve[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces#to-resolve "Direct link to To resolve")
  1. **Upgrade Python Version (Recommended)** If possible, upgrade to Python 3.11 or later for automatic context propagation.
  2. **Manual Context Propagation** If upgrading isn't an option, you'll need to manually propagate the tracing context. The method varies depending on your setup:
a) **Using LangGraph or LangChain** Pass the parent `config` to the child call:
```
import asynciofrom langchain_core.runnables import RunnableConfig, RunnableLambda@RunnableLambdaasyncdefmy_child_runnable(       inputs:str,# The config arg (present in parent_runnable below) is optional):yield"A"yield"response"@RunnableLambdaasyncdefparent_runnable(inputs:str, config: RunnableConfig):asyncfor chunk in my_child_runnable.astream(inputs, config):yield chunkasyncdefmain():return[val asyncfor val in parent_runnable.astream("call")]asyncio.run(main())
```

b) **Using LangSmith Directly** Pass the run tree directly:
```
import asyncioimport langsmith as ls@ls.traceableasyncdefmy_child_function(inputs:str):yield"A"yield"response"@ls.traceableasyncdefparent_function(       inputs:str,# The run tree can be auto-populated by the decorator       run_tree: ls.RunTree,):asyncfor chunk in my_child_function(inputs, langsmith_extra={"parent": run_tree}):yield chunkasyncdefmain():return[val asyncfor val in parent_function("call")]asyncio.run(main())
```

c) **Combining Decorated Code with LangGraph/LangChain** Use a combination of techniques for manual handoff:
```
import asyncioimport langsmith as lsfrom langchain_core.runnables import RunnableConfig, RunnableLambda@RunnableLambdaasyncdefmy_child_runnable(inputs:str):yield"A"yield"response"@ls.traceableasyncdefmy_child_function(inputs:str, run_tree: ls.RunTree):with ls.tracing_context(parent=run_tree):asyncfor chunk in my_child_runnable.astream(inputs):yield chunk@RunnableLambdaasyncdefparent_runnable(inputs:str, config: RunnableConfig):# @traceable decorated functions can directly accept a RunnableConfig when passed in via "config"asyncfor chunk in my_child_function(inputs, langsmith_extra={"config": config}):yield chunk@ls.traceableasyncdefparent_function(inputs:str, run_tree: ls.RunTree):# You can set the tracing context manuallywith ls.tracing_context(parent=run_tree):asyncfor chunk in parent_runnable.astream(inputs):yield chunkasyncdefmain():return[val asyncfor val in parent_function("call")]asyncio.run(main())
```



### Context propagation using threading[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces#context-propagation-using-threading "Direct link to Context propagation using threading")
It's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib `ThreadPoolExecutor` by default breaks tracing.
#### Why[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces#why-1 "Direct link to Why")
Python's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:
#### To resolve[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces#to-resolve-1 "Direct link to To resolve")
  1. **Using LangSmith's ContextThreadPoolExecutor**
LangSmith provides a `ContextThreadPoolExecutor` that automatically handles context propagation:
```
from langsmith.utils import ContextThreadPoolExecutorfrom langsmith import traceable@traceabledefouter_func():with ContextThreadPoolExecutor()as executor:    inputs =[1,2]    r =list(executor.map(inner_func, inputs))@traceabledefinner_func(x):print(x)outer_func()
```

  2. **Manually providing the parent run tree**
Alternatively, you can manually pass the parent run tree to the inner function:
```
from langsmith import traceable, get_current_run_treefrom concurrent.futures import ThreadPoolExecutor@traceabledefouter_func():  rt = get_current_run_tree()with ThreadPoolExecutor()as executor:    r =list(      executor.map(lambda x: inner_func(x, langsmith_extra={"parent": rt}),[1,2]))@traceabledefinner_func(x):print(x)outer_func()
```

In this approach, we use `get_current_run_tree()` to obtain the current run tree and pass it to the inner function using the `langsmith_extra` parameter.


Both methods ensure that the inner function calls are correctly aggregated under the initial trace stack, even when executed in separate threads.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousCalculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)[Next[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
  * [Python](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces#python)
    * [Context propagation using asyncio](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces#context-propagation-using-asyncio)
    * [Context propagation using threading](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces#context-propagation-using-threading)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Online Evaluation


On this page
# Set up online evaluations
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
  * Running [online evaluations](https://docs.smith.langchain.com/evaluation/concepts#online-evaluation)


Online evaluations provide real-time feedback on your production traces. This is useful to continuously monitor the performance of your application - to identify issues, measure improvements, and ensure consistent quality over time.
There are two types of online evaluations supported in LangSmith:
  * **[LLM-as-a-judge](https://docs.smith.langchain.com/evaluation/concepts#llm-as-judge)** : Use an LLM to evaluate your traces. Used as a scalable way to provide human-like judgement to your output (e.g. toxicity, hallucination, correctness, etc.).
  * **Custom Code** : Write an evaluator in Python directly in LangSmith. Often used for validating structure or statistical properties of your data.


Online evaluations are configured using [automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules).
## Get started with online evaluators[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#get-started-with-online-evaluators "Direct link to Get started with online evaluators")
#### 1. Open the [tracing project](https://docs.smith.langchain.com/observability/concepts#projects) you want to configure the online evalautor on[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#1-open-the-tracing-project-you-want-to-configure-the-online-evalautor-on "Direct link to 1-open-the-tracing-project-you-want-to-configure-the-online-evalautor-on")
#### 2. Select the Add Rules button (top right)[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#2-select-the-add-rules-button-top-right "Direct link to 2. Select the Add Rules button \(top right\)")
#### 3. Configure your rule[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#3-configure-your-rule "Direct link to 3. Configure your rule")
  * Add an evaluator name
  * Optionally filter runs that you would like to apply your evaluator on or configure a sampling rate. For example, it is commmon to apply specific evaluators based on runs that a user indicated the response was unsatisfactory, runs with a specific model, etc.
  * Select **Apply Evaluator**


### Configure a LLM-as-a-judge online evaluator[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#configure-a-llm-as-a-judge-online-evaluator "Direct link to Configure a LLM-as-a-judge online evaluator")
View this guide to configure on configuring an [LLM-as-a-judge evaluator](https://docs.smith.langchain.com/evaluation/how_to_guides/llm_as_judge).
### Configure a custom code evaluator[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#configure-a-custom-code-evaluator "Direct link to Configure a custom code evaluator")
Select **custom code** evaluator.
#### Write your evaluation function[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#write-your-evaluation-function "Direct link to Write your evaluation function")
Custom code evaluators restrictions.
**Allowed Libraries** : You can import all standard library functions, as well as the following public packages:
```
 numpy (v2.2.2): "numpy" pandas (v1.5.2): "pandas" jsonschema (v4.21.1): "jsonschema" scipy (v1.14.1): "scipy" sklearn (v1.26.4): "scikit-learn"
```

**Network Access** : You cannot access the internet from a custom code evaluator.
Custom code evaluators must be written inline. We reccomend testing locally before setting up your custom code evaluator in LangSmith.
In the UI, you will see a panel that lets you write your code inline, with some starter code:
![](https://docs.smith.langchain.com/assets/images/online-eval-custom-code-a4ebb57603069f5ff0a1e5941a150222.png)
Custom code evaluators take in one arguments:
  * A `Run` ([reference](https://docs.smith.langchain.com/reference/data_formats/run_data_format)). This represents the sampled run to evaluate.


They return a single value:
  * Feedback(s) Dictionary: A dictionary whose keys are the type of feedback you want to return, and values are the score you will give for that feedback key. For example, `{"correctness": 1, "silliness": 0}` would create two types of feedback on the run, one saying it is correct, and the other saying it is not silly.


In the below screenshot, you can see an example of a simple function that validates that each run in the experiment has a known json field:
```
import jsondef perform_eval(run): output_to_validate = run['outputs'] is_valid_json = 0 # assert you can serialize/deserialize as json try:  json.loads(json.dumps(output_to_validate)) except Exception as e:  return { "formatted": False } # assert output facts exist if "facts" not in output_to_validate:  return { "formatted": False } # assert required fields exist if "years_mentioned" not in output_to_validate["facts"]:  return { "formatted": False } return {"formatted": True}
```

#### Test and save your evaluation function[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#test-and-save-your-evaluation-function "Direct link to Test and save your evaluation function")
Before saving, you can test your evaluator function on a recent run by clicking **Test Code** to make sure that your code executes properly.
Once you **Save** , your online evaluator will run over newly sampled runs (or backfilled ones too if you chose the backfill option).
If you prefer a video tutorial, check out the [Online Evaluations video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/online_evaluations%3E).
[PreviousSet up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)[NextSet a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
  * [Get started with online evaluators](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#get-started-with-online-evaluators)
    * [Configure a LLM-as-a-judge online evaluator](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#configure-a-llm-as-a-judge-online-evaluator)
    * [Configure a custom code evaluator](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations#configure-a-custom-code-evaluator)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * How to print detailed logs (Python SDK)


On this page
# How to print detailed logs (Python SDK)
The LangSmith package uses Python's built in [`logging`](https://docs.python.org/3/library/logging.html) mechanism to output logs about its behavior to standard output.
## Ensure logging is configured[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs#ensure-logging-is-configured "Direct link to Ensure logging is configured")
note
By default, Jupyter notebooks send logs to standard error instead of standard output, which means your logs will not show up in your notebook cell output unless you configure logging as we do below.
If logging is not currently configured to send logs to standard output for your Python environment, you'll need to explicitly turn it on as follows:
  * Python


```
import logging# Note: this will affect _all_ packages that use python's built-in logging mechanism, #    so may increase your log volume. Pick the right log level for your use case.logging.basicConfig(level=logging.WARNING)
```

## Increase the logger's verbosity[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs#increase-the-loggers-verbosity "Direct link to Increase the logger's verbosity")
When debugging an issue, it's helpful to increase logs to a higher level verbosity so more info is outputted to standard output. Python loggers default to using `WARNING` log level, but you can choose different values to get different levels of verbosity. The values, from least verbose to most, are `ERROR`, `WARNING`, `INFO`, and `DEBUG`. You can set this as follows:
  * Python


```
import langsmithimport logging# Loggers are hierarchical, so setting the log level on "langsmith" will# set it on all modules inside the "langsmith" packagelangsmith_logger = logging.getLogger("langsmith")langsmith_logger.setLevel(level=logging.DEBUG)
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/output_detailed_logs%3E).
[PreviousConfiguring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)[NextTrace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
  * [Ensure logging is configured](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs#ensure-logging-is-configured)
  * [Increase the logger's verbosity](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs#increase-the-loggers-verbosity)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/rules

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/rules#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/rules)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/rules)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/rules)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Set up automation rules


On this page
# Set up automation rules
While you can manually sift through and process production logs from our LLM application, it often becomes difficult as your application scales to more users. LangSmith provides a powerful feature called automations that allow you to trigger certain actions on your trace data. At a high level, automations are defined by a **filter** , **sampling rate** , and **action**.
Automation rules can trigger actions such as online evaluation, adding inputs/outputs of traces to a dataset, adding to an annotation queue, and triggering a webhook.
An example of an automation you can set up can be _"trigger an online evaluation that grades on vagueness for all of my downvoted traces."_
## Create a rule[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/rules#create-a-rule "Direct link to Create a rule")
We will outline the steps for creating an automation rule in LangSmith below.
### Step 1: Navigate to rule creation[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/rules#step-1-navigate-to-rule-creation "Direct link to Step 1: Navigate to rule creation")
To create a rule, head click on **Rules** in the top right corner of any project details page, then scroll to the bottom and click on **+ Add Rule**.
![](https://docs.smith.langchain.com/assets/images/add_automation_rule-0c0ef9b379005d13684634332636e758.png)
_Alternatively_ , you can access rules in settings by navigating to [this link](https://smith.langchain.com/settings/workspaces/rules), click on **+ Add Rule** , then **Project Rule**.
note
There are currently two types of rules you can create: **Project Rule** and **Dataset Rule**.
  * **Project Rule** : This rule will apply to traces in the specified project. Actions allowed are adding to a dataset, adding to an annotation queue, running online evaluation, and triggering a webhook.
  * **Dataset Rule** : This rule will apply to traces that are part of an experiment in the specified dataset. Actions allowed are only running an evaluator on the experiment results. To see this in action, you can follow [this guide](https://docs.smith.langchain.com/evaluation/how_to_guides/run_evaluation_from_prompt_playground).


Give your rule a name, for example "my_rule":
![](https://docs.smith.langchain.com/assets/images/give_rule_name-b4dd6615be519422d890b7f4834ca438.png)
### Step 2: Define the filter[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/rules#step-2-define-the-filter "Direct link to Step 2: Define the filter")
You can create a filter as you normally would to filter traces in the project. For more information on filters, you can refer to [this guide](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application).
![](https://docs.smith.langchain.com/assets/images/rules_filter-3450cda01142e59c5b6ac5ea2080bcea.png)
### (Optional) Step 3: Apply Rule to Past Runs[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/rules#optional-step-3-apply-rule-to-past-runs "Direct link to \(Optional\) Step 3: Apply Rule to Past Runs")
When creating a new rule, you can apply the rule to past runs as well. To do this, select **Apply to Past Runs** checkbox and enter **Backfill From** date as the start date to apply the rule.
This will start from the **Backfill From** date and apply the run rules until it is caught up with the latest runs.
![](https://docs.smith.langchain.com/assets/images/rules_past_runs-304ff5e82bcc8f8825393aa73aaba491.png)
Note that you will have to expand the date range for logs if you wanted to look at the progress of the backfill, see [View logs for your automations](https://docs.smith.langchain.com/observability/how_to_guides/rules#view-logs-for-your-automations) for details.
### Step 4: Define the sampling rate[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/rules#step-4-define-the-sampling-rate "Direct link to Step 4: Define the sampling rate")
You can specify a sampling rate (between 0 and 1) for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action.
### Step 5: Define the action[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/rules#step-5-define-the-action "Direct link to Step 5: Define the action")
There are four actions you can take with an automation rule:
  * **Add to dataset** : Add the inputs and outputs of the trace to a dataset.
  * **Add to annotation queue** : Add the trace to an annotation queue.
  * **Run online evaluation** : Run an online evaluation on the trace. For more information on online evaluations, you can refer to [this guide](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations).
  * **Trigger webhook** : Trigger a webhook with the trace data. For more information on webhooks, you can refer to [this guide](https://docs.smith.langchain.com/observability/how_to_guides/webhooks).
  * **Extend data retention** : Extends the data retention period on matching traces that use base retention [(see data retention docs for more details)](https://docs.smith.langchain.com/administration/concepts#data-retention). Note that all other rules will also extend data retention on matching traces through the auto-upgrade mechanism described in the aforementioned data retention docs, but this rule takes no additional action.


## View logs for your automations[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/rules#view-logs-for-your-automations "Direct link to View logs for your automations")
You can view logs for your automations by going to `Settings` -> `Rules` and click on the `Logs` button in any row.
You can also get to logs by clicking on `Rules` in the top right hand corner of any project details page, then clicking on `See Logs` for any rule.
Logs allow you to gain confidence that your rules are working as expected. You can now view logs that list all runs processed by a given rule for the past day. For rules that apply online evaluation scores, you can easily see the output score and navigate to the run. For rules that add runs as examples to datasets, you can view the example produced. If a particular rule execution has triggered an error, you can view the error message by hovering over the error icon.
![Logs_Gif](https://docs.smith.langchain.com/assets/images/rules_logs-f007f2fcb6fdcee543ede0179d998340.gif)
![Logs](https://docs.smith.langchain.com/assets/images/rules_logs-185421b78d6946142d5f44def24ef41e.png)
By default, rule logs only show results for runs that occurred in the last day. To see results for older runs, you can select **Last 1 day** and enter the desired date range. When applying a rule to past runs, the processing will start from the start date and go forward, so this would be needed to view logs while the backfill is proceeding.
![](https://docs.smith.langchain.com/assets/images/rules_past_runs_logs-15a932828e71d3b0e47221a10a1169f8.png)
If you prefer a video tutorial, check out the [Automations video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/rules%3E).
[PreviousLog traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)[NextOnline Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
  * [Create a rule](https://docs.smith.langchain.com/observability/how_to_guides/rules#create-a-rule)
    * [Step 1: Navigate to rule creation](https://docs.smith.langchain.com/observability/how_to_guides/rules#step-1-navigate-to-rule-creation)
    * [Step 2: Define the filter](https://docs.smith.langchain.com/observability/how_to_guides/rules#step-2-define-the-filter)
    * [(Optional) Step 3: Apply Rule to Past Runs](https://docs.smith.langchain.com/observability/how_to_guides/rules#optional-step-3-apply-rule-to-past-runs)
    * [Step 4: Define the sampling rate](https://docs.smith.langchain.com/observability/how_to_guides/rules#step-4-define-the-sampling-rate)
    * [Step 5: Define the action](https://docs.smith.langchain.com/observability/how_to_guides/rules#step-5-define-the-action)
  * [View logs for your automations](https://docs.smith.langchain.com/observability/how_to_guides/rules#view-logs-for-your-automations)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/sample_traces

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Set a sampling rate for traces


# Set a sampling rate for traces
note
This section is relevant for those using the LangSmith SDK or LangChain, not for those logging directly with the LangSmith API.
By default, all traces are logged to LangSmith. To down-sample the number of traces logged to LangSmith, set the `LANGSMITH_TRACING_SAMPLING_RATE` environment variable to any float between `0` (no traces) and `1` (all traces). For instance, setting the following environment variable will log 75% of the traces.
```
export LANGSMITH_TRACING_SAMPLING_RATE=0.75
```

This works for the `traceable` decorator and `RunTree` objects.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/sample_traces%3E).
[PreviousOnline Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)[NextAdd metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Trace JS functions in serverless environments


On this page
# Trace JS functions in serverless environments
note
This section is relevant for those using the LangSmith JS SDK version 0.2.0 and higher. If you are tracing using LangChain.js or LangGraph.js in serverless environments, see [this guide](https://js.langchain.com/docs/how_to/callbacks_serverless).
When tracing JavaScript functions, LangSmith will trace runs in the background by default to avoid adding latency. In serverless environments where the execution context may be terminated abruptly, it's important to ensure that all tracing data is properly flushed before the function completes.
To make sure this occurs, you can either:
  * Set an environment variable named `LANGSMITH_TRACING_BACKGROUND` to `"false"`. This will cause your traced functions to wait for tracing to complete before returning. 
    * Note that this is named differently from the [environment variable](https://js.langchain.com/docs/how_to/callbacks_serverless) in LangChain.js because LangSmith can be used without LangChain.
  * Pass a custom client into your traced runs and `await` the `client.awaitPendingTraceBatches();` method.


Here's an example of using `awaitPendingTraceBatches` alongside the [`traceable`](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code) method:
```
import{ Client }from"langsmith";import{ traceable }from"langsmith/traceable";const langsmithClient =newClient({});const tracedFn =traceable(async()=>{return"Some return value";},{  client: langsmithClient,});const res =awaittracedFn();await langsmithClient.awaitPendingTraceBatches();
```

## Rate limits at high concurrency[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments#rate-limits-at-high-concurrency "Direct link to Rate limits at high concurrency")
By default, the LangSmith client will batch operations as your traced run executions, sending a new batch every few milliseconds.
This works well in most situations, but if your traced function is long-running and you have very high concurrency, you may also hit rate limits related to overall request count.
If you are seeing rate limit errors related to this, you can try setting `manualFlushMode: true` in your client like this:
```
import{ Client }from"langsmith";const langsmithClient =newClient({ manualFlushMode:true,});const myTracedFunc =traceable(async()=>{// Your logic here...},{ client: langsmithClient });
```

And then manually calling `client.flush()` like this before your serverless function closes:
```
try{awaitmyTracedFunc();}finally{await langsmithClient.flush();}
```

Note that this will prevent runs from appearing in the LangSmith UI until you call `.flush()`.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/serverless_environments%3E).
[PreviousHow to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)[NextSet up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
  * [Rate limits at high concurrency](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments#rate-limits-at-high-concurrency)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/share_trace

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/share_trace#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Share or unshare a trace publicly


# Share or unshare a trace publicly
caution
Sharing a trace publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information. This feature is only available in the cloud-hosted version of LangSmith.
To share a trace publicly, simply click on the **Share** button in the upper right hand side of any trace view. ![](https://docs.smith.langchain.com/assets/images/share_trace-15a34368c102d00cc213dd6608935fcc.png)
This will open a dialog where you can copy the link to the trace.
Shared traces will be accessible to anyone with the link, even if they don't have a LangSmith account. They will be able to view the trace, but not edit it.
To "unshare" a trace, either
  1. Click on **Unshare** by click on **Public** in the upper right hand corner of any publicly shared trace, then **Unshare** in the dialog. ![](https://docs.smith.langchain.com/assets/images/unshare_trace-9e00bf94f8b9056b2676a2f4dd5ae661.png)
  2. Navigate to your organization's list of publicly shared traces, either by clicking on **Settings** -> **Shared URLs** or [this link](https://smith.langchain.com/settings/shared), then click on **Unshare** next to the trace you want to unshare. ![](https://docs.smith.langchain.com/assets/images/unshare_trace_list-2c9ae0fc85ad285b0620ab08d128f79b.png)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/share_trace%3E).
[PreviousQuery traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)[NextCompare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/threads

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/threads#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/threads)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Set up threads


On this page
# Set up threads
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)


Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the `Threads` feature in LangSmith.
## Group traces into threads[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/threads#group-traces-into-threads "Direct link to Group traces into threads")
A `Thread` is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.
To associate traces together, you need to pass in a special `metadata` key where the value is the unique identifier for that thread.
The key value is the unique identifier for that conversation. The key name should be one of:
  * `session_id`
  * `thread_id`
  * `conversation_id`.


The value can be any string you want, but we recommend using UUIDs, such as `f47ac10b-58cc-4372-a567-0e02b2c3d479`.
### Code example[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/threads#code-example "Direct link to Code example")
This example demonstrates how to log and retrieve conversation history from LangSmith to maintain long-running chats.
You can [add metadata to your traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags) in LangSmith in a variety of ways, this code will show how to do so dynamically, but read the previously linked guide to learn about all the ways you can add thread identifier metadata to your traces.
  * Python
  * TypeScript


```
import openaifrom langsmith import traceablefrom langsmith import Clientimport langsmith as lsfrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())langsmith_client = Client()# Config used for this examplelangsmith_project ="project-with-threads"session_id ="thread-id-1"langsmith_extra={"project_name": langsmith_project,"metadata":{"session_id": session_id}}# gets a history of all LLM calls in the thread to construct conversation historydefget_thread_history(thread_id:str, project_name:str):# Filter runs by the specific thread and projectfilter_string =f'and(in(metadata_key, ["session_id","conversation_id","thread_id"]), eq(metadata_value, "{thread_id}"))'# Only grab the LLM runsruns =[r for r in langsmith_client.list_runs(project_name=project_name,filter=filter_string, run_type="llm")]# Sort by start time to get the most recent interaction runs =sorted(runs, key=lambda run: run.start_time, reverse=True)# The current state of the conversationreturn runs[0].inputs['messages']+[runs[0].outputs['choices'][0]['message']]# if an existing conversation is continued, this function looks up the current run‚Äôs metadata to get the session_id, calls get_thread_history, and appends the new user question before making a call to the chat model@traceable(name="Chat Bot")defchat_pipeline(question:str, get_chat_history:bool=False):# Whether to continue an existing thread or start a new oneif get_chat_history:run_tree = ls.get_current_run_tree()messages = get_thread_history(run_tree.extra["metadata"]["session_id"],run_tree.session_name)+[{"role":"user","content": question}]else:messages =[{"role":"user","content": question}]# Invoke the model chat_completion = client.chat.completions.create(   model="gpt-4o-mini", messages=messages)return chat_completion.choices[0].message.content# Start the conversationchat_pipeline("Hi, my name is Bob", langsmith_extra=langsmith_extra)
```

```
import OpenAI from"openai";import{ traceable, getCurrentRunTree }from"langsmith/traceable";import{ Client }from"langsmith";import{ wrapOpenAI }from"langsmith/wrappers";// Config used for this exampleconst langsmithProject ="project-with-threads";const threadId ="thread-id-1";const client =wrapOpenAI(newOpenAI(),{project_name: langsmithProject,metadata:{ session_id: threadId }});const langsmithClient =newClient();asyncfunctiongetThreadHistory(threadId:string, projectName:string){// Filter runs by the specific thread and projectconst filterString =`and(in(metadata_key, ["session_id","conversation_id","thread_id"]), eq(metadata_value, "${threadId}"))`;// Only grab the LLM runsconst runs = langsmithClient.listRuns({projectName: projectName,filter: filterString,runType:"llm"});// Sort by start time to get the most recent interactionconst runsArray =[];forawait(const run of runs){runsArray.push(run);}const sortedRuns = runsArray.sort((a, b)=>newDate(b.start_time).getTime()-newDate(a.start_time).getTime());// The current state of the conversationreturn[...sortedRuns[0].inputs.messages,sortedRuns[0].outputs.choices[0].message];}const chatPipeline =traceable(async(question:string,options:{getChatHistory?:boolean;}={})=>{const{getChatHistory =false,}= options;let messages =[];// Whether to continue an existing thread or start a new oneif(getChatHistory){const runTree =awaitgetCurrentRunTree();const historicalMessages =awaitgetThreadHistory(   runTree.extra.metadata.session_id,   runTree.project_name);  messages =[...historicalMessages,{ role:"user", content: question }];}else{  messages =[{ role:"user", content: question }];}// Invoke the modelconst chatCompletion =await client.chat.completions.create({  model:"gpt-4o-mini",  messages: messages});return chatCompletion.choices[0].message.content;},{name:"Chat Bot",project_name: langsmithProject,metadata:{ session_id: threadId }});// Start the conversationawaitchatPipeline("Hi, my name is Bob");
```

After waiting a few seconds, you can make the following calls to contineu the conversation. By passing `getChatHistory: true`, you can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it, instead of just responding to the latest message.
  * Python
  * TypeScript


```
# Continue the conversation (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE FRIST TRACE CAN BE INGESTED)chat_pipeline("What is my name?", get_chat_history=True, langsmith_extra=langsmith_extra)# Keep the conversation going (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE PREVIOUS TRACE CAN BE INGESTED)chat_pipeline("What was the first message I sent you", get_chat_history=True, langsmith_extra=langsmith_extra)
```

```
// Continue the conversation (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE FRIST TRACE CAN BE INGESTED)awaitchatPipeline("What is my name?",{ getChatHistory:true});// Keep the conversation going (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE PREVIOUS TRACE CAN BE INGESTED)awaitchatPipeline("What was the first message I sent you",{ getChatHistory:true});
```

## View threads[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/threads#view-threads "Direct link to View threads")
You can view threads by clicking on the `Threads` tab in any project details page. You will then see a list of all threads, sorted by the most recent activity.
![Thread Tab](https://docs.smith.langchain.com/assets/images/convo_tab-c8ef1ebd2b8b6987bb00204c8fac2b45.png)
You can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs.
![Conversation](https://docs.smith.langchain.com/assets/images/convo-b6f7b5ed3df4aaf3e1ec0908ea8e929e.png)
You can open up the trace or annotate the trace in a side panel by clicking on `Annotate` and `Open trace`, respectively.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousTrace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)[NextToubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
  * [Group traces into threads](https://docs.smith.langchain.com/observability/how_to_guides/threads#group-traces-into-threads)
    * [Code example](https://docs.smith.langchain.com/observability/how_to_guides/threads#code-example)
  * [View threads](https://docs.smith.langchain.com/observability/how_to_guides/threads#view-threads)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Toubleshooting variable caching


On this page
# Toubleshooting variable caching
If you‚Äôre not seeing traces in your tracing project or notice traces logged to the wrong project/workspace, the issue might be due to LangSmith‚Äôs default environment variable caching. This is especially common when running LangSmith within a Jupyter notebook. Follow these steps to diagnose and resolve the issue:
## 1. Verify Your Environment Variables[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching#1-verify-your-environment-variables "Direct link to 1. Verify Your Environment Variables")
First, check that the environment variables are set correctly by running:
```
from langsmith import utilsprint(utils.get_env_var("LANGSMITH_PROJECT"))print(utils.get_env_var("LANGSMITH_TRACING_V2"))print(utils.get_env_var("LANGSMITH_ENDPOINT"))print(utils.get_env_var("LANGSMITH_API_KEY"))
```

If the output does not match what‚Äôs defined in your .env file, it‚Äôs likely due to environment variable caching.
## 2. Clear the cache[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching#2-clear-the-cache "Direct link to 2. Clear the cache")
Clear the cached environment variables with the following command:
```
utils.get_env_var.cache_clear()
```

## 3. Reload the Environment Variables[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching#3-reload-the-environment-variables "Direct link to 3. Reload the Environment Variables")
Reload your environment variables from the .env file by executing:
```
from dotenv import load_dotenvimport osload_dotenv(<path to .env file>, override=True)
```

After reloading, your environment variables should be set correctly.
If you continue to experience issues, please reach out to us via a shared Slack channel or email support (available for Plus and Enterprise plans), or in [LangChain's community Slack](https://langchaincommunity.slack.com/)(sign up [here](https://www.langchain.com/join-community) if you're not already a member).
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/toubleshooting_variable_caching%3E).
[PreviousSet up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)[NextTrace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
  * [1. Verify Your Environment Variables](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching#1-verify-your-environment-variables)
  * [2. Clear the cache](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching#2-clear-the-cache)
  * [3. Reload the Environment Variables](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching#3-reload-the-environment-variables)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Trace generator functions


On this page
# Trace generator functions
In most LLM applications, you will want to stream outputs to minimize the time to the first token seen by the user.
LangSmith's tracing functionality natively supports streamed outputs via `generator` functions. Below is an example.
  * Python
  * TypeScript


```
from langsmith import traceable@traceabledefmy_generator():for chunk in["Hello","World","!"]:yield chunk# Stream to the userfor output in my_generator():print(output)# It also works with async functionsimport asyncio@traceableasyncdefmy_async_generator(): hunk in["Hello","World","!"]:yield chunk# Stream to the userasyncdefmain():asyncfor output in my_async_generator():print(output)asyncio.run(main())
```

```
import{ traceable }from"langsmith/traceable";const myGenerator =traceable(function*(){for(const chunk of["Hello","World","!"]){yield chunk;}});for(const output ofmyGenerator()){console.log(output);}
```

## Aggregate Results[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions#aggregate-results "Direct link to Aggregate Results")
By default, the `outputs` of the traced function are aggregated into a single array in LangSmith. If you want to customize how it is stored (for instance, concatenating the outputs into a single string), you can use the `aggregate` option (`reduce_fn` in python). This is especially useful for aggregating streamed LLM outputs.
note
Aggregating outputs **only** impacts the traced representation of the outputs. It doesn not alter the values returned by your function.
  * Python
  * TypeScript


```
from langsmith import traceabledefconcatenate_strings(outputs:list):return"".join(outputs)@traceable(reduce_fn=concatenate_strings)defmy_generator():for chunk in["Hello","World","!"]:yield chunk# Stream to the userfor output in my_generator():print(output)# It also works with async functionsimport asyncio@traceable(reduce_fn=concatenate_strings)asyncdefmy_async_generator():for chunk in["Hello","World","!"]:yield chunk# Stream to the userasyncdefmain():asyncfor output in my_async_generator():print(output)asyncio.run(main())
```

```
import{ traceable }from"langsmith/traceable";constconcatenateStrings=(outputs:string[])=> outputs.join("");const myGenerator =traceable(function*(){for(const chunk of["Hello","World","!"]){yield chunk;}},{ aggregator: concatenateStrings });for(const output ofawaitmyGenerator()){console.log(output);}
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/trace_generator_functions%3E).
[PreviousCompare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)[NextTrace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
  * [Aggregate Results](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions#aggregate-results)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Trace LangChain with OpenTelemetry


On this page
# Trace LangChain with OpenTelemetry
The LangSmith Python SDK provides built-in OpenTelemetry integration, allowing you to trace your LangChain and LangGraph applications using the OpenTelemetry standard and send those traces to any OTel-compatible platform.
## 1. Installation[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#1-installation "Direct link to 1. Installation")
info
Requires Python SDK version `langsmith>=0.3.18`.
Install the LangSmith package with OpenTelemetry support:
```
pip install "langsmith[otel]"pip install langchain
```

## 2. Enable the OpenTelemetry integration[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#2-enable-the-opentelemetry-integration "Direct link to 2. Enable the OpenTelemetry integration")
You can enable the OpenTelemetry integration by setting the `LANGSMITH_OTEL_ENABLED` environment variable:
```
LANGSMITH_OTEL_ENABLED=trueLANGSMITH_TRACING=trueLANGSMITH_ENDPOINT=https://api.smith.langchain.comLANGSMITH_API_KEY=<your_langsmith_api_key>
```

## 3. Create a LangChain application with tracing[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#3-create-a-langchain-application-with-tracing "Direct link to 3. Create a LangChain application with tracing")
Here's a simple example showing how to use the OpenTelemetry integration with LangChain:
```
import osfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplate# LangChain will automatically use OpenTelemetry to send traces to LangSmith# because the LANGSMITH_OTEL_ENABLED environment variable is set# Create a chainprompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")model = ChatOpenAI()chain = prompt | model# Run the chainresult = chain.invoke({"topic":"programming"})print(result.content)
```

## 4. View the traces in LangSmith[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#4-view-the-traces-in-langsmith "Direct link to 4. View the traces in LangSmith")
Once your application runs, you'll see the traces in your LangSmith dashboard [like this one](https://smith.langchain.com/public/a762af6c-b67d-4f22-90a0-728df16baeba/r).
## Distributed Tracing with LangChain and OpenTelemetry[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#distributed-tracing-with-langchain-and-opentelemetry "Direct link to Distributed Tracing with LangChain and OpenTelemetry")
Distributed tracing is essential when your LLM application spans multiple services or processes. OpenTelemetry's context propagation capabilities ensure that traces remain connected across service boundaries.
### Context Propagation in Distributed Tracing[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#context-propagation-in-distributed-tracing "Direct link to Context Propagation in Distributed Tracing")
In distributed systems, context propagation passes trace metadata between services so that related spans are linked to the same trace:
  * **Trace ID** : A unique identifier for the entire trace
  * **Span ID** : A unique identifier for the current span
  * **Sampling Decision** : Indicates whether this trace should be sampled


### Setting up Distributed Tracing with LangChain[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#setting-up-distributed-tracing-with-langchain "Direct link to Setting up Distributed Tracing with LangChain")
To enable distributed tracing across multiple services:
```
import osfrom opentelemetry import tracefrom opentelemetry.propagate import inject, extractfrom opentelemetry.sdk.trace import TracerProviderfrom opentelemetry.sdk.trace.export import BatchSpanProcessorfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporterimport requestsfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplate# Set up OpenTelemetry trace providerprovider = TracerProvider()otlp_exporter = OTLPSpanExporter(  endpoint="https://api.smith.langchain.com/otel/v1/traces",  headers={"x-api-key": os.getenv("LANGSMITH_API_KEY"),"Langsmith-Project":"my_project"})processor = BatchSpanProcessor(otlp_exporter)provider.add_span_processor(processor)trace.set_tracer_provider(provider)tracer = trace.get_tracer(__name__)# Service A: Create a span and propagate context to Service Bdefservice_a():with tracer.start_as_current_span("service_a_operation")as span:# Create a chain    prompt = ChatPromptTemplate.from_template("Summarize: {text}")    model = ChatOpenAI()    chain = prompt | model# Run the chain    result = chain.invoke({"text":"OpenTelemetry is an observability framework"})# Propagate context to Service B    headers ={}    inject(headers)# Inject trace context into headers# Call Service B with the trace context    response = requests.post("http://service-b.example.com/process",      headers=headers,      json={"summary": result.content})return response.json()# Service B: Extract the context and continue the tracefrom flask import Flask, request, jsonifyapp = Flask(__name__)@app.route("/process", methods=["POST"])defservice_b_endpoint():# Extract the trace context from the request headers  context = extract(request.headers)with tracer.start_as_current_span("service_b_operation", context=context)as span:    data = request.json    summary = data.get("summary","")# Process the summary with another LLM chain    prompt = ChatPromptTemplate.from_template("Analyze the sentiment of: {text}")    model = ChatOpenAI()    chain = prompt | model    result = chain.invoke({"text": summary})return jsonify({"analysis": result.content})if __name__ =="__main__":  app.run(port=5000)
```

## Sending Traces to Alternate Providers[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#sending-traces-to-alternate-providers "Direct link to Sending Traces to Alternate Providers")
While LangSmith is the default destination for OpenTelemetry traces, you can also configure OpenTelemetry to send traces to other observability platforms.
### Using Environment Variables or Global Configuration[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#using-environment-variables-or-global-configuration "Direct link to Using Environment Variables or Global Configuration")
By default, the LangSmith OpenTelemetry exporter will send data to the LangSmith API OTEL endpoint, but this can be customized by setting standard OTEL environment variables:
```
OTEL_EXPORTER_OTLP_ENDPOINT: Override the endpoint URLOTEL_EXPORTER_OTLP_HEADERS: Add custom headers (LangSmith API keys and Project are added automatically)OTEL_SERVICE_NAME: Set a custom service name (defaults to "langsmith")
```

LangSmith uses the HTTP trace exporter by default. If you'd like to use your own tracing provider, you can either:
  1. Set the OTEL environment variables as shown above, or
  2. Set a global trace provider before initializing LangChain components, which LangSmith will detect and use instead of creating its own.


### Configuring Alternate OTLP Endpoints[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#configuring-alternate-otlp-endpoints "Direct link to Configuring Alternate OTLP Endpoints")
To send traces to a different provider, configure the OTLP exporter with your provider's endpoint:
```
import osfrom opentelemetry import tracefrom opentelemetry.sdk.trace import TracerProviderfrom opentelemetry.sdk.trace.export import BatchSpanProcessorfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporterfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplate# Set environment variables for LangChainos.environ["LANGSMITH_OTEL_ENABLED"]="true"os.environ["LANGSMITH_TRACING"]="true"# Configure the OTLP exporter for your custom endpointprovider = TracerProvider()otlp_exporter = OTLPSpanExporter(# Change to your provider's endpoint  endpoint="https://otel.your-provider.com/v1/traces",# Add any required headers for authentication  headers={"api-key":"your-api-key"})processor = BatchSpanProcessor(otlp_exporter)provider.add_span_processor(processor)trace.set_tracer_provider(provider)# Create and run a LangChain applicationprompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")model = ChatOpenAI()chain = prompt | modelresult = chain.invoke({"topic":"programming"})print(result.content)
```

### Using OpenTelemetry Collector for Fan-out[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#using-opentelemetry-collector-for-fan-out "Direct link to Using OpenTelemetry Collector for Fan-out")
For more advanced scenarios, you can use the OpenTelemetry Collector to fan out your telemetry data to multiple destinations. This is a more scalable approach than configuring multiple exporters in your application code.
  1. **Install the OpenTelemetry Collector** : Follow the [official installation instructions](https://opentelemetry.io/docs/collector/getting-started/) for your environment.
  2. **Configure the Collector** : Create a configuration file (e.g., `otel-collector-config.yaml`) that exports to multiple destinations:


```
receivers:otlp:protocols:grpc:endpoint: 0.0.0.0:4317http:endpoint: 0.0.0.0:4318processors:batch:exporters:otlphttp/langsmith:endpoint: https://api.smith.langchain.com/otel/v1/tracesheaders:x-api-key: ${env:LANGSMITH_API_KEY}Langsmith-Project: my_projectotlphttp/other_provider:endpoint: https://otel.your-provider.com/v1/tracesheaders:api-key: ${env:OTHER_PROVIDER_API_KEY}service:pipelines:traces:receivers:[otlp]processors:[batch]exporters:[otlphttp/langsmith, otlphttp/other_provider]
```

  1. **Configure your application to send to the collector** :


```
import osfrom opentelemetry import tracefrom opentelemetry.sdk.trace import TracerProviderfrom opentelemetry.sdk.trace.export import BatchSpanProcessorfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporterfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplate# Point to your local OpenTelemetry Collectorotlp_exporter = OTLPSpanExporter(  endpoint="http://localhost:4318/v1/traces")provider = TracerProvider()processor = BatchSpanProcessor(otlp_exporter)provider.add_span_processor(processor)trace.set_tracer_provider(provider)# Set environment variables for LangChainos.environ["LANGSMITH_OTEL_ENABLED"]="true"os.environ["LANGSMITH_TRACING"]="true"# Create and run a LangChain applicationprompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")model = ChatOpenAI()chain = prompt | modelresult = chain.invoke({"topic":"programming"})print(result.content)
```

This approach offers several advantages:
  * Centralized configuration for all your telemetry destinations
  * Reduced overhead in your application code
  * Better scalability and resilience
  * Ability to add or remove destinations without changing application code


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/trace_langchain_with_otel%3E).
[PreviousToubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)[NextSet up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
  * [1. Installation](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#1-installation)
  * [2. Enable the OpenTelemetry integration](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#2-enable-the-opentelemetry-integration)
  * [3. Create a LangChain application with tracing](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#3-create-a-langchain-application-with-tracing)
  * [4. View the traces in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#4-view-the-traces-in-langsmith)
  * [Distributed Tracing with LangChain and OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#distributed-tracing-with-langchain-and-opentelemetry)
    * [Context Propagation in Distributed Tracing](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#context-propagation-in-distributed-tracing)
    * [Setting up Distributed Tracing with LangChain](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#setting-up-distributed-tracing-with-langchain)
  * [Sending Traces to Alternate Providers](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#sending-traces-to-alternate-providers)
    * [Using Environment Variables or Global Configuration](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#using-environment-variables-or-global-configuration)
    * [Configuring Alternate OTLP Endpoints](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#configuring-alternate-otlp-endpoints)
    * [Using OpenTelemetry Collector for Fan-out](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel#using-opentelemetry-collector-for-fan-out)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Trace using the LangSmith REST API


On this page
# Trace using the LangSmith REST API
It is HIGHLY recommended to use our Python or TypeScript SDKs to send traces to LangSmith. We have designed these SDKs with optimizations like batching and backgrounding to ensure that your application's performance is not impacted by sending traces to LangSmith. However, if you are unable to use our SDKs, you can use the LangSmith REST API to send traces. Performance may be impacted if you send traces synchronously in your application. This guide will show you how to trace a request using the LangSmith REST API. Please view our API documentation [here](https://api.smith.langchain.com/redoc) for a full list of endpoints and request/response schemas.
## Basic tracing[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api#basic-tracing "Direct link to Basic tracing")
The simplest way to log runs is via the POST and PATCH `/runs` endpoint. These routes expect minimal contextual information about the tree structure to
note
When using the LangSmith REST API, you will need to provide your API key in the request headers as `"x-api-key"`.
In the simple example, you do not need to set the `dotted_order` opr `trace_id` fields in the request body. These fields will be automatically generated by the system. Though this is simpler, it is slower and has a lower rate limit in LangSmith.
The following example shows how you might leverage our API directly in Python. The same principles apply to other languages.
```
import openaiimport osimport requestsfrom datetime import datetimefrom uuid import uuid4# Send your API Key in the request headersheaders ={"x-api-key": os.environ["LANGSMITH_API_KEY"]}defpost_run(run_id, name, run_type, inputs, parent_id=None):"""Function to post a new run to the API."""  data ={"id": run_id.hex,"name": name,"run_type": run_type,"inputs": inputs,"start_time": datetime.utcnow().isoformat(),}if parent_id:    data["parent_run_id"]= parent_id.hex  requests.post("https://api.smith.langchain.com/runs",# Update appropriately for self-hosted installations or the EU region    json=data,    headers=headers)defpatch_run(run_id, outputs):"""Function to patch a run with outputs."""  requests.patch(f"https://api.smith.langchain.com/runs/{run_id}",    json={"outputs": outputs,"end_time": datetime.now(timezone.utc).isoformat(),},    headers=headers,)# This can be a user input to your appquestion ="Can you summarize this morning's meetings?"# This can be retrieved in a retrieval stepcontext ="During this morning's meeting, we solved all world conflict."messages =[{"role":"system","content":"You are a helpful assistant. Please respond to the user's request only based on the given context."},{"role":"user","content":f"Question: {question}\\nContext: {context}"}]# Create parent runparent_run_id = uuid4()post_run(parent_run_id,"Chat Pipeline","chain",{"question": question})# Create child runchild_run_id = uuid4()post_run(child_run_id,"OpenAI Call","llm",{"messages": messages}, parent_run_id)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(model="gpt-4o-mini", messages=messages)# End runspatch_run(child_run_id, chat_completion.dict())patch_run(parent_run_id,{"answer": chat_completion.choices[0].message.content})
```

See the doc on the [Run (span) data format](https://docs.smith.langchain.com/reference/data_formats/run_data_format) for more information.
## Batch Ingestion[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api#batch-ingestion "Direct link to Batch Ingestion")
For faster ingestion of runs and higher rate limits, you can use the POST `/runs/multipart` [link](https://api.smith.langchain.com/redoc#tag/run/operation/multipart_ingest_runs_api_v1_runs_multipart_post) endpoint. Below is an example. It requires `orjson` (for fast json ) and `requests_toolbelt` to run
```
import jsonimport osimport uuidfrom datetime import datetime, timezonefrom typing import Dict, List, Optionalimport requestsfrom requests_toolbelt import MultipartEncoderdefcreate_dotted_order(  start_time: Optional[datetime]=None, run_id: Optional[uuid.UUID]=None)->str:"""Create a dotted order string for run ordering and hierarchy.  The dotted order is used to establish the sequence and relationships between runs.  It combines a timestamp with a unique identifier to ensure proper ordering and tracing.  """  st = start_time or datetime.now(timezone.utc)  id_ = run_id or uuid.uuid4()returnf"{st.strftime('%Y%m%dT%H%M%S%fZ')}{id_}"defcreate_run_base(  name:str, run_type:str, inputs:dict, start_time: datetime)->dict:"""Create the base structure for a run."""  run_id = uuid.uuid4()return{"id":str(run_id),"trace_id":str(run_id),"name": name,"start_time": start_time.isoformat(),"inputs": inputs,"run_type": run_type,}defconstruct_run(  name:str,  run_type:str,  inputs:dict,  parent_dotted_order: Optional[str]=None,)->dict:"""Construct a run dictionary with the given parameters.  This function creates a run with a unique ID and dotted order, establishing its place  in the trace hierarchy if it's a child run.  """  start_time = datetime.now(timezone.utc)  run = create_run_base(name, run_type, inputs, start_time)  current_dotted_order = create_dotted_order(start_time, uuid.UUID(run["id"]))if parent_dotted_order:    current_dotted_order =f"{parent_dotted_order}.{current_dotted_order}"    run["trace_id"]= parent_dotted_order.split(".")[0].split("Z")[1]    run["parent_run_id"]= parent_dotted_order.split(".")[-1].split("Z")[1]  run["dotted_order"]= current_dotted_orderreturn rundefserialize_run(operation:str, run_data:dict)-> List[tuple]:"""Serialize a run for the multipart request.  This function separates the run data into parts for efficient transmission and storage.  The main run data and optional fields (inputs, outputs, events) are serialized separately.  """  run_id = run_data.get("id",str(uuid.uuid4()))# Separate optional fields  inputs = run_data.pop("inputs",None)  outputs = run_data.pop("outputs",None)  events = run_data.pop("events",None)  parts =[]# Serialize main run data  run_data_json = json.dumps(run_data).encode("utf-8")  parts.append((f"{operation}.{run_id}",(None,        run_data_json,"application/json",{"Content-Length":str(len(run_data_json))},),))# Serialize optional fieldsfor key, value in[("inputs", inputs),("outputs", outputs),("events", events)]:if value:      serialized_value = json.dumps(value).encode("utf-8")      parts.append((f"{operation}.{run_id}.{key}",(None,            serialized_value,"application/json",{"Content-Length":str(len(serialized_value))},),))return partsdefbatch_ingest_runs(  api_url:str,  api_key:str,  posts: Optional[List[dict]]=None,  patches: Optional[List[dict]]=None,)->None:"""Ingest multiple runs in a single batch request.  This function handles both creating new runs (posts) and updating existing runs (patches).  It's more efficient for ingesting multiple runs compared to individual API calls.  """  boundary = uuid.uuid4().hex  all_parts =[]for operation, runs inzip(("post","patch"),(posts, patches)):if runs:      all_parts.extend([part for run in runs for part in serialize_run(operation, run)])  encoder = MultipartEncoder(fields=all_parts, boundary=boundary)  headers ={"Content-Type": encoder.content_type,"x-api-key": api_key}try:    response = requests.post(f"{api_url}/runs/multipart", data=encoder, headers=headers)    response.raise_for_status()print("Successfully ingested runs.")except requests.RequestException as e:print(f"Error ingesting runs: {e}")# In a production environment, you might want to log this error or handle it more robustly# Configure API URL and key# For production use, consider using a configuration file or environment variablesapi_url ="https://api.smith.langchain.com"api_key = os.environ.get("LANGSMITH_API_KEY")ifnot api_key:raise ValueError("LANGSMITH_API_KEY environment variable is not set")# Create a parent runparent_run = construct_run(  name="Parent Run",  run_type="chain",  inputs={"main_question":"Tell me about France"},)# Create a child run, linked to the parentchild_run = construct_run(  name="Child Run",  run_type="llm",  inputs={"question":"What is the capital of France?"},  parent_dotted_order=parent_run["dotted_order"],)# First, post the runs to create themposts =[parent_run, child_run]batch_ingest_runs(api_url, api_key, posts=posts)# Then, update the runs with their end times and any outputschild_run_update ={**child_run,"end_time": datetime.now(timezone.utc).isoformat(),"outputs":{"answer":"Paris is the capital of France."},}parent_run_update ={**parent_run,"end_time": datetime.now(timezone.utc).isoformat(),"outputs":{"summary":"Discussion about France, including its capital."},}patches =[parent_run_update, child_run_update]batch_ingest_runs(api_url, api_key, patches=patches)# Note: This example requires the `requests` and `requests_toolbelt` libraries.# You can install them using pip:# pip install requests requests_toolbelt
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/trace_with_api%3E).
[PreviousTrace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)[NextTrace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
  * [Basic tracing](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api#basic-tracing)
  * [Batch Ingestion](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api#batch-ingestion)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Trace with Instructor (Python only)


# Trace with `Instructor` (Python only)
We provide a convenient integration with [Instructor](https://jxnl.github.io/instructor/), a popular open-source library for generating structured outputs with LLMs.
In order to use, you first need to set your LangSmith API key.
```
export LANGSMITH_API_KEY=<your-api-key>
```

Next, you will need to install the LangSmith SDK:
```
pip install -U langsmith
```

Wrap your OpenAI client with `langsmith.wrappers.wrap_openai`
```
from openai import OpenAIfrom langsmith import wrappersclient = wrappers.wrap_openai(OpenAI())
```

After this, you can patch the wrapped OpenAI client using `instructor`:
```
import instructorclient = instructor.patch(client)
```

Now, you can use `instructor` as you normally would, but now everything is logged to LangSmith!
```
from pydantic import BaseModelclassUserDetail(BaseModel):  name:str  age:intuser = client.chat.completions.create(  model="gpt-4o-mini",  response_model=UserDetail,  messages=[{"role":"user","content":"Extract Jason is 25 years old"},])
```

Oftentimes, you use `instructor` inside of other functions. You can get nested traces by using this wrapped client and decorating those functions with `@traceable`. Please see [this guide](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code) for more information on how to annotate your code for tracing with the `@traceable` decorator.
```
# You can customize the run name with the `name` keyword argument@traceable(name="Extract User Details")defmy_function(text:str)-> UserDetail:return client.chat.completions.create(    model="gpt-4o-mini",    response_model=UserDetail,    messages=[{"role":"user","content":f"Extract {text}"},])my_function("Jason is 25 years old")
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/trace_with_instructor%3E).
[PreviousTrace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)[NextTrace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Trace with LangChain (Python and JS/TS)


On this page
# Trace with `LangChain` (Python and JS/TS)
LangSmith integrates seamlessly with LangChain ([Python](https://python.langchain.com/) and [JS](https://js.langchain.com/docs/get_started/introduction)), the popular open-source framework for building LLM applications.
## Installation[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#installation "Direct link to Installation")
Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).
For a full list of packages available, see the [LangChain Python docs](https://python.langchain.com/docs/integrations/platforms/) and [LangChain JS docs](https://js.langchain.com/docs/integrations/platforms/).
  * pip
  * yarn
  * npm
  * pnpm


```
pip install langchain_openai langchain_core
```

```
yarn add @langchain/openai @langchain/core
```

```
npm install @langchain/openai @langchain/core
```

```
pnpm add @langchain/openai @langchain/core
```

## Quick start[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#quick-start "Direct link to Quick start")
### 1. Configure your environment[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#1-configure-your-environment "Direct link to 1. Configure your environment")
  * Python
  * TypeScript


```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key># This example uses OpenAI, but you can use any LLM provider of choiceexport OPENAI_API_KEY=<your-openai-api-key>
```

```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key># This example uses OpenAI, but you can use any LLM provider of choiceexport OPENAI_API_KEY=<your-openai-api-key>
```

info
If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:
`export LANGCHAIN_CALLBACKS_BACKGROUND=true`
If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:
`export LANGCHAIN_CALLBACKS_BACKGROUND=false`
See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.
### 2. Log a trace[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#2-log-a-trace "Direct link to 2. Log a trace")
No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would.
  * Python
  * TypeScript


```
from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([("system","You are a helpful assistant. Please respond to the user's request only based on the given context."),("user","Question: {question}\nContext: {context}")])model = ChatOpenAI(model="gpt-4o-mini")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion ="Can you summarize this morning's meetings?"context ="During this morning's meeting, we solved all world conflict."chain.invoke({"question": question,"context": context})
```

```
import{ ChatOpenAI }from"@langchain/openai";import{ ChatPromptTemplate }from"@langchain/core/prompts";import{ StringOutputParser }from"@langchain/core/output_parsers";const prompt = ChatPromptTemplate.fromMessages([["system","You are a helpful assistant. Please respond to the user's request only based on the given context."],["user","Question: {question}\nContext: {context}"],]);const model =newChatOpenAI({ modelName:"gpt-4o-mini"});const outputParser =newStringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question ="Can you summarize this morning's meetings?"const context ="During this morning's meeting, we solved all world conflict."await chain.invoke({ question: question, context: context });
```

### 3. View your trace[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#3-view-your-trace "Direct link to 3. View your trace")
By default, the trace will be logged to the project with the name `default`. An example of a trace logged using the above code is made public and can be viewed [here](https://smith.langchain.com/public/e6a46eb2-d785-4804-a1e3-23f167a04300/r).
![](https://docs.smith.langchain.com/assets/images/langchain_trace-906c6783b28da0d523b2675ee0c02eef.png)
## Trace selectively[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#trace-selectively "Direct link to Trace selectively")
The [previous section](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#quick-start) showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application.
There are two ways to do this in Python: by manually passing in a `LangChainTracer` ([reference docs](https://api.python.langchain.com/en/latest/tracers/langchain_core.tracers.langchain.LangChainTracer.html#langchain_core.tracers.langchain.LangChainTracer)) instance as a callback, or by using the `tracing_v2_enabled` context manager ([reference docs](https://api.python.langchain.com/en/latest/tracers/langchain_core.tracers.context.tracing_v2_enabled.html)).
In JS/TS, you can pass a `LangChainTracer` ([reference docs](https://api.js.langchain.com/classes/langchain_core_tracers_tracer_langchain.LangChainTracer.html)) instance as a callback.
  * Python
  * TypeScript


```
# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({"question":"Am I using a callback?","context":"I'm using a callback"}, config={"callbacks":[tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(): chain.invoke({"question":"Am I using a context manager?","context":"I'm using a context manager"})# This will NOT be traced (assuming LANGSMITH_TRACING is not set)chain.invoke({"question":"Am I being traced?","context":"I'm not being traced"})
```

```
// You can configure a LangChainTracer instance to trace a specific invocation.import{ LangChainTracer }from"@langchain/core/tracers/tracer_langchain";const tracer =newLangChainTracer();await chain.invoke({ question:"Am I using a callback?", context:"I'm using a callback"},{ callbacks:[tracer]});
```

## Log to a specific project[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#log-to-a-specific-project "Direct link to Log to a specific project")
### Statically[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#statically "Direct link to Statically")
As mentioned in the [tracing conceptual guide](https://docs.smith.langchain.com/observability/concepts) LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the `LANGSMITH_PROJECT` environment variable to configure a custom project name for an entire application run. This should be done before executing your application.
```
export LANGSMITH_PROJECT=my-project
```

SDK compatibility in JS
The `LANGSMITH_PROJECT` flag is only supported in JS SDK versions >= 0.2.16, use `LANGCHAIN_PROJECT` instead if you are using an older version.
### Dynamically[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#dynamically "Direct link to Dynamically")
This largely builds off of the [previous section](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#trace-selectively) and allows you to set the project name for a specific `LangChainTracer` instance or as parameters to the `tracing_v2_enabled` context manager in Python.
  * Python
  * TypeScript


```
# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name="My Project")chain.invoke({"question":"Am I using a callback?","context":"I'm using a callback"}, config={"callbacks":[tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name="My Project"): chain.invoke({"question":"Am I using a context manager?","context":"I'm using a context manager"})
```

```
// You can set the project name for a specific tracer instance:import{ LangChainTracer }from"@langchain/core/tracers/tracer_langchain";const tracer =newLangChainTracer({ projectName:"My Project"});await chain.invoke({ question:"Am I using a callback?", context:"I'm using a callback"},{ callbacks:[tracer]});
```

## Add metadata and tags to traces[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#add-metadata-and-tags-to-traces "Direct link to Add metadata and tags to traces")
You can send annotate your traces with arbitrary metadata and tags by providing them in the [Config](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain-core-runnables-config-runnableconfig). This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it. For information on how to query traces and runs by metadata and tags, see [this guide](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
note
When you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable.
  * Python
  * TypeScript


```
from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([("system","You are a helpful AI."),("user","{input}")])# The tag "model-tag" and metadata {"model-key": "model-value"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({"tags":["model-tag"],"metadata":{"model-key":"model-value"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain =(prompt | chat_model | output_parser).with_config({"tags":["config-tag"],"metadata":{"config-key":"config-value"}})# Tags and metadata can also be passed at runtimechain.invoke({"input":"What is the meaning of life?"},{"tags":["invoke-tag"],"metadata":{"invoke-key":"invoke-value"}})
```

```
import{ ChatOpenAI }from"@langchain/openai";import{ ChatPromptTemplate }from"@langchain/core/prompts";import{ StringOutputParser }from"@langchain/core/output_parsers";const prompt = ChatPromptTemplate.fromMessages([["system","You are a helpful AI."],["user","{input}"]])// The tag "model-tag" and metadata {"model-key": "model-value"} will be attached to the ChatOpenAI run onlyconst model =newChatOpenAI().withConfig({ tags:["model-tag"], metadata:{"model-key":"model-value"}});const outputParser =newStringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain =(prompt.pipe(model).pipe(outputParser)).withConfig({"tags":["config-tag"],"metadata":{"config-key":"top-level-value"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input:"What is the meaning of life?"},{tags:["invoke-tag"], metadata:{"invoke-key":"invoke-value"}})
```

## Customize run name[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#customize-run-name "Direct link to Customize run name")
You can customize the name of a given run when invoking or streaming your LangChain code by providing it in the [Config](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain-core-runnables-config-runnableconfig). This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI. This can be done by setting a `run_name` in the `RunnableConfig` object at construction or by passing a `run_name` in the invocation parameters in JS/TS.
note
This feature is not currently supported directly for LLM objects.
  * Python
  * TypeScript


```
# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({"run_name":"MyCustomChain"})configured_chain.invoke({"input":"What is the meaning of life?"})# You can also configure the run name at invocation time, like belowchain.invoke({"input":"What is the meaning of life?"},{"run_name":"MyCustomChain"})
```

```
// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName:"MyCustomChain"});await configuredChain.invoke({ input:"What is the meaning of life?"});// You can also configure the run name at invocation time, like belowawait chain.invoke({ input:"What is the meaning of life?"},{runName:"MyCustomChain"})
```

## Customize run ID[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#customize-run-id "Direct link to Customize run ID")
You can customize the ID of a given run when invoking or streaming your LangChain code by providing it in the [Config](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain-core-runnables-config-runnableconfig). This ID is used to uniquely identify the run in LangSmith and can be used to query specific runs. The ID can be useful for linking runs across different systems or for implementing custom tracking logic. This can be done by setting a `run_id` in the `RunnableConfig` object at construction or by passing a `run_id` in the invocation parameters in JS/TS.
note
This feature is not currently supported directly for LLM objects.
  * Python
  * TypeScript


```
import uuidmy_uuid = uuid.uuid4()# You can configure the run ID at invocation time:chain.invoke({"input":"What is the meaning of life?"},{"run_id": my_uuid})
```

```
import{ v4 as uuidv4 }from'uuid';const myUuid =uuidv4();// You can configure the run ID at invocation time, like belowawait chain.invoke({ input:"What is the meaning of life?"},{ runId: myUuid });
```

Note that if you do this at the **root** of a trace (i.e., the top-level run, that run ID will be used as the `trace_id`).
## Access run (span) ID for LangChain invocations[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#access-run-span-id-for-langchain-invocations "Direct link to Access run \(span\) ID for LangChain invocations")
When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith.
In Python, you can use the `collect_runs` context manager to access the run ID.
In JS/TS, you can use a `RunCollectorCallbackHandler` instance to access the run ID.
  * Python
  * TypeScript


```
from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([("system","You are a helpful assistant. Please respond to the user's request only based on the given context."),("user","Question: {question}\n\nContext: {context}")])model = ChatOpenAI(model="gpt-4o-mini")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion ="Can you summarize this morning's meetings?"context ="During this morning's meeting, we solved all world conflict."with collect_runs()as cb:result = chain.invoke({"question": question,"context": context})# Get the root run idrun_id = cb.traced_runs[0].idprint(run_id)
```

```
import{ ChatOpenAI }from"@langchain/openai";import{ ChatPromptTemplate }from"@langchain/core/prompts";import{ StringOutputParser }from"@langchain/core/output_parsers";import{ RunCollectorCallbackHandler }from"@langchain/core/tracers/run_collector";const prompt = ChatPromptTemplate.fromMessages([["system","You are a helpful assistant. Please respond to the user's request only based on the given context."],["user","Question: {question\n\nContext: {context}"],]);const model =newChatOpenAI({ modelName:"gpt-4o-mini"});const outputParser =newStringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector =newRunCollectorCallbackHandler();const question ="Can you summarize this morning's meetings?"const context ="During this morning's meeting, we solved all world conflict."await chain.invoke({ question: question, context: context },{ callbacks:[runCollector]});const runId = runCollector.tracedRuns[0].id;console.log(runId);
```

## Ensure all traces are submitted before exiting[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting "Direct link to Ensure all traces are submitted before exiting")
In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.
You can make callbacks synchronous by setting the `LANGCHAIN_CALLBACKS_BACKGROUND` environment variable to `"false"`.
For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application. Below is an example:
  * Python
  * TypeScript


```
from langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try: llm.invoke("Hello, World!")finally: wait_for_all_tracers()
```

```
import{ ChatOpenAI }from"@langchain/openai";import{ awaitAllCallbacks }from"@langchain/core/callbacks/promises";try{const llm =newChatOpenAI();const response =await llm.invoke("Hello, World!");}catch(e){// handle error}finally{awaitawaitAllCallbacks();}
```

## Trace without setting environment variables[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#trace-without-setting-environment-variables "Direct link to Trace without setting environment variables")
As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:
  * `LANGSMITH_TRACING`
  * `LANGSMITH_API_KEY`
  * `LANGSMITH_ENDPOINT`
  * `LANGSMITH_PROJECT`


However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.
This largely builds off of the [previous section](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#trace-selectively).
  * Python
  * TypeScript


```
from langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client( api_key="YOUR_API_KEY",# This can be retrieved from a secrets manager api_url="https://api.smith.langchain.com",# Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name="test-no-env")chain.invoke({"question":"Am I using a callback?","context":"I'm using a callback"}, config={"callbacks":[tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name="test-no-env"): chain.invoke({"question":"Am I using a context manager?","context":"I'm using a context manager"})
```

```
import{ LangChainTracer }from"@langchain/core/tracers/tracer_langchain";import{ Client }from"langsmith";// You can create a client instance with an api key and api urlconst client =newClient({   apiKey:"YOUR_API_KEY",   apiUrl:"https://api.smith.langchain.com",});// You can pass the client and project_name to the LangChainTracer instanceconst tracer =newLangChainTracer({client, projectName:"test-no-env"});await chain.invoke({ question:"Am I using a callback?", context:"I'm using a callback",},{ callbacks:[tracer]});
```

## Distributed tracing with LangChain (Python)[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#distributed-tracing-with-langchain-python "Direct link to Distributed tracing with LangChain \(Python\)")
LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications. The principles are similar to the [distributed tracing guide](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing) for the LangSmith SDK.
```
import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindefchild_chain(inputs):return inputs["test"]+1defchild_wrapper(x, headers):with langsmith.tracing_context(parent=headers):    child_chain.invoke({"test": x})# -- This code should be in a separate file or service --@chaindefparent_chain(inputs):  rt = get_current_run_tree()  headers = rt.to_headers()# ... make a request to another service with the headers# The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({"test":1})
```

## Interoperability between LangChain (Python) and LangSmith SDK[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk "Direct link to Interoperability between LangChain \(Python\) and LangSmith SDK")
If you are using LangChain for part of your application and the LangSmith SDK (see [this guide](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)) for other parts, you can still trace the entire application seamlessly.
LangChain objects will be traced when invoked within a `traceable` function and be bound as a child run of the `traceable` function.
```
from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langsmith import traceableprompt = ChatPromptTemplate.from_messages([("system","You are a helpful assistant. Please respond to the user's request only based on the given context."),("user","Question: {question}\nContext: {context}")])model = ChatOpenAI(model="gpt-4o-mini")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(  tags=["openai","chat"],  metadata={"foo":"bar"})definvoke_runnnable(question, context):  result = chain.invoke({"question": question,"context": context})return"The response is: "+ resultinvoke_runnnable("Can you summarize this morning's meetings?","During this morning's meeting, we solved all world conflict.")
```

This will produce the following trace tree: ![](https://docs.smith.langchain.com/assets/images/trace_tree_python_interop-b5c85bda5c4610f4a6b131bd3b2492cd.png)
## Interoperability between LangChain.JS and LangSmith SDK[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk "Direct link to Interoperability between LangChain.JS and LangSmith SDK")
### Tracing LangChain objects inside `traceable` (JS only)[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#tracing-langchain-objects-inside-traceable-js-only "Direct link to tracing-langchain-objects-inside-traceable-js-only")
Starting with `langchain@0.2.x`, LangChain objects are traced automatically when used inside `@traceable` functions, inheriting the client, tags, metadata and project name of the traceable function.
For older versions of LangChain below `0.2.x`, you will need to manually pass an instance `LangChainTracer` created from the tracing context found in `@traceable`.
```
import{ ChatOpenAI }from"@langchain/openai";import{ ChatPromptTemplate }from"@langchain/core/prompts";import{ StringOutputParser }from"@langchain/core/output_parsers";import{ getLangchainCallbacks }from"langsmith/langchain";const prompt = ChatPromptTemplate.fromMessages([["system","You are a helpful assistant. Please respond to the user's request only based on the given context.",],["user","Question: {question}\nContext: {context}"],]);const model =newChatOpenAI({ modelName:"gpt-4o-mini"});const outputParser =newStringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main =traceable(async(input:{ question:string; context:string})=>{const callbacks =awaitgetLangchainCallbacks();const response =await chain.invoke(input,{ callbacks });return response;},{ name:"main"});
```

### Tracing LangChain child runs via `traceable` / RunTree API (JS only)[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#tracing-langchain-child-runs-via-traceable--runtree-api-js-only "Direct link to tracing-langchain-child-runs-via-traceable--runtree-api-js-only")
note
We're working on improving the interoperability between `traceable` and LangChain. The following limitations are present when using combining LangChain with `traceable`:
  1. Mutating RunTree obtained from `getCurrentRunTree()` of the RunnableLambda context will result in a no-op.
  2. It's discouraged to traverse the RunTree obtained from RunnableLambda via `getCurrentRunTree()` as it may not contain all the RunTree nodes.
  3. Different child runs may have the same `execution_order` and `child_execution_order` value. Thus in extreme circumstances, some runs may end up in a different order, depending on the `start_time`.


In some uses cases, you might want to run `traceable` functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the `RunTree` API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke `traceable`-wrapped functions within RunnableLambda.
```
import{ traceable }from"langsmith/traceable";import{ RunnableLambda }from"@langchain/core/runnables";import{ RunnableConfig }from"@langchain/core/runnables";const tracedChild =traceable((input:string)=>`Child Run: ${input}`,{ name:"Child Run",});const parrot =newRunnableLambda({func:async(input:{ text:string}, config?: RunnableConfig)=>{returnawaittracedChild(input.text);},});
```

![Trace Tree](https://docs.smith.langchain.com/assets/images/trace_tree_manual_tracing-2d0109064a77410ad2321852e0f3f4af.png)
Alternatively, you can convert LangChain's `RunnableConfig` to a equivalent RunTree object by using `RunTree.fromRunnableConfig` or pass the `RunnableConfig` as the first argument of `traceable`-wrapped function.
  * Traceable
  * Run Tree


```
import{ traceable }from"langsmith/traceable";import{ RunnableLambda }from"@langchain/core/runnables";import{ RunnableConfig }from"@langchain/core/runnables";const tracedChild =traceable((input:string)=>`Child Run: ${input}`,{ name:"Child Run",});const parrot =newRunnableLambda({func:async(input:{ text:string}, config?: RunnableConfig)=>{// Pass the config to existing traceable functionawaittracedChild(config, input.text);return input.text;},});
```

```
import{ RunTree }from"langsmith/run_trees";import{ RunnableLambda }from"@langchain/core/runnables";import{ RunnableConfig }from"@langchain/core/runnables";const parrot =newRunnableLambda({func:async(input:{ text:string}, config?: RunnableConfig)=>{// create the RunTree from the RunnableConfig of the RunnableLambdaconst childRunTree = RunTree.fromRunnableConfig(config,{   name:"Child Run",});  childRunTree.inputs ={ input: input.text };await childRunTree.postRun();  childRunTree.outputs ={ output:`Child Run: ${input.text}`};await childRunTree.patchRun();return input.text;},});
```

If you prefer a video tutorial, check out the [Alternative Ways to Trace video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousTrace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)[NextTrace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
  * [Installation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#installation)
  * [Quick start](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#quick-start)
    * [1. Configure your environment](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#1-configure-your-environment)
    * [2. Log a trace](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#2-log-a-trace)
    * [3. View your trace](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#3-view-your-trace)
  * [Trace selectively](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#trace-selectively)
  * [Log to a specific project](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#log-to-a-specific-project)
    * [Statically](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#statically)
    * [Dynamically](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#dynamically)
  * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#add-metadata-and-tags-to-traces)
  * [Customize run name](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#customize-run-name)
  * [Customize run ID](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#customize-run-id)
  * [Access run (span) ID for LangChain invocations](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#access-run-span-id-for-langchain-invocations)
  * [Ensure all traces are submitted before exiting](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting)
  * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#trace-without-setting-environment-variables)
  * [Distributed tracing with LangChain (Python)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#distributed-tracing-with-langchain-python)
  * [Interoperability between LangChain (Python) and LangSmith SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk)
  * [Interoperability between LangChain.JS and LangSmith SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk)
    * [Tracing LangChain objects inside `traceable` (JS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#tracing-langchain-objects-inside-traceable-js-only)
    * [Tracing LangChain child runs via `traceable` / RunTree API (JS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain#tracing-langchain-child-runs-via-traceable--runtree-api-js-only)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Trace with LangGraph (Python and JS/TS)


On this page
# Trace with `LangGraph` (Python and JS/TS)
LangSmith smoothly integrates with LangGraph ([Python](https://langchain-ai.github.io/langgraph/) and [JS](https://langchain-ai.github.io/langgraphjs/)) to help you trace agentic workflows, whether you're using [LangChain modules](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain) or [other SDKs](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#use-traceable--traceable).
## With LangChain[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#with-langchain "Direct link to With LangChain")
If you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing.
This guide will walk through a basic example. For more detailed information on configuration, see the [Trace With LangChain](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain) guide.
### 1. Installation[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#1-installation "Direct link to 1. Installation")
Install the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).
For a full list of packages available, see the [LangChain Python docs](https://python.langchain.com/docs/integrations/platforms/) and [LangChain JS docs](https://js.langchain.com/docs/integrations/platforms/).
  * pip
  * yarn
  * npm
  * pnpm


```
pip install langchain_openai langgraph
```

```
yarn add @langchain/openai @langchain/langgraph
```

```
npm install @langchain/openai @langchain/langgraph
```

```
pnpm add @langchain/openai @langchain/langgraph
```

### 2. Configure your environment[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#2-configure-your-environment "Direct link to 2. Configure your environment")
  * Python
  * TypeScript


```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key># This example uses OpenAI, but you can use any LLM provider of choiceexport OPENAI_API_KEY=<your-openai-api-key>
```

```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key># This example uses OpenAI, but you can use any LLM provider of choiceexport OPENAI_API_KEY=<your-openai-api-key>
```

info
If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:
`export LANGCHAIN_CALLBACKS_BACKGROUND=true`
If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:
`export LANGCHAIN_CALLBACKS_BACKGROUND=false`
See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.
### 3. Log a trace[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#3-log-a-trace "Direct link to 3. Log a trace")
Once you've set up your environment, you can call LangChain runnables as normal. LangSmith will infer the proper tracing config:
  * Python
  * TypeScript


```
from typing import Literalfrom langchain_core.messages import HumanMessagefrom langchain_openai import ChatOpenAIfrom langchain_core.tools import toolfrom langgraph.graph import StateGraph, MessagesStatefrom langgraph.prebuilt import ToolNode@tooldefsearch(query:str):"""Call to surf the web."""if"sf"in query.lower()or"san francisco"in query.lower():return"It's 60 degrees and foggy."return"It's 90 degrees and sunny."tools =[search]tool_node = ToolNode(tools)model = ChatOpenAI(model="gpt-4o", temperature=0).bind_tools(tools)defshould_continue(state: MessagesState)-> Literal["tools","__end__"]:  messages = state['messages']  last_message = messages[-1]if last_message.tool_calls:return"tools"return"__end__"defcall_model(state: MessagesState):  messages = state['messages']# Invoking `model` will automatically infer the correct tracing context  response = model.invoke(messages)return{"messages":[response]}workflow = StateGraph(MessagesState)workflow.add_node("agent", call_model)workflow.add_node("tools", tool_node)workflow.add_edge("__start__","agent")workflow.add_conditional_edges("agent",  should_continue,)workflow.add_edge("tools",'agent')app = workflow.compile()final_state = app.invoke({"messages":[HumanMessage(content="what is the weather in sf")]},  config={"configurable":{"thread_id":42}})final_state["messages"][-1].content
```

```
import{ HumanMessage, AIMessage }from"@langchain/core/messages";import{ tool }from"@langchain/core/tools";import{ z }from"zod";import{ ChatOpenAI }from"@langchain/openai";import{ StateGraph, StateGraphArgs }from"@langchain/langgraph";import{ ToolNode }from"@langchain/langgraph/prebuilt";interfaceAgentState{ messages: HumanMessage[];}const graphState: StateGraphArgs<AgentState>["channels"]={ messages:{reducer:(x: HumanMessage[], y: HumanMessage[])=> x.concat(y),},};const searchTool =tool(async({ query }:{ query:string})=>{if(query.toLowerCase().includes("sf")|| query.toLowerCase().includes("san francisco")){return"It's 60 degrees and foggy."}return"It's 90 degrees and sunny."},{ name:"search", description:"Call to surf the web.", schema: z.object({  query: z.string().describe("The query to use in your search."),}),});const tools =[searchTool];const toolNode =newToolNode<AgentState>(tools);const model =newChatOpenAI({ model:"gpt-4o", temperature:0,}).bindTools(tools);functionshouldContinue(state: AgentState){const messages = state.messages;const lastMessage = messages[messages.length -1]as AIMessage;if(lastMessage.tool_calls?.length){return"tools";}return"__end__";}asyncfunctioncallModel(state: AgentState){const messages = state.messages;// Invoking `model` will automatically infer the correct tracing contextconst response =await model.invoke(messages);return{ messages:[response]};}const workflow =newStateGraph<AgentState>({ channels: graphState }).addNode("agent", callModel).addNode("tools", toolNode).addEdge("__start__","agent").addConditionalEdges("agent", shouldContinue).addEdge("tools","agent");const app = workflow.compile();const finalState =await app.invoke({ messages:[newHumanMessage("what is the weather in sf")]},{ configurable:{ thread_id:"42"}});finalState.messages[finalState.messages.length -1].content;
```

An example trace from running the above code [looks like this](https://smith.langchain.com/public/10863294-ee79-484a-927f-0558230f1547/r):
![Trace tree for a LangGraph run with LangChain](https://docs.smith.langchain.com/assets/images/langgraph_with_langchain_trace-fc850a609ceda555dafb450e4176cfea.png)
## Without LangChain[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#without-langchain "Direct link to Without LangChain")
If you are using other SDKs or custom functions within LangGraph, you will need to [wrap or decorate them appropriately](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#use-traceable--traceable) (with the `@traceable` decorator in Python or the `traceable` function in JS, or something like e.g. `wrap_openai` for SDKs). If you do so, LangSmith will automatically nest traces from those wrapped methods.
Here's an example. You can also see this page for more information.
### 1. Installation[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#1-installation-1 "Direct link to 1. Installation")
Install the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below).
  * pip
  * yarn
  * npm
  * pnpm


```
pip install openai langsmith langgraph
```

```
yarn add openai langsmith @langchain/langgraph
```

```
npm install openai langsmith @langchain/langgraph
```

```
pnpm add openai langsmith @langchain/langgraph
```

### 2. Configure your environment[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#2-configure-your-environment-1 "Direct link to 2. Configure your environment")
  * Python
  * TypeScript


```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key># This example uses OpenAI, but you can use any LLM provider of choiceexport OPENAI_API_KEY=<your-openai-api-key>
```

```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key># This example uses OpenAI, but you can use any LLM provider of choiceexport OPENAI_API_KEY=<your-openai-api-key>
```

info
If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:
`export LANGCHAIN_CALLBACKS_BACKGROUND=true`
If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:
`export LANGCHAIN_CALLBACKS_BACKGROUND=false`
See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.
### 3. Log a trace[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#3-log-a-trace-1 "Direct link to 3. Log a trace")
Once you've set up your environment, [wrap or decorate the custom functions/SDKs](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#use-traceable--traceable) you want to trace. LangSmith will then infer the proper tracing config:
  * Python
  * TypeScript


```
import jsonimport openaiimport operatorfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaifrom typing import Annotated, Literal, TypedDictfrom langgraph.graph import StateGraphclassState(TypedDict):  messages: Annotated[list, operator.add]tool_schema ={"type":"function","function":{"name":"search","description":"Call to surf the web.","parameters":{"type":"object","properties":{"query":{"type":"string"}},"required":["query"],},},}# Decorating the tool function will automatically trace it with the correct context@traceable(run_type="tool", name="Search Tool")defsearch(query:str):"""Call to surf the web."""if"sf"in query.lower()or"san francisco"in query.lower():return"It's 60 degrees and foggy."return"It's 90 degrees and sunny."tools =[search]defcall_tools(state):  function_name_to_function ={"search": search}  messages = state["messages"]  tool_call = messages[-1]["tool_calls"][0]  function_name = tool_call["function"]["name"]  function_arguments = tool_call["function"]["arguments"]  arguments = json.loads(function_arguments)  function_response = function_name_to_function[function_name](**arguments)  tool_message ={"tool_call_id": tool_call["id"],"role":"tool","name": function_name,"content": function_response,}return{"messages":[tool_message]}wrapped_client = wrap_openai(openai.Client())defshould_continue(state: State)-> Literal["tools","__end__"]:  messages = state["messages"]  last_message = messages[-1]if last_message["tool_calls"]:return"tools"return"__end__"defcall_model(state: State):  messages = state["messages"]# Calling the wrapped client will automatically infer the correct tracing context  response = wrapped_client.chat.completions.create(    messages=messages, model="gpt-4o-mini", tools=[tool_schema])  raw_tool_calls = response.choices[0].message.tool_calls  tool_calls =[tool_call.to_dict()for tool_call in raw_tool_calls]if raw_tool_calls else[]  response_message ={"role":"assistant","content": response.choices[0].message.content,"tool_calls": tool_calls,}return{"messages":[response_message]}workflow = StateGraph(State)workflow.add_node("agent", call_model)workflow.add_node("tools", call_tools)workflow.add_edge("__start__","agent")workflow.add_conditional_edges("agent",  should_continue,)workflow.add_edge("tools",'agent')app = workflow.compile()final_state = app.invoke({"messages":[{"role":"user","content":"what is the weather in sf"}]})final_state["messages"][-1]["content"]
```

**Note:** The below example requires `langsmith>=0.1.39` and `@langchain/langgraph>=0.0.31`
```
import OpenAI from"openai";import{ StateGraph }from"@langchain/langgraph";import{ wrapOpenAI }from"langsmith/wrappers/openai";import{ traceable }from"langsmith/traceable";typeGraphState={ messages: OpenAI.ChatCompletionMessageParam[];};const wrappedClient =wrapOpenAI(newOpenAI({}));const toolSchema: OpenAI.ChatCompletionTool ={ type:"function",function:{  name:"search",  description:"Use this tool to query the web.",  parameters:{   type:"object",   properties:{    query:{     type:"string",},},   required:["query"],}}};// Wrapping the tool function will automatically trace it with the correct contextconst search =traceable(async({ query }:{ query:string})=>{if(  query.toLowerCase().includes("sf")||  query.toLowerCase().includes("san francisco")){return"It's 60 degrees and foggy.";}return"It's 90 degrees and sunny."},{ run_type:"tool", name:"Search Tool"});constcallTools=async({ messages }: GraphState)=>{const mostRecentMessage = messages[messages.length -1];const toolCalls =(mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;if(toolCalls ===undefined|| toolCalls.length ===0){thrownewError("No tool calls passed to node.");}const toolNameMap ={  search,};const functionName = toolCalls[0].function.name;const functionArguments =JSON.parse(toolCalls[0].function.arguments);const response =await toolNameMap[functionName](functionArguments);const toolMessage ={  tool_call_id: toolCalls[0].id,  role:"tool",  name: functionName,  content: response,}return{ messages:[toolMessage]};}constcallModel=async({ messages }: GraphState)=>{// Calling the wrapped client will automatically infer the correct tracing contextconst response =await wrappedClient.chat.completions.create({  messages,  model:"gpt-4o-mini",  tools:[toolSchema],});const responseMessage ={  role:"assistant",  content: response.choices[0].message.content,  tool_calls: response.choices[0].message.tool_calls ??[],};return{ messages:[responseMessage]};}constshouldContinue=({ messages }: GraphState)=>{const lastMessage =  messages[messages.length -1]as OpenAI.ChatCompletionAssistantMessageParam;if(  lastMessage?.tool_calls !==undefined&&  lastMessage?.tool_calls.length >0){return"tools";}return"__end__";}const workflow =newStateGraph<GraphState>({ channels:{  messages:{reducer:(a:any, b:any)=> a.concat(b),}}});const graph = workflow.addNode("model", callModel).addNode("tools", callTools).addEdge("__start__","model").addConditionalEdges("model", shouldContinue,{  tools:"tools",  __end__:"__end__",}).addEdge("tools","model").compile();await graph.invoke({ messages:[{ role:"user", content:"what is the weather in sf"}]});
```

An example trace from running the above code [looks like this](https://smith.langchain.com/public/353f27da-c221-4b67-b9ec-ede3777f3271/r):
![Trace tree for a LangGraph run without LangChain](https://docs.smith.langchain.com/assets/images/langgraph_without_langchain_trace-916302f8471cd3bf045d14b48fc172fa.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousTrace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)[NextTrace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
  * [With LangChain](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#with-langchain)
    * [1. Installation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#1-installation)
    * [2. Configure your environment](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#2-configure-your-environment)
    * [3. Log a trace](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#3-log-a-trace)
  * [Without LangChain](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#without-langchain)
    * [1. Installation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#1-installation-1)
    * [2. Configure your environment](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#2-configure-your-environment-1)
    * [3. Log a trace](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph#3-log-a-trace-1)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Trace with OpenAI Agents SDK


On this page
# Trace with OpenAI Agents SDK
The [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) allows you to build agentic applications powered by OpenAI's models.
## Installation[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk#installation "Direct link to Installation")
info
Requires Python SDK version `langsmith>=0.3.15`.
Install LangSmith with OpenAI Agents support:
```
pip install "langsmith[openai-agents]"
```

This will install both the LangSmith library and the OpenAI Agents SDK.
## Quick Start[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk#quick-start "Direct link to Quick Start")
You can integrate LangSmith tracing with the OpenAI Agents SDK by using the `OpenAIAgentsTracingProcessor` class.
```
import asynciofrom agents import Agent, Runner, set_trace_processorsfrom langsmith.wrappers import OpenAIAgentsTracingProcessorasyncdefmain():  agent = Agent(    name="Captain Obvious",    instructions="You are Captain Obvious, the world's most literal technical support agent.",)  question ="Why is my code failing when I try to divide by zero? I keep getting this error message."  result =await Runner.run(agent, question)print(result.final_output)if __name__ =="__main__":  set_trace_processors([OpenAIAgentsTracingProcessor()])  asyncio.run(main())
```

The agent's execution flow, including all spans and their details, will be logged to LangSmith.
![OpenAI Agents SDK Trace in LangSmith](https://docs.smith.langchain.com/assets/images/agent_trace-e915e199cfb66f774994e2db42420b27.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/trace_with_openai_agents_sdk%3E).
[PreviousTrace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)[NextCalculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
  * [Installation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk#installation)
  * [Quick Start](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk#quick-start)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Trace with OpenTelemetry


On this page
# Trace with OpenTelemetry
LangSmith can accept traces from OpenTelemetry based clients. This guide will walk through examples on how to achieve this.
## Logging Traces with a basic OpenTelemetry client[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#logging-traces-with-a-basic-opentelemetry-client "Direct link to Logging Traces with a basic OpenTelemetry client")
This first section covers how to use a standard OpenTelemetry client to log traces to LangSmith.
### 1. Installation[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#1-installation "Direct link to 1. Installation")
Install the OpenTelemetry SDK, OpenTelemetry exporter packages, as well as the OpenAI package:
```
pip install openaipip install opentelemetry-sdkpip install opentelemetry-exporter-otlp
```

### 2. Configure your environment[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#2-configure-your-environment "Direct link to 2. Configure your environment")
Setup environment variables for the endpoint, substitute your specific values:
```
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.smith.langchain.com/otelOTEL_EXPORTER_OTLP_HEADERS="x-api-key=<your langsmith api key>"
```

#### **Optional: Specify a custom project name other than "default"**[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#optional-specify-a-custom-project-name-other-than-default "Direct link to optional-specify-a-custom-project-name-other-than-default")
```
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.smith.langchain.com/otelOTEL_EXPORTER_OTLP_HEADERS="x-api-key=<your langsmith api key>,Langsmith-Project=<project name>"
```

### 3. Log a trace[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#3-log-a-trace "Direct link to 3. Log a trace")
This code sets up an OTEL tracer and exporter that will send traces to LangSmith. It then calls OpenAI and sends the required OpenTelemetry attributes.
```
from openai import OpenAIfrom opentelemetry import tracefrom opentelemetry.sdk.trace import TracerProviderfrom opentelemetry.sdk.trace.export import(  BatchSpanProcessor,)from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporterclient = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))otlp_exporter = OTLPSpanExporter(  timeout=10,)trace.set_tracer_provider(TracerProvider())trace.get_tracer_provider().add_span_processor(  BatchSpanProcessor(otlp_exporter))tracer = trace.get_tracer(__name__)defcall_openai():  model ="gpt-4o-mini"with tracer.start_as_current_span("call_open_ai")as span:    span.set_attribute("langsmith.span.kind","LLM")    span.set_attribute("langsmith.metadata.user_id","user_123")    span.set_attribute("gen_ai.system","OpenAI")    span.set_attribute("gen_ai.request.model", model)    span.set_attribute("llm.request.type","chat")    messages =[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Write a haiku about recursion in programming."}]for i, message inenumerate(messages):      span.set_attribute(f"gen_ai.prompt.{i}.content",str(message["content"]))      span.set_attribute(f"gen_ai.prompt.{i}.role",str(message["role"]))    completion = client.chat.completions.create(      model=model,      messages=messages)    span.set_attribute("gen_ai.response.model", completion.model)    span.set_attribute("gen_ai.completion.0.content",str(completion.choices[0].message.content))    span.set_attribute("gen_ai.completion.0.role","assistant")    span.set_attribute("gen_ai.usage.prompt_tokens", completion.usage.prompt_tokens)    span.set_attribute("gen_ai.usage.completion_tokens", completion.usage.completion_tokens)    span.set_attribute("gen_ai.usage.total_tokens", completion.usage.total_tokens)return completion.choices[0].messageif __name__ =="__main__":  call_openai()
```

You should see a trace in your LangSmith dashboard [like this one](https://smith.langchain.com/public/4f2890b1-f105-44aa-a6cf-c777dcc27a37/r).
## Supported OpenTelemetry Attribute Mapping[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#supported-opentelemetry-attribute-mapping "Direct link to Supported OpenTelemetry Attribute Mapping")
When sending traces to LangSmith via OpenTelemetry, the following attributes are mapped to LangSmith fields.
OpenTelemetry Attribute| LangSmith Field| Notes  
---|---|---  
`langsmith.trace.name`| Run Name| Overrides the span name for the run  
`langsmith.span.kind`| Run Type| Values: `llm`, `chain`, `tool`, `retriever`, `embedding`, `prompt`, `parser`  
`langsmith.span.id`| Run ID| Unique identifier for the span  
`langsmith.trace.id`| Trace ID| Unique identifier for the trace  
`langsmith.span.dotted_order`| Dotted Order| Position in the execution tree  
`langsmith.span.parent_id`| Parent Run ID| ID of the parent span  
`langsmith.trace.session_id`| Session ID| Session identifier for related traces  
`langsmith.trace.session_name`| Session Name| Name of the session  
`langsmith.span.tags`| Tags| Custom tags attached to the span  
`gen_ai.system`| `metadata.ls_provider`| The GenAI system (e.g., "openai", "anthropic")  
`gen_ai.prompt`| `inputs`| The input prompt sent to the model  
`gen_ai.completion`| `outputs`| The output generated by the model  
`gen_ai.prompt.{n}.role`| `inputs.messages[n].role`| Role for the nth input message  
`gen_ai.prompt.{n}.content`| `inputs.messages[n].content`| Content for the nth input message  
`gen_ai.completion.{n}.role`| `outputs.messages[n].role`| Role for the nth output message  
`gen_ai.completion.{n}.content`| `outputs.messages[n].content`| Content for the nth output message  
`gen_ai.request.model`| `invocation_params.model`| The model name used for the request  
`gen_ai.response.model`| `invocation_params.model`| The model name returned in the response  
`gen_ai.request.temperature`| `invocation_params.temperature`| Temperature setting  
`gen_ai.request.top_p`| `invocation_params.top_p`| Top-p sampling setting  
`gen_ai.request.max_tokens`| `invocation_params.max_tokens`| Maximum tokens setting  
`gen_ai.request.frequency_penalty`| `invocation_params.frequency_penalty`| Frequency penalty setting  
`gen_ai.request.presence_penalty`| `invocation_params.presence_penalty`| Presence penalty setting  
`gen_ai.request.seed`| `invocation_params.seed`| Random seed used for generation  
`gen_ai.request.stop_sequences`| `invocation_params.stop`| Sequences that stop generation  
`gen_ai.request.top_k`| `invocation_params.top_k`| Top-k sampling parameter  
`gen_ai.request.encoding_formats`| `invocation_params.encoding_formats`| Output encoding formats  
`gen_ai.usage.input_tokens`| `usage_metadata.input_tokens`| Number of input tokens used  
`gen_ai.usage.output_tokens`| `usage_metadata.output_tokens`| Number of output tokens used  
`gen_ai.usage.total_tokens`| `usage_metadata.total_tokens`| Total number of tokens used  
`gen_ai.usage.prompt_tokens`| `usage_metadata.input_tokens`| Number of input tokens used (deprecated)  
`gen_ai.usage.completion_tokens`| `usage_metadata.output_tokens`| Number of output tokens used (deprecated)  
`input.value`| `inputs`| Full input value, can be string or JSON  
`output.value`| `outputs`| Full output value, can be string or JSON  
`langsmith.metadata.{key}`| `metadata.{key}`| Custom metadata  
## Logging Traces with the Traceloop SDK[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#logging-traces-with-the-traceloop-sdk "Direct link to Logging Traces with the Traceloop SDK")
The Traceloop SDK is an OpenTelemetry compatible SDK that covers a range of models, vector databases and frameworks. If there are integrations that you are interested in instrumenting that are covered by this SDK, you can use this SDK with OpenTelemetry to log traces to LangSmith.
To see what integrations are supported by the Traceloop SDK, see the [Traceloop SDK documentation](https://www.traceloop.com/docs/openllmetry/tracing/supported).
To get started, follow these steps:
### 1. Installation[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#1-installation-1 "Direct link to 1. Installation")
```
pip install traceloop-sdkpip install openai
```

### 2. Configure your environment[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#2-configure-your-environment-1 "Direct link to 2. Configure your environment")
Setup environment variables:
```
TRACELOOP_BASE_URL=https://api.smith.langchain.com/otelTRACELOOP_HEADERS=x-api-key=<your_langsmith_api_key>
```

#### **Optional: Specify a custom project name other than "default"**[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#optional-specify-a-custom-project-name-other-than-default-1 "Direct link to optional-specify-a-custom-project-name-other-than-default-1")
```
TRACELOOP_HEADERS=x-api-key=<your_langsmith_api_key>,Langsmith-Project=<langsmith_project_name>
```

### 3. Initialize the SDK[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#3-initialize-the-sdk "Direct link to 3. Initialize the SDK")
To use the SDK, you need to initialize it before logging traces:
```
from traceloop.sdk import TraceloopTraceloop.init()
```

### 4. Log a trace[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#4-log-a-trace "Direct link to 4. Log a trace")
Here is a complete example using an OpenAI chat completion:
```
import osfrom openai import OpenAIfrom traceloop.sdk import Traceloopclient = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))Traceloop.init()completion = client.chat.completions.create(  model="gpt-4o-mini",  messages=[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Write a haiku about recursion in programming."}])print(completion.choices[0].message)
```

You should see a trace in your LangSmith dashboard [like this one](https://smith.langchain.com/public/106f5bed-edca-4357-91a5-80089252c9ed/r).
## Tracing using the Arize SDK[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#tracing-using-the-arize-sdk "Direct link to Tracing using the Arize SDK")
With the Arize SDK and OpenTelemetry, you can log traces from multiple other frameworks to LangSmith. Below is an example of tracing CrewAI to LangSmith, you can find a full list of supported frameworks [here](https://docs.arize.com/phoenix/tracing/integrations-tracing). To make this example work with other frameworks, you just need to change the instrumentor to match the framework.
### 1. Installation[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#1-installation-2 "Direct link to 1. Installation")
First, install the required packages:
```
pip install -qU arize-phoenix-otel openinference-instrumentation-crewai crewai crewai-tools
```

### 2. Configure your environment[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#2-configure-your-environment-2 "Direct link to 2. Configure your environment")
Next, set the following environment variables:
```
OPENAI_API_KEY=<your_openai_api_key>SERPER_API_KEY=<your_serper_api_key>
```

### 3. Set up the instrumentor[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#3-set-up-the-instrumentor "Direct link to 3. Set up the instrumentor")
Before running any application code let's set up our instrumentor (you can replace this with any of the frameworks supported [here](https://docs.arize.com/phoenix/tracing/integrations-tracing))
```
from opentelemetry.sdk.trace import TracerProviderfrom opentelemetry.sdk.trace.export import BatchSpanProcessorfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter# Add LangSmith API Key for tracingLANGSMITH_API_KEY ="YOUR_API_KEY"# Set the endpoint for OTEL collectionENDPOINT ="https://api.smith.langchain.com/otel/v1/traces"# Select the project to trace toLANGSMITH_PROJECT ="YOUR_PROJECT_NAME"# Create the OTLP exporterotlp_exporter = OTLPSpanExporter(  endpoint=ENDPOINT,  headers={"x-api-key": LANGSMITH_API_KEY,"Langsmith-Project": LANGSMITH_PROJECT})# Set up the trace providerprovider = TracerProvider()processor = BatchSpanProcessor(otlp_exporter)provider.add_span_processor(processor)# Now instrument CrewAIfrom openinference.instrumentation.crewai import CrewAIInstrumentorCrewAIInstrumentor().instrument(tracer_provider=provider)
```

### 4. Log a trace[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#4-log-a-trace-1 "Direct link to 4. Log a trace")
Now, you can run a CrewAI workflow and the trace will automatically be logged to LangSmith
```
from crewai import Agent, Task, Crew, Processfrom crewai_tools import SerperDevToolsearch_tool = SerperDevTool()# Define your agents with roles and goalsresearcher = Agent( role='Senior Research Analyst', goal='Uncover cutting-edge developments in AI and data science', backstory="""You work at a leading tech think tank. Your expertise lies in identifying emerging trends. You have a knack for dissecting complex data and presenting actionable insights.""", verbose=True, allow_delegation=False,# You can pass an optional llm attribute specifying what model you wanna use.# llm=ChatOpenAI(model_name="gpt-3.5", temperature=0.7), tools=[search_tool])writer = Agent( role='Tech Content Strategist', goal='Craft compelling content on tech advancements', backstory="""You are a renowned Content Strategist, known for your insightful and engaging articles. You transform complex concepts into compelling narratives.""", verbose=True, allow_delegation=True)# Create tasks for your agentstask1 = Task( description="""Conduct a comprehensive analysis of the latest advancements in AI in 2024. Identify key trends, breakthrough technologies, and potential industry impacts.""", expected_output="Full analysis report in bullet points", agent=researcher)task2 = Task( description="""Using the insights provided, develop an engaging blog post that highlights the most significant AI advancements. Your post should be informative yet accessible, catering to a tech-savvy audience. Make it sound cool, avoid complex words so it doesn't sound like AI.""", expected_output="Full blog post of at least 4 paragraphs", agent=writer)# Instantiate your crew with a sequential processcrew = Crew( agents=[researcher, writer], tasks=[task1, task2], verbose=False, process = Process.sequential)# Get your crew to work!result = crew.kickoff()print("######################")print(result)
```

You should see a trace in your LangSmith project that looks like this:
![](https://docs.smith.langchain.com/assets/images/trace_arize-5a2a843e88f09fede7b022fef0c671ef.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/trace_with_opentelemetry%3E).
[PreviousTrace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)[NextTrace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
  * [Logging Traces with a basic OpenTelemetry client](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#logging-traces-with-a-basic-opentelemetry-client)
    * [1. Installation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#1-installation)
    * [2. Configure your environment](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#2-configure-your-environment)
    * [3. Log a trace](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#3-log-a-trace)
  * [Supported OpenTelemetry Attribute Mapping](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#supported-opentelemetry-attribute-mapping)
  * [Logging Traces with the Traceloop SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#logging-traces-with-the-traceloop-sdk)
    * [1. Installation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#1-installation-1)
    * [2. Configure your environment](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#2-configure-your-environment-1)
    * [3. Initialize the SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#3-initialize-the-sdk)
    * [4. Log a trace](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#4-log-a-trace)
  * [Tracing using the Arize SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#tracing-using-the-arize-sdk)
    * [1. Installation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#1-installation-2)
    * [2. Configure your environment](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#2-configure-your-environment-2)
    * [3. Set up the instrumentor](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#3-set-up-the-instrumentor)
    * [4. Log a trace](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry#4-log-a-trace-1)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
    * [Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Trace with the Vercel AI SDK (JS/TS only)


On this page
# Trace with the Vercel AI SDK (JS/TS only)
You can use LangSmith to trace runs from the [Vercel AI SDK](https://sdk.vercel.ai/docs/introduction) with our built-in `AISDKExporter` OpenTelemetry trace exporter. This guide will walk through an example.
note
The `AISDKExporter` class is only available in `langsmith` JS SDK version `>=0.2.1`.
## 0. Installation[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#0-installation "Direct link to 0. Installation")
Install the Vercel AI SDK. We use their OpenAI integration for the code snippets below, but you can use any of their other options as well.
  * yarn
  * npm
  * pnpm


```
yarn add ai @ai-sdk/openai zod
```

```
npm install ai @ai-sdk/openai zod
```

```
pnpm add ai @ai-sdk/openai zod
```

## 1. Configure your environment[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#1-configure-your-environment "Direct link to 1. Configure your environment")
  * Shell


```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key># This example uses OpenAI, but you can use any LLM provider of choiceexport OPENAI_API_KEY=<your-openai-api-key>
```

## 2. Log a trace[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#2-log-a-trace "Direct link to 2. Log a trace")
### Next.js[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#nextjs "Direct link to Next.js")
First, create a `instrumentation.js` file in your project root. Learn more how to setup OpenTelemetry instrumentation within your Next.js app [here](https://nextjs.org/docs/app/api-reference/file-conventions/instrumentation).
```
import{ registerOTel }from"@vercel/otel";import{ AISDKExporter }from"langsmith/vercel";exportfunctionregister(){registerOTel({  serviceName:"langsmith-vercel-ai-sdk-example",  traceExporter:newAISDKExporter(),});}
```

Afterwards, add the `experimental_telemetry` argument to your AI SDK calls that you want to trace. For convenience, we've included the `AISDKExporter.getSettings()` method which appends additional metadata for LangSmith.
```
import{ AISDKExporter }from"langsmith/vercel";import{ streamText }from"ai";import{ openai }from"@ai-sdk/openai";awaitstreamText({ model:openai("gpt-4o-mini"), prompt:"Write a vegetarian lasagna recipe for 4 people.", experimental_telemetry: AISDKExporter.getSettings(),});
```

You should see a trace in your LangSmith dashboard [like this one](https://smith.langchain.com/public/a9d9521a-4f97-4843-b1e2-b87c3a125503/r).
You can also trace runs with tool calls:
```
import{ AISDKExporter }from"langsmith/vercel";import{ generateText, tool }from"ai";import{ openai }from"@ai-sdk/openai";import{ z }from"zod";awaitgenerateText({ model:openai("gpt-4o-mini"), messages:[{   role:"user",   content:"What are my orders and where are they? My user ID is 123",},], tools:{  listOrders:tool({   description:"list all orders",   parameters: z.object({ userId: z.string()}),execute:async({ userId })=>`User ${userId} has the following orders: 1`,}),  viewTrackingInformation:tool({   description:"view tracking information for a specific order",   parameters: z.object({ orderId: z.string()}),execute:async({ orderId })=>`Here is the tracking information for ${orderId}`,}),}, experimental_telemetry: AISDKExporter.getSettings(), maxSteps:10,});
```

Which results in a trace like [this one](https://smith.langchain.com/public/4d3add36-756d-4c8c-845d-4ad701a315bb/r).
### Node.js[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#nodejs "Direct link to Node.js")
First, learn more how to setup OpenTelemetry instrumentation within your Node.js app [here](https://opentelemetry.io/docs/languages/js/getting-started/nodejs/#setup).
In particular, you will need to ensure that OTEL setup and configuration run before your application logic. One tool commonly used for this task are Node's [`--require`](https://nodejs.org/api/cli.html#-r---require-module) or `--import` flags.
Add the `AISDKExporter` to the trace exporter to your OpenTelemetry setup.
```
import{ AISDKExporter }from"langsmith/vercel";import{ NodeSDK }from"@opentelemetry/sdk-node";import{ getNodeAutoInstrumentations }from"@opentelemetry/auto-instrumentations-node";const sdk =newNodeSDK({ traceExporter:newAISDKExporter(), instrumentations:[getNodeAutoInstrumentations()],});sdk.start();
```

Afterwards, add the `experimental_telemetry` argument to your AI SDK calls that you want to trace.
info
Do not forget to call `await sdk.shutdown()` before your application shuts down in order to flush any remaining traces to LangSmith.
```
import{ generateText }from"ai";import{ openai }from"@ai-sdk/openai";import{ AISDKExporter }from"langsmith/vercel";const result =awaitgenerateText({ model:openai("gpt-4o-mini"), prompt:"Write a vegetarian lasagna recipe for 4 people.", experimental_telemetry: AISDKExporter.getSettings(),});await sdk.shutdown();
```

### Sentry[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#sentry "Direct link to Sentry")
If you're using Sentry, you can attach the LangSmith trace exporter to Sentry's default OpenTelemetry instrumentation as follows:
```
import*as Sentry from"@sentry/node";import{ BatchSpanProcessor }from"@opentelemetry/sdk-trace-base";import{ AISDKExporter }from"langsmith/vercel";const client = Sentry.init({ dsn:"[Sentry DSN]", tracesSampleRate:1.0,});client?.traceProvider?.addSpanProcessor(newBatchSpanProcessor(newAISDKExporter()));
```

Alternatively, you can use your existing OpenTelemetry setup by setting `skipOpenTelemetrySetup: true` in your `Sentry.init()` call. In this case, we recommend following the official [Sentry OpenTelemetry Setup documentation](https://docs.sentry.io/platforms/javascript/guides/node/opentelemetry/custom-setup/).
### Cloudflare Workers[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#cloudflare-workers "Direct link to Cloudflare Workers")
To instrument AI SDK calls within Cloudflare Workers, you can use the `AISDKExporter` with `@microlabs/otel-cf-workers`. See the documentation for [`otel-cf-workers` here](https://github.com/evanderkoogh/otel-cf-workers).
```
import{ Client }from"langsmith";import{ instrument }from"@microlabs/otel-cf-workers";import{ AISDKExporter }from"langsmith/vercel";import{ createOpenAI }from"@ai-sdk/openai";import{ generateText }from"ai";interfaceEnv{OPENAI_API_KEY:string;LANGSMITH_TRACING:string;LANGSMITH_ENDPOINT:string;LANGSMITH_API_KEY:string;}const handler ={asyncfetch(request, env):Promise<Response>{const openai =createOpenAI({ apiKey: env.OPENAI_API_KEY});const model =openai("gpt-4o-mini");const response =awaitgenerateText({   model,   prompt:"Tell me a joke",   experimental_telemetry: AISDKExporter.getSettings({// As `process.env.LANGSMITH_TRACING` is undefined in Cloudflare Workers,// we need to check the environment variable directly.    isEnabled: env.LANGSMITH_TRACING==="true",}),});returnnewResponse(response.text);},} satisfies ExportedHandler<Env>;exportdefaultinstrument<Env,unknown,unknown>(handler,(env)=>({ exporter:newAISDKExporter({  client:newClient({// Batching is handled by OTEL by default, we need to// disable LangSmith batch tracing to avoid losing traces   autoBatchTracing:false,   apiKey: env.LANGSMITH_API_KEY,   apiUrl: env.LANGSMITH_ENDPOINT,}),}), service:{ name:"ai-sdk-service"},}));
```

You should see a trace in your LangSmith dashboard [like this one](https://smith.langchain.com/public/a9d9521a-4f97-4843-b1e2-b87c3a125503/r).
## Customize run name[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#customize-run-name "Direct link to Customize run name")
You can customize the run name by passing the `runName` argument to the `AISDKExporter.getSettings()` method.
```
import{ AISDKExporter }from"langsmith/vercel";awaitgenerateText({ model:openai("gpt-4o-mini"), prompt:"Write a vegetarian lasagna recipe for 4 people.", experimental_telemetry: AISDKExporter.getSettings({  runName:"my-custom-run-name",}),});
```

## Customize run ID[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#customize-run-id "Direct link to Customize run ID")
You can customize the run ID by passing the `runId` argument to the `AISDKExporter.getSettings()` method. This is especially useful if you want to know the run ID before the run has been completed. Note that the run ID has to be a valid UUID.
```
import{ AISDKExporter }from"langsmith/vercel";import{ v4 as uuidv4 }from"uuid";awaitgenerateText({ model:openai("gpt-4o-mini"), prompt:"Write a vegetarian lasagna recipe for 4 people.", experimental_telemetry: AISDKExporter.getSettings({  runId:uuidv4(),}),});
```

## Nesting runs[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#nesting-runs "Direct link to Nesting runs")
You can also nest runs within other traced functions to create a hierarchy of associated runs. Here's an example using the [`traceable`](https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code#use-traceable--traceable) method:
```
import{ AISDKExporter }from"langsmith/vercel";import{ openai }from"@ai-sdk/openai";import{ generateText }from"ai";import{ traceable }from"langsmith/traceable";const wrappedGenerateText =traceable(async(content:string)=>{const{ text }=awaitgenerateText({   model:openai("gpt-4o-mini"),   messages:[{ role:"user", content }],   experimental_telemetry: AISDKExporter.getSettings(),});const reverseText =traceable(async(text:string)=>{return text.split("").reverse().join("");},{ name:"reverseText"});const reversedText =awaitreverseText(text);return{ text, reversedText };},{ name:"parentTraceable"});const result =awaitwrappedGenerateText("What color is the sky? Respond with one word.");
```

The resulting trace will look like [this one](https://smith.langchain.com/public/c0466ed5-3932-4140-83b1-cf11e998fa6a/r).
## Custom LangSmith client[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#custom-langsmith-client "Direct link to Custom LangSmith client")
You can also pass a LangSmith client instance into the `AISDKExporter` constructor:
```
import{ AISDKExporter }from"langsmith/vercel";import{ Client }from"langsmith";import{ NodeSDK }from"@opentelemetry/sdk-node";import{ getNodeAutoInstrumentations }from"@opentelemetry/auto-instrumentations-node";const langsmithClient =newClient({});const sdk =newNodeSDK({ traceExporter:newAISDKExporter({ client: langsmithClient }), instrumentations:[getNodeAutoInstrumentations()],});sdk.start();awaitgenerateText({ model:openai("gpt-4o-mini"), prompt:"Write a vegetarian lasagna recipe for 4 people.", experimental_telemetry: AISDKExporter.getSettings(),});
```

## Debugging Exporter[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#debugging-exporter "Direct link to Debugging Exporter")
You can enable debug logs for the `AISDKExporter` by passing the `debug` argument to the constructor.
```
const traceExporter =newAISDKExporter({ debug:true});
```

Alternatively, you can set the `OTEL_LOG_LEVEL=DEBUG` environment variable to enable debug logs for the exporter as well as the rest of the OpenTelemetry stack.
## Adding metadata[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#adding-metadata "Direct link to Adding metadata")
You can add metadata to your traces to help organize and filter them in the LangSmith UI:
```
import{ AISDKExporter }from"langsmith/vercel";import{ generateText }from"ai";import{ openai }from"@ai-sdk/openai";awaitgenerateText({ model:openai("gpt-4o-mini"), prompt:"Write a vegetarian lasagna recipe for 4 people.", experimental_telemetry: AISDKExporter.getSettings({  metadata:{ userId:"123", language:"english"},}),});
```

Metadata will be visible in your LangSmith dashboard and can be used to filter and search for specific traces.
## `wrapAISDKModel` (deprecated)[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#wrapaisdkmodel-deprecated "Direct link to wrapaisdkmodel-deprecated")
note
The `wrapAISDKModel` method is deprecated and will be removed in a future release.
The `wrapAISDKModel` method wraps the Vercel model wrapper and intercept model invocation to send traces to LangSmith. This method is useful if you are using an older version of LangSmith or if you are using `streamUI` / Vercel AI RSC, which currently does not support `experimental_telemetry`.
```
import{ wrapAISDKModel }from"langsmith/wrappers/vercel";import{ openai }from"@ai-sdk/openai";import{ generateText }from"ai";const vercelModel =openai("gpt-4o-mini");const modelWithTracing =wrapAISDKModel(vercelModel);awaitgenerateText({ model: modelWithTracing, prompt:"Write a vegetarian lasagna recipe for 4 people.",});
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/how_to_guides/trace_with_vercel_ai_sdk%3E).
[PreviousTrace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)[NextTrace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
  * [0. Installation](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#0-installation)
  * [1. Configure your environment](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#1-configure-your-environment)
  * [2. Log a trace](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#2-log-a-trace)
    * [Next.js](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#nextjs)
    * [Node.js](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#nodejs)
    * [Sentry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#sentry)
    * [Cloudflare Workers](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#cloudflare-workers)
  * [Customize run name](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#customize-run-name)
  * [Customize run ID](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#customize-run-id)
  * [Nesting runs](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#nesting-runs)
  * [Custom LangSmith client](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#custom-langsmith-client)
  * [Debugging Exporter](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#debugging-exporter)
  * [Adding metadata](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#adding-metadata)
  * [`wrapAISDKModel` (deprecated)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk#wrapaisdkmodel-deprecated)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Trace without setting environment variables


# Trace without setting environment variables
As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:
  * `LANGSMITH_TRACING`
  * `LANGSMITH_API_KEY`
  * `LANGSMITH_ENDPOINT`
  * `LANGSMITH_PROJECT`


In some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.
Recently changed behavior
Due to a number of asks for finer-grained control of tracing using the `trace` context manager, **we changed the behavior** of `with trace` to honor the `LANGSMITH_TRACING` environment variable in version **0.1.95** of the Python SDK. You can find more details in the [release notes](https://github.com/langchain-ai/langsmith-sdk/releases/tag/v0.1.95). The recommended way to disable/enable tracing without setting environment variables is to use the `with tracing_context` context manager, as shown in the example below.
  * Python
  * TypeScript


The recommended way to do this in Python is to use the `tracing_context` context manager. This works for both code annotated with `traceable` and code within the `trace` context manager.
```
import openaifrom langsmith import Client, tracing_context, traceablefrom langsmith.wrappers import wrap_openailangsmith_client = Client( api_key="YOUR_LANGSMITH_API_KEY",# This can be retrieved from a secrets manager api_url="https://api.smith.langchain.com",# Update appropriately for self-hosted installations or the EU region)client = wrap_openai(openai.Client())@traceable(run_type="tool", name="Retrieve Context")defmy_tool(question:str)->str:return"During this morning's meeting, we solved all world conflict."@traceabledefchat_pipeline(question:str): context = my_tool(question) messages =[{"role":"system","content":"You are a helpful assistant. Please respond to the user's request only based on the given context."},{"role":"user","content": f"Question:{question}Context:{context}"}] chat_completion = client.chat.completions.create(   model="gpt-4o-mini", messages=messages)return chat_completion.choices[0].message.content# Can set to False to disable tracing here without changing code structurewith tracing_context(enabled=True):# Use langsmith_extra to pass in a custom client chat_pipeline("Can you summarize this morning's meetings?", langsmith_extra={"client": langsmith_client})
```

In TypeScript, you can pass in both the client and the `tracingEnabled` flag to the `traceable` decorator.
```
import{ Client }from"langsmith";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers";import{ OpenAI }from"openai";const client =newClient({ apiKey:"YOUR_API_KEY",// This can be retrieved from a secrets manager apiUrl:"https://api.smith.langchain.com",// Update appropriately for self-hosted installations or the EU region});const openai =wrapOpenAI(newOpenAI());const tool =traceable((question:string)=>{return"During this morning's meeting, we solved all world conflict.";},{ name:"Retrieve Context", runType:"tool"});const pipeline =traceable(async(question:string)=>{const context =awaittool(question);const completion =await openai.chat.completions.create({   model:"gpt-4o-mini",   messages:[{     role:"system"asconst,     content:"You are a helpful assistant. Please respond to the user's request only based on the given context.",},{     role:"user"asconst,     content:`Question: ${question}\nContext: ${context}`,},],});return completion.choices[0].message.content;},{ name:"Chat", client, tracingEnabled:true});awaitpipeline("Can you summarize this morning's meetings?");
```

If you prefer a video tutorial, check out the [Alternative Ways to Trace video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousTrace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)[NextTrace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Upload files with traces


# Upload files with traces
Recommended Reading
Before diving into this content, it would be helpful to read the following guides:
  * [Trace with LangSmith using the traceable decorator or wrapper](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#use-traceable--traceable)


Minimum SDK Versions
The following features are available in the following SDK versions:
  * Python SDK: >=0.1.141
  * JS/TS SDK: >=0.2.5


LangSmith supports uploading binary files (such as images, audio, videos, PDFs, and CSVs) with your traces. This is particularly useful when working with LLM pipelines using multimodal inputs or outputs.
In both the Python and TypeScript SDKs, attachments can be added to your traces by specifying the MIME type and binary content of each file. This guide explains how to define and trace attachments using the `Attachment` type in Python and `Uint8Array` / `ArrayBuffer` in TypeScript.
  * Python
  * TypeScript


In the Python SDK, you can use the `Attachment` type to add files to your traces. Each `Attachment` requires:
  * `mime_type` (str): The MIME type of the file (e.g., `"image/png"`).
  * `data` (bytes | Path): The binary content of the file, or the file path.


You can also define an attachment with a tuple tuple of the form `(mime_type, data)` for convenience. 
Simply decorate a function with `@traceable` and include your `Attachment` instances as arguments. Note that to use the file path instead of the raw bytes, you need to set the `dangerously_allow_filesystem` flag to `True` in your traceable decorator.
```
from langsmith import traceablefrom langsmith.schemas import Attachmentfrom pathlib import Pathimport os# Must set dangerously_allow_filesystem to True if you want to use file paths@traceable(dangerously_allow_filesystem=True)deftrace_with_attachments( val:int, text:str, image: Attachment, audio: Attachment, video: Attachment, pdf: Attachment, csv: Attachment,):returnf"Processed: {val}, {text}, {len(image.data)}, {len(audio.data)}, {len(video.data)}, {len(pdf.data),{len(csv.data)}}"# Helper function to load files as bytesdefload_file(file_path:str)->bytes:withopen(file_path,"rb")as f:return f.read()# Load files and create attachmentsimage_data = load_file("my_image.png")audio_data = load_file("my_mp3.mp3")video_data = load_file("my_video.mp4")pdf_data = load_file("my_document.pdf")image_attachment = Attachment(mime_type="image/png", data=image_data)audio_attachment = Attachment(mime_type="audio/mpeg", data=audio_data)video_attachment = Attachment(mime_type="video/mp4", data=video_data)pdf_attachment =("application/pdf", pdf_data)# Can just define as tuple of (mime_type, data)csv_attachment = Attachment(mime_type="text/csv", data=Path(os.getcwd())/"my_csv.csv")# Define other parametersval =42text ="Hello, world!"# Call the function with traced attachmentsresult = trace_with_attachments(val=val,text=text,image=image_attachment,audio=audio_attachment,video=video_attachment,pdf=pdf_attachment,csv=csv_attachment,)
```

In the TypeScript SDK, you can add attachments to traces by using `Uint8Array` or `ArrayBuffer` as data types. Each attachment's MIME type is specified within `extractAttachments`:
  * `Uint8Array`: Useful for handling binary data directly.
  * `ArrayBuffer`: Represents fixed-length binary data, which can be converted to `Uint8Array` as needed.


Wrap your function with `traceable` and include your attachments within the `extractAttachments` option.
In the TypeScript SDK, the `extractAttachments` function is an optional parameter in the `traceable` configuration. When the traceable-wrapped function is invoked, it extracts binary data (e.g., images, audio files) from your inputs and logs them alongside other trace data, specifying their MIME types.
Note that you cannot directly pass in a file path in the TypeScript SDK, as accessing local files is not supported in all runtime environments.
```
type AttachmentData = Uint8Array | ArrayBuffer;
type Attachments = Record<string, [string, AttachmentData]>;
extractAttachments?: (
...args: Parameters<Func>
) => [Attachments | undefined, KVMap];

```

```
import{ traceable }from"langsmith/traceable";const traceableWithAttachments =traceable((val:number,text:string,attachment: Uint8Array,attachment2: ArrayBuffer,attachment3: Uint8Array,attachment4: ArrayBuffer,attachment5: Uint8Array,)=>`Processed: ${val}, ${text}, ${attachment.length}, ${attachment2.byteLength}, ${attachment3.length}, ${attachment4.byteLength}, ${attachment5.byteLength}`,{name:"traceWithAttachments",extractAttachments:(val:number,text:string,attachment: Uint8Array,attachment2: ArrayBuffer,attachment3: Uint8Array,attachment4: ArrayBuffer,attachment5: Uint8Array,)=>[{"image inputs":["image/png", attachment],"mp3 inputs":["audio/mpeg",newUint8Array(attachment2)],"video inputs":["video/mp4", attachment3],"pdf inputs":["application/pdf",newUint8Array(attachment4)],"csv inputs":["text/csv",newUint8Array(attachment5)]},{ val, text },],});const fs = Deno // or Node.js fs moduleconst image =await fs.readFile("my_image.png");// Uint8Arrayconst mp3Buffer =await fs.readFile("my_mp3.mp3");const mp3ArrayBuffer = mp3Buffer.buffer;// Convert to ArrayBufferconst video =await fs.readFile("my_video.mp4");// Uint8Arrayconst pdfBuffer =await fs.readFile("my_document.pdf");const pdfArrayBuffer = pdfBuffer.buffer;// Convert to ArrayBufferconst csv =await fs.readFile("test-vals.csv");// Uint8Array// Define example parametersconst val =42;const text ="Hello, world!";// Call traceableWithAttachments with the filesconst result =awaittraceableWithAttachments(val, text, image, mp3ArrayBuffer, video, pdfArrayBuffer, csv);
```

Here is how the above would look in the LangSmith UI. You can expand each attachment to view its contents. ![](https://docs.smith.langchain.com/assets/images/trace_with_attachments-fcee2de7720714915d3d264b61f2064d.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousFilter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)[NextUse monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Use monitoring charts


On this page
# Use monitoring charts
LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the `Monitor` tab within a particular project.
## Change the time period[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts#change-the-time-period "Direct link to Change the time period")
You can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days.
## Slice data by metadata or tag[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts#slice-data-by-metadata-or-tag "Direct link to Slice data by metadata or tag")
By default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs. This can be useful to compare how two different prompts or models are performing.
In order to do this, you first need to make sure you are [attaching appropriate tags or metadata](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags) to these runs when logging them. After that, you can click the `Tag` or `Metadata` tab at the top to group runs accordingly.
![Subsets Monitor](https://docs.smith.langchain.com/assets/images/subsets_monitor-b35c71b35135007bff93d619bb765a6a.png)
## Drill down into specific subsets[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts#drill-down-into-specific-subsets "Direct link to Drill down into specific subsets")
Monitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard.
From there, you will be brought back to the `Traces` tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into.
![Drill Monitor](https://docs.smith.langchain.com/assets/images/drill_monitor-72788343ba315878ab45312b03b53836.png)
If you prefer a video tutorial, check out the [Monitoring video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousUpload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)[NextDashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
  * [Change the time period](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts#change-the-time-period)
  * [Slice data by metadata or tag](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts#slice-data-by-metadata-or-tag)
  * [Drill down into specific subsets](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts#drill-down-into-specific-subsets)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/how_to_guides/webhooks

[Skip to main content](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * Set up webhook notifications for rules


On this page
# Set up webhook notifications for rules
When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs.
![](https://docs.smith.langchain.com/assets/images/webhook-62ab900a1a3806c8513ce6ad3147b2aa.png)
## Webhook payload[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#webhook-payload "Direct link to Webhook payload")
The payload we send to your webhook endpoint contains
  * `"rule_id"` this is the ID of the automation that sent this payload
  * `"start_time"` and `"end_time"` these are the time boundaries where we found matching runs
  * `"runs"` this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.
  * `"feedback_stats"` this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below.


```
"feedback_stats":{"about_langchain":{"n":1,"avg":0.0,"show_feedback_arrow":true,"values":{}},"category":{"n":0,"avg":null,"show_feedback_arrow":true,"values":{"CONCEPTUAL":1}},"user_score":{"n":2,"avg":0.0,"show_feedback_arrow":false,"values":{}},"vagueness":{"n":1,"avg":0.0,"show_feedback_arrow":true,"values":{}}},
```

fetching from S3 URLs
Depending on how recent your runs are, the `inputs_s3_urls` and `outputs_s3_urls` fields may contain S3 URLs to the actual data instead of the data itself.
The `inputs` and `outputs` can be fetched by the `ROOT.presigned_url` provided in `inputs_s3_urls` and `outputs_s3_urls` respectively.
This is an example of the entire payload we send to your webhook endpoint:
```
{"rule_id":"d75d7417-0c57-4655-88fe-1db3cda3a47a","start_time":"2024-04-05T01:28:54.734491+00:00","end_time":"2024-04-05T01:28:56.492563+00:00","runs":[{"status":"success","is_root":true,"trace_id":"6ab80f10-d79c-4fa2-b441-922ed6feb630","dotted_order":"20230505T051324571809Z6ab80f10-d79c-4fa2-b441-922ed6feb630","run_type":"tool","modified_at":"2024-04-05T01:28:54.145062","tenant_id":"2ebda79f-2946-4491-a9ad-d642f49e0815","end_time":"2024-04-05T01:28:54.085649","name":"Search","start_time":"2024-04-05T01:28:54.085646","id":"6ab80f10-d79c-4fa2-b441-922ed6feb630","session_id":"6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5","parent_run_ids":[],"child_run_ids":null,"direct_child_run_ids":null,"total_tokens":0,"completion_tokens":0,"prompt_tokens":0,"total_cost":null,"completion_cost":null,"prompt_cost":null,"first_token_time":null,"app_path":"/o/2ebda79f-2946-4491-a9ad-d642f49e0815/projects/p/6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5/r/6ab80f10-d79c-4fa2-b441-922ed6feb630?trace_id=6ab80f10-d79c-4fa2-b441-922ed6feb630&start_time=2023-05-05T05:13:24.571809","in_dataset":false,"last_queued_at":null,"inputs":null,"inputs_s3_urls":null,"outputs":null,"outputs_s3_urls":null,"extra":null,"events":null,"feedback_stats":null,"serialized":null,"share_token":null}]}
```

### Webhook Security[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#webhook-security "Direct link to Webhook Security")
We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications.
An example would be
```
https://api.example.com/langsmith_webhook?secret=38ee77617c3a489ab6e871fbeb2ec87d
```

### Webhook custom HTTP headers[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#webhook-custom-http-headers "Direct link to Webhook custom HTTP headers")
If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the `Headers` option next to the URL field and add your headers.
note
Headers are stored in encrypted format.
![](https://docs.smith.langchain.com/assets/images/webhook_headers-7ce2425dfc9201bf7fd233eedf033526.png)
### Webhook Delivery[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#webhook-delivery "Direct link to Webhook Delivery")
When delivering events to your webhook endpoint we follow these guidelines
  * If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.
  * If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .
  * If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.
  * If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.
  * Anything your endpoint returns in the body will be ignored


## Example with Modal[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#example-with-modal "Direct link to Example with Modal")
### Setup[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#setup "Direct link to Setup")
For an example of how to set this up, we will use [Modal](https://modal.com/). Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here.
First, create a Modal account. Then, locally install the Modal SDK:
```
pip install modal
```

To finish setting up your account, run the command:
```
modal setup
```

and follow the instructions
### Secrets[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#secrets "Direct link to Secrets")
Next, you will need to set up some secrets in Modal.
First, LangSmith will need to authenticate to Modal by passing in a secret. The easiest way to do this is to pass in a secret in the query parameters. To validate this secret, we will need to add a secret in _Modal_ to validate it. We will do that by creating a Modal secret. You can see instructions for secrets [here](https://modal.com/docs/guide/secrets). For this purpose, let's call our secret `ls-webhook` and have it set an environment variable with the name `LS_WEBHOOK`.
We can also set up a LangSmith secret - luckily there is already an integration template for this!
![LangSmith Modal Template](https://docs.smith.langchain.com/assets/images/modal_langsmith_secret-423427ddebfb97eae4269687372b3b70.png)
### Service[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#service "Direct link to Service")
After that, you can create a Python file that will serve as your endpoint. An example is below, with comments explaining what is going on:
```
from fastapi import HTTPException, status, Request, Queryfrom modal import Secret, Stub, web_endpoint, Imagestub = Stub("auth-example", image=Image.debian_slim().pip_install("langsmith"))@stub.function(  secrets=[Secret.from_name("ls-webhook"), Secret.from_name("my-langsmith-secret")])# We want this to be a `POST` endpoint since we will post data here@web_endpoint(method="POST")# We set up a `secret` query parameterdeff(data:dict, secret:str= Query(...)):# You can import dependencies you don't have locally inside Modal functionsfrom langsmith import Client# First, we validate the secret key we passimport osif secret != os.environ["LS_WEBHOOK"]:raise HTTPException(      status_code=status.HTTP_401_UNAUTHORIZED,      detail="Incorrect bearer token",      headers={"WWW-Authenticate":"Bearer"},)# This is where we put the logic for what should happen inside this webhook  ls_client = Client()  runs = data["runs"]  ids =[r["id"]for r in runs]  feedback =list(ls_client.list_feedback(run_ids=ids))for r, f inzip(runs, feedback):try:      ls_client.create_example(        inputs=r["inputs"],        outputs={"output": f.correction},        dataset_name="classifier-github-issues",)except Exception:raise ValueError(f"{r} and {f}")# Function bodyreturn"success!"
```

We can now deploy this easily with `modal deploy ...` (see docs [here](https://modal.com/docs/guide/managing-deployments)).
You should now get something like:
```
‚úì Created objects.‚îú‚îÄ‚îÄ üî® Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py‚îú‚îÄ‚îÄ üî® Created mount PythonPackage:langsmith‚îî‚îÄ‚îÄ üî® Created f => https://hwchase17--auth-example-f.modal.run‚úì App deployed! üéâView Deployment: https://modal.com/apps/hwchase17/auth-example
```

The important thing to remember is `https://hwchase17--auth-example-f.modal.run` - the function we created to run. NOTE: this is NOT the final deployment URL, make sure not to accidentally use that.
### Hooking it up[‚Äã](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#hooking-it-up "Direct link to Hooking it up")
We can now take the function URL we create above and add it as a webhook. We have to remember to also pass in the secret key as a query parameter. Putting it all together, it should look something like:
```
https://hwchase17--auth-example-f-dev.modal.run?secret={SECRET}
```

Replace `{SECRET}` with the secret key you created to access the Modal service.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousTrace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)[NextConceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Webhook payload](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#webhook-payload)
    * [Webhook Security](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#webhook-security)
    * [Webhook custom HTTP headers](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#webhook-custom-http-headers)
    * [Webhook Delivery](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#webhook-delivery)
  * [Example with Modal](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#example-with-modal)
    * [Setup](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#setup)
    * [Secrets](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#secrets)
    * [Service](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#service)
    * [Hooking it up](https://docs.smith.langchain.com/observability/how_to_guides/webhooks#hooking-it-up)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/tutorials

[Skip to main content](https://docs.smith.langchain.com/observability/tutorials#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/tutorials)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/observability/tutorials)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/observability/tutorials)
    * [Evaluation](https://docs.smith.langchain.com/observability/tutorials)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/observability/tutorials)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * Tutorials


# Observability tutorials
New to LangSmith or to LLM app development in general? Read this material to quickly get up and running.
  * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/observability/tutorials%3E).
[PreviousQuick Start](https://docs.smith.langchain.com/observability)[NextAdd observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/observability/tutorials/observability

[Skip to main content](https://docs.smith.langchain.com/observability/tutorials/observability#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/observability/tutorials/observability)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
    * [Quick Start](https://docs.smith.langchain.com/observability)
    * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
      * [Add observability to your LLM application](https://docs.smith.langchain.com/observability/tutorials/observability)
    * [How-to Guides](https://docs.smith.langchain.com/observability/how_to_guides)
      * [Annotate code for tracing](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code)
      * [Filter traces in the application](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application)
      * [Upload files with traces](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces)
      * [Use monitoring charts](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts)
      * [Dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards)
      * [Log traces to specific project](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project)
      * [Set up automation rules](https://docs.smith.langchain.com/observability/how_to_guides/rules)
      * [Online Evaluation](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)
      * [Set a sampling rate for traces](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces)
      * [Add metadata and tags to traces](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags)
      * [Implement distributed tracing](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing)
      * [Access the current run (span) within a traced function](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span)
      * [Log multimodal traces](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces)
      * [Log retriever traces](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace)
      * [Log custom LLM traces](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace)
      * [Prevent logging of sensitive data in traces](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs)
      * [Query traces](https://docs.smith.langchain.com/observability/how_to_guides/export_traces)
      * [Share or unshare a trace publicly](https://docs.smith.langchain.com/observability/how_to_guides/share_trace)
      * [Compare traces](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces)
      * [Trace generator functions](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions)
      * [Trace with LangChain (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain)
      * [Trace with LangGraph (Python and JS/TS)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)
      * [Trace with Instructor (Python only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor)
      * [Trace with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry)
      * [Trace with the Vercel AI SDK (JS/TS only)](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk)
      * [Trace without setting environment variables](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars)
      * [Trace using the LangSmith REST API](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api)
      * [Trace with OpenAI Agents SDK](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)
      * [Calculate token-based costs for traces](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs)
      * [Troubleshoot trace nesting](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces)
      * [[Beta] Bulk Exporting Trace Data](https://docs.smith.langchain.com/observability/how_to_guides/data_export)
      * [Alerts in LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/alerts)
      * [Configuring PagerDuty Integration for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_pagerduty)
      * [Configuring Webhook Notifications for LangSmith Alerts](https://docs.smith.langchain.com/observability/how_to_guides/alerts_webhook)
      * [How to print detailed logs (Python SDK)](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs)
      * [Trace JS functions in serverless environments](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments)
      * [Set up threads](https://docs.smith.langchain.com/observability/how_to_guides/threads)
      * [Toubleshooting variable caching](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching)
      * [Trace LangChain with OpenTelemetry](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel)
      * [Set up webhook notifications for rules](https://docs.smith.langchain.com/observability/how_to_guides/webhooks)
    * [Conceptual Guide](https://docs.smith.langchain.com/observability/concepts)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Tutorials](https://docs.smith.langchain.com/observability/tutorials)
  * Add observability to your LLM application


On this page
# Add observability to your LLM application
Observability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.
Luckily, this is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights into your application.
Note that observability is important throughout all stages of application development - from prototyping, to beta testing, to production. There are different considerations at all stages, but they are all intricately tied together. In this tutorial we walk through the natural progression.
Let's assume that we're building a simple RAG application using the OpenAI SDK. The simple application we're adding observability to looks like:
  * Python
  * TypeScript


```
from openai import OpenAIopenai_client = OpenAI()# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdefretriever(query:str):  results =["Harrison worked at Kensho"]return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdefrag(question):  docs = retriever(question)  system_message ="""Answer the users question using only the provided information below:  {docs}""".format(docs="\n".join(docs))return openai_client.chat.completions.create(    messages=[{"role":"system","content": system_message},{"role":"user","content": question},],    model="gpt-4o-mini",)
```

```
import{ OpenAI }from"openai";const openAIClient =newOpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasyncfunctionretriever(query:string){return["This is a document"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasyncfunctionrag(question:string){const docs =awaitretriever(question);const systemMessage ="Answer the users question using only the provided information below:\n\n"+  docs.join("\n");returnawait openAIClient.chat.completions.create({  messages:[{ role:"system", content: systemMessage },{ role:"user", content: question },],  model:"gpt-4o-mini",});}
```

## Prototyping[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#prototyping "Direct link to Prototyping")
Having observability set up from the start can you help iterate **much** more quickly than you would otherwise be able to. It allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using. In this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping.
### Set up your environment[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#set-up-your-environment "Direct link to Set up your environment")
First, create an API key by navigating to the [settings page](https://smith.langchain.com/settings).
Next, install the LangSmith SDK:
  * Python SDK
  * TypeScript SDK


```
pip install langsmith
```

```
npm install langsmith
```

Finally, set up the appropriate environment variables. This will log traces to the `default` project (though you can easily change that).
```
export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key>export LANGSMITH_PROJECT=default
```

note
You may see these variables referenced as `LANGCHAIN_*` in other places. These are all equivalent, however the best practice is to use `LANGSMITH_TRACING`, `LANGSMITH_API_KEY`, `LANGSMITH_PROJECT`.
The `LANGSMITH_PROJECT` flag is only supported in JS SDK versions >= 0.2.16, use `LANGCHAIN_PROJECT` instead if you are using an older version.
### Trace your LLM calls[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#trace-your-llm-calls "Direct link to Trace your LLM calls")
The first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like:
  * Python
  * TypeScript


```
from openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdefretriever(query:str):  results =["Harrison worked at Kensho"]return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdefrag(question):  docs = retriever(question)  system_message ="""Answer the users question using only the provided information below:  {docs}""".format(docs="\n".join(docs))return openai_client.chat.completions.create(    messages=[{"role":"system","content": system_message},{"role":"user","content": question},],    model="gpt-4o-mini",)
```

```
import{ OpenAI }from"openai";import{ wrapOpenAI }from"langsmith/wrappers";const openAIClient =wrapOpenAI(newOpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasyncfunctionretriever(query:string){return["This is a document"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasyncfunctionrag(question:string){const docs =awaitretriever(question);const systemMessage ="Answer the users question using only the provided information below:\n\n"+  docs.join("\n");returnawait openAIClient.chat.completions.create({  messages:[{ role:"system", content: systemMessage },{ role:"user", content: question },],  model:"gpt-4o-mini",});}
```

Notice how we import `from langsmith.wrappers import wrap_openai` and use it to wrap the OpenAI client (`openai_client = wrap_openai(OpenAI())`).
What happens if you call it in the following way?
```
rag("where did harrison work")
```

This will produce a trace of just the OpenAI call - it should look something like [this](https://smith.langchain.com/public/e7b7d256-10fe-4d49-a8d5-36ca8e5af0d2/r)
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_openai-667b87c0df7e5bd45538c165314d7e22.png)
### Trace the whole chain[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#trace-the-whole-chain "Direct link to Trace the whole chain")
Great - we've traced the LLM call. But it's often very informative to trace more than that. LangSmith is **built** for tracing the entire LLM pipeline - so let's do that! We can do this by modifying the code to now look something like this:
  * Python
  * TypeScript


```
from openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())defretriever(query:str):  results =["Harrison worked at Kensho"]return results@traceabledefrag(question):  docs = retriever(question)  system_message ="""Answer the users question using only the provided information below:  {docs}""".format(docs="\n".join(docs))return openai_client.chat.completions.create(    messages=[{"role":"system","content": system_message},{"role":"user","content": question},],    model="gpt-4o-mini",)
```

```
import{ OpenAI }from"openai";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers";const openAIClient =wrapOpenAI(newOpenAI());asyncfunctionretriever(query:string){return["This is a document"];}const rag =traceable(asyncfunctionrag(question:string){const docs =awaitretriever(question);const systemMessage ="Answer the users question using only the provided information below:\n\n"+  docs.join("\n");returnawait openAIClient.chat.completions.create({  messages:[{ role:"system", content: systemMessage },{ role:"user", content: question },],  model:"gpt-4o-mini",});});
```

Notice how we import `from langsmith import traceable` and use it decorate the overall function (`@traceable`).
What happens if you call it in the following way?
```
rag("where did harrison work")
```

This will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like [this](https://smith.langchain.com/public/2174f4e9-48ab-4f9e-a8c4-470372d976f1/r)
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_chain-5023f6584725ddccf4052f7fc050977c.png)
### Trace the retrieval step[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#trace-the-retrieval-step "Direct link to Trace the retrieval step")
There's one last part of the application we haven't traced - the retrieval step! Retrieval is a key part of LLM applications, and we've made it easy to log retrieval steps as well. All we have to do is modify our code to look like:
  * Python
  * TypeScript


```
from openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())@traceable(run_type="retriever")defretriever(query:str):  results =["Harrison worked at Kensho"]return results@traceabledefrag(question):  docs = retriever(question)  system_message ="""Answer the users question using only the provided information below:  {docs}""".format(docs="\n".join(docs))return openai_client.chat.completions.create(    messages=[{"role":"system","content": system_message},{"role":"user","content": question},],    model="gpt-4o-mini",)
```

```
import{ OpenAI }from"openai";import{ traceable }from"langsmith/traceable";import{ wrapOpenAI }from"langsmith/wrappers";const openAIClient =wrapOpenAI(newOpenAI());const retriever =traceable(asyncfunctionretriever(query:string){return["This is a document"];},{ run_type:"retriever"})const rag =traceable(asyncfunctionrag(question:string){const docs =awaitretriever(question);const systemMessage ="Answer the users question using only the provided information below:\n\n"+  docs.join("\n");returnawait openAIClient.chat.completions.create({  messages:[{ role:"system", content: systemMessage },{ role:"user", content: question },],  model:"gpt-4o-mini",});});
```

Notice how we import `from langsmith import traceable` and use it decorate the overall function (`@traceable(run_type="retriever")`).
What happens if you call it in the following way?
```
rag("where did harrison work")
```

This will produce a trace of the whole chain including the retrieval step - it should look something like [this](https://smith.langchain.com/public/7b3fe3b1-b287-49ec-9e3b-feecbe460144/r)
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_retriever-5f1b82cc2624110a90b6a7527b4d8845.png)
## Beta Testing[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#beta-testing "Direct link to Beta Testing")
The next stage of LLM application development is beta testing your application. This is when you release it to a few initial users. Having good observability set up here is crucial as often you don't know exactly how users will actually use your application, so this allows you get insights into how they do so. This also means that you probably want to make some changes to your tracing set up to better allow for that. This extends the observability you set up in the previous section
### Collecting Feedback[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#collecting-feedback "Direct link to Collecting Feedback")
A huge part of having good observability during beta testing is collecting feedback. What feedback you collect is often application specific - but at the very least a simple thumbs up/down is a good start. After logging that feedback, you need to be able to easily associate it with the run that caused that. Luckily LangSmith makes it easy to do that.
First, you need to log the feedback from your app. An easy way to do this is to keep track of a run ID for each run, and then use that to log feedback. Keeping track of the run ID would look something like:
```
import uuidrun_id =str(uuid.uuid4())rag("where did harrison work",  langsmith_extra={"run_id": run_id})
```

Associating feedback with that run would look something like:
```
from langsmith import Clientls_client = Client()ls_client.create_feedback(  run_id,  key="user-score",  score=1.0,)
```

Once the feedback is logged, you can then see it associated with each run by clicking into the `Metadata` tab when inspecting the run. It should look something like [this](https://smith.langchain.com/public/8cafba6a-1a6d-4a73-8565-483186f31c29/r)
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_feedback-847a70007d2c66f1aa852b815f4e6f49.png)
You can also query for all runs with positive (or negative) feedback by using the filtering logic in the runs table. You can do this by creating a filter like the following:
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_filtering-80640215eedd16650b606d3d15ce809e.png)
### Logging Metadata[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#logging-metadata "Direct link to Logging Metadata")
It is also a good idea to start logging metadata. This allows you to start keep track of different attributes of your app. This is important in allowing you to know what version or variant of your app was used to produce a given result.
For this example, we will log the LLM used. Oftentimes you may be experimenting with different LLMs, so having that information as metadata can be useful for filtering. In order to do that, we can add it as such:
```
from openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())@traceable(run_type="retriever")defretriever(query:str):  results =["Harrison worked at Kensho"]return results@traceable(metadata={"llm":"gpt-4o-mini"})defrag(question):  docs = retriever(question)  system_message ="""Answer the users question using only the provided information below:  {docs}""".format(docs='\n'.join(docs))return openai_client.chat.completions.create(messages =[{"role":"system","content": system_message},{"role":"user","content": question},], model="gpt-4o-mini")
```

Notice we added `@traceable(metadata={"llm": "gpt-4o-mini"})` to the `rag` function.
Keeping track of metadata in this way assumes that it is known ahead of time. This is fine for LLM types, but less desirable for other types of information - like a User ID. In order to log information that, we can pass it in at run time with the run ID.
```
import uuidrun_id =str(uuid.uuid4())rag("where did harrison work",  langsmith_extra={"run_id": run_id,"metadata":{"user_id":"harrison"}})
```

Now that we've logged these two pieces of metadata, we should be able to see them both show up in the UI [here](https://smith.langchain.com/public/37adf7e5-97aa-42d0-9850-99c0199bddf6/r).
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_metadata-c7033f6f303eaf9fa030cd1973fdf04d.png)
We can filter for these pieces of information by constructing a filter like the following:
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_metadata_filtering-aa19d199babe5bddd3c243c050268be5.png)
## Production[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#production "Direct link to Production")
Great - you've used this newfound observability to iterate quickly and gain confidence that your app is performing well. Time to ship it to production! What new observability do you need to add?
First of all, let's note that the same observability you've already added will keep on providing value in production. You will continue to be able to drill down into particular runs.
In production you likely have a LOT more traffic. So you don't really want to be stuck looking at datapoints one at a time. Luckily, LangSmith has a set of tools to help with observability in production.
### Monitoring[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#monitoring "Direct link to Monitoring")
If you click on the `Monitor` tab in a project, you will see a series of monitoring charts. Here we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc. You can view these over time across a few different time bins.
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_monitor-2b5f65a2804e259db29c5225f3e07e04.png)
### A/B Testing[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#ab-testing "Direct link to A/B Testing")
note
Group-by functionality for A/B testing requires at least 2 different values to exist for a given metadata key.
You can also use this tab to perform a version of A/B Testing. In the previous tutorial we starting tracking a few different metadata attributes - one of which was `llm`. We can group the monitoring charts by ANY metadata attribute, and instantly get grouped charts over time. This allows us to experiment with different LLMs (or prompts, or other) and track their performance over time.
In order to do this, we just need to click on the `Metadata` button at the top. This will give us a drop down of options to choose from to group by:
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_monitor_metadata-f80a110ec91cac0cbc9293871f41c7e1.png)
Once we select this, we will start to see charts grouped by this attribute:
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_monitor_grouped-a2fed9e2e364633d61f875b6b60bf5f8.png)
### Drilldown[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#drilldown "Direct link to Drilldown")
One of the awesome abilities that LangSmith provides is the ability to easily drilldown into datapoints that you identify as problematic while looking at monitoring charts. In order to do this, you can simply hover over a datapoint in the monitoring chart. When you do this, you will be able to click the datapoint. This will lead you back to the runs table with a filtered view:
![](https://docs.smith.langchain.com/assets/images/tracing_tutorial_monitor_drilldown-9a963dac945979f67efea49ab50aaeab.png)
## Conclusion[‚Äã](https://docs.smith.langchain.com/observability/tutorials/observability#conclusion "Direct link to Conclusion")
In this tutorial you saw how to set up your LLM application with best-in-class observability. No matter what stage your application is in, you will still benefit from observability.
If you have more in-depth questions about observability, check out the [how-to section](https://docs.smith.langchain.com/observability/how_to_guides) for guides on topics like testing, prompt management, and more.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousObservability tutorials](https://docs.smith.langchain.com/observability/tutorials)[NextObservability how-to guides](https://docs.smith.langchain.com/observability/how_to_guides)
  * [Prototyping](https://docs.smith.langchain.com/observability/tutorials/observability#prototyping)
    * [Set up your environment](https://docs.smith.langchain.com/observability/tutorials/observability#set-up-your-environment)
    * [Trace your LLM calls](https://docs.smith.langchain.com/observability/tutorials/observability#trace-your-llm-calls)
    * [Trace the whole chain](https://docs.smith.langchain.com/observability/tutorials/observability#trace-the-whole-chain)
    * [Trace the retrieval step](https://docs.smith.langchain.com/observability/tutorials/observability#trace-the-retrieval-step)
  * [Beta Testing](https://docs.smith.langchain.com/observability/tutorials/observability#beta-testing)
    * [Collecting Feedback](https://docs.smith.langchain.com/observability/tutorials/observability#collecting-feedback)
    * [Logging Metadata](https://docs.smith.langchain.com/observability/tutorials/observability#logging-metadata)
  * [Production](https://docs.smith.langchain.com/observability/tutorials/observability#production)
    * [Monitoring](https://docs.smith.langchain.com/observability/tutorials/observability#monitoring)
    * [A/B Testing](https://docs.smith.langchain.com/observability/tutorials/observability#ab-testing)
    * [Drilldown](https://docs.smith.langchain.com/observability/tutorials/observability#drilldown)
  * [Conclusion](https://docs.smith.langchain.com/observability/tutorials/observability#conclusion)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/pricing/faq

[Skip to main content](https://docs.smith.langchain.com/pricing/faq#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/pricing/faq)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
    * [Plans](https://docs.smith.langchain.com/pricing/plans)
    * [FAQ](https://docs.smith.langchain.com/pricing/faq)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/pricing/faq)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/pricing/faq)
    * [Evaluation](https://docs.smith.langchain.com/pricing/faq)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/pricing/faq)


  * [](https://docs.smith.langchain.com/)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * FAQ


On this page
# Frequently Asked Questions
## Questions and Answers[‚Äã](https://docs.smith.langchain.com/pricing/faq#questions-and-answers "Direct link to Questions and Answers")
### I‚Äôve been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?[‚Äã](https://docs.smith.langchain.com/pricing/faq#ive-been-using-langsmith-since-before-pricing-took-effect-for-new-users-when-will-pricing-go-into-effect-for-my-account "Direct link to I‚Äôve been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?")
If you‚Äôve been using LangSmith already, your usage will be billable starting in July 2024. At that point if you want to add seats or use more than the monthly allotment of free traces, you will need to add a credit card to LangSmith or contact sales. If you are interested in the Enterprise plan with higher rate limits and special deployment options, you can learn more or make a purchase by reaching out to sales@langchain.dev.
### Which plan is right for me?[‚Äã](https://docs.smith.langchain.com/pricing/faq#which-plan-is-right-for-me "Direct link to Which plan is right for me?")
If you‚Äôre an individual developer, the Developer plan is a great choice for small projects.
For teams that want to collaborate in LangSmith, check out the Plus plan. **If you are an early-stage startup building an AI application** , you may be eligible for our Startup plan with discounted prices and a generous free monthly trace allotment. Please reach out via our [Startup Contact Form](https://airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form) for more details.
If you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you. Please reach out via our [Sales Contact Form](https://www.langchain.com/contact-sales) for more details.
### What is a seat?[‚Äã](https://docs.smith.langchain.com/pricing/faq#what-is-a-seat "Direct link to What is a seat?")
A seat is a distinct user inside your organization. We consider the total number of users (including invited users) to determine the number of seats to bill.
### What is a trace?[‚Äã](https://docs.smith.langchain.com/pricing/faq#what-is-a-trace "Direct link to What is a trace?")
A trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Here is an [example](https://smith.langchain.com/public/17c24270-9f74-47e7-b70c-d508afc448fa/r) of a single trace.
### What is an ingested event?[‚Äã](https://docs.smith.langchain.com/pricing/faq#what-is-an-ingested-event "Direct link to What is an ingested event?")
An ingested event is any distinct, trace-related data sent to LangSmith. This includes:
  * Inputs, outputs and metadata sent at the start of a run step within a trace
  * Inputs, outputs and metadata sent at the end of a run step within a trace
  * Feedback on run steps or traces


### I‚Äôve hit my rate or usage limits. What can I do?[‚Äã](https://docs.smith.langchain.com/pricing/faq#ive-hit-my-rate-or-usage-limits-what-can-i-do "Direct link to I‚Äôve hit my rate or usage limits. What can I do?")
If you‚Äôve consumed the monthly allotment of free traces in your account, you can add a credit card on the Developer and Plus plans to continue sending traces to LangSmith. If you‚Äôve hit the rate limits on your tier, you can upgrade to a higher plan to get higher limits, or reach out to support@langchain.dev with questions.
### I have a developer account, can I upgrade my account to the Plus or Enterprise plan?[‚Äã](https://docs.smith.langchain.com/pricing/faq#i-have-a-developer-account-can-i-upgrade-my-account-to-the-plus-or-enterprise-plan "Direct link to I have a developer account, can I upgrade my account to the Plus or Enterprise plan?")
Every user will have a unique personal account on the Developer plan. **We cannot upgrade a Developer account to the Plus or Enterprise plans.** If you‚Äôre interested in working as a team, create a separate LangSmith Organization on the Plus plan. This plan can upgraded to the Enterprise plan at a later date.
### How does billing work?[‚Äã](https://docs.smith.langchain.com/pricing/faq#how-does-billing-work "Direct link to How does billing work?")
**Seats**
Seats are billed monthly on the first of the month. Additional seats purchased mid-month are pro-rated and billed within one day of the purchase. Seats removed mid-month will not be credited.
**Traces**
As long as you have a card on file in your account, we‚Äôll service your traces and bill you on the first of the month for traces that you submitted in the previous month. You will be able to set usage limits if you so choose to limit the maximum charges you could incur in any given month.
### Can I limit how much I spend on tracing?[‚Äã](https://docs.smith.langchain.com/pricing/faq#can-i-limit-how-much-i-spend-on-tracing "Direct link to Can I limit how much I spend on tracing?")
You can set limits on the number of traces that can be sent to LangSmith per month on the [Plans and Billing](https://smith.langchain.com/settings/payments) settings page.
note
While we do show you the dollar value of your usage limit for convenience, this limit evaluated in terms of number of traces instead of dollar amount. For example, if you are approved for our startup plan tier where you are given a generous allotment of free traces, your usage limit will not automatically change.
You are not currently able to set a spend limit in the product.
### How can my track my usage so far this month?[‚Äã](https://docs.smith.langchain.com/pricing/faq#how-can-my-track-my-usage-so-far-this-month "Direct link to How can my track my usage so far this month?")
Under the Settings section for your Organization you will see subsection for **Usage**. There, you will able to see a graph of the daily number of billable LangSmith traces from the last 30, 60, or 90 days. Note that this data is delayed by 1-2 hours and so may trail your actual number of runs slightly for the current day.
### I have a question about my bill...[‚Äã](https://docs.smith.langchain.com/pricing/faq#i-have-a-question-about-my-bill "Direct link to I have a question about my bill...")
Customers on the Developer and Plus plan tiers should email support@langchain.dev. Customers on the Enterprise plan should contact their sales representative directly.
Enterprise plan customers are billed annually by invoice.
### What can I expect from Support?[‚Äã](https://docs.smith.langchain.com/pricing/faq#what-can-i-expect-from-support "Direct link to What can I expect from Support?")
On the Developer plan, community-based support is available on [Discord](https://discord.com/invite/6adMQxSpJS).
On the Plus plan, you will also receive preferential, email support at support@langchain.dev for LangSmith-related questions only and we'll do our best to respond within the next business day.
On the Enterprise plan, you‚Äôll get white-glove support with a Slack channel, a dedicated customer success manager, and monthly check-ins to go over LangSmith and LangChain questions. We can help with anything from debugging, agent and RAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the add-on to run LangSmith in your environment, we‚Äôll also support deployments and new releases with our infra engineering team on-call.
### Where is my data stored?[‚Äã](https://docs.smith.langchain.com/pricing/faq#where-is-my-data-stored "Direct link to Where is my data stored?")
You may choose to sign up in either the US or EU region. See the [cloud architecture reference](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability) for more details. If you‚Äôre on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment.
### Which security frameworks is LangSmith compliant with?[‚Äã](https://docs.smith.langchain.com/pricing/faq#which-security-frameworks-is-langsmith-compliant-with "Direct link to Which security frameworks is LangSmith compliant with?")
We are SOC 2 Type II, GDPR, and HIPAA compliant.
You can request more information about our security policies and posture at [trust.langchain.com](https://trust.langchain.com). Please note we only enter into BAAs with customers on our Enterprise plan.
### Will you train on the data that I send LangSmith?[‚Äã](https://docs.smith.langchain.com/pricing/faq#will-you-train-on-the-data-that-i-send-langsmith "Direct link to Will you train on the data that I send LangSmith?")
We will not train on your data, and you own all rights to your data. See [LangSmith Terms of Service](https://langchain.dev/terms-of-service) for more information.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/pricing/faq%3E).
[PreviousPlans](https://docs.smith.langchain.com/pricing/plans)[NextReference](https://docs.smith.langchain.com/reference)
  * [Questions and Answers](https://docs.smith.langchain.com/pricing/faq#questions-and-answers)
    * [I‚Äôve been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?](https://docs.smith.langchain.com/pricing/faq#ive-been-using-langsmith-since-before-pricing-took-effect-for-new-users-when-will-pricing-go-into-effect-for-my-account)
    * [Which plan is right for me?](https://docs.smith.langchain.com/pricing/faq#which-plan-is-right-for-me)
    * [What is a seat?](https://docs.smith.langchain.com/pricing/faq#what-is-a-seat)
    * [What is a trace?](https://docs.smith.langchain.com/pricing/faq#what-is-a-trace)
    * [What is an ingested event?](https://docs.smith.langchain.com/pricing/faq#what-is-an-ingested-event)
    * [I‚Äôve hit my rate or usage limits. What can I do?](https://docs.smith.langchain.com/pricing/faq#ive-hit-my-rate-or-usage-limits-what-can-i-do)
    * [I have a developer account, can I upgrade my account to the Plus or Enterprise plan?](https://docs.smith.langchain.com/pricing/faq#i-have-a-developer-account-can-i-upgrade-my-account-to-the-plus-or-enterprise-plan)
    * [How does billing work?](https://docs.smith.langchain.com/pricing/faq#how-does-billing-work)
    * [Can I limit how much I spend on tracing?](https://docs.smith.langchain.com/pricing/faq#can-i-limit-how-much-i-spend-on-tracing)
    * [How can my track my usage so far this month?](https://docs.smith.langchain.com/pricing/faq#how-can-my-track-my-usage-so-far-this-month)
    * [I have a question about my bill...](https://docs.smith.langchain.com/pricing/faq#i-have-a-question-about-my-bill)
    * [What can I expect from Support?](https://docs.smith.langchain.com/pricing/faq#what-can-i-expect-from-support)
    * [Where is my data stored?](https://docs.smith.langchain.com/pricing/faq#where-is-my-data-stored)
    * [Which security frameworks is LangSmith compliant with?](https://docs.smith.langchain.com/pricing/faq#which-security-frameworks-is-langsmith-compliant-with)
    * [Will you train on the data that I send LangSmith?](https://docs.smith.langchain.com/pricing/faq#will-you-train-on-the-data-that-i-send-langsmith)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/pricing/plans

![Revisit consent button](https://uploads-ssl.webflow.com/65ff950538088944d66126b3/662ef3209b872e92e41212f6_cookieicon.png)
We value your privacy 
We use cookies to analyze our traffic. By clicking "Accept All", you consent to our use of cookies. [Privacy Policy](https://www.langchain.com/privacy-policy)
Customize Reject All Accept All
Customize Consent Preferences ![Close](https://cdn-cookieyes.com/assets/images/close.svg)
We may use cookies to help you navigate efficiently and perform certain functions. You will find detailed information about all cookies under each consent category below.
The cookies that are categorized as "Necessary" are stored on your browser as they are essential for enabling the basic functionalities of the site.... Show more
NecessaryAlways Active
Necessary cookies are required to enable the basic features of this site, such as providing secure log-in or adjusting your consent preferences. These cookies do not store any personally identifiable data.
Functional
Functional cookies help perform certain functionalities like sharing the content of the website on social media platforms, collecting feedback, and other third-party features.
Analytics
Analytical cookies are used to understand how visitors interact with the website. These cookies help provide information on metrics such as the number of visitors, bounce rate, traffic source, etc.
Performance
Performance cookies are used to understand and analyze the key performance indexes of the website which helps in delivering a better user experience for the visitors.
Advertisement
Advertisement cookies are used to provide visitors with customized advertisements based on the pages you visited previously and to analyze the effectiveness of the ad campaigns.
Uncategorized
Other uncategorized cookies are those that are being analyzed and have not been classified into a category as yet.
Reject All  Save My Preferences  Accept All 
[](https://docs.smith.langchain.com/)
Products
[LangGraph](https://docs.smith.langchain.com/langgraph)[LangSmith](https://docs.smith.langchain.com/langsmith)[LangChain](https://docs.smith.langchain.com/langchain)
Resources
[Resources Hub](https://docs.smith.langchain.com/resources)[Blog](https://blog.langchain.dev/)[Customer Stories](https://docs.smith.langchain.com/customers)[LangChain Academy](https://academy.langchain.com/)[Community](https://docs.smith.langchain.com/community)[Experts](https://docs.smith.langchain.com/experts)[Changelog](https://changelog.langchain.com/)
Docs
Python
[LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)[LangSmith](https://docs.smith.langchain.com/)[LangChain](https://python.langchain.com/docs/introduction/)
JavaScript
[LangGraph](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/)[LangSmith](https://docs.smith.langchain.com/)[LangChain](https://js.langchain.com/docs/introduction/)
Company
[About](https://docs.smith.langchain.com/about)[Careers](https://docs.smith.langchain.com/careers)
Pricing
[LangSmith](https://docs.smith.langchain.com/pricing-langsmith)[LangGraph Platform](https://docs.smith.langchain.com/pricing-langgraph-platform)
[Get a demo](https://docs.smith.langchain.com/contact-sales)
[Sign up](https://smith.langchain.com/)
# LangSmith plans for teams of any size
###### Looking for [LangGraph Platform pricing](https://docs.smith.langchain.com/pricing-langgraph-platform)?
Startups
Designed for early stage startups building AI applications
Reach out for starter pricing and get shipping today
[Reach out](https://airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form)
What to expect:
We want all early stage companies to build with LangSmith. LangSmith for Startups offers discounted prices and a generous free, monthly trace allotment, so you can have the right tooling in place as you grow your business.
Developer
Designed for hobbyists who want to start their adventure solo
Free for 1 user
5k traces per month included, pay as you go thereafter
First 5k traces included, starting at $0.50 per 1k base traces thereafter.
[Sign up](https://smith.langchain.com/)
Key features:
1 Developer seat
Debugging traces
Dataset collection
Testing and evaluation
Prompt management
Monitoring
Plus
Everything in Developer, plus team features and better rate limits.
$39/user per month
10k traces per month included, pay as you go thereafter
First 10k traces included, starting at $0.50 per 1k base traces thereafter.
[Sign up](https://smith.langchain.com/)
Key features:
All features in Developer tier
Up to 10 seats
Higher rate limits
Email support
Enterprise
Designed for teams with more security, deployment, and support needs
Custom
[Get a demo](https://docs.smith.langchain.com/contact-sales)
Key features:
All features in Plus tier
Custom Single Sign On (SSO)
SLA
Self-hosted deployment options
Custom rate limits
Team trainings
Shared Slack channel
Architectural guidance
Dedicated customer success manager
**LangSmith for Startups and Education.** Seed stage startups and educational institutions, reach out for starter pricing and get shipping today.
Reach out to learn about startup pricing for a period of time.
[Startups](https://airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form)[Education](https://airtable.com/app8ZrGLtHAtFVO1o/paginNsvj5jx2VaFh/form)[Enquire about startup pricing](https://airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form)
## LangSmith for Startups and Education
## Seed stage startups and educational institutions, reach out for starter pricing and get shipping today.
[Startups](https://airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form)[Education](https://airtable.com/app8ZrGLtHAtFVO1o/paginNsvj5jx2VaFh/form)
## Compare plans
Developer
Plus
Enterprise
Features
Debugging Traces
Dataset Collection
Human Labeling
Testing and Evaluation
Prompt Management
Monitoring
Bulk Data Export
Team
Developer Seats
Maximum 1 seatFree
Maximum 10 seats$39 per seat/month
Custom pricing
Workspaces
Single, default Workspace under Personal Organization
Up to 3 Workspaces per Organization
Up to 10 Workspaces per Organization (contact for more)
Usage
Traces 
Prices for traces vary depending on the data retention period you've set. 
First 5k base traces and extended upgrades per month for freePay as you go thereafter: $0.50 per 1k base traces (14-day retention)Additional $4.50 per 1k extended traces (400-day retention)
First 10k base traces and extended upgrades per month for freePay as you go thereafter: $0.50 per 1k base traces (14-day retention)Additional $4.50 per 1k extended traces (400-day retention)
Custom
Max Ingested Events / Hour
50k / 250k (with payment method on file) 
500k
Custom
Total Trace Size Stored / Hour
500MB / 2.5GB (with payment method on file)
5GB
Custom
Security Controls
SSO
Google, GitHub
Custom SSO
Role Based Access Control
Organization Roles (User and Admin)
Deployment
Hosted in US or EU
Hosted in US or EU
Add-on for self-hosted deployment in customer's VPC
Support
Email Support
Community Discord
Shared Slack Channel Support
Team Trainings
Application Architectural Guidance
Dedicated Customer Success Manager
SLA
Procurement
Billing
Monthly, self-serve
Monthly, self-serve
Annual invoice
Custom Terms and Data Privacy Agreement
Infosec Review
### Hear from our happy customers
LangChain, LangGraph, and LangSmith help teams of all sizes, across all industries - from ambitious startups to established enterprises.
![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308a04d37cf7d3eb1341_Rakuten_Global_Brand_Logo.png)
‚ÄúBy combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we‚Äôre able to identify the right approaches of using LLMs in an enterprise-setting faster.‚Äù
![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308a8b6137d44c621cb4_Yusuke%20Kaji.png)
Yusuke Kaji
General Manager of AI
![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308aea1371b447cc4af9_elastic-ar21.png)
‚ÄúWorking with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the development and shipping experience. We couldn‚Äôt have achieved the product experience delivered to our customers without LangChain, and we couldn‚Äôt have done it at the same pace without LangSmith.‚Äù
![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c5308a4095d5a871de7479_James%20Spiteri.png)
James Spiteri
Director of Security Products
![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c530539f4824b828357352_Logo_de_Fintual%201.png)
‚ÄúAs soon as we heard about LangSmith, we moved our entire development stack onto it. We could have built evaluation, testing and monitoring tools in house, but with LangSmith it took us 10x less time to get a 1000x better tool.‚Äù
![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c53058acbff86f4c2dcee2_jose%20pena.png)
Jose Pe√±a
Senior Manager
## Have a question about our plans?
Which plan is right for me?
If you‚Äôre an individual developer, the Developer plan is a great choice for small projects. For teams that want to self-serve LangSmith, check out the Plus plan. If you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you.
What is a seat?
Seats are named to a specific developer. All user invitations in an account will be counted as a seat.
What is a trace? Can it contain many events?
A trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Traces can include many calls to an LLM or other tracked events. We don't want to penalize you for complexity, so go ahead and design the chain or agent capable of accomplishing sophisticated tasks. Here is an [example](https://smith.langchain.com/public/17c24270-9f74-47e7-b70c-d508afc448fa/r) of a single trace. 
What is the difference between a base trace and an extended trace?
Base traces have a shorter retention period of 14 days and cost $0.50 per 1k traces. Extended traces have a longer retention period of 400 days and cost $5.00 per 1k traces. You can "upgrade" base traces to extended traces at $4.50 per 1k traces. 
Why might I want to upgrade a base trace to an extended trace?
Majority of traces sent to LangSmith may have utility only for a short period of time. For example, you're actively debugging your application or you're actively responding to a customer question about an AI generated response. We default these base traces' retention to 14-days and offer a better price point.Some traces, however, will have utility well beyond the initial creation. These traces often have feedback associated with them -- either explicitly from a user, auto-created from an online evaluator, or labeled by a human annotator. Feedback on traces is critical for improving your application, and LangSmith will automatically upgrade these traces to have a longer retention period of 400 days so that you can make use of the enriched trace. The longer retention comes with a higher price point, but also more utility. You may have your own reasons for wanting some traces to be short lived (as base traces are) vs long lived (as extended traces are). LangSmith provides the tools to let you optimize your trace retention, so you can better align utility and spend.Read more detail in our [documentation](https://docs.smith.langchain.com/pricing). 
I‚Äôve hit my usage limits. What can I do?
If you‚Äôve consumed the monthly allotment of free traces in your account, you can purchase credits via credit card on the Developer and Plus plans to continue sending traces to LangSmith. If you‚Äôve hit the performance usage limits on your tier, you can upgrade to a higher plan to get better limits, or reach out to support@langchain.dev with questions.
Can I pay by ACH?
Yes, we offer ACH payment on the Enterprise plan, and we‚Äôre happy to work with your procurement team to onboard as a new vendor.
How will billing work?
Seats are billed monthly on the first of the month or pro-rated if additional seats are purchased in the middle of the month. Churned seats mid month won‚Äôt get credited. You can purchase LangSmith credits for your tracing usage. As long as you have credits in your account, we‚Äôll service your traces and deduct from your credit balance. You‚Äôll be able to set alerts and auto-top ups on credits if you choose.Enterprise plan customers are billed annually by invoice.
What can I expect from support?
On the Plus plan, you receive preferential, email support at support@langchain.dev. On the Enterprise plan, you‚Äôll get white-glove support with a Slack channel, a dedicated customer success engineer, and monthly check-ins to go over LangSmith and LangChain questions. We can help with anything from debugging, agent and RAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the add-on to run LangSmith in your environment, we‚Äôll also support deployments and new releases with our infra engineering team on-call.
Where is my data stored?
When using LangSmith hosted at smith.langchain.com, data is stored in GCP us-central-1. If you‚Äôre on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment. For more information, check out our [documentation](https://docs.smith.langchain.com/self_hosting/kubernetes).
Which security frameworks is LangSmith compliant with?
We are SOC 2 Type II, HIPAA, and GDPR compliant. We're able to establish a BAA with customers on our Enterprise plan. You can request more information about our security policies and posture at [trust.langchain.com](https://docs.smith.langchain.com/pricing/plans#trust.langchain.com).
Will you train on the data that I send LangSmith?
We will not train on your data, and you own all rights to your data. See LangSmith [Terms of Service](https://docs.smith.langchain.com/terms-of-service) for more information.
## Ready to start shipping reliable GenAI apps faster?
Get started with LangChain, LangSmith, and LangGraph to enhance your LLM app development, from prototype to production.
[Get a demo](https://docs.smith.langchain.com/contact-sales)[Sign up for free](https://smith.langchain.com/)
Products
[LangChain](https://docs.smith.langchain.com/langchain)[LangSmith](https://docs.smith.langchain.com/langsmith)[LangGraph](https://docs.smith.langchain.com/langgraph)[Agents](https://docs.smith.langchain.com/agents)[Evaluation](https://docs.smith.langchain.com/evaluation)[Retrieval](https://docs.smith.langchain.com/retrieval)
Resources
[Python Docs](https://python.langchain.com/)[JS/TS Docs](https://js.langchain.com/docs/get_started/introduction/)[GitHub](https://github.com/langchain-ai)[Integrations](https://python.langchain.com/docs/integrations/providers/)[Changelog](https://changelog.langchain.com/)[Community](https://docs.smith.langchain.com/join-community)[LangSmith Trust Portal](https://trust.langchain.com/)
Company
[About](https://docs.smith.langchain.com/about)[Careers](https://docs.smith.langchain.com/careers)[Blog](https://blog.langchain.dev/)[Twitter](https://twitter.com/LangChainAI)[LinkedIn](https://www.linkedin.com/company/langchain/)[YouTube](https://www.youtube.com/@LangChain)[Marketing Assets](https://drive.google.com/drive/folders/17xybjzmVBdsQA-VxouuGLxF6bDsHDe80?usp=sharing)
Sign up for our newsletter to stay up to date
Thank you! Your submission has been received!
Oops! Something went wrong while submitting the form.
![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65c6a38f9c53ec71f5fc73de_langchain-word.svg)
[All systems degraded_performance](https://status.smith.langchain.com/)[Privacy Policy](https://docs.smith.langchain.com/privacy-policy)[Terms of Service](https://docs.smith.langchain.com/terms-of-service)


---

## https://docs.smith.langchain.com/prompt_engineering/concepts

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/concepts#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/prompt_engineering/concepts)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/prompt_engineering/concepts)
    * [Evaluation](https://docs.smith.langchain.com/prompt_engineering/concepts)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/prompt_engineering/concepts)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * Conceptual Guide


On this page
# Concepts
Prompt engineering is one the core pillars of LangSmith. While traditional software application are built by writing code, AI applications often involve a good amount of writing prompts. We aim to make this as easy possible by providing a set of tools designed to enable and facilitate prompt engineering.
## Why prompt engineering?[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#why-prompt-engineering "Direct link to Why prompt engineering?")
A prompt sets the stage for the model, like an audience member at an improv show directing the actor's next performance - it guides the model's behavior without changing its underlying capabilities. Just as telling an actor to "be a pirate" determines how they act, a prompt provides instructions, examples, and context that shape how the model responds.
Prompt engineering is important because it allows you to change the way the model behaves. While there are other ways to change the model's behavior (like fine-tuning), prompt engineering is usually the simplest to get started with and often provides the highest ROI.
We often see that prompt engineering is multi-disciplinary. Sometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager or another domain expert. It is important to have the proper tooling and infrastructure to support this cross-disciplinary building.
## Prompts vs Prompt Templates[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#prompts-vs-prompt-templates "Direct link to Prompts vs Prompt Templates")
Although we often use these terms interchangably, it is important to understand the difference between "prompts" and "prompt templates".
Prompts refer to the messages that are passed into the language model.
Prompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates can include variables for few shot examples, outside context, or any other external data that is needed in your prompt.
![](https://docs.smith.langchain.com/assets/images/prompt_vs_prompt_template-3e48c649bfe69dcd49d5fd7381aab163.png)
## Prompts in LangSmith[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#prompts-in-langsmith "Direct link to Prompts in LangSmith")
You can store and version prompts templates in LangSmith. There are few key aspects of a prompt template to understand.
### Chat vs Completion[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#chat-vs-completion "Direct link to Chat vs Completion")
There are two different types of prompts: `chat` style prompts and `completion` style prompts.
Chat style prompts are a **list of messages**. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.
Completion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.
### F-string vs mustache[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#f-string-vs-mustache "Direct link to F-string vs mustache")
You can format your prompt with input variables using either [f-string](https://realpython.com/python-f-strings/) or [mustache](https://mustache.github.io/mustache.5.html) format. Here is an example prompt with f-string format:
```
Hello,{name}!
```

And here is one with mustache:
```
Hello,{{name}}!
```

Mustache format
Mustache format gives your more flexbility around conditional variables, loops, and nested keys. Read [the documentation](https://mustache.github.io/mustache.5.html)
### Tools[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#tools "Direct link to Tools")
Tools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description, and JSON schema of arguments used to call the tool.
### Structured Output[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#structured-output "Direct link to Structured Output")
Structured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they stick to a specified schema. This may or may not use [Tools](https://docs.smith.langchain.com/prompt_engineering/concepts#tools) under the hood.
Structured Output vs Tools
Structured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM **always** responds in this format. With tools, the LLM may select **multiple** tools; with structured output, only one response is generate.
### Model[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#model "Direct link to Model")
Optionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).
## Prompt Versioning[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-versioning "Direct link to Prompt Versioning")
Verisioning is a key part of iterating and collaborating on your different prompts.
### Commits[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#commits "Direct link to Commits")
Every saved update to a prompt creates a new commit. You can view previous commits, making it easy to review earlier prompt versions or revert to a previous state if needed. In the SDK, you can access a specific commit of a prompt by specifying the commit hash along with the prompt name (e.g. `prompt_name:commit_hash`).
In the UI, you can compare a commit with its previous version by toggling the "diff" button in the top-right corner of the Commits tab. ![](https://docs.smith.langchain.com/assets/images/commit_diff-e6773531883507d745eb22d08bab0056.png)
### Tags[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#tags "Direct link to Tags")
You may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt with `dev` or `prod` tags. This allows you to track which versions of prompts are used where.
## Prompt Playground[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-playground "Direct link to Prompt Playground")
The prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.
In the playground you can:
  * Change the model being used
  * Change prompt template being used
  * Change the output schema
  * Change the tools available
  * Enter the input variables to run through the prompt template
  * Run the prompt through the model
  * Observe the outputs


## Testing multiple prompts[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#testing-multiple-prompts "Direct link to Testing multiple prompts")
You can add more prompts to your playground to easily compare outputs and decide which version is better:
![](https://docs.smith.langchain.com/assets/images/add_prompt_to_playground-72edfaaf4ebadc6404b5dd1c5c3e5680.gif)
## Testing over a dataset[‚Äã](https://docs.smith.langchain.com/prompt_engineering/concepts#testing-over-a-dataset "Direct link to Testing over a dataset")
To test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results are streamed back as well as how many repitions there are in the test.
![](https://docs.smith.langchain.com/assets/images/test_over_dataset_in_playground-d9300964035796aede159c8bf87d3910.gif)
You can click on the "View Experiment" button to dive deeper into the results of the test.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/prompt_engineering/concepts%3E).
[PreviousPrompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)[NextDeployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Why prompt engineering?](https://docs.smith.langchain.com/prompt_engineering/concepts#why-prompt-engineering)
  * [Prompts vs Prompt Templates](https://docs.smith.langchain.com/prompt_engineering/concepts#prompts-vs-prompt-templates)
  * [Prompts in LangSmith](https://docs.smith.langchain.com/prompt_engineering/concepts#prompts-in-langsmith)
    * [Chat vs Completion](https://docs.smith.langchain.com/prompt_engineering/concepts#chat-vs-completion)
    * [F-string vs mustache](https://docs.smith.langchain.com/prompt_engineering/concepts#f-string-vs-mustache)
    * [Tools](https://docs.smith.langchain.com/prompt_engineering/concepts#tools)
    * [Structured Output](https://docs.smith.langchain.com/prompt_engineering/concepts#structured-output)
    * [Model](https://docs.smith.langchain.com/prompt_engineering/concepts#model)
  * [Prompt Versioning](https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-versioning)
    * [Commits](https://docs.smith.langchain.com/prompt_engineering/concepts#commits)
    * [Tags](https://docs.smith.langchain.com/prompt_engineering/concepts#tags)
  * [Prompt Playground](https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-playground)
  * [Testing multiple prompts](https://docs.smith.langchain.com/prompt_engineering/concepts#testing-multiple-prompts)
  * [Testing over a dataset](https://docs.smith.langchain.com/prompt_engineering/concepts#testing-over-a-dataset)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/how_to_guides

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
    * [Evaluation](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * How-to Guides


On this page
# Prompt engineering how-to guides
Step-by-step guides that cover key tasks and operations for doing prompt engineering LangSmith.
## Prompt hub[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#prompt-hub "Direct link to Prompt hub")
Organize and manage prompts in LangSmith to streamline your LLM development workflow.
  * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
  * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
  * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
  * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)


## Playground[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#playground "Direct link to Playground")
Quickly iterate on prompts and models in the LangSmith Playground.
  * [Run an evaluation in the Playground](https://docs.smith.langchain.com/evaluation?mode=ui)
  * [Manage prompt settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
  * [Iterate on your prompts with the Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
  * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
  * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)


## Few shot prompting[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#few-shot-prompting "Direct link to Few shot prompting")
Use LangSmith datasets to serve few shot examples to your application
  * [Index a dataset for few shot example selection](https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/prompt_engineering/how_to_guides%3E).
[PreviousOptimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)[NextCreate a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
  * [Prompt hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#prompt-hub)
  * [Playground](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#playground)
  * [Few shot prompting](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#few-shot-prompting)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
    * [Evaluation](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
  * Create a prompt


On this page
# Create a prompt
Navigate to the **Prompts** section in the left-hand sidebar or from the application homepage. Click the "+ Prompt" button to enter the Playground. The dropdown next to the button gives you a choice between a chat style prompt and an instructional prompt - chat is the default. ![](https://docs.smith.langchain.com/assets/images/blank_prompts_page-c20c133f3a5e9fc9b30aba59eb02e4ac.png)
## Compose your prompt[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt#compose-your-prompt "Direct link to Compose your prompt")
After choosing a prompt type, you're brought to the playground to develop your prompt. On the left is an editable view of the prompt. You can add more messages, change the template format (f-string or mustache), and add an output schema (which makes your prompt a StructuredPrompt type).
To the right, we can enter sample inputs for our prompt variables and then run our prompt against a model.
(If you haven't yet, you'll need to enter an API key for whichever model you want to run your prompt with.)
To see the response from the model, click "Start".
![](https://docs.smith.langchain.com/assets/images/create_prompt_playground-4f33ab0226596e1a4b19d4027d11c24a.png)
## Save your prompt[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt#save-your-prompt "Direct link to Save your prompt")
To save your prompt, click the "Save as" button, name your prompt, and decide if you want it to be "private" or "public". Private prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub. Click save to create your prompt.
The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version.
Public Prompts
The first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace.
![](https://docs.smith.langchain.com/assets/images/save_prompt-95a53879cedf5411d75c05ec4fe4fb92.png)
## View your prompts[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt#view-your-prompts "Direct link to View your prompts")
You've just created your first prompt! View a table of your prompts in the prompts tab.
![](https://docs.smith.langchain.com/assets/images/prompt_table-3e01df311d62e7f2b7474cdc39f29221.png)
## Add metadata[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt#add-metadata "Direct link to Add metadata")
To add metadata to your prompt, click the prompt and then click the "Edit" pencil icon next to the name. This brings you to where you can add additional information about the prompt, including a description, a README, and use cases. For public prompts this information will be visible to anyone who views your prompt in the LangChain Hub.
![](https://docs.smith.langchain.com/assets/images/edit_prompt-4702a9323ad6c3e90192b708f1d78315.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/prompt_engineering/how_to_guides/create_a_prompt%3E).
[PreviousPrompt engineering how-to guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)[NextRun the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
  * [Compose your prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt#compose-your-prompt)
  * [Save your prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt#save-your-prompt)
  * [View your prompts](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt#view-your-prompts)
  * [Add metadata](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt#add-metadata)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
    * [Evaluation](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
  * Run the playground against a custom LangServe model server


On this page
# Run the playground against a custom LangServe model server
The LangSmith playground allows you to use your own custom models. You can deploy a model server that exposes your model's API via [LangServe](https://github.com/langchain-ai/langserve), an open source library for serving LangChain applications. Behind the scenes, the playground will interact with your model server to generate responses.
## Deploy a custom model server[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint#deploy-a-custom-model-server "Direct link to Deploy a custom model server")
For your convenience, we have provided a sample model server that you can use as a reference. You can find the sample model server [here](https://github.com/langchain-ai/langsmith-model-server) We highly recommend using the sample model server as a starting point.
Depending on your model is an instruct-style or chat-style model, you will need to implement either `custom_model.py` or `custom_chat_model.py` respectively.
## Adding configurable fields[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint#adding-configurable-fields "Direct link to Adding configurable fields")
It is often useful to configure your model with different parameters. These might include temperature, model_name, max_tokens, etc.
To make your model configurable in the LangSmith playground, you need to add configurable fields to your model server. These fields can be used to change model parameters from the playground.
You can add configurable fields by implementing the `with_configurable_fields` function in the `config.py` file. You can
```
defwith_configurable_fields(self)-> Runnable:"""Expose fields you want to be configurable in the playground. We will automatically expose these to the  playground. If you don't want to expose any fields, you can remove this method."""return self.configurable_fields(n=ConfigurableField(id="n",    name="Num Characters",    description="Number of characters to return from the input prompt.",))
```

## Use the model in the LangSmith Playground[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint#use-the-model-in-the-langsmith-playground "Direct link to Use the model in the LangSmith Playground")
Once you have deployed a model server, you can use it in the LangSmith Playground. Enter the playground and select either the `ChatCustomModel` or the `CustomModel` provider for chat-style model or instruct-style models.
Enter the `URL`. The playground will automatically detect the available endpoints and configurable fields. You can then invoke the model with the desired parameters.
![ChatCustomModel in Playground](https://docs.smith.langchain.com/assets/images/playground_custom_model-4237b77bfc148e67d4a2c9dfe58788d5.png)
If everything is set up correctly, you should see the model's response in the playground as well as the configurable fields specified in the `with_configurable_fields`.
See how to store your model configuration for later use [here](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations).
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/prompt_engineering/how_to_guides/custom_endpoint%3E).
[PreviousCreate a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)[NextRun the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
  * [Deploy a custom model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint#deploy-a-custom-model-server)
  * [Adding configurable fields](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint#adding-configurable-fields)
  * [Use the model in the LangSmith Playground](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint#use-the-model-in-the-langsmith-playground)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
    * [Evaluation](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
  * Run the playground against an OpenAI-compliant model provider/proxy


On this page
# Run the playground against an OpenAI-compliant model provider/proxy
The LangSmith playground allows you to use any model that is compliant with the OpenAI API. You can utilize your model by setting the Proxy Provider for `OpenAI` in the playground.
## Deploy an OpenAI compliant model[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model#deploy-an-openai-compliant-model "Direct link to Deploy an OpenAI compliant model")
Many providers offer OpenAI compliant models or proxy services. Some examples of this include:
  * [LiteLLM Proxy](https://github.com/BerriAI/litellm?tab=readme-ov-file#quick-start-proxy---cli)
  * [Ollama](https://ollama.com/)


You can use these providers to deploy your model and get an API endpoint that is compliant with the OpenAI API.
Take a look at the full [specification](https://platform.openai.com/docs/api-reference/chat) for more information.
## Use the model in the LangSmith Playground[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model#use-the-model-in-the-langsmith-playground "Direct link to Use the model in the LangSmith Playground")
Once you have deployed a model server, you can use it in the LangSmith Playground. Enter the playground and select the `Proxy Provider` inside the `OpenAI` modal.
![OpenAI Proxy Provider](https://docs.smith.langchain.com/assets/images/openai_proxy_provider-69efe92e79a8bff3d690cec9493d22d2.png)
If everything is set up correctly, you should see the model's response in the playground. You can also use this functionality to invoke downstream pipelines as well.
See how to store your model configuration for later use [here](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations).
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/prompt_engineering/how_to_guides/custom_openai_compliant_model%3E).
[PreviousRun the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)[NextUpdate a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
  * [Deploy an OpenAI compliant model](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model#deploy-an-openai-compliant-model)
  * [Use the model in the LangSmith Playground](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model#use-the-model-in-the-langsmith-playground)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
    * [Evaluation](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
  * LangChain Hub


# LangChain Hub
Navigate to the **LangChain Hub** section of the left-hand sidebar.
![](https://docs.smith.langchain.com/assets/images/langchain_hub-6be81660d24f2695b7b2849050787777.png)
Here you'll find all of the publicly listed prompts in the LangChain Hub. You can search for prompts by name, handle, use cases, descriptions, or models. You can fork prompts to your personal organization, view the prompt's details, and run the prompt in the playground. You can [pull any public prompt into your code](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically) using the SDK.
To view prompts tied to your workspace, visit the Prompts tab in the sidebar.
![](https://docs.smith.langchain.com/assets/images/prompts_tab-5ec24d24d44c67c35588d7cecd5da526.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/prompt_engineering/how_to_guides/langchain_hub%3E).
[PreviousOpen a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)[NextPrompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
  * Manage prompts programmatically


On this page
# Manage prompts programmatically
You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.
note
Previously this functionality lived in the `langchainhub` package which is now deprecated. All functionality going forward will live in the `langsmith` package.
## Install packages[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#install-packages "Direct link to Install packages")
In Python, you can directly use the LangSmith SDK (_recommended, full functionality_) or you can use through the LangChain package (limited to pushing and pulling prompts).
In TypeScript, you must use the LangChain npm package for pulling prompts (it also allows pushing). For all other functionality, use the LangSmith package.
  * Python
  * LangChain (Python)
  * TypeScript


```
pip install -U langsmith # version >= 0.1.99
```

```
pip install -U langchain langsmith # langsmith version >= 0.1.99 and langchain >= 0.2.13
```

```
yarn add langsmith langchain // langsmith version >= 0.1.99 and langchain version >= 0.2.14
```

## Configure environment variables[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#configure-environment-variables "Direct link to Configure environment variables")
If you already have `LANGSMITH_API_KEY` set to your current workspace's api key from LangSmith, you can skip this step.
Otherwise, get an API key for your workspace by navigating to `Settings > API Keys > Create API Key` in LangSmith.
Set your environment variable.
```
export LANGSMITH_API_KEY="lsv2_..."
```

Terminology
What we refer to as "prompts" used to be called "repos", so any references to "repo" in the code are referring to a prompt.
## Push a prompt[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#push-a-prompt "Direct link to Push a prompt")
To create a new prompt or update an existing prompt, you can use the `push prompt` method.
  * Python
  * LangChain (Python)
  * TypeScript


```
from langsmith import Clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")url = client.push_prompt("joke-generator",object=prompt)# url is a link to the prompt in the UIprint(url)
```

```
from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")url = prompts.push("joke-generator", prompt)# url is a link to the prompt in the UIprint(url)
```

```
import*as hub from"langchain/hub";import{ ChatPromptTemplate }from"@langchain/core/prompts";const prompt = ChatPromptTemplate.fromTemplate("tell me a joke about {topic}");const url = hub.push("joke-generator",{object: prompt,});// url is a link to the prompt in the UIconsole.log(url);
```

You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground. (see settings here: [Supported Providers](https://langsmith.com/playground))
  * Python
  * LangChain (Python)
  * TypeScript


```
from langsmith import Clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model="gpt-4o-mini")prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")chain = prompt | modelclient.push_prompt("joke-generator-with-model",object=chain)
```

```
from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model="gpt-4o-mini")prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")chain = prompt | modelurl = prompts.push("joke-generator-with-model", chain)# url is a link to the prompt in the UIprint(url)
```

```
import*as hub from"langchain/hub";import{ ChatPromptTemplate }from"@langchain/core/prompts";import{ ChatOpenAI }from"@langchain/openai";const model =newChatOpenAI({ model:"gpt-4o-mini"});const prompt = ChatPromptTemplate.fromTemplate("tell me a joke about {topic}");const chain = prompt.pipe(model);await hub.push("joke-generator-with-model",{object: chain});
```

## Pull a prompt[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#pull-a-prompt "Direct link to Pull a prompt")
To pull a prompt, you can use the `pull prompt` method, which returns a the prompt as a langchain `PromptTemplate`.
To pull a **private prompt** you do not need to specify the owner handle (though you can, if you have one set).
To pull a **public prompt** from the LangChain Hub, you need to specify the handle of the prompt's author.
  * Python
  * LangChain (Python)
  * TypeScript


```
from langsmith import Clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt("joke-generator")model = ChatOpenAI(model="gpt-4o-mini")chain = prompt | modelchain.invoke({"topic":"cats"})
```

```
from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull("joke-generator")model = ChatOpenAI(model="gpt-4o-mini")chain = prompt | modelchain.invoke({"topic":"cats"})
```

```
import*as hub from"langchain/hub";import{ ChatOpenAI }from"@langchain/openai";const prompt =await hub.pull("joke-generator");const model =newChatOpenAI({ model:"gpt-4o-mini"});const chain = prompt.pipe(model);await chain.invoke({"topic":"cats"});
```

Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model. Just specify include_model when pulling the prompt. If the stored prompt includes a model, it will be returned as a RunnableSequence. Make sure you have the proper environment variables set for the model you are using.
  * Python
  * LangChain (Python)
  * TypeScript


```
from langsmith import Clientclient = Client()chain = client.pull_prompt("joke-generator-with-model", include_model=True)chain.invoke({"topic":"cats"})
```

```
from langchain import hub as promptschain = prompts.pull("joke-generator-with-model", include_model=True)chain.invoke({"topic":"cats"})
```

```
import*as hub from"langchain/hub";import{ Runnable }from"@langchain/core/runnables";const chain =await hub.pull<Runnable>("joke-generator-with-model",{ includeModel:true});await chain.invoke({"topic":"cats"});
```

When pulling a prompt, you can also specify a specific commit hash or [prompt tag](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags) to pull a specific version of the prompt.
  * Python
  * LangChain (Python)
  * TypeScript


```
prompt = client.pull_prompt("joke-generator:12344e88")
```

```
prompt = prompts.pull("joke-generator:12344e88")
```

```
const prompt =await hub.pull("joke-generator:12344e88")
```

To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author.
  * Python
  * LangChain (Python)
  * TypeScript


```
prompt = client.pull_prompt("efriis/my-first-prompt")
```

```
prompt = prompts.pull("efriis/my-first-prompt")
```

```
const prompt =await hub.pull("efriis/my-first-prompt")
```

Important Note for JavaScript Users
For pulling prompts, if you are using Node.js or an environment that supports dynamic imports, we recommend using the `langchain/hub/node` entrypoint, as it handles deserialization of models associated with your prompt configuration automatically.
If you are in a non-Node environment, "includeModel" is not supported for non-OpenAI models and you should use the base `langchain/hub` entrypoint.
## Use a prompt without LangChain[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#use-a-prompt-without-langchain "Direct link to Use a prompt without LangChain")
If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods. These convert your prompt into the payload required for the OpenAI or Anthropic API.
These conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency in addition to your official SDK of choice. Here are some examples:
### OpenAI[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#openai "Direct link to OpenAI")
  * Python
  * TypeScript


```
pip install -U langchain_openai
```

```
yarn add @langchain/openai @langchain/core // @langchain/openai version >= 0.3.2
```

  * Python
  * TypeScript


```
from openai import OpenAIfrom langsmith.client import Client, convert_prompt_to_openai_format# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt("joke-generator")prompt_value = prompt.invoke({"topic":"cats"})openai_payload = convert_prompt_to_openai_format(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)
```

```
import*as hub from"langchain/hub";import{ convertPromptToOpenAI }from"@langchain/openai";import OpenAI from"openai";const prompt =await hub.pull("jacob/joke-generator");const formattedPrompt =await prompt.invoke({topic:"cats",});const{ messages }=convertPromptToOpenAI(formattedPrompt);const openAIClient =newOpenAI();const openAIResponse =await openAIClient.chat.completions.create({model:"gpt-4o-mini",messages,});
```

### Anthropic[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#anthropic "Direct link to Anthropic")
  * Python
  * TypeScript


```
pip install -U langchain_anthropic
```

```
yarn add @langchain/anthropic @langchain/core // @langchain/anthropic version >= 0.3.3
```

  * Python
  * TypeScript


```
from anthropic import Anthropicfrom langsmith.client import Client, convert_prompt_to_anthropic_format# langsmith clientclient = Client()# anthropic clientanthropic_client = Anthropic()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt("joke-generator")prompt_value = prompt.invoke({"topic":"cats"})anthropic_payload = convert_prompt_to_anthropic_format(prompt_value)anthropic_response = anthropic_client.messages.create(**anthropic_payload)
```

```
import*as hub from"langchain/hub";import{ convertPromptToAnthropic }from"@langchain/anthropic";import Anthropic from"@anthropic-ai/sdk";const prompt =await hub.pull("jacob/joke-generator");const formattedPrompt =await prompt.invoke({topic:"cats",});const{ messages, system }=convertPromptToAnthropic(formattedPrompt);const anthropicClient =newAnthropic();const anthropicResponse =await anthropicClient.messages.create({model:"claude-3-haiku-20240307",system,messages,max_tokens:1024,stream:false,});
```

## List, delete, and like prompts[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#list-delete-and-like-prompts "Direct link to List, delete, and like prompts")
You can also list, delete, and like/unlike prompts using the `list prompts`, `delete prompt`, `like prompt` and `unlike prompt` methods. See the [LangSmith SDK client](https://github.com/langchain-ai/langsmith-sdk) for extensive documentation on these methods.
  * Python
  * TypeScript


```
# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include "joke"prompts = client.list_prompts(query="joke", is_public=False)# Delete a promptclient.delete_prompt("joke-generator")# Like a promptclient.like_prompt("efriis/my-first-prompt")# Unlike a promptclient.unlike_prompt("efriis/my-first-prompt")
```

```
// List all prompts in my workspaceimport Client from"langsmith";const client =newClient({ apiKey:"lsv2_..."});const prompts = client.listPrompts();forawait(const prompt of prompts){console.log(prompt);}// List my private prompts that include "joke"const private_joke_prompts = client.listPrompts({ query:"joke", isPublic:false});// Delete a promptclient.deletePrompt("joke-generator");// Like a promptclient.likePrompt("efriis/my-first-prompt");// Unlike a promptclient.unlikePrompt("efriis/my-first-prompt");
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousUpdate a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)[NextManaging Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
  * [Install packages](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#install-packages)
  * [Configure environment variables](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#configure-environment-variables)
  * [Push a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#push-a-prompt)
  * [Pull a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#pull-a-prompt)
  * [Use a prompt without LangChain](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#use-a-prompt-without-langchain)
    * [OpenAI](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#openai)
    * [Anthropic](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#anthropic)
  * [List, delete, and like prompts](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically#list-delete-and-like-prompts)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
  * Managing Prompt Settings


On this page
# Managing Prompt Settings
The LangSmith playground enables you to control various settings for your prompt. These include the model configuration, tool settings, and prompt formatting.
![Prompt Settings](https://docs.smith.langchain.com/assets/images/prompt_settings-041cc4809434e305f3f297add3d1999e.png)
## Model Configurations[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#model-configurations "Direct link to Model Configurations")
Model configurations are the set of parameters against which your prompt is run. For example, they include the provider, model, and temperature, among others. The LangSmith playground allows you to save and manage your model configurations, making it easy to reuse preferred settings across multiple prompts and sessions.
### Creating Saved Configurations[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#creating-saved-configurations "Direct link to Creating Saved Configurations")
  1. Adjust the model configuration as desired
  2. Click the `Save As` button in the top bar
  3. Enter a name and optional description for your configuration and confirm.


Your configuration is now saved and ready to be accessed by anyone in your organization's workspace. All saved configurations are available in the `Model configuration` dropdown.
### Default Configurations[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#default-configurations "Direct link to Default Configurations")
Once you have created a saved configuration, you can optionally set it as your default, so any new prompt you create will automatically use this configuration. To set a configuration as your default, click the `Set as default` button next to the dropdown.
![Setting Default Configuration](https://docs.smith.langchain.com/assets/images/set_default_config-af0255a161071a2d21c74155951fb9e6.png)
### Editing Configurations[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#editing-configurations "Direct link to Editing Configurations")
  * To rename or update the description: Click the configuration name or description and make your changes.


  * To update the current configuration's parameters: Make any desired to the parameters and click the `Save` button at the top.


### Deleting Configurations[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#deleting-configurations "Direct link to Deleting Configurations")
  1. Select the configuration you want to remove
  2. Click the trash can icon to delete it


## Tool Settings[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#tool-settings "Direct link to Tool Settings")
Tools enable your LLM to perform tasks like searching the web, looking up information, and more. Here you can manage the ways your LLM can utilize and access the tools you have defined in your prompt. Learn more about tools [here](https://docs.smith.langchain.com/prompt_engineering/concepts#tools).
## Prompt Formatting[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#prompt-formatting "Direct link to Prompt Formatting")
For information on chat and completion prompts, see [here](https://docs.smith.langchain.com/prompt_engineering/concepts#chat-vs-completion). For information about prompt templating and using variables, see [here](https://docs.smith.langchain.com/prompt_engineering/concepts#f-string-vs-mustache).
## Extra Parameters[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#extra-parameters "Direct link to Extra Parameters")
The **Extra Parameters** field allows you to pass additional model parameters that aren't directly supported in the LangSmith interface. This is particularly useful in two scenarios:
  1. When model providers release new parameters that haven't yet been integrated into the LangSmith interface. You can specify these parameters in JSON format to use them right away.


![Extra Params](https://docs.smith.langchain.com/assets/images/extra_params-544de3114ed19a30449bf3e17c4767fd.png)
  1. When troubleshooting parameter-related errors in the playground. If you receive an error about unnecessary parameters (more common when using LangChainJS for run tracing), you can use this field to remove the extra parameters.


![Extra Params](https://docs.smith.langchain.com/assets/images/extra_params_error-0be24963963b558ce279124404e0117b.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousManage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)[NextPrompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
  * [Model Configurations](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#model-configurations)
    * [Creating Saved Configurations](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#creating-saved-configurations)
    * [Default Configurations](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#default-configurations)
    * [Editing Configurations](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#editing-configurations)
    * [Deleting Configurations](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#deleting-configurations)
  * [Tool Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#tool-settings)
  * [Prompt Formatting](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#prompt-formatting)
  * [Extra Parameters](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations#extra-parameters)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
  * Open a prompt from a trace


# Open a prompt from a trace
If you pull a prompt into your code and begin logging traces that use it, you can find a link to the prompt in the Trace UI.
In the run that used the prompt, hover over the Prompt tag. Clicking on this will take you to the prompt. (If you used a LangChain Hub prompt, this tag will say Hub) ![](https://docs.smith.langchain.com/assets/images/trace_with_prompt_link-4ac8e15ca2617e82c24f3a7eb9ad26a5.png)]
In the metadata of the run, you can see more details. Click on an individual prompt metadata value to filter your traces by that attribute. You can filter by prompt handle, prompt name, or prompt commit hash. ![](https://docs.smith.langchain.com/assets/images/run_metadata-9770b3ad83a2426110b56692e771a557.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousPrompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)[NextLangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
  * Prompt Canvas


On this page
# Prompt Canvas
The prompt canvas makes it easy to edit a prompt with the help of an LLM. This allows you to iterate faster on long prompts and also makes it easier to make overarching stylisting or tonal changes to your prompt. You can enter the promp canvas by clicking the glowing wand over any message in your prompt:
![](https://docs.smith.langchain.com/assets/images/prompt_canvas_open-1f4ee00de4116761fdc60bb20860837e.gif)
## Chat sidebar[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas#chat-sidebar "Direct link to Chat sidebar")
You can use the chat sidebar to ask questions about your prompt, or to give instructions in natural language to the LLM for how to rewrite your prompt.
![](https://docs.smith.langchain.com/assets/images/prompt_canvas_rewrite-63f6f747175551dadebbdf717dea9c09.gif)
Write Directly
You can also edit the prompt directly - you don't **need** to use the LLM. This is useful if you know what edits you want to make and just want to make them directly
## Quick actions[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas#quick-actions "Direct link to Quick actions")
There are quick actions to change the reading level or length of the prompt with a single mouse click:
![](https://docs.smith.langchain.com/assets/images/prompt_canvas_quick_actions-8c873fd6715397171030767d54b27927.gif)
## Custom quick actions[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas#custom-quick-actions "Direct link to Custom quick actions")
You can also save your own custom quick actions, for ease of use across all the prompts you are working on in LangSmith:
![](https://docs.smith.langchain.com/assets/images/prompt_canvas_custom_quick_action-3fbc8dd98c06f336c5323092801cbc67.gif)
## Diffing[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas#diffing "Direct link to Diffing")
You can also see the specific differences between each version of your prompt by selecting the diff slider in the top right of the canvas:
![](https://docs.smith.langchain.com/assets/images/prompt_canvas_diff-c1ea4921287c4b36b0ef803989f09fba.gif)
## Saving and using prompts[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas#saving-and-using-prompts "Direct link to Saving and using prompts")
Lastly, you can save the prompt you have created in the canvas by clicking the "Use this Version" button in the bottom right:
![](https://docs.smith.langchain.com/assets/images/prompt_canvas_save-f9de1dd20a376b45787c7014b9c2e1b3.gif)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousLangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)[NextConceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Chat sidebar](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas#chat-sidebar)
  * [Quick actions](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas#quick-actions)
  * [Custom quick actions](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas#custom-quick-actions)
  * [Diffing](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas#diffing)
  * [Saving and using prompts](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas#saving-and-using-prompts)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
    * [Evaluation](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
  * Prompt Tags


On this page
# Prompt Tags
Prompt tags are labels that attached to specific commits in your prompt's version history. They help you mark significant versions and control which versions run in different environments. By referencing tags rather than commit IDs in your code, you can easily update which version is being used without modifying the code itself.
## Overview[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#overview "Direct link to Overview")
  * A tag is a named reference to a specific commit
  * Each tag points to exactly one commit at a time
  * Tags can be moved between commits
  * Common uses include marking commits for different environments (e.g., "production", "staging") or marking stable versions


## Managing Tags[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#managing-tags "Direct link to Managing Tags")
### Create a tag[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#create-a-tag "Direct link to Create a tag")
To create a tag, navigate to the commits tab of a prompt. Click on the tag icon next to the commit you want to tag. Click "New Tag" and enter the name of the tag.
![](https://docs.smith.langchain.com/assets/images/commits_tab-6c39f66e5734451dbc7e614119a0e5c7.png) ![](https://docs.smith.langchain.com/assets/images/create_new_prompt_tag-5c6a58ee5730d29b4b022cf011a2659c.png)
### Move a tag[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#move-a-tag "Direct link to Move a tag")
To point a tag to a different commit, click on the tag icon next to the destination commit, and select the tag you want to move. This will automatically update the tag to point to the new commit.
![](https://docs.smith.langchain.com/assets/images/move_prompt_tag-62a99e58991405ba424653735eacf117.png)
## Delete a tag[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#delete-a-tag "Direct link to Delete a tag")
To delete a tag, click on the delete icon next to the tag you want to delete. Note that this will delete the tag altogether and it will no longer be associated with any commit.
## Using tags in code[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#using-tags-in-code "Direct link to Using tags in code")
Tags provide a stable way to reference specific versions of your prompts in code. Instead of using commit hashes directly, you can reference tags which can be updated without changing your code.
See [managing prompts programatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically) for more information on how to use prompts in code.
Here is an example of pulling a prompt by tag in Python:
```
prompt = client.pull_prompt("joke-generator:prod")# If prod tag points to commit a1b2c3d4, this is equivalent to:prompt = client.pull_prompt("joke-generator:a1b2c3d4")
```

## Common use cases[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#common-use-cases "Direct link to Common use cases")
  1. **Environment-specific tags** : Use tags like "prod" or "staging" to mark versions for different environments. This makes it easy to switch between different versions without changing your code.
  2. **Version control** : Use tags to mark stable versions of your prompts (ex. v1, v2). This makes it easy to reference specific versions in your code and track changes over time.
  3. **Collaboration** : Use tags to mark versions ready for review. This makes it easy to share specific versions with collaborators and get feedback.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/prompt_engineering/how_to_guides/prompt_tags%3E).
[PreviousManaging Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)[NextOpen a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
  * [Overview](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#overview)
  * [Managing Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#managing-tags)
    * [Create a tag](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#create-a-tag)
    * [Move a tag](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#move-a-tag)
  * [Delete a tag](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#delete-a-tag)
  * [Using tags in code](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#using-tags-in-code)
  * [Common use cases](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags#common-use-cases)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
    * [Evaluation](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
  * Update a prompt


On this page
# Update a prompt
Navigate to the **Prompts** section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit.
## Update metadata[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt#update-metadata "Direct link to Update metadata")
To update the prompt metadata (description, use cases, etc.) click the "Edit" pencil icon.
![](https://docs.smith.langchain.com/assets/images/metadata_edit_button-a30214b3741be0a533e00b7139192d85.png)
Your prompt metadata will be updated upon save.
![](https://docs.smith.langchain.com/assets/images/update_prompt_form-9661d07fa9e0aeec3dbb96a7b624cc48.png)
## Update the prompt content[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt#update-the-prompt-content "Direct link to Update the prompt content")
To update the prompt content itself, you need to enter the prompt playground. Click "Edit in playground". Now you can make changes to the prompt and test it with different inputs. When you're happy with the prompt, click "Commit" to save it.
![](https://docs.smith.langchain.com/assets/images/edit_in_playground-234bee74c4e53194d299bd48d2f68057.png) ![](https://docs.smith.langchain.com/assets/images/prompt_playground_edit_commit-8149ad7050bf02844e4a0ac2e2d3184b.png)
## Version a prompt[‚Äã](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt#version-a-prompt "Direct link to Version a prompt")
When you add a commit to a prompt, a new version of the prompt is created. You can view all historical versions by clicking the "Commits" tab in the prompt view.
![](https://docs.smith.langchain.com/assets/images/prompt_commits_tab-b3a602826968f746e9389a0fc72ed025.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/prompt_engineering/how_to_guides/update_a_prompt%3E).
[PreviousRun the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)[NextManage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
  * [Update metadata](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt#update-metadata)
  * [Update the prompt content](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt#update-the-prompt-content)
  * [Version a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt#version-a-prompt)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
      * [Quick Start (UI)](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
      * [Quick Start (SDK)](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * Quick Start (SDK)


On this page
# Prompt Engineering Quick Start (SDK)
This quick start will walk through how to create, test, and iterate on prompts using the SDK. In this tutorial we will use OpenAI, but you can use whichever LLM you want.
QuickStart
This tutorial uses the SDK for prompt engineering, if you are interested in using the UI instead, read [this guide](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui).
## 1. Setup[‚Äã](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk#1-setup "Direct link to 1. Setup")
First, install the required packages:
  * Python
  * TypeScript


```
pip install -qU langsmith openai langchain_core
```

```
yarn add langsmith @langchain/core langchain openai
```

Next, make sure you have signed up for a [LangSmith](https://langsmith.com) account, then [create](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key#create-an-api-key) and set your API key. You will also want to sign up for an OpenAI API key to run the code in this tutorial.
```
LANGSMITH_API_KEY = '<your_api_key>'OPENAI_API_KEY = '<your_api_key>'
```

## 2. Create a prompt[‚Äã](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk#2-create-a-prompt "Direct link to 2. Create a prompt")
To create a prompt in LangSmith, define the list of messages you want in your prompt and then wrap them using the `ChatPromptTemplate` function ([Python](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)) or [TypeScript](https://v03.api.js.langchain.com/classes/_langchain_core.prompts.ChatPromptTemplate.html) function. Then all you have to do is call [`push_prompt`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.push_prompt) (Python) or [`pushPrompt`](https://langsmith-docs-7jgx2bq8f-langchain.vercel.app/reference/js/classes/client.Client#pushprompt) (TypeScript) to send your prompt to LangSmith!
  * Python
  * TypeScript


```
from langsmith import Clientfrom langchain_core.prompts import ChatPromptTemplate# Connect to the LangSmith clientclient = Client()# Define the promptprompt = ChatPromptTemplate([("system","You are a helpful chatbot."),("user","{question}"),])# Push the promptclient.push_prompt("my-prompt",object=prompt)
```

```
import{ Client }from"langsmith";import{ ChatPromptTemplate }from"@langchain/core/prompts";// Connect to the LangSmith clientconst client =newClient();// Define the promptconst prompt = ChatPromptTemplate.fromMessages([["system","You are a helpful chatbot."],["user","{question}"],]);// Push the promptawait client.pushPrompt("my-prompt",{ object: prompt,});
```

## 3. Test a prompt[‚Äã](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk#3-test-a-prompt "Direct link to 3. Test a prompt")
To test a prompt, you need to pull the prompt, invoke it with the input values you want to test and then call the model with those input values. your LLM or application expects.
  * Python
  * TypeScript


```
from langsmith import Clientfrom openai import OpenAIfrom langchain_core.messages import convert_to_openai_messages# Connect to LangSmith and OpenAIclient = Client()oai_client = OpenAI()# Pull the prompt to use# You can also specify a specific commit by passing the commit hash "my-prompt:<commit-hash>"prompt = client.pull_prompt("my-prompt")# Since our prompt only has one variable we could also pass in the value directly# The code below is equivalent to formatted_prompt = prompt.invoke("What is the color of the sky?")formatted_prompt = prompt.invoke({"question":"What is the color of the sky?"})# Test the promptresponse = oai_client.chat.completions.create(model="gpt-4o",messages=convert_to_openai_messages(formatted_prompt.messages),)
```

```
import{ OpenAI }from"openai";import{ pull }from"langchain/hub";import{ convertPromptToOpenAI }from"@langchain/openai";// Connect to LangSmith and OpenAIconst oaiClient =newOpenAI();// Pull the prompt to use// You can also specify a specific commit by passing the commit hash "my-prompt:<commit-hash>"const prompt =awaitpull("my-prompt");// Format the prompt with the questionconst formattedPrompt =await prompt.invoke({ question:"What is the color of the sky?",});// Test the promptconst response =await oaiClient.chat.completions.create({ model:"gpt-4o", messages:convertPromptToOpenAI(formattedPrompt).messages,});
```

## 4. Iterate on a prompt[‚Äã](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk#4-iterate-on-a-prompt "Direct link to 4. Iterate on a prompt")
LangSmith makes it easy to iterate on prompts with your entire team. Members of your workspace can select a prompt to iterate on, and once they are happy with their changes, they can simply save it as a new commit.
To improve your prompts:
  * We recommend referencing the documentation provided by your model provider for best practices in prompt creation, such as [Best practices for prompt engineering with the OpenAI API](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api) and [Gemini‚Äôs Introduction to prompt design](https://ai.google.dev/gemini-api/docs/prompting-intro).
  * To help with iterating on your prompts in LangSmith, we've created Prompt Canvas ‚Äî an interactive tool to build and optimize your prompts. Learn about how to use [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-canvas).


To add a new commit to a prompt, you can use the same [`push_prompt`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.push_prompt) (Python) or [`pushPrompt`](https://langsmith-docs-7jgx2bq8f-langchain.vercel.app/reference/js/classes/client.Client#pushprompt) (TypeScript) methods as when you first created the prompt.
  * Python
  * TypeScript


```
from langsmith import Clientfrom langchain_core.prompts import ChatPromptTemplate# Connect to the LangSmith clientclient = Client()# Define the prompt to updatenew_prompt = ChatPromptTemplate([("system","You are a helpful chatbot. Respond in Spanish."),("user","{question}"),])# Push the updated prompt making sure to use the correct prompt name# Tags can help you remember specific versions in your commit historyclient.push_prompt("my-prompt",object=new_prompt, tags=["Spanish"])
```

```
import{ Client }from"langsmith";import{ ChatPromptTemplate }from"@langchain/core/prompts";// Connect to the LangSmith clientconst client =newClient();// Define the promptconst newPrompt = ChatPromptTemplate.fromMessages([["system","You are a helpful chatbot. Speak in Spanish."],["user","{question}"],]);// Push the updated prompt making sure to use the correct prompt name// Tags can help you remember specific versions in your commit historyawait client.pushPrompt("my-prompt",{ object: newPrompt, tags:["Spanish"],});
```

## 5. Next steps[‚Äã](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk#5-next-steps "Direct link to 5. Next steps")
  * Learn more about how to store and manage prompts using the Prompt Hub in [these how-to guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#prompt-hub)
  * Learn more about how to use the playground for prompt engineering in [these how-to guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#playground)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousQuick Start (UI)](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)[NextPrompt engineering tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
  * [1. Setup](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk#1-setup)
  * [2. Create a prompt](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk#2-create-a-prompt)
  * [3. Test a prompt](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk#3-test-a-prompt)
  * [4. Iterate on a prompt](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk#4-iterate-on-a-prompt)
  * [5. Next steps](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk#5-next-steps)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
      * [Quick Start (UI)](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
      * [Quick Start (SDK)](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Evaluation](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)


  * [](https://docs.smith.langchain.com/)
  * Prompt Engineering


On this page
# Prompt Engineering Quick Start (UI)
This quick start will walk through how to create, test, and iterate on prompts in LangSmith.
QuickStart
This tutorial uses the UI for prompt engineering, if you are interested in using the SDK instead, read [this guide](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk).
## 1. Setup[‚Äã](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#1-setup "Direct link to 1. Setup")
The only setup needed for this guide is to make sure you have signed up for a [LangSmith](https://langsmith.com) account.
## 2. Create a prompt[‚Äã](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#2-create-a-prompt "Direct link to 2. Create a prompt")
To create a prompt in LangSmith, navigate to the **Prompts** section of the left-hand sidebar and click on the ‚Äú+ New Prompt‚Äù button. You can then modify the prompt by editing/adding messages and input variables.
![](https://docs.smith.langchain.com/assets/images/create_prompt_ui-cbf004294a153b92d9bf31e05c31cb9e.gif)
## 3. Test a prompt[‚Äã](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#3-test-a-prompt "Direct link to 3. Test a prompt")
To test a prompt, set the model configuration you want to use, add your LLM provider's API key, specify the prompt input values you want to test, and then click "Start".
To learn about more options for configuring your prompt in the playground, check out this [guide](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations). If you are interested in testing how your prompt performs over a dataset instead of individual examples, read [this page](https://docs.smith.langchain.com/evaluation?mode=ui).
![](https://docs.smith.langchain.com/assets/images/test_prompt_ui-2e14355570ca743ccd19a947d859a2c3.gif)
## 4. Save a prompt[‚Äã](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#4-save-a-prompt "Direct link to 4. Save a prompt")
One you have run some tests and made your desired changes to your prompt you can click the ‚ÄúSave‚Äù button to save your prompt for future use.
![](https://docs.smith.langchain.com/assets/images/save_prompt_ui-1b8e22452b7e60bd004a3415abbfe21b.gif)
## 5. Iterate on a prompt[‚Äã](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#5-iterate-on-a-prompt "Direct link to 5. Iterate on a prompt")
LangSmith makes it easy to iterate on prompts with your entire team. Members of your workspace can select a prompt to iterate on in the playground, and once they are happy with their changes, they can simply save it as a new commit.
To improve your prompts:
  * We recommend referencing the documentation provided by your model provider for best practices in prompt creation, such as [Best practices for prompt engineering with the OpenAI API](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api) and [Gemini‚Äôs Introduction to prompt design](https://ai.google.dev/gemini-api/docs/prompting-intro).
  * To help with iterating on your prompts in LangSmith, we've created Prompt Canvas ‚Äî an interactive tool to build and optimize your prompts. Learn about how to use [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/concepts#prompt-canvas).


![](https://docs.smith.langchain.com/assets/images/save_prompt_commit_ui-4769dd50d77ded4d8b0647b6cf6d0fd8.gif)
You can also tag specific commits to mark important moments in your commit history:
![](https://docs.smith.langchain.com/assets/images/tag_prompt_ui-c03cd8243cce44568982a85e64ce6b20.gif)
## 6. Next steps[‚Äã](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#6-next-steps "Direct link to 6. Next steps")
  * Learn more about how to store and manage prompts using the Prompt Hub in [these how-to guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#prompt-hub)
  * Learn more about how to use the playground for prompt engineering in [these how-to guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#playground)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/prompt_engineering/quickstarts/quickstart_ui%3E).
[PreviousConceptual Guide](https://docs.smith.langchain.com/evaluation/concepts)[NextQuick Start (UI)](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [1. Setup](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#1-setup)
  * [2. Create a prompt](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#2-create-a-prompt)
  * [3. Test a prompt](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#3-test-a-prompt)
  * [4. Save a prompt](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#4-save-a-prompt)
  * [5. Iterate on a prompt](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#5-iterate-on-a-prompt)
  * [6. Next steps](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui#6-next-steps)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/tutorials

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/tutorials#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/tutorials)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/prompt_engineering/tutorials)
    * [Evaluation](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/prompt_engineering/tutorials)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * Tutorials


# Prompt engineering tutorials
New to LangSmith or to LLM app development in general? Read this material to quickly get up and running.
  * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/prompt_engineering/tutorials%3E).
[PreviousQuick Start (SDK)](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_sdk)[NextOptimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier

[Skip to main content](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Quickstarts](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
    * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
      * [Optimize a classifier](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [How-to Guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
      * [Create a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt)
      * [Run the playground against a custom LangServe model server](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint)
      * [Run the playground against an OpenAI-compliant model provider/proxy](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model)
      * [Update a prompt](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt)
      * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically)
      * [Managing Prompt Settings](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations)
      * [Prompt Tags](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags)
      * [Open a prompt from a trace](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace)
      * [LangChain Hub](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub)
      * [Prompt Canvas](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas)
    * [Conceptual Guide](https://docs.smith.langchain.com/prompt_engineering/concepts)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
    * [Evaluation](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier)


  * [](https://docs.smith.langchain.com/)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)
  * Optimize a classifier


On this page
# Optimize a classifier
This tutorial walks through optimizing a classifier based on user a feedback. Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback. That is exactly what we will do in this example.
## The objective[‚Äã](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier#the-objective "Direct link to The objective")
In this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.
## Getting started[‚Äã](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier#getting-started "Direct link to Getting started")
To get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:
```
import osos.environ["LANGSMITH_PROJECT"]="classifier"
```

We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.
```
import openaifrom langsmith import traceable, Clientimport uuidclient = openai.Client()available_topics =["bug","improvement","new_feature","documentation","integration",]prompt_template ="""Classify the type of the issue as one of {topics}.Issue: {text}"""@traceable(  run_type="chain",  name="Classifier",)deftopic_classifier(  topic:str):return client.chat.completions.create(    model="gpt-4o-mini",    temperature=0,    messages=[{"role":"user","content": prompt_template.format(          topics=','.join(available_topics),          text=topic,)}],).choices[0].message.content
```

We can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.
Here's how we can invoke the application:
```
run_id = uuid.uuid4()topic_classifier("fix bug in LCEL",  langsmith_extra={"run_id": run_id})
```

Here's how we can attach feedback after. We can collect feedback in two forms.
First, we can collect "positive" feedback - this is for examples that the model got right.
```
ls_client = Client()run_id = uuid.uuid4()topic_classifier("fix bug in LCEL",  langsmith_extra={"run_id": run_id})ls_client.create_feedback(  run_id,  key="user-score",  score=1.0,)
```

Next, we can focus on collecting feedback that corresponds to a "correction" to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.
```
ls_client = Client()run_id = uuid.uuid4()topic_classifier("fix bug in documentation",  langsmith_extra={"run_id": run_id})ls_client.create_feedback(  run_id,  key="correction",  correction="documentation")
```

## Set up automations[‚Äã](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier#set-up-automations "Direct link to Set up automations")
We can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.
The first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Let's create a dataset called `classifier-github-issues` to add this data to.
![Optimization Negative](https://docs.smith.langchain.com/assets/images/class-optimization-neg-67fd3aae244b7f73980ae39d58d1df87.png)
The second will take all runs with a correction and use a webhook to add them to a dataset. When creating this webhook, we will select the option to "Use Corrections". This option will make it so that when creating a dataset from a run, rather than using the output of the run as the gold-truth output of the datapoint, it will use the correction.
![Optimization Positive](https://docs.smith.langchain.com/assets/images/class-optimization-pos-2be610a400f5a8a60deb4da8af1205d2.png)
## Update the application[‚Äã](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier#update-the-application "Direct link to Update the application")
We can now update our code to pull down the dataset we are sending runs to. Once we pull it down, we can create a string with the examples in it. We can then put this string as part of the prompt!
```
### NEW CODE #### Initialize the LangSmith Client so we can use to get the datasetls_client = Client()# Create a function that will take in a list of examples and format them into a stringdefcreate_example_string(examples):  final_strings =[]for e in examples:    final_strings.append(f"Input: {e.inputs['topic']}\n> {e.outputs['output']}")return"\n\n".join(final_strings)### NEW CODE ###client = openai.Client()available_topics =["bug","improvement","new_feature","documentation","integration",]prompt_template ="""Classify the type of the issue as one of {topics}.Here are some examples:{examples}Begin!Issue: {text}>"""@traceable(  run_type="chain",  name="Classifier",)deftopic_classifier(  topic:str):# We can now pull down the examples from the dataset# We do this inside the function so it always get the most up-to-date examples,# But this can be done outside and cached for speed if desired  examples =list(ls_client.list_examples(dataset_name="classifier-github-issues"))# <- New Code  example_string = create_example_string(examples)return client.chat.completions.create(    model="gpt-4o-mini",    temperature=0,    messages=[{"role":"user","content": prompt_template.format(          topics=','.join(available_topics),          text=topic,          examples=example_string,)}],).choices[0].message.content
```

If now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as `documentation`
```
ls_client = Client()run_id = uuid.uuid4()topic_classifier("address bug in documentation",  langsmith_extra={"run_id": run_id})
```

## Semantic search over examples[‚Äã](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier#semantic-search-over-examples "Direct link to Semantic search over examples")
One additional thing we can do is only use the most semantically similar examples. This is useful when you start to build up a lot of examples.
In order to do this, we can first define an example to find the `k` most similar examples:
```
import numpy as npdeffind_similar(examples, topic, k=5):  inputs =[e.inputs['topic']for e in examples]+[topic]  vectors = client.embeddings.create(input=inputs, model="text-embedding-3-small")  vectors =[e.embedding for e in vectors.data]  vectors = np.array(vectors)  args = np.argsort(-vectors.dot(vectors[-1])[:-1])[:5]  examples =[examples[i]for i in args]return examples
```

We can then use that in the application
```
ls_client = Client()defcreate_example_string(examples):  final_strings =[]for e in examples:    final_strings.append(f"Input: {e.inputs['topic']}\n> {e.outputs['output']}")return"\n\n".join(final_strings)client = openai.Client()available_topics =["bug","improvement","new_feature","documentation","integration",]prompt_template ="""Classify the type of the issue as one of {topics}.Here are some examples:{examples}Begin!Issue: {text}>"""@traceable(  run_type="chain",  name="Classifier",)deftopic_classifier(  topic:str):  examples =list(ls_client.list_examples(dataset_name="classifier-github-issues"))  examples = find_similar(examples, topic)  example_string = create_example_string(examples)return client.chat.completions.create(    model="gpt-4o-mini",    temperature=0,    messages=[{"role":"user","content": prompt_template.format(          topics=','.join(available_topics),          text=topic,          examples=example_string,)}],).choices[0].message.content
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/prompt_engineering/tutorials/optimize_classifier%3E).
[PreviousPrompt engineering tutorials](https://docs.smith.langchain.com/prompt_engineering/tutorials)[NextPrompt engineering how-to guides](https://docs.smith.langchain.com/prompt_engineering/how_to_guides)
  * [The objective](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier#the-objective)
  * [Getting started](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier#getting-started)
  * [Set up automations](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier#set-up-automations)
  * [Update the application](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier#update-the-application)
  * [Semantic search over examples](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier#semantic-search-over-examples)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/reference

[Skip to main content](https://docs.smith.langchain.com/reference#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/reference)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference)
    * [Evaluation](https://docs.smith.langchain.com/reference)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference)


  * [](https://docs.smith.langchain.com/)
  * Reference


On this page
# Reference
Technical reference that covers components, APIs, and other aspects of LangSmith.
### API[‚Äã](https://docs.smith.langchain.com/reference#api "Direct link to API")
  * [API Reference](https://api.smith.langchain.com/redoc)


### SDK[‚Äã](https://docs.smith.langchain.com/reference#sdk "Direct link to SDK")
  * [Python SDK Reference](https://docs.smith.langchain.com/reference/python)
  * [JS/TS SDK Reference](https://docs.smith.langchain.com/reference/js)
  * [LangChain off-the-shelf evaluators (Python only)](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


### Common data types[‚Äã](https://docs.smith.langchain.com/reference#common-data-types "Direct link to Common data types")
  * [Run](https://docs.smith.langchain.com/reference/data_formats/run_data_format)
  * [Feedback](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)
  * [Example](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
  * [Run query syntax](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)
  * [Dataset schema prebuilt types](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)
  * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)


### Architecture[‚Äã](https://docs.smith.langchain.com/reference#architecture "Direct link to Architecture")
  * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
  * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)


### Organization settings[‚Äã](https://docs.smith.langchain.com/reference#organization-settings "Direct link to Organization settings")
  * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/reference%3E).
[PreviousFAQ](https://docs.smith.langchain.com/pricing/faq)[NextCloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
  * [API](https://docs.smith.langchain.com/reference#api)
  * [SDK](https://docs.smith.langchain.com/reference#sdk)
  * [Common data types](https://docs.smith.langchain.com/reference#common-data-types)
  * [Architecture](https://docs.smith.langchain.com/reference#architecture)
  * [Organization settings](https://docs.smith.langchain.com/reference#organization-settings)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods

[Skip to main content](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [Evaluation](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)


  * [](https://docs.smith.langchain.com/)
  * [Reference](https://docs.smith.langchain.com/reference)
  * Authz and Authn
  * Authentication methods


On this page
# Authentication methods
LangSmith supports multiple authentication methods for easy sign-up and login.
## Cloud[‚Äã](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#cloud "Direct link to Cloud")
### Email/Password[‚Äã](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#emailpassword "Direct link to Email/Password")
Users can use an email address and password to sign up and login to LangSmith.
### Social Providers[‚Äã](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#social-providers "Direct link to Social Providers")
Users can alternatively use their credentials from GitHub, Google, or Discord.
### SAML SSO[‚Äã](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#saml-sso "Direct link to SAML SSO")
Enterprise customers can configure [SAML SSO](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_saml_sso)
## Self-Hosted[‚Äã](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#self-hosted "Direct link to Self-Hosted")
Self-hosted customers have more control over how their users can login to LangSmith. For more in-depth coverage of configuration options, see [the self-hosting docs](https://docs.smith.langchain.com/self_hosting) and [Helm chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith).
### SSO with OAuth 2.0 and OIDC[‚Äã](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#sso-with-oauth-20-and-oidc "Direct link to SSO with OAuth 2.0 and OIDC")
Production installations should configure SSO in order to use an external identity provider. This enables users to login through an identity platform like Auth0/Okta. LangSmith supports almost any OIDC-compliant provider. Learn more about configuring SSO in the [SSO configuration guide](https://docs.smith.langchain.com/self_hosting/configuration/sso)
### Email/Password a.k.a. basic auth[‚Äã](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#basic-auth "Direct link to Email/Password a.k.a. basic auth")
This auth method requires very little configuration as it does not require an external identity provider. It is most appropriate to use for self-hosted trials. Learn more in the [basic auth configuration guide](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
### None[‚Äã](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#none "Direct link to None")
warning
This authentication mode will be removed after the launch of Basic Auth.
If zero authentication methods are enabled, a self-hosted installation does not require any login/sign-up. This configuration should only be used for verifying installation at the infrastructure level, as the feature set supported in this mode is restricted with only a single organization and workspace.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/reference/authentication_authorization/authentication_methods%3E).
[PreviousCloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)[NextExample data format](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
  * [Cloud](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#cloud)
    * [Email/Password](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#emailpassword)
    * [Social Providers](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#social-providers)
    * [SAML SSO](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#saml-sso)
  * [Self-Hosted](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#self-hosted)
    * [SSO with OAuth 2.0 and OIDC](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#sso-with-oauth-20-and-oidc)
    * [Email/Password a.k.a. basic auth](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#basic-auth)
    * [None](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#none)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability

[Skip to main content](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Evaluation](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)


  * [](https://docs.smith.langchain.com/)
  * [Reference](https://docs.smith.langchain.com/reference)
  * Cloud architecture and scalability


On this page
# Cloud architecture and scalability
Cloud-managed solution
This section is only relevant for the cloud-managed LangSmith services available at <https://smith.langchain.com> and <https://eu.smith.langchain.com>.
For information on the self-hosted LangSmith solution, please refer to the [self-hosted documentation](https://docs.smith.langchain.com/self_hosting).
LangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for both LLM application observability and evaluation.
## Architecture[‚Äã](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#architecture "Direct link to Architecture")
The US-based LangSmith service is deployed in the `us-central1` (Iowa) region of GCP.
**NOTE:** The [EU-based LangSmith service](https://eu.smith.langchain.com) is now available (as of mid-July 2024) and is deployed in the `europe-west4` (Netherlands) region of GCP. If you are interested in an enterprise plan in this region, please contact us at sales@langchain.dev.
### Regional storage[‚Äã](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#regional-storage "Direct link to Regional storage")
The resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU). Cloud-managed LangSmith uses [Supabase](https://supabase.com) for authentication/authorization and [ClickHouse Cloud](https://clickhouse.com/cloud) for data warehouse.
US| EU  
---|---  
URL| <https://smith.langchain.com>| <https://eu.smith.langchain.com>  
API URL| <https://api.smith.langchain.com>| <https://eu.api.smith.langchain.com>  
GCP| us-central1 (Iowa)| europe-west4 (Netherlands)  
Supabase| AWS us-east-1 (N. Virginia)| AWS eu-central-1 (Germany)  
ClickHouse Cloud| us-central1 (Iowa)| europe-west4 (Netherlands)  
[LangGraph Cloud](https://docs.smith.langchain.com/langgraph_cloud)| us-central1 (Iowa)| europe-west4 (Netherlands)  
See the [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq) for more information.
### Region-independent storage[‚Äã](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#region-independent-storage "Direct link to Region-independent storage")
Data listed here is stored exclusively in the US:
  * Payment and billing information with Stripe and Metronome


### GCP services[‚Äã](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#gcp-services "Direct link to GCP services")
LangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE):
  * LangSmith Frontend: serves the LangSmith UI.
  * LangSmith Backend: serves the LangSmith API.
  * LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)
  * LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.
  * LangSmith Queue: handles processing of asynchronous tasks. (Internal service)


LangSmith uses the following GCP storage services:
  * Google Cloud Storage (GCS) for runs inputs and outputs.
  * Google Cloud SQL PostgreSQL for transactional workloads.
  * Google Cloud Memorystore for Redis for queuing and caching.
  * Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint.


Some additional GCP services we use include:
  * Google Cloud Load Balancer for routing traffic to the LangSmith services.
  * Google Cloud CDN for caching static assets.
  * Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to [this guide](https://docs.smith.langchain.com/administration/concepts#rate-limits).


![](https://docs.smith.langchain.com/assets/images/cloud-arch-68cc65b66ea86af5126c9d052bd90d85.png)
## Scalability[‚Äã](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#scalability "Direct link to Scalability")
LangSmith is designed to be scalable and performant.
As of load testing done in February 2024, LangSmith can comfortably process 500K+ runs (spans) per minute. We anticipate that LangSmith can process 750K+ runs per minute with the optimizations we've made since then.
## Whitelisting IP addresses[‚Äã](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#whitelisting-ip-addresses "Direct link to Whitelisting IP addresses")
All traffic leaving LangSmith services will be routed through a NAT gateway. All traffic will appear to originate from the following IP addresses:
US| EU  
---|---  
34.59.65.97| 34.13.192.67  
34.67.51.221| 34.147.105.64  
34.46.212.37| 34.90.22.166  
34.132.150.88| 34.147.36.213  
35.188.222.201| 34.32.137.113  
34.58.194.127| 34.91.238.184  
34.59.97.173| 35.204.101.241  
104.198.162.55| 35.204.48.32  
It may be helpful to whitelist these IP addresses if connecting to your own AzureOpenAI service or other endpoints that may be required by the Playground or Online Evaluation.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/reference/cloud_architecture_and_scalability%3E).
[PreviousReference](https://docs.smith.langchain.com/reference)[NextAuthentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
  * [Architecture](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#architecture)
    * [Regional storage](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#regional-storage)
    * [Region-independent storage](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#region-independent-storage)
    * [GCP services](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#gcp-services)
  * [Scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#scalability)
  * [Whitelisting IP addresses](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#whitelisting-ip-addresses)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/reference/data_formats/dataset_json_types

[Skip to main content](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)
      * [Example data format](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
      * [Run (span) data format](https://docs.smith.langchain.com/reference/data_formats/run_data_format)
      * [Feedback data format](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)
      * [Trace query syntax](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)
      * [Dataset prebuilt JSON schema types](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)
    * [Evaluation](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)


  * [](https://docs.smith.langchain.com/)
  * [Reference](https://docs.smith.langchain.com/reference)
  * data_formats
  * Dataset prebuilt JSON schema types


# Dataset prebuilt JSON schema types
LangSmith recommends that you set a schema on the inputs and outputs of your dataset schemas to ensure data consistency and that your examples are in the right format for downstream processing, like running evals.
In order to better support LLM workflows, LangSmith has support for a few different predefined prebuilt types. These schemas are hosted publicly by the LangSmith API, and can be defined in your dataset schemas using [JSON Schema references](https://json-schema.org/understanding-json-schema/structuring#dollarref). The table of available schemas can be seen below
Type| JSON Schema Reference Link| Usage  
---|---|---  
Message| <https://api.smith.langchain.com/public/schemas/v1/message.json>| Represents messages sent to a chat model, following the OpenAI standard format.  
Tool| <https://api.smith.langchain.com/public/schemas/v1/tooldef.json>| Tool definitions available to chat models for function calling, defined in OpenAI's JSON Schema inspired function format.  
LangSmith lets you define a series of transformations that collect the above prebuilt types from your traces and add them to your dataset. For more info on available transformations, see our [reference](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/reference/data_formats/dataset_json_types%3E).
[PreviousTrace query syntax](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)[NextDataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/reference/data_formats/example_data_format

[Skip to main content](https://docs.smith.langchain.com/reference/data_formats/example_data_format#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
      * [Example data format](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
      * [Run (span) data format](https://docs.smith.langchain.com/reference/data_formats/run_data_format)
      * [Feedback data format](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)
      * [Trace query syntax](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)
      * [Dataset prebuilt JSON schema types](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Reference](https://docs.smith.langchain.com/reference)
  * data_formats
  * Example data format


# Example data format
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Conceptual guide on evaluation](https://docs.smith.langchain.com/evaluation/concepts)


LangSmith stores examples in datasets as follows:
Field Name| Type| Description  
---|---|---  
**id**|  UUID| Unique identifier for the example.  
**name**|  string| The name of the example.  
**created_at**|  datetime| The time this example was created  
**modified_at**|  datetime| The last time this example was modified  
**inputs**|  object| A map of inputs for the example.  
**outputs**|  object| A map or set of outputs generated by the run.  
**dataset_id**|  UUID| The dataset the example belongs to  
**source_run_id**|  UUID| If this example was created from a LangSmith [`Run`](https://docs.smith.langchain.com/reference/data_formats/run_data_format), the ID of said run  
**metadata**|  object| A map of additional, user or SDK defined information that can be stored on an example.  
To learn more about how examples are used in evaluation, read our how-to guide on [evaluating LLM applications](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application).
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousAuthentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)[NextRun (span) data format](https://docs.smith.langchain.com/reference/data_formats/run_data_format)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/reference/data_formats/feedback_data_format

[Skip to main content](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)
      * [Example data format](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
      * [Run (span) data format](https://docs.smith.langchain.com/reference/data_formats/run_data_format)
      * [Feedback data format](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)
      * [Trace query syntax](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)
      * [Dataset prebuilt JSON schema types](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)
    * [Evaluation](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)


  * [](https://docs.smith.langchain.com/)
  * [Reference](https://docs.smith.langchain.com/reference)
  * data_formats
  * Feedback data format


# Feedback data format
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Conceptual guide on tracing and feedback](https://docs.smith.langchain.com/observability/concepts)


**Feedback** is LangSmith's way of storing the criteria and scores from evaluation on a particular trace or intermediate run (span). Feedback can be produced from a variety of ways, such as:
  1. [Sent up along with a trace](https://docs.smith.langchain.com/evaluation/how_to_guides/attach_user_feedback) from the LLM application
  2. Generated by a user in the app [inline](https://docs.smith.langchain.com/evaluation/how_to_guides/annotate_traces_inline) or in an [annotation queue](https://docs.smith.langchain.com/evaluation/how_to_guides/annotation_queues)
  3. Generated by an automatic evaluator during [offline evaluation](https://docs.smith.langchain.com/evaluation/how_to_guides/evaluate_llm_application)
  4. Generated by an [online evaluator](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations)


Feedback is stored in a simple format with the following fields:
Field Name| Type| Description  
---|---|---  
id| UUID| Unique identifier for the record itself  
created_at| datetime| Timestamp when the record was created  
modified_at| datetime| Timestamp when the record was last modified  
session_id| UUID| Unique identifier for the experiment or tracing project the run was a part of  
run_id| UUID| Unique identifier for a specific run within a session  
key| string| A key describing the criteria of the feedback, eg "correctness"  
score| number| Numerical score associated with the feedback key  
value| string| Reserved for storing a value associated with the score. Useful for categorical feedback.  
comment| string| Any comment or annotation associated with the record. This can be a justification for the score given.  
correction| object| Reserved for storing correction details, if any  
feedback_source| object| Object containing information about the feedback source  
feedback_source.type| string| The type of source where the feedback originated, eg "api", "app", "evaluator"  
feedback_source.metadata| object| Reserved for additional metadata, currently  
feedback_source.user_id| UUID| Unique identifier for the user providing feedback  
Here is an example JSON representation of a feedback record in the above format:
```
{"created_at":"2024-05-05T23:23:11.077838","modified_at":"2024-05-05T23:23:11.232962","session_id":"c919298b-0af2-4517-97a2-0f98ed4a48f8","run_id":"e26174e5-2190-4566-b970-7c3d9a621baa","key":"correctness","score":1.0,"value":null,"comment":"I gave this score because the answer was correct.","correction":null,"id":"62104630-c7f5-41dc-8ee2-0acee5c14224","feedback_source":{"type":"app","metadata":null,"user_id":"ad52b092-1346-42f4-a934-6e5521562fab"}}
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/reference/data_formats/feedback_data_format%3E).
[PreviousRun (span) data format](https://docs.smith.langchain.com/reference/data_formats/run_data_format)[NextTrace query syntax](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/reference/data_formats/run_data_format

[Skip to main content](https://docs.smith.langchain.com/reference/data_formats/run_data_format#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/reference/data_formats/run_data_format)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
      * [Example data format](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
      * [Run (span) data format](https://docs.smith.langchain.com/reference/data_formats/run_data_format)
      * [Feedback data format](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)
      * [Trace query syntax](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)
      * [Dataset prebuilt JSON schema types](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Reference](https://docs.smith.langchain.com/reference)
  * data_formats
  * Run (span) data format


On this page
# Run (span) data format
Recommended Reading
Before diving into this content, it might be helpful to read the following:
  * [Conceptual guide on tracing and runs](https://docs.smith.langchain.com/observability/concepts)


LangSmith stores and processes trace data in a simple format that is easy to export and import.
Many of these fields are optional or not important to know about but are included for completeness. The **bolded** fields are the most important ones to know about.
Field Name| Type| Description  
---|---|---  
**id**|  UUID| Unique identifier for the span.  
**name**|  string| The name associated with the run.  
**inputs**|  object| A map or set of inputs provided to the run.  
**run_type**|  string| Type of run, e.g., "llm", "chain", "tool".  
**start_time**|  datetime| Start time of the run.  
**end_time**|  datetime| End time of the run.  
**extra**|  object| Any extra information run.  
**error**|  string| Error message if the run encountered an error.  
**outputs**|  object| A map or set of outputs generated by the run.  
**events**|  array of objects| A list of event objects associated with the run. This is relevant for runs executed with streaming.  
**tags**|  array of strings| Tags or labels associated with the run.  
**trace_id**|  UUID| Unique identifier for the trace the run is a part of. This is also the `id` field of the root run of the trace  
**dotted_order**|  string| Ordering string, hierarchical. Format: `run_start_time`Z`run_uuid`.`child_run_start_time`Z`child_run_uuid`...  
**status**|  string| Current status of the run execution, e.g., "error", "pending", "success"  
**child_run_ids**|  array of UUIDs| List of IDs for all child runs.  
**direct_child_run_ids**|  array of UUIDs| List of IDs for direct children of this run.  
**parent_run_ids**|  array of UUIDs| List of IDs for all parent runs.  
**feedback_stats**|  object| Aggregations of feedback statistics for this run  
**reference_example_id**|  UUID| ID of a reference example associated with the run. This is usually only present for evaluation runs.  
**total_tokens**|  integer| Total number of tokens processed by the run.  
**prompt_tokens**|  integer| Number of tokens in the prompt of the run.  
**completion_tokens**|  integer| Number of tokens in the completion of the run.  
**total_cost**|  string| Total cost associated with processing the run.  
**prompt_cost**|  string| Cost associated with the prompt part of the run.  
**completion_cost**|  string| Cost associated with the completion of the run.  
**first_token_time**|  datetime| Time when the first token was generated.  
**session_id**|  string| Session identifier for the run.  
**in_dataset**|  boolean| Indicates whether the run is included in a dataset.  
**parent_run_id**|  UUID| Unique identifier of the parent run.  
execution_order (deprecated)| integer| The order in which this run was executed within the trace.  
serialized| object| Serialized state of the object executing the run if applicable.  
manifest_id (deprecated)| UUID| Identifier for a manifest associated with the span.  
manifest_s3_id| UUID| S3 identifier for the manifest.  
inputs_s3_urls| object| S3 URLs for the inputs.  
outputs_s3_urls| object| S3 URLs for the outputs.  
price_model_id| UUID| Identifier for the pricing model applied to the run.  
app_path| string| Application (UI) path for this run.  
last_queued_at| datetime| Last time the span was queued.  
share_token| string| Token for sharing access to the run's data.  
Here is an example of a JSON representation of a run in the above format:
```
{"id":"497f6eca-6276-4993-bfeb-53cbbbba6f08","name":"string","inputs":{},"run_type":"llm","start_time":"2024-04-29T00:49:12.090000","end_time":"2024-04-29T00:49:12.459000","extra":{},"error":"string","execution_order":1,"serialized":{},"outputs":{},"parent_run_id":"f8faf8c1-9778-49a4-9004-628cdb0047e5","manifest_id":"82825e8e-31fc-47d5-83ce-cd926068341e","manifest_s3_id":"0454f93b-7eb6-4b9d-a203-f1261e686840","events":[{}],"tags":["foo"],"inputs_s3_urls":{},"outputs_s3_urls":{},"trace_id":"df570c03-5a03-4cea-8df0-c162d05127ac","dotted_order":"20240429T004912090000Z497f6eca-6276-4993-bfeb-53cbbbba6f08","status":"string","child_run_ids":["497f6eca-6276-4993-bfeb-53cbbbba6f08"],"direct_child_run_ids":["497f6eca-6276-4993-bfeb-53cbbbba6f08"],"parent_run_ids":["f8faf8c1-9778-49a4-9004-628cdb0047e5"],"feedback_stats":{"correctness":{"n":1,"avg":1.0}},"reference_example_id":"9fb06aaa-105f-4c87-845f-47d62ffd7ee6","total_tokens":0,"prompt_tokens":0,"completion_tokens":0,"total_cost":"string","prompt_cost":"string","completion_cost":"string","price_model_id":"0b5d9575-bec3-4256-b43a-05893b8b8440","first_token_time":null,"session_id":"1ffd059c-17ea-40a8-8aef-70fd0307db82","app_path":"string","last_queued_at":null,"in_dataset":true,"share_token":"d0430ac3-04a1-4e32-a7ea-57776ad22c1c"}
```

#### What is `dotted_order`?[‚Äã](https://docs.smith.langchain.com/reference/data_formats/run_data_format#what-is-dotted_order "Direct link to what-is-dotted_order")
A run's dotted order is a sortable key that fully specifies its location within the tracing hierarchy.
Take the following example:
```
import langsmith as ls@ls.traceabledefgrandchild():  p("grandchild")@ls.traceabledefchild():  grandchild()@ls.traceabledefparent():  child()
```

If you print out the IDs at each stage, you may get the following:
```
parent	run_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7	trace_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7 parent_run_id=null	dotted_order=20240919T171648521691Z0e01bf50-474d-4536-810f-67d3ee7ea3e7child	run_id=a8024e23-5b82-47fd-970e-f6a5ba3f5097	trace_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7 parent_run_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7	dotted_order=20240919T171648521691Z0e01bf50-474d-4536-810f-67d3ee7ea3e7.20240919T171648523407Za8024e23-5b82-47fd-970e-f6a5ba3f5097grandchild	run_id=0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6	trace_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7 parent_run_id=a8024e23-5b82-47fd-970e-f6a5ba3f5097	dotted_order=20240919T171648521691Z0e01bf50-474d-4536-810f-67d3ee7ea3e7.20240919T171648523407Za8024e23-5b82-47fd-970e-f6a5ba3f5097.20240919T171648523563Z0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6
```

Note a few invariants:
  * The "id" is equal to the last 36 characters of the dotted order (the suffix after the final "Z"). See `0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6` for example in the grandchild.
  * The "trace_id" is equal to the first UUID in the dotted order (i.e., `dotted_order.split('.')[0].split('Z')[1]`)
  * If "parent_run_id" exists, it is the penultimate UUID in the dotted order. See `a8024e23-5b82-47fd-970e-f6a5ba3f5097` in the grandchild, for an example.
  * If you split the dotted_order on the dots, each segment is formatted as (`<run_start_time>Z<run_id>`)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousExample data format](https://docs.smith.langchain.com/reference/data_formats/example_data_format)[NextFeedback data format](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax

[Skip to main content](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)
      * [Example data format](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
      * [Run (span) data format](https://docs.smith.langchain.com/reference/data_formats/run_data_format)
      * [Feedback data format](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)
      * [Trace query syntax](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)
      * [Dataset prebuilt JSON schema types](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)
    * [Evaluation](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax)


  * [](https://docs.smith.langchain.com/)
  * [Reference](https://docs.smith.langchain.com/reference)
  * data_formats
  * Trace query syntax


On this page
# Trace query syntax
Using the `list_runs` method in the SDK or `/runs/query` endpoint in the API, you can filter runs to analyze and export.
## Filter arguments[‚Äã](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-arguments "Direct link to Filter arguments")
Keys| Description  
---|---  
`project_id` / `project_name`| The project(s) to fetch runs from - can be a single project or a list of projects.  
`trace_id`| Fetch runs that are part of a specific trace.  
`run_type`| The type of run to get, such as `llm`, `chain`, `tool`, `retriever`, etc.  
`dataset_name` / `dataset_id`| Fetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.  
`reference_example_id`| Fetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.  
`parent_run_id`| Fetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.  
`error`| Fetch runs that errored or did not error.  
`run_ids`| Fetch runs with a given list of run ids. Note: **This will ignore all other filtering arguments.**  
`filter`|  Fetch runs that match a given structured filter statement. See the guide below for more information.  
`trace_filter`| Filter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of the root run within a trace.  
`tree_filter`| Filter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of any run within a trace.  
`is_root`| Only return root runs.  
`select`| Select the fields to return in the response. By default, all fields are returned.  
`query` (_experimental_)| Natural language query, which translates your query into a filter statement.  
## Filter query language[‚Äã](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-query-language "Direct link to Filter query language")
LangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs.
The filtering grammar is based on common comparators on fields in the run object. Supported comparators include:
  * `gte` (greater than or equal to)
  * `gt` (greater than)
  * `lte` (less than or equal to)
  * `lt` (less than)
  * `eq` (equal to)
  * `neq` (not equal to)
  * `has` (check if run contains a tag or metadata json blob)
  * `search` (search for a substring in a string field)


Additionally, you can combine multiple comparisons through `and` and `or` operators.
These can be applied on fields of the run object, such as its `id`, `name`, `run_type`, `start_time` / `end_time`, `latency`, `total_tokens`, `error`, `execution_order`, `tags`, and any associated feedback through `feedback_key` and `feedback_score`.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/reference/data_formats/trace_query_syntax%3E).
[PreviousFeedback data format](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format)[NextDataset prebuilt JSON schema types](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)
  * [Filter arguments](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-arguments)
  * [Filter query language](https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-query-language)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/reference/evaluation/dataset_transformations

[Skip to main content](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Reference](https://docs.smith.langchain.com/reference)
  * Evaluation
  * Dataset transformations


On this page
# Dataset transformations
LangSmith allows you to attach transformations to fields in your dataset's schema that apply to your data before it is added to your dataset, whether that be from UI, API, or run rules.
Coupled with [LangSmith's prebuilt JSON schema types](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types), these allow you to do easy preprocessing of your data before saving it into your datasets.
## Transformation types[‚Äã](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#transformation-types "Direct link to Transformation types")
Transformation Type| Target Types| Functionality  
---|---|---  
remove_system_messages| Array[Message]| Filters a list of messages to remove any system messages.  
convert_to_openai_message| Message Array[Message]| Converts any incoming data from LangChain's internal serialization format to OpenAI's standard message format using langchain's [convert_to_openai_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.convert_to_openai_messages.html). If the target field is marked as required, and no matching message is found upon entry, it will attempt to extract a message (or list of messages) from several well-known LangSmith tracing formats (e.g., any traced LangChain [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) run or traced run from the [LangSmith OpenAI wrapper](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#wrap-the-openai-client)), and remove the original key containing the message.  
convert_to_openai_tool| Array[Tool] Only available on top level fields in the inputs dictionary.| Converts any incoming data into OpenAI standard tool formats here using langchain's [convert_to_openai_tool](https://python.langchain.com/api_reference/core/utils/langchain_core.utils.function_calling.convert_to_openai_tool.html) Will extract tool definitions from a run's invocation parameters if present / no tools are found at the specified key. This is useful because LangChain chat models trace tool definitions to the `extra.invocation_params` field of the run rather than inputs.  
remove_extra_fields| Object| Removes any field not defined in the schema for this target object.  
## Chat Model prebuilt schema[‚Äã](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#chat-model-prebuilt-schema "Direct link to Chat Model prebuilt schema")
The main use case for transformations is to simplify collecting production traces into datasets in a format that can be standardized across model providers for usage in evaluations / few shot prompting / etc downstream.
To simplify setup of transformations for our end users, LangSmith offers a pre-defined schema that will do the following:
  * Extract messages from your collected runs and transform them into the openai standard format, which makes them compatible all LangChain ChatModels and most model providers' SDK for downstream evaluation and experimentation
  * Extract any tools used by your LLM and add them to your example's input to be used for reproducability in downstream evaluation


tip
Users who want to iterate on their system prompts often also add the Remove System Messages transformation on their input messages when using our Chat Model schema, which will prevent you from saving the system prompt to your dataset.
### Compatibility[‚Äã](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#compatibility "Direct link to Compatibility")
The LLM run collection schema is built to collect data from LangChain [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) runs or traced runs from the [LangSmith OpenAI wrapper](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code#wrap-the-openai-client).
Please reach out to support@langchain.dev if you have an LLM run you are tracing that is not compatible and we can extend support.
If you want to apply transformations to other sorts of runs (for example, representing LangGraph state with message history), please define your schema directly and manually add the relevant transformations.
### Enablement[‚Äã](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#enablement "Direct link to Enablement")
When adding a run from a tracing project or annotation queue to a dataset, if it has the LLM run type, we will apply the Chat Model schema by default.
For enablement on new datasets, see our [dataset management how-to guide](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application).
### Specs[‚Äã](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#specs "Direct link to Specs")
For the full API specs of the prebuilt schema, see the below sections:
#### Input Schema[‚Äã](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#input-schema "Direct link to Input Schema")
```
{"type":"object","properties":{"messages":{"type":"array","items":{"$ref":"https://api.smith.langchain.com/public/schemas/v1/message.json"}},"tools":{"type":"array","items":{"$ref":"https://api.smith.langchain.com/public/schemas/v1/tooldef.json"}}},"required":["messages"]}
```

#### Output Schema[‚Äã](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#output-schema "Direct link to Output Schema")
```
{"type":"object","properties":{"message":{"$ref":"https://api.smith.langchain.com/public/schemas/v1/message.json"}},"required":["message"]}
```

#### Transformations[‚Äã](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#transformations "Direct link to Transformations")
And the transformations look as follows:
```
[{"path":["inputs"],"transformation_type":"remove_extra_fields"},{"path":["inputs","messages"],"transformation_type":"convert_to_openai_message"},{"path":["inputs","tools"],"transformation_type":"convert_to_openai_tool"},{"path":["outputs"],"transformation_type":"remove_extra_fields"},{"path":["outputs","message"],"transformation_type":"convert_to_openai_message"}]
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousDataset prebuilt JSON schema types](https://docs.smith.langchain.com/reference/data_formats/dataset_json_types)[NextRegions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
  * [Transformation types](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#transformation-types)
  * [Chat Model prebuilt schema](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#chat-model-prebuilt-schema)
    * [Compatibility](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#compatibility)
    * [Enablement](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#enablement)
    * [Specs](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations#specs)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/reference/regions_faq

[Skip to main content](https://docs.smith.langchain.com/reference/regions_faq#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/reference/regions_faq)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/regions_faq)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/regions_faq)
    * [Evaluation](https://docs.smith.langchain.com/reference/regions_faq)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/regions_faq)


  * [](https://docs.smith.langchain.com/)
  * [Reference](https://docs.smith.langchain.com/reference)
  * Regions FAQ


On this page
# Regions FAQ
note
See the [cloud architecture reference](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#architecture) for additional details.
## Legal and compliance[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#legal-and-compliance "Direct link to Legal and compliance")
#### _What privacy and data protection frameworks does LangSmith, including its EU instance, comply with?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#what-privacy-and-data-protection-frameworks-does-langsmith-including-its-eu-instance-comply-with "Direct link to what-privacy-and-data-protection-frameworks-does-langsmith-including-its-eu-instance-comply-with")
LangSmith complies with the General Data Protection Regulation (GDPR) and other laws and regulations applicable to the LangSmith service. We are also SOC 2 Type 2 certified and are HIPAA compliant. You can request more information about our security policies and posture at [trust.langchain.com](https://trust.langchain.com). If you would like to sign a Data Processing Addendum (DPA) with us, please reach out to support@langchain.dev. Please note we only enter into Business Associate Agreements (BAAs) with customers on our Enterprise plan.
#### _My company isn‚Äôt based in the EU, can I still have my data hosted there?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#my-company-isnt-based-in-the-eu-can-i-still-have-my-data-hosted-there "Direct link to my-company-isnt-based-in-the-eu-can-i-still-have-my-data-hosted-there")
Yes, you can host your LangSmith data in the EU instance independent of your location.
#### _Do you have a legal entity in the EU that we can contract with?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#do-you-have-a-legal-entity-in-the-eu-that-we-can-contract-with "Direct link to do-you-have-a-legal-entity-in-the-eu-that-we-can-contract-with")
We do not have a legal entity in the EU for customer contracting today.
#### _Do different legal terms apply if I choose the EU region?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#do-different-legal-terms-apply-if-i-choose-the-eu-region "Direct link to do-different-legal-terms-apply-if-i-choose-the-eu-region")
The terms are the same for the EU and US regions.
## Features[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#features "Direct link to Features")
#### _How do I use the EU instance?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#how-do-i-use-the-eu-instance "Direct link to how-do-i-use-the-eu-instance")
Follow the instructions [here](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key) to create an account and an API key (make sure to change the region to EU in the dropdown)
#### _Are there any functional differences between US and EU cloud-managed LangSmith?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#are-there-any-functional-differences-between-us-and-eu-cloud-managed-langsmith "Direct link to are-there-any-functional-differences-between-us-and-eu-cloud-managed-langsmith")
There may be a small delay between launches to each region depending on the feature. Besides that, they are functionally equivalent - all features supported in the US are supported in the EU and vice versa.
#### _Can an organization have workspaces in different regions?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#can-an-organization-have-workspaces-in-different-regions "Direct link to can-an-organization-have-workspaces-in-different-regions")
LangSmith does not support this at the moment, but if you are interested, please contact support@langchain.dev and share your use case.
#### _Can I connect an EU organization to a US organization and share billing?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#can-i-connect-an-eu-organization-to-a-us-organization-and-share-billing "Direct link to can-i-connect-an-eu-organization-to-a-us-organization-and-share-billing")
LangSmith does not support this at the moment, but if you are interested, please contact support@langchain.dev and share your use case.
#### _What data will be stored in my selected region?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#what-data-will-be-stored-in-my-selected-region "Direct link to what-data-will-be-stored-in-my-selected-region")
See the [cloud architecture reference](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability#architecture) for details.
#### _How can I see my organization's region?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#how-can-i-see-my-organizations-region "Direct link to how-can-i-see-my-organizations-region")
Check your URL - any organizations on <https://eu.smith.langchain.com> are in the EU, and any on <https://smith.langchain.com> are in the US.
#### _Can I switch my organization from the US to EU or vice versa?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#can-i-switch-my-organization-from-the-us-to-eu-or-vice-versa "Direct link to can-i-switch-my-organization-from-the-us-to-eu-or-vice-versa")
We do not support migration between regions at this time, but if you are interested in this feature, please reach out to support@langchain.dev.
## Plans and pricing[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#plans-and-pricing "Direct link to Plans and pricing")
#### _Is the EU region available on all LangSmith plans?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#is-the-eu-region-available-on-all-langsmith-plans "Direct link to is-the-eu-region-available-on-all-langsmith-plans")
Yes, you can sign up for the EU region on all plans including free plans.
#### _Is pricing different for the EU region compared to the US region?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#is-pricing-different-for-the-eu-region-compared-to-the-us-region "Direct link to is-pricing-different-for-the-eu-region-compared-to-the-us-region")
No, pricing is the same for the EU and US regions.
#### _What currency is used for payment if I use the EU region?_[‚Äã](https://docs.smith.langchain.com/reference/regions_faq#what-currency-is-used-for-payment-if-i-use-the-eu-region "Direct link to what-currency-is-used-for-payment-if-i-use-the-eu-region")
All LangSmith plans are paid in USD.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/reference/regions_faq%3E).
[PreviousDataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)[NextLangChain off-the-shelf evaluators](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)
  * [Legal and compliance](https://docs.smith.langchain.com/reference/regions_faq#legal-and-compliance)
  * [Features](https://docs.smith.langchain.com/reference/regions_faq#features)
  * [Plans and pricing](https://docs.smith.langchain.com/reference/regions_faq#plans-and-pricing)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators

[Skip to main content](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)
    * [Evaluation](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)
      * [LangChain off-the-shelf evaluators](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Reference](https://docs.smith.langchain.com/reference)
  * sdk_reference
  * LangChain off-the-shelf evaluators


# LangChain off-the-shelf evaluators
LangChain's evaluation module provides evaluators you can use as-is for common evaluation scenarios. To learn how to use these evaluators, please refer to the [following guide](https://docs.smith.langchain.com/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old).
note
We currently support off-the-shelf evaluators in LangChain for Python only.
note
Most of these evaluators are useful but imperfect! We recommend against blind trust of any single automated metric and to always incorporate them as a part of a holistic testing and evaluation strategy. Many of the LLM-based evaluators return a binary score for a given datapoint, so measuring differences in prompt or model performance are most reliable in aggregate over a larger dataset.
The following table enumerates the off-the-shelf evaluators available in LangSmith, along with their output keys and a simple code sample.
Evaluator name| Output Key| Simple Code Example  
---|---|---  
Q&A| `correctness`| `LangChainStringEvaluator("qa")`  
Contextual Q&A| `contextual accuracy`| `LangChainStringEvaluator("context_qa")`  
Chain of Thought Q&A| `cot contextual accuracy`| `LangChainStringEvaluator("cot_qa")`  
Criteria| Depends on criteria key| `LangChainStringEvaluator("criteria", config={ "criteria": <criterion> })``criterion` may be one of the default implemented criteria: `conciseness`, `relevance`, `correctness`, `coherence`, `harmfulness`, `maliciousness`, `helpfulness`, `controversiality`, `misogyny`, and `criminality`.Or, you may define your own criteria in a custom dict as follows:`{ "criterion_key": "criterion description" }`  
Labeled Criteria| Depends on criteria key| `LangChainStringEvaluator("labeled_criteria", config={ "criteria": <criterion> })``criterion` may be one of the default implemented criteria: `conciseness`, `relevance`, `correctness`, `coherence`, `harmfulness`, `maliciousness`, `helpfulness`, `controversiality`, `misogyny`, and `criminality`.Or, you may define your own criteria in a custom dict as follows:`{ "criterion_key": "criterion description" }`  
Score| Depends on criteria key| `LangChainStringEvaluator("score_string", config={ "criteria": <criterion>, "normalize_by": 10 })``criterion` may be one of the default implemented criteria: `conciseness`, `relevance`, `correctness`, `coherence`, `harmfulness`, `maliciousness`, `helpfulness`, `controversiality`, `misogyny`, and `criminality`.Or, you may define your own criteria in a custom dict as follows:`{ "criterion_key": "criterion description" }`. Scores are out of 10, so normalize_by will cast this to a score from 0 to 1.  
Labeled Score| Depends on criteria key| `LangChainStringEvaluator("labeled_score_string", config={ "criteria": <criterion>, "normalize_by": 10 })``criterion` may be one of the default implemented criteria: `conciseness`, `relevance`, `correctness`, `coherence`, `harmfulness`, `maliciousness`, `helpfulness`, `controversiality`, `misogyny`, and `criminality`.Or, you may define your own criteria in a custom dict as follows:`{ "criterion_key": "criterion description" }`. Scores are out of 10, so normalize_by will cast this to a score from 0 to 1.  
Embedding distance| `embedding_cosine_distance`| `LangChainStringEvaluator("embedding_distance")`  
String Distance| `string_distance`| `LangChainStringEvaluator("string_distance", config={"distance": "damerau_levenshtein" })` `distance` defines the string difference metric to be applied, such as `levenshtein` or `jaro_winkler`.  
Exact Match| `exact_match`| `LangChainStringEvaluator("exact_match")`  
Regex Match| `regex_match`| `LangChainStringEvaluator("regex_match")`  
Json Validity| `json_validity`| `LangChainStringEvaluator("json_validity")`  
Json Equality| `json_equality`| `LangChainStringEvaluator("json_equality")`  
Json Edit Distance| `json_edit_distance`| `LangChainStringEvaluator("json_edit_distance")`  
Json Schema| `json_schema`| `LangChainStringEvaluator("json_schema")`  
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/reference/sdk_reference/langchain_evaluators%3E).
[PreviousRegions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting

[Skip to main content](https://docs.smith.langchain.com/self_hosting#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting)


  * [](https://docs.smith.langchain.com/)
  * Self-hosting


# Self-Hosting LangSmith
Step-by-step guides that cover the installation, configuration, and scaling of your Self-Hosted LangSmith instance.
  * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview): A high-level overview of the LangSmith architecture. 
    * [Storage services](https://docs.smith.langchain.com/self_hosting/architectural_overview#datastores): The storage services used by LangSmith.
    * [Services](https://docs.smith.langchain.com/self_hosting/architectural_overview#services): The services that make up LangSmith.
  * [Installation](https://docs.smith.langchain.com/self_hosting/installation): How to install LangSmith on your own infrastructure. 
    * [Kubernetes](https://docs.smith.langchain.com/self_hosting/installation/kubernetes): Deploy LangSmith on Kubernetes.
    * [Docker](https://docs.smith.langchain.com/self_hosting/installation/docker): Deploy LangSmith using Docker.
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration): How to configure your self-hosted instance of LangSmith. 
    * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso): Configure LangSmith to use OAuth2.0 and OIDC for SSO.
    * [Connect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse): Configure LangSmith to use an external ClickHouse database.
    * [Connect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres): Configure LangSmith to use an external Postgres database.
    * [Connect to an external Redis instance](https://docs.smith.langchain.com/self_hosting/configuration/external_redis): Configure LangSmith to use an external Redis instance.
    * [Email/password a.k.a. basic auth (beta)](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth): Configure LangSmith to use email/password authentication.
    * [Blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage): Configure LangSmith to use blob storage.
    * [TTLs](https://docs.smith.langchain.com/self_hosting/configuration/ttl): Configure LangSmith to use TTLs.
  * [Usage](https://docs.smith.langchain.com/self_hosting/usage): How to use your self-hosted instance of LangSmith.
  * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts): View trace counts for your organization and workspaces.
  * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades): How to upgrade your self-hosted instance of LangSmith.
  * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress): Egress requirements for Subscription Metrics and Operational Metadata.
  * [Release notes](https://docs.smith.langchain.com/self_hosting/release_notes): The latest release notes for LangSmith.
  *     * [Week of August 26, 2024 - LangSmith v0.7](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-august-26-2024---langsmith-v07): Release notes for version 0.7 of LangSmith.
    * [Week of June 17, 2024 - LangSmith v0.6](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05): Release notes for version 0.6 of LangSmith.
    * [Week of May 13, 2024 - LangSmith v0.5](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05): Release notes for version 0.5 of LangSmith.
    * [Week of March 25, 2024 - LangSmith v0.4](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04): Release notes for version 0.4 of LangSmith.
    * [Week of Februrary 21, 2024 - LangSmith v0.3](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03): Release notes for version 0.3 of LangSmith.
    * [Week of January 29, 2024 - LangSmith v0.2](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02): Release notes for version 0.2 of LangSmith.
  * [FAQ](https://docs.smith.langchain.com/self_hosting/faq): Frequently asked questions about LangSmith.
  * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting): Troubleshooting common issues with your Self-Hosted LangSmith instance.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting%3E).
[PreviousConceptual Guide](https://docs.smith.langchain.com/administration/concepts)[NextArchitectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/architectural_overview

[Skip to main content](https://docs.smith.langchain.com/self_hosting/architectural_overview#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/architectural_overview)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/architectural_overview)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/architectural_overview)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/architectural_overview)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * Architectural overview


On this page
# Architectural overview
Enterprise License Required
Self-Hosted LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our [pricing page](https://www.langchain.com/pricing) for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment.
LangSmith can be run via Kubernetes (recommended) or Docker in a Cloud environment that you control.
The LangSmith application consists of several components including 5 LangSmith servers and 3 stateful services:
  * LangSmith Frontend
  * LangSmith Backend
  * LangSmith Platform Backend
  * LangSmith Playground
  * LangSmith Queue
  * LangSmith ACE(Arbitrary Code Execution) Backend
  * ClickHouse
  * Postgres
  * Redis


![./static/self_hosted_architecture_diagram.png](https://docs.smith.langchain.com/assets/images/self_hosted_architecture_diagram-fbad3c8ee6b44e73b0eb451e003bab1b.png)
To access the LangSmith UI and send API requests, you will need to expose the LangSmith Frontend service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine.
## Storage Services[‚Äã](https://docs.smith.langchain.com/self_hosting/architectural_overview#storage-services "Direct link to Storage Services")
note
LangSmith Self-Hosted will bundle all storage services by default. LangSmith can be configured to use external versions of all storage services. In a production setting, we **strongly recommend using external Storage Services**.
### ClickHouse[‚Äã](https://docs.smith.langchain.com/self_hosting/architectural_overview#clickhouse "Direct link to ClickHouse")
[ClickHouse](https://clickhouse.com/docs/en/intro) is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP).
LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data).
### PostgreSQL[‚Äã](https://docs.smith.langchain.com/self_hosting/architectural_overview#postgresql "Direct link to PostgreSQL")
[PostgreSQL](https://www.postgresql.org/about/) is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads
LangSmith uses Postgres as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback).
### Redis[‚Äã](https://docs.smith.langchain.com/self_hosting/architectural_overview#redis "Direct link to Redis")
[Redis](https://github.com/redis/redis) is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching.
LangSmith uses Redis to back queuing/caching operations.
## Services[‚Äã](https://docs.smith.langchain.com/self_hosting/architectural_overview#services "Direct link to Services")
### LangSmith Frontend[‚Äã](https://docs.smith.langchain.com/self_hosting/architectural_overview#langsmith-frontend "Direct link to LangSmith Frontend")
The frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users.
### LangSmith Backend[‚Äã](https://docs.smith.langchain.com/self_hosting/architectural_overview#langsmith-backend "Direct link to LangSmith Backend")
The backend is the primary entrypoint for API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and sdk, preparing traces for ingestion, and supporting the hub API.
### LangSmith Queue[‚Äã](https://docs.smith.langchain.com/self_hosting/architectural_overview#langsmith-queue "Direct link to LangSmith Queue")
The queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database.
### LangSmith Platform Backend[‚Äã](https://docs.smith.langchain.com/self_hosting/architectural_overview#langsmith-platform-backend "Direct link to LangSmith Platform Backend")
The platform backend is an internal service that primarily handles authentication and other high-volume tasks. The user should not need to interact with this service directly.
### LangSmith Playground[‚Äã](https://docs.smith.langchain.com/self_hosting/architectural_overview#langsmith-playground "Direct link to LangSmith Playground")
The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers.
### LangSmith ACE(Arbitrary Code Execution) Backend[‚Äã](https://docs.smith.langchain.com/self_hosting/architectural_overview#langsmith-acearbitrary-code-execution-backend "Direct link to LangSmith ACE\(Arbitrary Code Execution\) Backend")
The ACE backend is a service that handles executing arbitrary code in a secure environment. This is used to support running custom code within LangSmith.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/architectural_overview%3E).
[PreviousSelf-hosting](https://docs.smith.langchain.com/self_hosting)[NextScripts](https://docs.smith.langchain.com/self_hosting/scripts)
  * [Storage Services](https://docs.smith.langchain.com/self_hosting/architectural_overview#storage-services)
    * [ClickHouse](https://docs.smith.langchain.com/self_hosting/architectural_overview#clickhouse)
    * [PostgreSQL](https://docs.smith.langchain.com/self_hosting/architectural_overview#postgresql)
    * [Redis](https://docs.smith.langchain.com/self_hosting/architectural_overview#redis)
  * [Services](https://docs.smith.langchain.com/self_hosting/architectural_overview#services)
    * [LangSmith Frontend](https://docs.smith.langchain.com/self_hosting/architectural_overview#langsmith-frontend)
    * [LangSmith Backend](https://docs.smith.langchain.com/self_hosting/architectural_overview#langsmith-backend)
    * [LangSmith Queue](https://docs.smith.langchain.com/self_hosting/architectural_overview#langsmith-queue)
    * [LangSmith Platform Backend](https://docs.smith.langchain.com/self_hosting/architectural_overview#langsmith-platform-backend)
    * [LangSmith Playground](https://docs.smith.langchain.com/self_hosting/architectural_overview#langsmith-playground)
    * [LangSmith ACE(Arbitrary Code Execution) Backend](https://docs.smith.langchain.com/self_hosting/architectural_overview#langsmith-acearbitrary-code-execution-backend)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/configuration

[Skip to main content](https://docs.smith.langchain.com/self_hosting/configuration#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/configuration)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Enable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Use custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Connect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Connect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Connect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
      * [Create an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [User management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/configuration)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * Configuration


# Configuration
Your LangSmith instance supports configuring a variety of parameters to suit your needs. This section contains guides for configuring your LangSmith instance.
  * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
  * [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
  * [User management features](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
  * [Connecting to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
  * [Connecting to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
  * [Connecting to an external Redis instance](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
  * [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
  * [Blob Storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
  * [Custom TLS Certificates for Model Providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
  * [Configuring an Ingress (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/configuration%3E).
[PreviousInstalling on Docker](https://docs.smith.langchain.com/self_hosting/installation/docker)[NextEmail/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/configuration/basic_auth

[Skip to main content](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Enable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Use custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Connect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Connect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Connect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
      * [Create an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [User management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
  * Email/password a.k.a. basic auth


On this page
# Email/password a.k.a. basic auth
LangSmith supports login via username/password with a few limitations:
  * You cannot change an existing installation from basic auth mode to OAuth with PKCE (deprecated) or vice versa - installations must be either one or the other. **A basic auth installation requires a completely fresh installation including a separate PostgreSQL database/schema, unless migrating from an existing`None` type installation (see below).**
  * Users must be given their initial auto-generated password once they are invited. This password may be changed later by any Organization Admin.
  * You cannot use both basic auth and OAuth with client secret at the same time.


## Requirements and features[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth#requirements-and-features "Direct link to Requirements and features")
  * There is a single `Default` organization that is provisioned during initial installation, and creating additional organizations is not supported
  * Your initial password (configured below) must be least 12 characters long and have at least one lowercase, uppercase, and symbol
  * There are no strict requirements for the secret used for signing JWTs, but we recommend securely generating a string of at least 32 characters. For example: `openssl rand -base64 32`


### Migrating from None auth[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth#migrating-from-none-auth "Direct link to Migrating from None auth")
**Only supported in versions 0.7 and above.**
Migrating an installation from [None](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#none) auth mode replaces the single "default" user with a user with the configured credentials and keeps all existing resources. The single pre-existing workspace ID post-migration remains `00000000-0000-0000-0000-000000000000`, but everything else about the migrated installation is standard for a basic auth installation.
To migrate, simply update your configuration as shown below and run `helm upgrade` (or `docker-compose up`) as usual.
### Configuration[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth#configuration "Direct link to Configuration")
note
Changing the JWT secret will log out your users
  * Helm
  * Docker


```
config:authType: mixedbasicAuth:enabled:trueinitialOrgAdminEmail: <YOUR EMAIL ADDRESS>initialOrgAdminPassword: <PASSWORD># Must be at least 12 characters long and have at least one lowercase, uppercase, and symboljwtSecret: <SECRET>
```

```
# In your .env fileAUTH_TYPE=mixedBASIC_AUTH_ENABLED=trueINITIAL_ORG_ADMIN_EMAIL=<YOUR EMAIL ADDRESS>INITIAL_ORG_ADMIN_PASSWORD=<PASSWORD> # Must be at least 12 characters long and have at least one lowercase, uppercase, and symbolBASIC_AUTH_JWT_SECRET=<SECRET>
```

Additionally, in docker-compose you will need to run the bootstrap command to create the initial organization and user:
```
docker-compose exec langchain-backend python hooks/auth_bootstrap.pyc
```

Once configured, you will see a login screen like the one below. You should be able to login with the `initialOrgAdminEmail` and `initialOrgAdminPassword` values, and your user will be auto-provisioned with role `Organization Admin`. See the [admin guide](https://docs.smith.langchain.com/administration/concepts#organization-roles) for more details on organization roles.
![LangSmith UI with basic auth](https://docs.smith.langchain.com/assets/images/langsmith_ui_basic_auth-3fb707f0d8ab906a867ef9efe01ce645.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/configuration/basic_auth%3E).
[PreviousConfiguration](https://docs.smith.langchain.com/self_hosting/configuration)[NextEnable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
  * [Requirements and features](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth#requirements-and-features)
    * [Migrating from None auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth#migrating-from-none-auth)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth#configuration)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/configuration/blob_storage

[Skip to main content](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Enable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Use custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Connect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Connect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Connect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
      * [Create an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [User management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
  * Enable blob storage


On this page
# Enable blob storage
By default, LangSmith stores run inputs, outputs, errors, manifests, extras, and events in ClickHouse. If you so choose, you can instead store this information in blob storage, which has a couple of notable benefits:
  1. In high trace environments, inputs, outputs, errors, manifests, extras, and events may balloon the size of your databases.
  2. If using LangSmith Managed ClickHouse, you may want sensitive information in blob storage that resides in your environment. To alleviate this, LangSmith supports storing run inputs, outputs, errors, manifests, extras, events, and attachments in an external blob storage system.


## Requirements[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#requirements "Direct link to Requirements")
note
Azure blob storage is available in Beta on Helm chart versions 0.8.9 and greater. While in Beta, [deleting trace projects](https://docs.smith.langchain.com/observability/concepts#deleting-traces-from-langsmith) is not supported.
  * Access to a valid blob storage service 
    * [Amazon S3](https://aws.amazon.com/s3/)
    * [Google Cloud Storage (GCS)](https://cloud.google.com/storage?hl=en)
    * [Azure Blob Storage [BETA]](https://azure.microsoft.com/en-us/products/storage/blobs)
  * A bucket/directory in your blob storage to store the data. We highly recommend creating a separate bucket/directory for LangSmith data. 
    * **If you are using TTLs** , you will need to set up a lifecycle policy to delete old data. You can find more information on configuring TTLs [here](https://docs.smith.langchain.com/self_hosting/configuration/ttl). These policies should mirror the TTLs you have set in your LangSmith configuration, or you may experience data loss. See [here](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#ttl-configuration) on how to setup the lifecycle rules for TTLs for blob storage.
  * Credentials to permit LangSmith Services to access the bucket/directory 
    * You will need to provide your LangSmith instance with the necessary credentials to access the bucket/directory. Read the authentication [section](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#authentication) below for more information.
  * If using S3 or GCS, an API url for your blob storage service 
    * This will be the URL that LangSmith uses to access your blob storage system
    * For Amazon S3, this will be the URL of the S3 endpoint. Something like: `https://s3.amazonaws.com` or `https://s3.us-west-1.amazonaws.com` if using a regional endpoint.
    * For Google Cloud Storage, this will be the URL of the GCS endpoint. Something like: `https://storage.googleapis.com`


## Authentication[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#authentication "Direct link to Authentication")
### Amazon S3[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#amazon-s3 "Direct link to Amazon S3")
To authenticate to [Amazon S3](https://aws.amazon.com/s3/), you will need to create an IAM policy granting admin permissions on your bucket. This will look something like the following:
```
{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Action":"s3:*","Resource":["arn:aws:s3:::your-bucket-name","arn:aws:s3:::your-bucket-name/*"]}]}
```

Once you have the correct policy, there are two ways to authenticate with Amazon S3:
  1. [**(Recommended) IAM Role for Service Account**](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html): You can create an IAM role for your LangSmith instance and attach the policy to that role. You can then provide the role to LangSmith. This is the recommended way to authenticate with Amazon S3 in production. 
    1. You will need to create an IAM role with the policy attached.
    2. You will need to allow LangSmith service accounts to assume the role. The `langsmith-queue`, `langsmith-backend`, and `langsmith-platform-backend` service accounts will need to be able to assume the role. 
Service Account Names
The service account names will be different if you are using a custom release name. You can find the service account names by running `kubectl get serviceaccounts` in your cluster.
    3. You will need to provide the role ARN to LangSmith. You can do this by adding the `eks.amazonaws.com/role-arn: "<role_arn>"` annotation to the `queue`, `backend`, and `platform-backend` services in your Helm Chart installation.
  2. [**Access Key and Secret Key**](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html): You can provide LangSmith with an access key and secret key. This is the simplest way to authenticate with Amazon S3. However, it is not recommended for production use as it is less secure. 
    1. You will need to create a user with the policy attached. Then you can provision an access key and secret key for that user.


### Google Cloud Storage[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#google-cloud-storage "Direct link to Google Cloud Storage")
To authenticate with [Google Cloud Storage](https://cloud.google.com/storage?hl=en), you will need to create a [`service account`](https://cloud.google.com/iam/docs/service-account-overview) with the necessary permissions to access your bucket.
Your service account will need the `Storage Admin` role or a custom role with equivalent permissions. This can be scoped to the bucket that LangSmith will be using.
Once you have a provisioned service account, you will need to generate a [`HMAC key`](https://cloud.google.com/storage/docs/authentication/hmackeys) for that service account. This key and secret will be used to authenticate with Google Cloud Storage.
### Azure Blob Storage[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#azure-blob-storage "Direct link to Azure Blob Storage")
To authenticate with [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), you will need to use one of the following methods to grant LangSmith workloads permission to access your [container](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction#containers) (listed in order of precedence):
  1. [Storage account and access key](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage)
  2. [Connection string](https://learn.microsoft.com/en-us/azure/storage/common/storage-configure-connection-string)
  3. [Workload identity](https://azure.github.io/azure-workload-identity/docs/introduction.html) (recommended), managed identity, or environment variables supported by [`DefaultAzureCredential`](https://learn.microsoft.com/en-us/azure/developer/go/azure-sdk-authentication?tabs=bash#2-authenticate-with-azure). This is the default authentication method when configuration for either option above is not present. 
    1. To use workload identity, add the label `azure.workload.identity/use: true` to the `queue`, `backend`, and `platform-backend` deployments. Additionally, add the `azure.workload.identity/client-id` annotation to the corresponding service accounts, which should be an existing Azure AD Application's client ID or user-assigned managed identity's client ID. See [Azure's documentation](https://azure.github.io/azure-workload-identity/docs/topics/service-account-labels-and-annotations.html) for additional details.


note
Some deployments may need further customization of the connection configuration using a Service URL Override instead of the default service URL (`https://<storage_account_name>.blob.core.windows.net/`). For example, this override is necessary in order to use a different blob storage domain (e.g. government or china).
## CH Search[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#ch-search "Direct link to CH Search")
By default, LangSmith will still store tokens for search in ClickHouse. If you are using LangSmith Managed Clickhouse, you may want to disable this feature to avoid sending potentially sensitive information to ClickHouse. You can do this in your blob storage configuration.
## Configuration[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#configuration "Direct link to Configuration")
After creating your bucket and obtaining the necessary credentials, you can configure LangSmith to use your blob storage system.
  * Helm
  * Docker


```
config:blobStorage:enabled:trueengine:"S3"# Or "Azure"chSearchEnabled:true# Set to false if you want to disable CH search (Recommended for LangSmith Managed Clickhouse)bucketName:"your-bucket-name"apiURL:"Your connection url"accessKey:"Your access key"# Optional. Only required if using S3 access key and secret keyaccessKeySecret:"Your access key secret"# Optional. Only required if using access key and secret key# The following blob storage configuration values are for Azure and require blobStorage.engine = "Azure". Omit otherwise.azureStorageAccountName:"Your storage account name"# Optional. Only required if using storage account and access key.azureStorageAccountKey:"Your storage account access key"# Optional. Only required if using storage account and access key.azureStorageContainerName:"your-container-name"# RequiredazureStorageConnectionString:""# Optional.azureStorageServiceUrlOverride:""# Optional backend:# Optional, only required if using IAM role for service account on AWS or workload identity on AKSdeployment:# Azure onlylabels:azure.workload.identity/use:trueserviceAccount:annotations:azure.workload.identity/client-id:"<client_id>"# Azure onlyeks.amazonaws.com/role-arn:"<role_arn>"# AWS only platformBackend:# Optional, only required if using IAM role for service account on AWS or workload identity on AKSdeployment:# Azure onlylabels:azure.workload.identity/use:trueserviceAccount:annotations:azure.workload.identity/client-id:"<client_id>"# Azure onlyeks.amazonaws.com/role-arn:"<role_arn>"# AWS only queue:# Optional, only required if using IAM role for service account on AWS or workload identity on AKSdeployment:# Azure onlylabels:azure.workload.identity/use:trueserviceAccount:annotations:azure.workload.identity/client-id:"<client_id>"# Azure onlyeks.amazonaws.com/role-arn:"<role_arn>"# AWS only
```

```
# In your .env fileFF_BLOB_STORAGE_ENABLED=false # Set to true if you want to enable blob storageBLOB_STORAGE_ENGINE=S3 # Or AzureBLOB_STORAGE_BUCKET_NAME=langsmith-blob-storage # Required for using S3. Change to your desired blob storage bucket nameBLOB_STORAGE_API_URL=https://s3.us-west-2.amazonaws.com # Change to your desired blob storage API URLBLOB_STORAGE_ACCESS_KEY=your-access-key # Change to your desired blob storage access keyBLOB_STORAGE_ACCESS_KEY_SECRET=your-access-key-secret # Change to your desired blob storage access key secretAZURE_STORAGE_ACCOUNT_NAME=your-storage-account-name # Optional. Only required if using storage account and access key.AZURE_STORAGE_ACCOUNT_KEY=your-storage-account-key # Optional. Only required if using storage account and access key.AZURE_STORAGE_CONTAINER_NAME=your-container-name # Required for using Azure blob storage. Change to your desired container nameAZURE_STORAGE_CONNECTION_STRING=BlobEndpoint=https://storagesample.blob.core.windows.net;SharedAccessSignature=signature; # Optional.AZURE_STORAGE_SERVICE_URL_OVERRIDE=https://your.override.domain.net # Optional
```

Using an existing secret
If using an access key and secret, you can also provide an existing Kubernetes secret that contains the authentication information. This is recommended over providing the access key and secret key directly in your config. See the [generated secret template](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/secrets.yaml) for the expected secret keys.
## TTL Configuration[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#ttl-configuration "Direct link to TTL Configuration")
If using the [TTL](https://docs.smith.langchain.com/self_hosting/configuration/ttl) feature with LangSmith, you'll also have to configure TTL rules for your blob storage. Trace information stored on blob storage is stored on a particular prefix path, which determines the TTL for the data. When a trace's retention is extended, its corresponding blob storage path changes to ensure that it matches the new extended retention.
The following TTL prefix are used:
  * `ttl_s/`: Short term TTL, configured for 14 days.
  * `ttl_l/`: Long term TTL, configured for 400 days.


If you have customized the TTLs in your LangSmith configuration, you will need to adjust the TTLs in your blob storage configuration to match.
### Amazon S3[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#amazon-s3-1 "Direct link to Amazon S3")
If using S3 for your blob storage, you will need to setup a filter lifecycle configuration that matches the prefixes above. You can find information for this [in the Amazon Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-filter).
As an example, if you are using Terraform to manage your S3 bucket, you would setup something like this:
```
 rule {  id   = "short-term-ttl"  prefix = "ttl_s/"  enabled = true  expiration {   days = 14  } } rule {  id   = "long-term-ttl"  prefix = "ttl_l/"  enabled = true  expiration {   days = 400  } }
```

### Google Cloud Storage[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#google-cloud-storage-1 "Direct link to Google Cloud Storage")
You will need to setup lifecycle conditions for your GCS buckets that you are using. You can find information for this [in the Google Documentation](https://cloud.google.com/storage/docs/lifecycle#conditions), specifically using matchesPrefix.
As an example, if you are using Terraform to manage your GCS bucket, you would setup something like this:
```
 lifecycle_rule {  condition {   age      = 14   matches_prefix = ["ttl_s"]  }  action {   type = "Delete"  } } lifecycle_rule {  condition {   age      = 400   matches_prefix = ["ttl_l"]  }  action {   type = "Delete"  } }
```

### Azure blob storage[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#azure-blob-storage-1 "Direct link to Azure blob storage")
You will need to configure a [lifecycle management policy](https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-policy-configure) on the container in order to expire objects matching the prefixes above.
As an example, if you are [using Terraform to manage your blob storage container](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_management_policy), you would setup something like this:
```
resource "azurerm_storage_management_policy" "example" { storage_account_id = "my-storage-account-id" rule {  name = "base"  enabled = true  type = "Lifecycle"  filters {   prefix_match = ["my-container/ttl_s"]   blob_types = ["blockBlob"]  }  actions {   base_blob {    delete_after_days_since_creation_greater_than = 14   }   snapshot {    delete_after_days_since_creation_greater_than = 14   }   version {    delete_after_days_since_creation_greater_than = 14   }  } } rule {  name = "extended"  enabled = true  type = "Lifecycle"  filters {   prefix_match = ["my-container/ttl_l"]   blob_types = ["blockBlob"]  }   actions {   base_blob {    delete_after_days_since_creation_greater_than = 400   }   snapshot {    delete_after_days_since_creation_greater_than = 400   }   version {    delete_after_days_since_creation_greater_than = 400   }  } }}
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/configuration/blob_storage%3E).
[PreviousEmail/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)[NextUse custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
  * [Requirements](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#requirements)
  * [Authentication](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#authentication)
    * [Amazon S3](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#amazon-s3)
    * [Google Cloud Storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#google-cloud-storage)
    * [Azure Blob Storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#azure-blob-storage)
  * [CH Search](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#ch-search)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#configuration)
  * [TTL Configuration](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#ttl-configuration)
    * [Amazon S3](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#amazon-s3-1)
    * [Google Cloud Storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#google-cloud-storage-1)
    * [Azure blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage#azure-blob-storage-1)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates

[Skip to main content](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Enable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Use custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Connect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Connect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Connect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
      * [Create an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [User management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
  * Use custom TLS certificates for model providers


# Use custom TLS certificates for model providers
Select providers only
This feature is currently only available for the following model providers:
  * Azure OpenAI
  * OpenAI
  * Custom (our custom model server). Refer to the [custom model server documentation](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint) for more information.


These TLS settings will apply to all invocations of the selected model providers including when used through Online Evaluation.
You can use custom TLS certificates to connect to model providers in the LangSmith playground. This is useful if you are using a self-signed certificate, a certificate from a custom certificate authority or mutual TLS authentication.
To use custom TLS certificates, you need to set the following environment variables. See the [self hosted deployment section](https://docs.smith.langchain.com/self_hosting) for more information on how to set up application configuration.
  * `LANGSMITH_PLAYGROUND_TLS_MODEL_PROVIDERS`: A comma-separated list of model providers that require custom TLS certificates. Note that `azure_openai`, `openai` and `custom` are currently the only supported model provider that supports custom TLS certificates, but more providers will be supported in the future.
  * `LANGSMITH_PLAYGROUND_TLS_CA`: The custom certificate authority (CA) certificate in PEM format. This must be a file path (for a mounted volume).
  * [Optional] `LANGSMITH_PLAYGROUND_TLS_KEY`: The private key in PEM format. This must be a file path (for a mounted volume). This is usually only necessary for mutual TLS authentication.
  * [Optional] `LANGSMITH_PLAYGROUND_TLS_CERT`: The certificate in PEM format. This must be a file path (for a mounted volume). This is usually only necessary for mutual TLS authentication.


Once you have set these environment variables, enter the playground and select the model provider that requires custom TLS certificates. Set your model provider configuration as usual, and the custom TLS certificates will be used when connecting to the model provider.
![](https://docs.smith.langchain.com/assets/images/azure_playground-d19d9eb84ae2cea448c721c650bde449.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/configuration/custom_tls_certificates%3E).
[PreviousEnable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)[NextConnect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse

[Skip to main content](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Enable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Use custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Connect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Connect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Connect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
      * [Create an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [User management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
  * Connect to an external ClickHouse database


On this page
# Connect to an external ClickHouse database
ClickHouse is a high-performance, column-oriented database system. It allows for fast ingestion of data and is optimized for analytical queries.
LangSmith uses ClickHouse as the primary data store for traces and feedback. By default, self-hosted LangSmith will use an internal ClickHouse database that is bundled with the LangSmith instance. This is run as a stateful set in the same Kubernetes cluster as the LangSmith application or as a Docker container on the same host as the LangSmith application.
However, you can configure LangSmith to use an external ClickHouse database for easier management and scaling. By configuring an external ClickHouse database, you can manage backups, scaling, and other operational tasks for your database. While Clickhouse is not yet a native service in Azure, AWS, or Google Cloud, you can run LangSmith with an external ClickHouse database in the following ways:
  * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
  * Provision a [ClickHouse Cloud](https://clickhouse.cloud/) either directly or through a cloud provider marketplace: 
    * [Azure Marketplace](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/clickhouse.clickhouse_cloud?tab=Overview)
    * [Google Cloud Marketplace](https://console.cloud.google.com/marketplace/product/clickhouse-public/clickhouse-cloud)
    * [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-jettukeanwrfc)
  * On a VM in your cloud provider


note
Using the first two options (LangSmith-managed ClickHouse or ClickHouse Cloud) will provision a Clickhouse service OUTSIDE of your VPC. However, both options support private endpoints, meaning that you can direct traffic to the ClickHouse service without exposing it to the public internet (eg via AWS PrivateLink, or GCP Private Service Connect).
Additionally, sensitive information can be configured to be not stored in Clickhouse. Please reach out to support@langchain.dev for more information.
## Requirements[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse#requirements "Direct link to Requirements")
  * A provisioned ClickHouse instance that your LangSmith application will have network access to (see above for options).
  * A user with admin access to the ClickHouse database. This user will be used to create the necessary tables, indexes, and views.
  * We support both standalone ClickHouse and externally managed clustered deployments. For clustered deployments, ensure all nodes are running the same version. Note that clustered setups are not supported with bundled ClickHouse installations.
  * We only support ClickHouse versions >= 23.9. Use of ClickHouse versions >= 24.2 requires LangSmith v0.6 or later. See the [LangSmith release notes](https://docs.smith.langchain.com/self_hosting/release_notes) for more information.
  * We rely on a few configuration parameters to be set on your ClickHouse instance. These are detailed below:


```
<profiles> <default>   <async_insert>1</async_insert># Turn on async insert   <async_insert_max_data_size>25000000</async_insert_max_data_size># Flush data to disk after 25MB. You may need to adjust this based on your workload.   <wait_for_async_insert>0</wait_for_async_insert># Disable waiting for async insert by default   <parallel_view_processing>1</parallel_view_processing># Enable parallel view processing   <materialize_ttl_after_modify>0</materialize_ttl_after_modify># Disable TTL materialization after modify   <wait_for_async_insert_timeout>120</wait_for_async_insert_timeout># Set the timeout for waiting for async insert   <lightweight_deletes_sync>0</lightweight_deletes_sync># Disable lightweight deletes sync </default></profiles>
```

Configuration parameters
Our system has been tuned to work with the above configuration parameters. Changing these parameters may result in unexpected behavior.
## HA Replicated Clickhouse Cluster[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse#ha-replicated-clickhouse-cluster "Direct link to HA Replicated Clickhouse Cluster")
warning
By default, the setup process above will only work with a single node Clickhouse cluster.
If you would like to use a multi-node Clickhouse cluster for HA, we support this with additional required configuration. This setup can use a Clickhouse cluster with multiple nodes where data replicated via Zookeeper or Clickhouse Keeper. For more information on Clickhouse replication, see [Clickhouse Data Replication Docs](https://clickhouse.com/docs/architecture/replication).
In order to setup LangSmith with a replicated multi-node Clickhouse setup:
  * You need to have a Clickhouse cluster that is setup with Keeper or Zookeeper for data replication and the appropriate settings. See [Clickhouse Replication Setup Docs](https://clickhouse.com/docs/architecture/replication).
  * You need to set the cluster setting in the [LangSmith Configuration](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse#configuration) section, specifically the `cluster` settings to match your Clickhouse Cluster name. This will use the `Replicated` table engines when running the Clickhouse migrations.
  * If in addition to HA, you would like to load balance among the Clickhouse nodes (to distribute reads or writes), we suggest using a load balancer or DNS load balancing to round robin among your Clickhouse servers.
  * **Note** : You will need to enable your `cluster` setting before launching LangSmith for the first time and running the Clickhouse migrations. This is a requirement since the table engine will need to be created as a `Replicated` table engine vs the non replicated engine type.


When running migrations with `cluster` enabled, the migration will create the `Replicated` table engine flavor. This means that data will be replicated among the servers in the cluster. This is a master-master setup where any server can process reads, writes, or merges.
## LangSmith-managed ClickHouse[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse#langsmith-managed-clickhouse "Direct link to LangSmith-managed ClickHouse")
  * If using LangSmith-managed ClickHouse, you will need to set up a VPC peering connection between the LangSmith VPC and the ClickHouse VPC. Please reach out to support@langchain.dev for more information.
  * You will also need to set up Blob Storage. You can read more about Blob Storage in the [Blob Storage documentation](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage).


note
ClickHouse installations managed by LangSmith use a SharedMerge engine, which automatically clusters them and separates compute from storage.
## Parameters[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse#parameters "Direct link to Parameters")
You will need to provide several parameters to your LangSmith installation to configure an external ClickHouse database. These parameters include:
  * Host: The hostname or IP address of the ClickHouse database
  * HTTP Port: The port that the ClickHouse database listens on for HTTP connections
  * Native Port: The port that the ClickHouse database listens on for [native connections](https://clickhouse.com/docs/en/interfaces/tcp)
  * Database: The name of the ClickHouse database that LangSmith should use
  * Username: The username to use to connect to the ClickHouse database
  * Password: The password to use to connect to the ClickHouse database
  * Cluster (Optional): The name of the ClickHouse cluster if using an external Clickhouse cluster. When set, LangSmith will run migrations on the cluster and replicate data across instances.


warning
Important considerations for clustered deployments:
  * Clustered setups must be configured on a fresh schema - existing standalone ClickHouse instances cannot be converted to clustered mode.
  * Clustering is only supported with externally managed ClickHouse deployments. It is not compatible with bundled ClickHouse installations as these do not include required ZooKeeper configurations.
  * When using a clustered deployment, LangSmith will automatically: 
    * Run database migrations across all nodes in the cluster
    * Configure tables for data replication across the cluster


Note that while data is replicated across nodes, LangSmith does not configure distributed tables or handle query routing - queries will be directed to the specified host. You will need to handle any load balancing or query distribution at the infrastructure level if desired.
## Configuration[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse#configuration "Direct link to Configuration")
With these parameters in hand, you can configure your LangSmith instance to use the provisioned ClickHouse database. You can do this by modifying the `config.yaml` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.
  * Helm
  * Docker


```
clickhouse:external:enabled:truehost:"host"port:"http port"nativePort:"native port"user:"default"password:"password"database:"default"tls:falsecluster:"my_cluster_name"# Optional: Set this if using an external Clickhouse cluster
```

```
# In your .env fileCLICKHOUSE_HOST=langchain-clickhouse # Change to your Clickhouse host if using external Clickhouse. Otherwise, leave it as isCLICKHOUSE_USER=default # Change to your Clickhouse user if neededCLICKHOUSE_DB=default # Change to your Clickhouse database if neededCLICKHOUSE_PORT=8123 # Change to your Clickhouse port if neededCLICKHOUSE_TLS=false # Change to true if you are using TLS to connect to Clickhouse. Otherwise, leave it as isCLICKHOUSE_PASSWORD=password # Change to your Clickhouse password if neededCLICKHOUSE_NATIVE_PORT=9000 # Change to your Clickhouse native port if neededCLICKHOUSE_CLUSTER=my_cluster_name # Optional: Set this if using an external Clickhouse cluster
```

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external ClickHouse database.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/configuration/external_clickhouse%3E).
[PreviousUse custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)[NextConnect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
  * [Requirements](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse#requirements)
  * [HA Replicated Clickhouse Cluster](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse#ha-replicated-clickhouse-cluster)
  * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse#langsmith-managed-clickhouse)
  * [Parameters](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse#parameters)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse#configuration)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/configuration/external_postgres

[Skip to main content](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Enable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Use custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Connect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Connect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Connect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
      * [Create an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [User management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
  * Connect to an external Postgres database


On this page
# Connect to an external Postgres database
LangSmith uses a Postgres database as the primary data store for transactional workloads and operational data (almost everything besides runs). By default, LangSmith Self-Hosted will use an internal Postgres database. However, you can configure LangSmith to use an external Postgres database (**strongly recommended in a production setting**). By configuring an external Postgres database, you can more easily manage backups, scaling, and other operational tasks for your database.
## Requirements[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres#requirements "Direct link to Requirements")
  * A provisioned Postgres database that your LangSmith instance will have network access to. We recommend using a managed Postgres service like: 
    * [Amazon RDS](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_GettingStarted.CreatingConnecting.PostgreSQL.html)
    * [Google Cloud SQL](https://cloud.google.com/curated-resources/cloud-sql#section-1)
    * [Azure Database for PostgreSQL](https://azure.microsoft.com/en-us/products/postgresql#features)
  * Note: We only officially support Postgres versions >= 14.
  * A user with admin access to the Postgres database. This user will be used to create the necessary tables, indexes, and schemas.
  * This user will also need to have the ability to create extensions in the database. We use/will try to install the btree_gin, btree_gist, pgcrypto, citext, and pg_trgm extensions.
  * If using a schema other than public, ensure that you do not have any other schemas with the extensions enabled, or you must include that in your search path.
  * By default, we recommend an instance with at least 2 vCPUs and 8GB of memory. However, the actual requirements will depend on your workload and the number of users you have. We recommend monitoring your Postgres instance and scaling up as needed.


## Connection String[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres#connection-string "Direct link to Connection String")
You will need to provide a connection string to your Postgres database. This connection string should include the following information:
  * Host
  * Port
  * Database
  * Username
  * Password(Make sure to url encode this if there are any special characters)
  * URL params


This will take the form of:
```
username:password@host:port/database?<url_params>
```

An example connection string might look like:
```
myuser:mypassword@myhost:5432/mydatabase?sslmode=disable
```

Without url parameters, the connection string would look like:
```
myuser:mypassword@myhost:5432/mydatabase
```

## Configuration[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres#configuration "Direct link to Configuration")
With your connection string in hand, you can configure your LangSmith instance to use an external Postgres database. You can do this by modifying the `values` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.
  * Helm
  * Docker


```
postgres:external:enabled:trueconnectionUrl:"Your connection url"
```

```
# In your .env filePOSTGRES_DATABASE_URI="Your connection url"
```

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external Postgres database.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/configuration/external_postgres%3E).
[PreviousConnect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)[NextConnect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
  * [Requirements](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres#requirements)
  * [Connection String](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres#connection-string)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres#configuration)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/configuration/external_redis

[Skip to main content](https://docs.smith.langchain.com/self_hosting/configuration/external_redis#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-white.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Enable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Use custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Connect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Connect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Connect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
      * [Create an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [User management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/reference/data_formats/example_data_format)
    * [Evaluation](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
  * Connect to an external Redis database


On this page
# Connect to an external Redis database
LangSmith uses Redis to back our queuing/caching operations. By default, LangSmith Self-Hosted will use an internal Redis instance. However, you can configure LangSmith to use an external Redis instance (**strongly recommended in a production setting**). By configuring an external Redis instance, you can more easily manage backups, scaling, and other operational tasks for your Redis instance.
## Requirements[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/external_redis#requirements "Direct link to Requirements")
  * A provisioned Redis instance that your LangSmith instance will have network access to. We recommend using a managed Redis service like: 
    * [Amazon ElastiCache](https://aws.amazon.com/elasticache/redis/)
    * [Google Cloud Memorystore](https://cloud.google.com/memorystore)
    * [Azure Cache for Redis](https://azure.microsoft.com/en-us/services/cache/)
  * Note: We only officially support Redis versions >= 5.
  * We do not support Redis Cluster.
  * By default, we recommend an instance with at least 2 vCPUs and 8GB of memory. However, the actual requirements will depend on your tracing workload. We recommend monitoring your Redis instance and scaling up as needed.


Redis Cluster Not Supported
Certain tiers of managed Redis services may use Redis Cluster under the hood, but you can point to a single node in the cluster. For example on Azure Cache for Redis, the `Premium` tier and above use Redis Cluster, so you will need to use a lower tier.
## Connection String[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/external_redis#connection-string "Direct link to Connection String")
We use `redis-py` to connect to Redis. This library supports a variety of connection strings. You can find more information on the connection string format [here](https://redis-py.readthedocs.io/en/stable/#redis.StrictRedis.from_url).
You will need to assemble the connection string for your Redis instance. This connection string should include the following information:
  * Host
  * Database
  * Port
  * URL params


This will take the form of:
```
"redis://host:port/db?<url_params>"
```

An example connection string might look like:
```
"redis://langsmith-redis:6379/0"
```

To use SSL, you can use the `rediss://` prefix. An example connection string with SSL might look like:
```
"rediss://langsmith-redis:6380/0?password=foo"
```

## Configuration[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/external_redis#configuration "Direct link to Configuration")
With your connection string in hand, you can configure your LangSmith instance to use an external Redis instance. You can do this by modifying the `values` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.
  * Helm
  * Docker


```
redis:external:enabled:trueconnectionUrl:"Your connection url"
```

```
# In your .env fileREDIS_DATABASE_URI="Your connection url"
```

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external Redis instance.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).
[PreviousConnect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)[NextCreate an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
  * [Requirements](https://docs.smith.langchain.com/self_hosting/configuration/external_redis#requirements)
  * [Connection String](https://docs.smith.langchain.com/self_hosting/configuration/external_redis#connection-string)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration/external_redis#configuration)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/configuration/ingress

[Skip to main content](https://docs.smith.langchain.com/self_hosting/configuration/ingress#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Enable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Use custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Connect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Connect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Connect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
      * [Create an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [User management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/configuration/ingress)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
  * Create an Ingress for your LangSmith installation (Kubernetes only)


On this page
# Create an Ingress for your LangSmith installation (Kubernetes only)
By default, LangSmith will provision a LoadBalancer service for the `langsmith-frontend`. Depending on your cloud provider, this may result in a public IP address being assigned to the service. If you would like to use a custom domain or have more control over the routing of traffic to your LangSmith installation, you can configure an Ingress.
## Requirements[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/ingress#requirements "Direct link to Requirements")
  * An existing Kubernetes cluster
  * An existing Ingress Controller installed in your Kubernetes cluster


## Parameters[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/ingress#parameters "Direct link to Parameters")
You may need to provide certain parameters to your LangSmith installation to configure the Ingress. Additionally, we will want to convert the `langsmith-frontend` service to a ClusterIP service.
  * _Hostname (optional)_ : The hostname that you would like to use for your LangSmith installation. E.g `"langsmith.example.com"`. If you leave this empty, the ingress will serve all traffic to the LangSmith installation.
  * _Subdomain (optional)_ : If you would like to serve LangSmith on a subdomain, you can specify it here. E.g adding `"langsmith"` will serve the application at `"langsmith.example.com/langsmith"`.
  * _IngressClassName (optional)_ : The name of the Ingress class that you would like to use. If not set, the default Ingress class will be used.
  * _Annotations (optional)_ : Additional annotations to add to the Ingress. Certain providers like AWS may use annotations to control things like TLS termination.
For example, you can add the following annotations using the AWS ALB Ingress Controller to attach an ACM certificate to the Ingress:
```
annotations:alb.ingress.kubernetes.io/certificate-arn:"<your-certificate-arn>"
```

  * _Labels (optional)_ : Additional labels to add to the Ingress.
  * _TLS (optional)_ : If you would like to serve LangSmith over HTTPS, you can add TLS configuration here (many Ingress controllers may have other ways of controlling TLS so this is often not needed). This should be an array of TLS configurations. Each TLS configuration should have the following fields:
    * hosts: An array of hosts that the certificate should be valid for. E.g ["langsmith.example.com"]
    * secretName: The name of the Kubernetes secret that contains the certificate and private key. This secret should have the following keys: 
      * tls.crt: The certificate
      * tls.key: The private key
    * You can read more about creating a TLS secret [here](https://kubernetes.io/docs/concepts/services-networking/ingress/#tls).


## Configuration[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/ingress#configuration "Direct link to Configuration")
With these parameters in hand, you can configure your LangSmith instance to use an Ingress. You can do this by modifying the `config.yaml` file for your LangSmith Helm Chart installation.
  * Helm


```
ingress:enabled:truehostname:""# Main domain for LangSmithsubdomain:""# If you want to serve langsmith on a subdomainingressClassName:""# If not set, the default ingress class will be usedannotations:{}# Add annotations here if neededlabels:{}# Add labels here if neededtls:[]# Add TLS configuration here if neededfrontend:service:type: ClusterIP
```

Once configured, you will need to update your LangSmith installation. If everything is configured correctly, your LangSmith instance should now be accessible via the Ingress. You can run the following to check the status of your Ingress:
```
kubectl get ingress
```

You should see something like this in the output:
```
NAME             CLASS  HOSTS  ADDRESS     PORTS   AGElangsmith-ingress      nginx  <host>  35.227.243.203  80, 443  95d
```

Adding the IP address to your DNS provider
If you do not have automated DNS setup, you will need to add the IP address to your DNS provider manually.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/configuration/ingress%3E).
[PreviousConnect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)[NextSSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
  * [Requirements](https://docs.smith.langchain.com/self_hosting/configuration/ingress#requirements)
  * [Parameters](https://docs.smith.langchain.com/self_hosting/configuration/ingress#parameters)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration/ingress#configuration)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/configuration/sso

[Skip to main content](https://docs.smith.langchain.com/self_hosting/configuration/sso#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/configuration/sso)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Enable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Use custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Connect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Connect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Connect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
      * [Create an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [User management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/configuration/sso)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/configuration/sso)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
  * SSO with OAuth2.0 and OIDC


On this page
# SSO with OAuth2.0 and OIDC
LangSmith Self-Hosted provides SSO via OAuth2.0 and OIDC. This will delegate authentication to your Identity Provider (IdP) to manage access to LangSmith.
Our implementation supports almost anything that is OIDC compliant, with a few exceptions. Once configured, you will see a login screen like this:
![LangSmith UI with OAuth SSO](https://docs.smith.langchain.com/assets/images/langsmith_ui_sso-59c95310eaba26254b5c5afa6b86a1ac.png)
## With Client Secret (Recommended)[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/sso#with-secret "Direct link to With Client Secret \(Recommended\)")
By default, LangSmith Self-Hosted supports the `Authorization Code` flow with `Client Secret`. In this version of the flow, your client secret is stored security in the LangSmith platform (not on the frontend) and used for authentication and establishing auth sessions.
### Requirements[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/sso#requirements "Direct link to Requirements")
note
You may upgrade a [basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth) installation to this mode, but not a [none auth](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods#none) installation. In order to upgrade, simply remove the basic auth configuration and add the required configuration parameters as shown below. Users may then login via OAuth _only_. **In order to maintain access post-upgrade, you must have access to login via OAuth using an email address that previously logged in via basic auth.**
warning
LangSmith does not support moving from SSO to basic auth mode in self-hosted at the moment. We also do not support moving from OAuth Mode with client secret to OAuth mode without a client secret and vice versa. Finally, we do not support having both basic auth and OAuth at the same time. Ensure you disable the basic auth configuration when enabling OAuth.
  * Your IdP must support the `Authorization Code` flow with `Client Secret`.
  * Your IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP.
  * You must provide the `OIDC`, `email`, and `profile` scopes to LangSmith. We use these to fetch the necessary user information and email for your users.
  * You will need to set the callback URL in your IdP to `http://<host>/api/v1/oauth/custom-oidc/callback`, where host is the domain or IP you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated.
  * You will need to provide the `oauthClientId`, `oauthClientSecret`, `hostname`, and `oauthIssuerUrl` in your `values.yaml` file. This is where you will configure your LangSmith instance.
  * Some IdPs may require the `offline_access` scope to be provided to LangSmith. This is used to refresh the user's token when it expires.


  * Helm
  * Docker


```
config:authType: mixedhostname: https://langsmith.example.comoauth:enabled:trueoauthClientId: <YOUR CLIENT ID>oauthClientSecret: <YOUR CLIENT SECRET>oauthIssuerUrl: <YOUR DISCOVERY URL>oauthScopes:"email,profile,openid"
```

```
# In your .env fileAUTH_TYPE=mixedLANGSMITH_URL=https://langsmith.example.comOAUTH_CLIENT_ID=your-client-idOAUTH_CLIENT_SECRET=your-client-secretOAUTH_ISSUER_URL=https://your-issuer-urlOAUTH_SCOPES=email,profile,openid
```

### Identity Provider (IdP) Setup[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/sso#identity-provider-idp-setup "Direct link to Identity Provider \(IdP\) Setup")
**Google Workspace**
You can use Google Workspace as a single sign-on (SSO) provider using [OAuth2.0 and OIDC](https://developers.google.com/identity/openid-connect/openid-connect) without PKCE.
note
You must have administrator-level access to your organization‚Äôs Google Cloud Platform (GCP) account to create a new project, or permissions to create and configure OAuth 2.0 credentials for an existing project. We recommend that you create a new project for managing access, since each GCP project has a single OAuth consent screen.
  1. Create a new GCP project, see the Google documentation topic [creating and managing projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects)
  2. After you have created the project, open the [Credentials](https://console.developers.google.com/apis/credentials) page in the Google API Console (making sure the project in the top left corner is correct)
  3. Create new credentials: `Create Credentials ‚Üí OAuth client ID`
  4. Choose `Web application` as the `Application type` and enter a name for the application e.g. `LangSmith`
  5. In `Authorized Javascript origins` put the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com`
  6. In `Authorized redirect URIs` put the domain of your LangSmith instance followed by `/api/v1/oauth/custom-oidc/callback` e.g. `https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback`
  7. Click `Create`, then download the JSON or copy and save the `Client ID` (ends with `.apps.googleusercontent.com`) and `Client secret` somewhere secure. **You will be able to access these later if needed**.
  8. Select `OAuth consent screen` from the navigation menu on the left 
    1. Choose the Application type as `Internal`. **If you select`Public` , anyone with a Google account can sign in.**
    2. Enter a descriptive `Application name`. This name is shown to users on the consent screen when they sign in. For example, use `LangSmith` or `<organization_name> SSO for LangSmith`.
    3. Verify that the Scopes for Google APIs only lists email, profile, and openid scopes. Only these scopes are required for single sign-on. If you grant additional scopes it increases the risk of exposing sensitive data.
  9. (Optional) control who within your organization has access to LangSmith: <https://admin.google.com/ac/owl/list?tab=configuredApps>. See [Google's documentation](https://support.google.com/a/answer/7281227?hl=en&fl=1&sjid=9554153972856467090-NA) for additional details.
  10. Configure LangSmith to use this OAuth application. For examples, here are the `config `values that would be used for Kubernetes configuration: 
    1. `oauthClientId`: `Client ID` (ends with `.apps.googleusercontent.com`)
    2. `oauthClientSecret`: `Client secret`
    3. `hostname`: the domain of your instance e.g. `https://langsmith.yourdomain.com` (no trailing slash)
    4. `oauthIssuerUrl`: `https://accounts.google.com`
    5. `oauth.enabled`: `true`
    6. `authType`: `mixed`


## Without Client Secret (PKCE) (Deprecated)[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/sso#without-secret "Direct link to Without Client Secret \(PKCE\) \(Deprecated\)")
We recommend running with a `Client Secret` if possible (previously we didn't support this). However, if your IdP does not support this, you can use the `Authorization Code with PKCE` flow.
This flow does _not_ require a `Client Secret` - see the flow [above](https://docs.smith.langchain.com/self_hosting/configuration/sso#with-secret) for the alternative that does.
### Requirements[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/sso#requirements-1 "Direct link to Requirements")
There are a couple of requirements for using OAuth SSO with LangSmith:
  * Your IdP must support the `Authorization Code with PKCE` [flow](https://www.oauth.com/oauth2-servers/pkce) (Google does not support this flow for example, but see [below](https://docs.smith.langchain.com/self_hosting/configuration/sso#with-secret) for an alternative configuration that Google supports). This is often displayed in your OAuth Provider as configuring a "Single Page Application (SPA)"
  * Your IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP.
  * You must provide the `OIDC`, `email`, and `profile` scopes to LangSmith. We use these to fetch the necessary user information and email for your users.
  * You will need to set the callback URL in your IdP to `http://<host>/oauth-callback`, where host is the domain or IP you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated.
  * You will need to provide the `oauthClientId` and `oauthIssuerUrl` in your `values.yaml` file. This is where you will configure your LangSmith instance.


  * Helm
  * Docker


```
config:oauth:enabled:trueoauthClientId: <YOUR CLIENT ID>oauthIssuerUrl: <YOUR DISCOVERY URL>
```

```
# In your .env fileAUTH_TYPE=oauthOAUTH_CLIENT_ID=your-client-idOAUTH_ISSUER_URL=https://your-issuer-url
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/configuration/sso%3E).
[PreviousCreate an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)[NextTTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
  * [With Client Secret (Recommended)](https://docs.smith.langchain.com/self_hosting/configuration/sso#with-secret)
    * [Requirements](https://docs.smith.langchain.com/self_hosting/configuration/sso#requirements)
    * [Identity Provider (IdP) Setup](https://docs.smith.langchain.com/self_hosting/configuration/sso#identity-provider-idp-setup)
  * [Without Client Secret (PKCE) (Deprecated)](https://docs.smith.langchain.com/self_hosting/configuration/sso#without-secret)
    * [Requirements](https://docs.smith.langchain.com/self_hosting/configuration/sso#requirements-1)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/configuration/ttl

[Skip to main content](https://docs.smith.langchain.com/self_hosting/configuration/ttl#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Enable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Use custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Connect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Connect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Connect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
      * [Create an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [User management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/configuration/ttl)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
  * TTL and Data Retention


On this page
# TTL and Data Retention
LangSmith Self-Hosted allows enablement of automatic TTL and Data Retention of traces. This can be useful if you're complying with data privacy regulations, or if you want to have more efficient space usage and auto cleanup of your traces. Traces will also have their data retention period automatically extended based on certain actions or run rule applications. For more details on Data Retention, take a look at the section on auto-upgrades in the [data retention guide](https://docs.smith.langchain.com/administration/concepts#data-retention).
## Requirements[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/ttl#requirements "Direct link to Requirements")
You can configure retention through helm or environment variable settings. There are a few options that are configurable:
  * _Enabled:_ Whether data retention is enabled or disabled. If enabled, via the UI you can your default organization and project TTL tiers to apply to traces (see [data retention guide](https://docs.smith.langchain.com/administration/concepts#data-retention) for details).
  * _Retention Periods:_ You can configure system-wide retention periods for shortlived and longlived traces. Once configured, you can manage the retention level at each project as well as set an organization-wide default for new projects.


  * Helm
  * Docker


```
config:ttl:enabled:truettl_period_seconds:# -- TTL seconds - 400 day longlived and 14 day shortlivedlonglived:"34560000"shortlived:"1209600"
```

```
# In your .env fileFF_TRACE_TIERS_ENABLED=trueTRACE_TIER_TTL_DURATION_SEC_MAP='{"longlived": 34560000, "shortlived": 1209600}'
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/configuration/ttl%3E).
[PreviousSSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)[NextUser management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
  * [Requirements](https://docs.smith.langchain.com/self_hosting/configuration/ttl#requirements)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/configuration/user_management

[Skip to main content](https://docs.smith.langchain.com/self_hosting/configuration/user_management#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
      * [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
      * [Enable blob storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage)
      * [Use custom TLS certificates for model providers](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates)
      * [Connect to an external ClickHouse database](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse)
      * [Connect to an external Postgres database](https://docs.smith.langchain.com/self_hosting/configuration/external_postgres)
      * [Connect to an external Redis database](https://docs.smith.langchain.com/self_hosting/configuration/external_redis)
      * [Create an Ingress for your LangSmith installation (Kubernetes only)](https://docs.smith.langchain.com/self_hosting/configuration/ingress)
      * [SSO with OAuth2.0 and OIDC](https://docs.smith.langchain.com/self_hosting/configuration/sso)
      * [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)
      * [User management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/configuration/user_management)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/configuration/user_management)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
  * User management


On this page
# User management
note
This guide assumes you have read the [admin guide](https://docs.smith.langchain.com/administration/concepts) and [organization setup guide](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_organization).
LangSmith offers additional customization features for user management using feature flags.
## Features[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/user_management#features "Direct link to Features")
### Workspace level invites to an organization[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/user_management#workspace-level-invites-to-an-organization "Direct link to Workspace level invites to an organization")
The default behavior in LangSmith requires a user to be an Organization Admin in order to invite new users to an organization, as this operation can increase cost by adding seats. For self-hosted customers that would like to delegate this responsibility to workspace Admins, a feature flag may be set that enables workspace Admins to invite new users to the organization as well as their specific workspace **at the workspace level**.
Once this feature is enabled via the configuration option below, workspace Admins may add new users in the `Workspace members` tab under `Settings` > `Workspaces`. Both of the following cases are supported when inviting at the workspace level, while the organization level invite functions the same as before.
  1. Invite users who are NOT already active in the organization: this will add the users as pending to the organization and specific workspace
  2. Invite users who ARE already active in the organization: adds the users directly to the workspace as an active member (no pending state).


Admins may invite users for both cases at the same time.
#### Configuration[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/user_management#configuration "Direct link to Configuration")
  * Helm
  * Docker


```
config:workspaceScopeOrgInvitesEnabled:true
```

```
# In your .env fileWORKSPACE_SCOPE_ORG_INVITES_ENABLED="true"
```

### Disabling Organization Creating[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/user_management#disabling-organization-creating "Direct link to Disabling Organization Creating")
By default, any user can create an organization in LangSmith. For self-hosted customers, an admin may want to restrict this ability after setting up initial organizations. This feature flag allows an admin to disable the ability for users to create new organizations.
#### Configuration[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/user_management#configuration-1 "Direct link to Configuration")
  * Helm
  * Docker


```
config:orgCreationDisabled:true
```

```
# In your .env fileORG_CREATION_DISABLED="true"
```

### Disabling Personal Organizations[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/user_management#disabling-personal-organizations "Direct link to Disabling Personal Organizations")
By default, any user who logs in to LangSmith will have a personal organization created for them. For self-hosted customers, an admin may want to restrict this ability. This feature flag allows an admin to disable the ability for users to create personal organizations.
#### Configuration[‚Äã](https://docs.smith.langchain.com/self_hosting/configuration/user_management#configuration-2 "Direct link to Configuration")
  * Helm
  * Docker


```
config:personalOrgsDisabled:true
```

```
# In your .env filePERSONAL_ORGS_DISABLED="true"
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/configuration/user_management%3E).
[PreviousTTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl)[NextUsage](https://docs.smith.langchain.com/self_hosting/usage)
  * [Features](https://docs.smith.langchain.com/self_hosting/configuration/user_management#features)
    * [Workspace level invites to an organization](https://docs.smith.langchain.com/self_hosting/configuration/user_management#workspace-level-invites-to-an-organization)
    * [Disabling Organization Creating](https://docs.smith.langchain.com/self_hosting/configuration/user_management#disabling-organization-creating)
    * [Disabling Personal Organizations](https://docs.smith.langchain.com/self_hosting/configuration/user_management#disabling-personal-organizations)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/egress

[Skip to main content](https://docs.smith.langchain.com/self_hosting/egress#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/egress)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/egress)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/egress)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/egress)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/egress)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * Egress for Subscription Metrics and Operational Metadata


On this page
Version Requirement
This section only applies to customers who are not running in offline mode and assumes you are using a self-hosted LangSmith instance serving version 0.9.0 or later. Previous versions of LangSmith did not have this feature.
# Egress for Subscription Metrics and Operational Metadata
Self-Hosted LangSmith instances store all information locally and will never send sensitive information outside of your network. We currently only track platform usage for billing purposes according to the entitlements in your order. In order to better remotely support our customers, we do require egress to `https://beacon.langchain.com`.
In the future, we will be introducing support diagnostics to help us ensure that the LangSmith platform is running at an optimal level within your environment.
Important
**This will require egress to`https://beacon.langchain.com` from your network.**
Generally, data that we send to Beacon can be categorized as follows:
  * Subscription Metrics 
    * Subscription metrics are used to determine level of access and utilization of LangSmith. This includes, but are not limited to: 
      * Number of traces
      * Seats allocated per contract
      * Seats in currently use
  * Operational Metadata 
    * This metadata will contain and collect the above subscription metrics to assist with remote support, allowing the LangChain team to diagnose and troubleshoot performance issues more effectively and proactively.


## Example Payloads[‚Äã](https://docs.smith.langchain.com/self_hosting/egress#example-payloads "Direct link to Example Payloads")
In an effort to maximize transparency, we provide sample payloads here:
### License Verification[‚Äã](https://docs.smith.langchain.com/self_hosting/egress#license-verification "Direct link to License Verification")
**Endpoint:**
`POST beacon.langchain.com/v1/beacon/verify`
**Request:**
```
{"license: "<YOUR_LICENSE_KEY>"}
```

**Response:**
```
{"token":"Valid JWT"//Short-lived JWT token to avoid repeated license checks}
```

### Usage Reporting[‚Äã](https://docs.smith.langchain.com/self_hosting/egress#usage-reporting "Direct link to Usage Reporting")
**Endpoint:**
`POST beacon.langchain.com/v1/beacon/ingest-traces`
**Request:**
```
{"license":"<YOUR_LICENSE_KEY>","trace_transactions":[{"id":"af28dfea-5358-463d-a2dc-37df1da72498","tenant_id":"3a1c2b6f-4430-4b92-8a5b-79b8b567bbc1","session_id":"b26ae531-cdb3-42a5-8bcf-05355199fe27","trace_count":5,"start_insertion_time":"2025-01-06T10:00:00Z","end_insertion_time":"2025-01-06T11:00:00Z","start_interval_time":"2025-01-06T09:00:00Z","end_interval_time":"2025-01-06T10:00:00Z","status":"completed","num_failed_send_attempts":0,"transaction_type":"type1","organization_id":"c5b5f53a-4716-4326-8967-d4f7f7799735"}]}
```

**Response:**
```
{"inserted_count":1//Number of transactions successfully ingested}
```

## Our Commitment[‚Äã](https://docs.smith.langchain.com/self_hosting/egress#our-commitment "Direct link to Our Commitment")
LangChain will not store any sensitive information in the Subscription Metrics or Operational Metadata. Any data collected will not be shared with a third party. If you have any concerns about the data being sent, please reach out to your account team.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/egress%3E).
[PreviousUpgrades](https://docs.smith.langchain.com/self_hosting/upgrades)[NextOrganization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
  * [Example Payloads](https://docs.smith.langchain.com/self_hosting/egress#example-payloads)
    * [License Verification](https://docs.smith.langchain.com/self_hosting/egress#license-verification)
    * [Usage Reporting](https://docs.smith.langchain.com/self_hosting/egress#usage-reporting)
  * [Our Commitment](https://docs.smith.langchain.com/self_hosting/egress#our-commitment)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/faq

[Skip to main content](https://docs.smith.langchain.com/self_hosting/faq#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/faq)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/faq)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/faq)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/faq)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/faq)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * Frequently asked questions


On this page
# Frequently Asked Questions:
### _I can't create API keys or manage users in the UI, what's wrong?_[‚Äã](https://docs.smith.langchain.com/self_hosting/faq#i-cant-create-api-keys-or-manage-users-in-the-ui-whats-wrong "Direct link to i-cant-create-api-keys-or-manage-users-in-the-ui-whats-wrong")
  * You have likely deployed LangSmith without setting up SSO. LangSmith requires SSO to manage users and API keys. You can find more information on setting up SSO in the [configuration section.](https://docs.smith.langchain.com/self_hosting/configuration/sso)


### _How does load balancing/ingress work_?[‚Äã](https://docs.smith.langchain.com/self_hosting/faq#how-does-load-balancingingress-work "Direct link to how-does-load-balancingingress-work")
  * You will need to expose the frontend container/service to your applications/users. This will handle routing to all downstream services.
  * You will need to terminate SSL at the ingress level. We recommend using a managed service like AWS ALB, GCP Load Balancer, or Nginx.


### _How can we authenticate to the application?_[‚Äã](https://docs.smith.langchain.com/self_hosting/faq#how-can-we-authenticate-to-the-application "Direct link to how-can-we-authenticate-to-the-application")
  * Currently, our self-hosted solution supports SSO with OAuth2.0 and OIDC as an authn solution. Note, we do offer a no-auth solution but highly recommend setting up oauth before moving into production.


You can find more information on setting up SSO in the [configuration section.](https://docs.smith.langchain.com/self_hosting/configuration/sso)
### _Can I use external storage services?_[‚Äã](https://docs.smith.langchain.com/self_hosting/faq#can-i-use-external-storage-services "Direct link to can-i-use-external-storage-services")
  * You can configure LangSmith to use external versions of all storage services. In a production setting, we strongly recommend using external storage services. Check out the [configuration section](https://docs.smith.langchain.com/self_hosting/configuration) for more information.


### _Does my application need egress to function properly?_[‚Äã](https://docs.smith.langchain.com/self_hosting/faq#does-my-application-need-egress-to-function-properly "Direct link to does-my-application-need-egress-to-function-properly")
Our deployment only needs egress for a few things (most of which can reside within your VPC):
  * Fetching images (If mirroring your images, this may not be needed)
  * Talking to any LLM endpoints
  * Talking to any external storage services you may have configured
  * Fetching OAuth information
  * Subscription Metrics and Operational Metadata (if not running in offline mode) 
    * Requires egress to `https://beacon.langchain.com`
    * See [Egress](https://docs.smith.langchain.com/self_hosting/egress) for more information


Your VPC can set up rules to limit any other access. Note: We require the `X-Organization-Id` and `X-Tenant-Id` headers to be allowed to be passed through to the backend service. These are used to determine which organization and workspace (previously called "tenant") the request is for.
### _Resource requirements for the application?_[‚Äã](https://docs.smith.langchain.com/self_hosting/faq#resource-requirements-for-the-application "Direct link to resource-requirements-for-the-application")
  * In kubernetes, we recommend a minimum helm configuration which can be found in [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/examples/medium_size.yaml). For docker, we recommend a minimum of 16GB of RAM and 4 CPUs.
  * For Postgres, we recommend a minimum of 8GB of RAM and 2 CPUs.
  * For Redis, we recommend 4GB of RAM and 2 CPUs.
  * For Clickhouse, we recommend 32GB of RAM and 8 CPUs.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/faq%3E).
[PreviousRelease notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)[NextLangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
  * [ _I can't create API keys or manage users in the UI, what's wrong?_](https://docs.smith.langchain.com/self_hosting/faq#i-cant-create-api-keys-or-manage-users-in-the-ui-whats-wrong)
  * [_How does load balancing/ingress work_?](https://docs.smith.langchain.com/self_hosting/faq#how-does-load-balancingingress-work)
  * [_How can we authenticate to the application?_](https://docs.smith.langchain.com/self_hosting/faq#how-can-we-authenticate-to-the-application)
  * [_Can I use external storage services?_](https://docs.smith.langchain.com/self_hosting/faq#can-i-use-external-storage-services)
  * [_Does my application need egress to function properly?_](https://docs.smith.langchain.com/self_hosting/faq#does-my-application-need-egress-to-function-properly)
  * [_Resource requirements for the application?_](https://docs.smith.langchain.com/self_hosting/faq#resource-requirements-for-the-application)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/installation

[Skip to main content](https://docs.smith.langchain.com/self_hosting/installation#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/installation)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
      * [Installing on Kubernetes](https://docs.smith.langchain.com/self_hosting/installation/kubernetes)
      * [Installing on Docker](https://docs.smith.langchain.com/self_hosting/installation/docker)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/installation)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/installation)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/installation)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/installation)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * Installation


# Installation
This section contains guides for installing LangSmith on your own infrastructure.
  * [Kubernetes](https://docs.smith.langchain.com/self_hosting/installation/kubernetes): Deploy LangSmith on Kubernetes.
  * [Docker](https://docs.smith.langchain.com/self_hosting/installation/docker): Deploy LangSmith using Docker.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/installation%3E).
[PreviousRunning Clickhouse Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)[NextInstalling on Kubernetes](https://docs.smith.langchain.com/self_hosting/installation/kubernetes)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/installation/docker

[Skip to main content](https://docs.smith.langchain.com/self_hosting/installation/docker#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/installation/docker)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
      * [Installing on Kubernetes](https://docs.smith.langchain.com/self_hosting/installation/kubernetes)
      * [Installing on Docker](https://docs.smith.langchain.com/self_hosting/installation/docker)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/installation/docker)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/installation/docker)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/installation/docker)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/installation/docker)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
  * Installing on Docker


On this page
# Self-hosting LangSmith with Docker
Enterprise License Required
Self-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our [pricing page](https://www.langchain.com/pricing) for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment.
This guide provides instructions for installing and setting up your environment to run LangSmith locally using Docker. You can do this either by using the LangSmith SDK or by using Docker Compose directly.
## Prerequisites[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/docker#prerequisites "Direct link to Prerequisites")
  1. Ensure Docker is installed and running on your system. You can verify this by running: 
```
docker info
```

If you don't see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon. 
    1. Recommended: At least 4 vCPUs, 16GB Memory available on your machine. 
       * You may need to tune resource requests/limits for all of our different services based off of organization size/usage
    2. Disk Space: LangSmith can potentially require a lot of disk space. Ensure you have enough disk space available.
  2. LangSmith License Key 
    1. You can get this from your Langchain representative. Contact us at sales@langchain.dev for more information.
  3. Api Key Salt 
    1. This is a secret key that you can generate. It should be a random string of characters.
    2. You can generate this using the following command:
```
openssl rand -base64 32
```

  4. Egress to `https://beacon.langchain.com` (if not running in offline mode) 
    1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](https://docs.smith.langchain.com/self_hosting/egress) section.
  5. Configuration 
    1. There are several configuration options that you can set in the `.env` file. You can find more information on the available configuration options in the [Configuration](https://docs.smith.langchain.com/self_hosting/configuration) section.


## Running via Docker Compose[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/docker#running-via-docker-compose "Direct link to Running via Docker Compose")
The following explains how to run the LangSmith using Docker Compose. This is the most flexible way to run LangSmith without Kubernetes. The default configuration for Docker Compose is intended for local testing only and not for instances where any services are exposed to the public internet. **In production, we highly recommend using a secured Kubernetes environment.**
### 1. Fetch the LangSmith `docker-compose.yml` file[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/docker#1-fetch-the-langsmith-docker-composeyml-file "Direct link to 1-fetch-the-langsmith-docker-composeyml-file")
You can find the `docker-compose.yml` file and related files in the LangSmith SDK repository here: [_LangSmith Docker Compose File_](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docker-compose/docker-compose.yaml)
Copy the `docker-compose.yml` file and all files in that directory from the LangSmith SDK to your project directory.
  * Ensure that you copy the `users.xml` file as well.


### 2. Configure environment variables[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/docker#2-configure-environment-variables "Direct link to 2. Configure environment variables")
  1. Copy the `.env.example` file from the LangSmith SDK to your project directory and rename it to `.env`.
  2. Configure the appropriate values in the `.env` file. You can find the available configuration options in the [Configuration](https://docs.smith.langchain.com/self_hosting/configuration) section.


You can also set these environment variables in the `docker-compose.yml` file directly or export them in your terminal. We recommend setting them in the `.env` file.
### 2. Start server[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/docker#2-start-server "Direct link to 2. Start server")
Start the LangSmith application by executing the following command in your terminal:
```
docker-compose up
```

You can also run the server in the background by running:
```
docker-compose up -d
```

### Validate your deployment:[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/docker#validate-your-deployment "Direct link to Validate your deployment:")
  1. Curl the exposed port of the `cli-langchain-frontend-1` container:
```
curl localhost:1980/info{"version":"0.5.7","license_expiration_time":"2033-05-20T20:08:06","batch_ingest_config":{"scale_up_qsize_trigger":1000,"scale_up_nthreads_limit":16,"scale_down_nempty_trigger":4,"size_limit":100,"size_limit_bytes":20971520}}
```

  2. Visit the exposed port of the `cli-langchain-frontend-1` container on your browser
The Langsmith UI should be visible/operational at `http://localhost:1980`
![.langsmith_ui.png](https://docs.smith.langchain.com/assets/images/langsmith_ui-a308960b13a121598b5c577e7587adfe.png)


### Checking the logs[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/docker#checking-the-logs "Direct link to Checking the logs")
If, at any point, you want to check if the server is running and see the logs, run
```
docker-compose logs
```

### Stopping the server[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/docker#stopping-the-server "Direct link to Stopping the server")
```
docker-compose down
```

## Using LangSmith[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/docker#using-langsmith "Direct link to Using LangSmith")
Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the [self-hosted usage guide](https://docs.smith.langchain.com/self_hosting/usage).
Your LangSmith instance is now running but may not be fully setup yet.
If you used one of the basic configs, you may have deployed a no-auth configuration. In this state, there is no authentication or concept of user accounts nor API keys and traces can be submitted directly without an API key so long as the hostname is passed to the LangChain tracer/LangSmith SDK.
As a next step, it is strongly recommended you work with your infrastructure administrators to:
  * Setup DNS for your LangSmith instance to enable easier access
  * Configure SSL to ensure in-transit encryption of traces submitted to LangSmith
  * Configure LangSmith for [oauth authentication](https://docs.smith.langchain.com/self_hosting/configuration/sso) or [basic authentication](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth) to secure your LangSmith instance
  * Secure access to your Docker environment to limit access to only the LangSmith frontend and API
  * Connect LangSmith to secured Postgres and Redis instances


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/installation/docker%3E).
[PreviousInstalling on Kubernetes](https://docs.smith.langchain.com/self_hosting/installation/kubernetes)[NextConfiguration](https://docs.smith.langchain.com/self_hosting/configuration)
  * [Prerequisites](https://docs.smith.langchain.com/self_hosting/installation/docker#prerequisites)
  * [Running via Docker Compose](https://docs.smith.langchain.com/self_hosting/installation/docker#running-via-docker-compose)
    * [1. Fetch the LangSmith `docker-compose.yml` file](https://docs.smith.langchain.com/self_hosting/installation/docker#1-fetch-the-langsmith-docker-composeyml-file)
    * [2. Configure environment variables](https://docs.smith.langchain.com/self_hosting/installation/docker#2-configure-environment-variables)
    * [2. Start server](https://docs.smith.langchain.com/self_hosting/installation/docker#2-start-server)
    * [Validate your deployment:](https://docs.smith.langchain.com/self_hosting/installation/docker#validate-your-deployment)
    * [Checking the logs](https://docs.smith.langchain.com/self_hosting/installation/docker#checking-the-logs)
    * [Stopping the server](https://docs.smith.langchain.com/self_hosting/installation/docker#stopping-the-server)
  * [Using LangSmith](https://docs.smith.langchain.com/self_hosting/installation/docker#using-langsmith)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/installation/kubernetes

[Skip to main content](https://docs.smith.langchain.com/self_hosting/installation/kubernetes#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/installation/kubernetes)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
      * [Installing on Kubernetes](https://docs.smith.langchain.com/self_hosting/installation/kubernetes)
      * [Installing on Docker](https://docs.smith.langchain.com/self_hosting/installation/docker)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/installation/kubernetes)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/installation/kubernetes)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/installation/kubernetes)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/installation/kubernetes)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
  * Installing on Kubernetes


On this page
# Self-hosting LangSmith on Kubernetes
Enterprise License Required
Self-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our [pricing page](https://www.langchain.com/pricing) for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment.
This guide will walk you through the process of deploying LangSmith to a Kubernetes cluster. We will use Helm to install LangSmith and its dependencies.
We've successfully tested LangSmith on the following Kubernetes distributions:
  * Google Kubernetes Engine (GKE)
  * Amazon Elastic Kubernetes Service (EKS)
  * Azure Kubernetes Service (AKS)
  * OpenShift
  * Minikube and Kind (for development purposes)


## Prerequisites[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/kubernetes#prerequisites "Direct link to Prerequisites")
Ensure you have the following tools/items ready. Some items are marked optional:
  1. A working Kubernetes cluster that you can access via `kubectl`. Your cluster should have the following minimum requirements:
    1. Recommended: At least 16 vCPUs, 64GB Memory available 
       * You may need to tune resource requests/limits for all of our different services based off of organization size/usage
       * We recommend using a cluster autoscaler to handle scaling up/down of nodes based on resource usage
       * We recommend setting up the metrics server so that autoscaling can be turned on
       * You must have a node with at least 4 vCPUs and 16GB of memory **allocatable** as ClickHouse will request this amount of resources by default.
    2. Valid Dynamic PV provisioner or PVs available on your cluster. You can verify this by running: 
       * We will be using a PostgreSQL database, Redis, and ClickHouse for storing traces. These services require persistent storage.
       * If using PVs in your cluster, we highly recommend setting up backups in a production environment.
       * **We strongly encourage using a storage class backed by SSDs for better performance. We recommend 7000 IOPS and 1000 MiB/s throughput.**
```
kubectl get storageclass
```

The output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:
```
 NAME      PROVISIONER       RECLAIMPOLICY  VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION  AGE gp2 (default)  kubernetes.io/aws-ebs  Delete     WaitForFirstConsumer  true          161d
```

note
We highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time.
Refer to the [Kubernetes documentation](https://kubernetes.io/docs/concepts/storage/storage-classes/) for more information on storage classes.
  2. Helm
    1. To install helm refer to the [Helm documentation](https://helm.sh/docs/intro/install/)
  3. LangSmith License Key
    1. You can get this from your Langchain representative. Contact us at sales@langchain.dev for more information.
  4. Api Key Salt
    1. This is a secret key that you can generate. It should be a random string of characters.
    2. You can generate this using the following command:
```
openssl rand -base64 32
```

  5. JWT Secret (Optional but used for basic auth)
    1. This is a secret key that you can generate. It should be a random string of characters.
    2. You can generate this using the following command:
```
openssl rand -base64 32
```

  6. Egress to `https://beacon.langchain.com` (if not running in offline mode)
    1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](https://docs.smith.langchain.com/self_hosting/egress) section.
  7. Configuration
    1. There are several configuration options that you can set in the `langsmith_config.yaml` file. You can find more information on specific configuration options in the [Configuration](https://docs.smith.langchain.com/self_hosting/configuration) section.
    2. If you are new to Kubernetes or Helm, we‚Äôd recommend starting with one of the example configurations in the examples directory of the Helm Chart repository here: [LangSmith helm chart examples](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/examples).
    3. You can see a full list of configuration options in the `values.yaml` file in the Helm Chart repository here: [LangSmith Helm Chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/values.yaml)


## Configure your Helm Charts:[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/kubernetes#configure-your-helm-charts "Direct link to Configure your Helm Charts:")
  1. Create a new file called `langsmith_config.yaml` with the configuration options from the previous step.
  2. At a minimum, you will need to set the following configuration options (using basic auth):
```
config:langsmithLicenseKey:"<your license key>"apiKeySalt:"<your api key salt>"authType: mixedbasicAuth:enabled:trueinitialOrgAdminEmail:"admin@langchain.dev"# Change this to your admin email addressinitialOrgAdminPassword:"secure-password"# Must be at least 12 characters long and have at least one lowercase, uppercase, and symboljwtSecret: <your jwt salt># A random string of characters used to sign JWT tokens for basic auth.
```



## Deploying to Kubernetes:[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/kubernetes#deploying-to-kubernetes "Direct link to Deploying to Kubernetes:")
  1. Verify that you can connect to your Kubernetes cluster(note: We highly suggest installing into an empty namespace)
    1. Run `kubectl get pods`
Output should look something like:
```
kubectl get pods                                                                                   ‚éà langsmith-eks-2vauP7wf 21:07:46No resources found in default namespace.
```

  2. Ensure you have the Langchain Helm repo added. (skip this step if you are using local charts)
helm repo add langchain <https://langchain-ai.github.io/helm/> "langchain" has been added to your repositories


Namespace
If you are using a namespace other than the default namespace, you will need to specify the namespace in the `helm` and `kubectl` commands by using the `-n <namespace` flag.
  1. Run `helm install langsmith langchain/langsmith --values langsmith_config.yaml --version <version> --debug`
     * Replace `<your-namespace>` with the namespace you want to deploy LangSmith to.
     * Replace `<version>` with the version of LangSmith you want to deploy. You can find the available versions in the [Helm Chart repository](https://github.com/langchain-ai/helm/releases). We generally recommend using the latest version. Output should look something like:
```
NAME: langsmithLAST DEPLOYED: Fri Sep 17 21:08:47 2021NAMESPACE: langsmithSTATUS: deployedREVISION: 1TEST SUITE: None
```

  2. Run `kubectl get pods` Output should now look something like:
```
langsmith-backend-6ff46c99c4-wz22d    1/1   Running  0     3h2mlangsmith-frontend-6bbb94c5df-8xrlr   1/1   Running  0     3h2mlangsmith-hub-backend-5cc68c888c-vppjj  1/1   Running  0     3h2mlangsmith-playground-6d95fd8dc6-x2d9b  1/1   Running  0     3h2mlangsmith-postgres-0           1/1   Running  0     9hlangsmith-queue-5898b9d566-tv6q8     1/1   Running  0     3h2mlangsmith-redis-0            1/1   Running  0     9h
```



## Validate your deployment:[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/kubernetes#validate-your-deployment "Direct link to Validate your deployment:")
  1. Run `kubectl get services`
Output should look something like:
```
NAME          TYPE      CLUSTER-IP    EXTERNAL-IP                                PORT(S)    AGElangsmith-backend    ClusterIP   172.20.140.77  <none>                                  1984/TCP    35hlangsmith-frontend   LoadBalancer  172.20.253.251  <external ip>                               80:31591/TCP  35hlangsmith-hub-backend  ClusterIP   172.20.112.234  <none>                                  1985/TCP    35hlangsmith-playground  ClusterIP   172.20.153.194  <none>                                  3001/TCP    9hlangsmith-postgres   ClusterIP   172.20.244.82  <none>                                  5432/TCP    35hlangsmith-redis     ClusterIP   172.20.81.217  <none>                                  6379/TCP    35h
```

  2. Curl the external ip of the `langsmith-frontend` service:
```
curl <external ip>/api/tenants[{"id":"00000000-0000-0000-0000-000000000000","has_waitlist_access":true,"created_at":"2023-09-13T18:25:10.488407","display_name":"Personal","config":{"is_personal":true,"max_identities":1},"tenant_handle":"default"}]%
```

  3. Visit the external ip for the `langsmith-frontend` service on your browser
The LangSmith UI should be visible/operational
![./static/langsmith_ui.png](https://docs.smith.langchain.com/assets/images/langsmith_ui-a308960b13a121598b5c577e7587adfe.png)


## Using LangSmith[‚Äã](https://docs.smith.langchain.com/self_hosting/installation/kubernetes#using-langsmith "Direct link to Using LangSmith")
Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the [self-hosted usage guide](https://docs.smith.langchain.com/self_hosting/usage).
Your LangSmith instance is now running but may not be fully setup yet.
If you used one of the basic configs, you may have deployed a no-auth configuration. In this state, there is no authentication or concept of user accounts nor API keys and traces can be submitted directly without an API key so long as the hostname is passed to the LangChain tracer/LangSmith SDK.
As a next step, it is strongly recommended you work with your infrastructure administrators to:
  * Setup DNS for your LangSmith instance to enable easier access
  * Configure SSL to ensure in-transit encryption of traces submitted to LangSmith
  * Configure LangSmith for [oauth authentication](https://docs.smith.langchain.com/self_hosting/configuration/sso) or [basic authentication](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth) to secure your LangSmith instance
  * Connect LangSmith to secured Postgres and Redis instances


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/installation/kubernetes%3E).
[PreviousInstallation](https://docs.smith.langchain.com/self_hosting/installation)[NextInstalling on Docker](https://docs.smith.langchain.com/self_hosting/installation/docker)
  * [Prerequisites](https://docs.smith.langchain.com/self_hosting/installation/kubernetes#prerequisites)
  * [Configure your Helm Charts:](https://docs.smith.langchain.com/self_hosting/installation/kubernetes#configure-your-helm-charts)
  * [Deploying to Kubernetes:](https://docs.smith.langchain.com/self_hosting/installation/kubernetes#deploying-to-kubernetes)
  * [Validate your deployment:](https://docs.smith.langchain.com/self_hosting/installation/kubernetes#validate-your-deployment)
  * [Using LangSmith](https://docs.smith.langchain.com/self_hosting/installation/kubernetes#using-langsmith)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse

[Skip to main content](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * LangSmith-managed ClickHouse


On this page
# LangSmith-managed ClickHouse
recommended reading
Please read the [LangSmith architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview) and [guide on connecting to external Clickhouse](https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse) before proceeding with this guide.
As mentioned in previous guides, LangSmith uses Clickhouse as the primary storage engine for **traces** and **feedback**. For easier management and scaling, it is recommended to connect a self-hosted LangSmith instance to an external Clickhouse instance. LangSmith-managed ClickHouse is an option that allows you to use a fully managed ClickHouse instance that is monitored and maintained by the LangSmith team.
## Architecture Overview[‚Äã](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse#architecture-overview "Direct link to Architecture Overview")
Using LangSmith Managed Clickhouse with your Self-Hosted LangSmith instance is fairly simple. The overall architecture is similar to using a fully self-hosted ClickHouse instance, with a few key differences:
  * You will need to set up a private network connection between your LangSmith instance and the LangSmith-managed ClickHouse instance. This is to ensure that your data is secure and that you can connect to the ClickHouse instance from your self-hosted LangSmith instance.
  * With this option, sensitive information (inputs and outputs) of your traces will be stored in cloud object storage (S3 or GCS) within your cloud instead of Clickhouse to ensure that sensitive information doesn't leave your VPC.


More on sensitive information
Clickhouse stores **runs** and **feedback** data.
This [reference doc](https://docs.smith.langchain.com/reference/data_formats/feedback_data_format) explains the format we use to store feedback, which is the LangSmith's way of representing evaluation scores and annotations on runs. This [reference doc](https://docs.smith.langchain.com/reference/data_formats/run_data_format) explains the format we use to store runs (spans), which are the building blocks of traces.
Our definition of sensitive information as it relates to application data are `inputs`, `outputs`, `errors`, `manifests`, `extras`, and `events` of a **run** , since these fields can contain prompts and completions from LLMs.
With LangSmith-managed ClickHouse, we store `inputs`, `outputs`, `errors`, `manifests`, `extras`, and `events` in cloud object storage (S3 or GCS) within your cloud and store the rest of the run data in ClickHouse. This ensures that sensitive information doesn't leave your VPC.
Please note, ALL **feedback** data is stored in ClickHouse. Please do not send sensitive information in feedback (scores and annotations/comments) or in any other **run** fields that are mentioned above.
  * The LangSmith team will monitor your ClickHouse instance and ensure that it is running smoothly. This allows us to track metrics like run-ingestion delay and query performance.


The overall architecture looks like this:
![LangSmith Managed ClickHouse Architecture](https://docs.smith.langchain.com/assets/images/langsmith_managed_clickhouse_architecture-ec8d2b6d3bee84152fc465e9a7bd9698.png)
## Requirements[‚Äã](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse#requirements "Direct link to Requirements")
  * **You must use a supported blob storage option.** Read the [blob storage guide](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage) for more information.
  * To use private endpoints, ensure that your VPC is in a ClickHouse Cloud supported [region](https://clickhouse.com/docs/en/cloud/reference/supported-regions). Otherwise, you will need to use a public endpoint we will secure with firewall rules. Your VPC will need to have a NAT gateway to allow us to whitelist your traffic.
  * You must have a VPC that can connect to the LangSmith-managed Clickhouse service. You will need to work with our team to set up the necessary networking.
  * You must have a LangSmith self-hosted instance running. You can use our managed ClickHouse service with both [Kubernetes](https://docs.smith.langchain.com/self_hosting/installation/kubernetes) and [Docker](https://docs.smith.langchain.com/self_hosting/installation/docker) installations.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/langsmith_managed_clickhouse%3E).
[PreviousFrequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)[NextTroubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Architecture Overview](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse#architecture-overview)
  * [Requirements](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse#requirements)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/organization_charts

[Skip to main content](https://docs.smith.langchain.com/self_hosting/organization_charts#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/organization_charts)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/organization_charts)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/organization_charts)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/organization_charts)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * Organization Charts


# Viewing trace counts across your organization
note
This feature is available on Helm chart versions 0.9.5 and later.
LangSmith automatically generates and syncs organization usage charts for self-hosted installations.
These charts are available under `Settings > Usage and billing > Usage graph`:
  * Usage by Workspace: this counts traces (root runs) by workspace
  * Organization Usage: this counts all traces (root runs) for the organization


The charts are refreshed to include any new workspaces every 5 minutes. Note that the charts are not editable.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/organization_charts%3E).
[PreviousEgress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)[NextRelease notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/release_notes

[Skip to main content](https://docs.smith.langchain.com/self_hosting/release_notes#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/release_notes)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/release_notes)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/release_notes)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/release_notes)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * Release notes (self-hosted)


On this page
# LangSmith Release Notes
note
**Reminder: API keys prefixed with`ls__` have been disabled in favor of `lsv2...` style keys as of LangSmith Helm release v0.8.** For more information see [the Admin concepts guide.](https://docs.smith.langchain.com/administration/concepts#api-keys)
## Week of October 28, 2024 - LangSmith v0.8[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-october-28-2024---langsmith-v08 "Direct link to Week of October 28, 2024 - LangSmith v0.8")
Release notes are available at our [new changelog](https://changelog.langchain.com/?categories=cat_ZWTyLBFVqdtSq).
## Week of August 26, 2024 - LangSmith v0.7[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-august-26-2024---langsmith-v07 "Direct link to Week of August 26, 2024 - LangSmith v0.7")
This release adds a number of new features, improves the performance of the Threads view, and adds password authentication support and adds support for setting a default Time To Live (TTL) on LangSmith traces.
### New Features since v0.6.0[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#new-features-since-v060 "Direct link to New Features since v0.6.0")
  * [Resource tags to organize your Workspace in LangSmith](https://changelog.langchain.com/announcements/resource-tags-to-organize-your-workspace-in-langsmith)
  * [Generate synthetic examples to enhance a LangSmith dataset](https://changelog.langchain.com/announcements/generate-synthetic-examples-to-enhance-a-langsmith-dataset)
  * [Enhanced trace comparison view and saveable custom trace filters](https://changelog.langchain.com/announcements/trace-comparison-view-saving-custom-trace-filters)
  * [Defining, validating and updating dataset schemas](https://changelog.langchain.com/announcements/define-validate-and-update-dataset-schemas-in-langsmith)
  * [Multiple annotators can review a run in LangSmith](https://changelog.langchain.com/announcements/multiple-annotators-can-review-a-run-in-langsmith)
  * [Support for filtering runs within the trace view](https://changelog.langchain.com/announcements/filtering-runs-within-the-trace-view)
  * [Enhanced key-value search](https://changelog.langchain.com/announcements/enhanced-key-value-search-matching-inputs-and-outputs)
  * [Webhook notifications for run rules](https://changelog.langchain.com/announcements/set-up-webhook-notifications-for-run-rules)
  * [Support for comparing multiple prompts and model configurations side-by-side in the LangSmith Playground](https://changelog.langchain.com/announcements/build-prompts-faster-and-compare-in-langsmith-playground)
  * [Support for storing the model and configuration when saving a Prompt](https://changelog.langchain.com/announcements/store-the-model-and-configuration-when-saving-a-prompt)


### Performance and Reliability Changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#performance-and-reliability-changes "Direct link to Performance and Reliability Changes")
  * Improved performance of Threads view for very large projects
  * Improved error handling in cases where the Clickhouse database is temporarily unavailable


### Infrastructure Changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#infrastructure-changes "Direct link to Infrastructure Changes")
  * Added a Helm configuration option for Time To Live for traces. When set, this setting will apply only to net-new ingested traces and by changing this setting, _traces will be automatically and irrevocably deleted from Clickhouse after expiration of the TTL._ For more details see [TTL and Data Retention](https://docs.smith.langchain.com/self_hosting/configuration/ttl). You may also need to change/audit your project default TTL settings.
  * Added configuration option to enable `blobStorage`. This will move run inputs, outputs, errors, manifests, extras, and events to blob storage to lower load on ClickHouse/reduce disk usage. Currently only S3 and GCP are supported. For more details see [Enable Blob Storage](https://docs.smith.langchain.com/self_hosting/configuration/blob_storage).
  * Default Resource/Limits for all resources. Note that you may need to tweak your cluster settings. 
    * By default we will use: 
      * 16 CPU
      * 64 GB RAM
      * You will need a node that can fit 4 CPU/16 GB RAM
    * To override these settings you can manually configure resources requests/limits yourself
  * Turned bundled `Redis` persistence on by default. If you are using the bundled version of `redis` you may need to recreate your `Redis` StatefulSet if you had not previously turned on persistence.
  * Updated `clickhouseMigration` command to wait for clickhouse initialization prior to running migrations.
  * Deprecation of `<domain>/api-hub url`. You can now use the `<domain>/api` url for all api interactions. This will be fully removed in the v0.8 release so make sure to migrate any apps using the previous url.
  * Health checks added to `queue` pods.
  * Updates to the `nginx` config.
  * Removed the OpenAI key configuration option.


### Admin Changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#admin-changes "Direct link to Admin Changes")
  * Added support for simple password authentication. For more details see [Email/password a.k.a. basic auth](https://docs.smith.langchain.com/self_hosting/configuration/basic_auth)
    * **Note that at this time there is no migration path from simple to OIDC authentication** ‚Äî we are working on this migration path for a subsequent release.
  * Added support to disable personal orgs.
  * Added support to disable org creation.
  * Added config option to allow workspace admins to add workspace users.
  * We have delayed deprecation of the v1 API Keys prefixed with `ls__` until the v0.8 release of LangSmith which should happen on or around October 1. Please update your API keys to Service Keys prefixed with `lsv2__sk` at your earliest convenience.


### Deprecation notices[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#deprecation-notices "Direct link to Deprecation notices")
With the release of v0.7:
  * The LangChain Hub SDK is now deprecated and its functionality folded into the [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk).
  * LangSmith v0.6.x and earlier are now in maintenance mode and may only receive critical security fixes.


## Week of June 17, 2024 - LangSmith v0.6[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v06 "Direct link to Week of June 17, 2024 - LangSmith v0.6")
LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations.
### New Features since v0.5[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#new-features-since-v05 "Direct link to New Features since v0.5")
  * Dataset splits for evaluation and filtering/editing dataset examples. [Learn More...](https://blog.langchain.dev/week-of-5-27-langchain-release-notes/#datasetsplits)
  * You can now run multiple repetitions of your experiment in LangSmith. [Learn More...](https://blog.langchain.dev/week-of-5-27-langchain-release-notes/#repetitions)
  * Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. [Learn More...](https://blog.langchain.dev/week-of-5-27-langchain-release-notes/#onlineevaluatorprompts)
  * Manage private prompts without a handle. [Learn More...](https://blog.langchain.dev/week-of-5-27-langchain-release-notes/#privateprompts)
  * Workspaces in LangSmith for improved collaboration & organization. [Learn More...](https://blog.langchain.dev/week-of-6-10-langchain-release-notes/#workspaces)
  * Enter the playground from scratch instead of from a trace or a prompt. [Learn More...](https://blog.langchain.dev/week-of-6-10-langchain-release-notes/#playground-from-scratch)
  * Variable mapping for online evaluator prompts. [Learn More...](https://blog.langchain.dev/week-of-6-10-langchain-release-notes/#variable-mapping)
  * Custom Model support in Playground. [Learn More...](https://docs.smith.langchain.com/how_to_guides/custom_endpoint)


### Performance and Reliability Changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#performance-and-reliability-changes-1 "Direct link to Performance and Reliability Changes")
  * Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.
  * Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rules
  * Improved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the [Hub SDK](https://github.com/langchain-ai/hub-sdk)


### Infrastructure changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#infrastructure-changes-1 "Direct link to Infrastructure changes")
  * [Docker Compose only] The default port has changed from 80 to 1980.
  * [Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.
  * [Helm] Added the ability to configure your probes in the `values.yaml` file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.
  * [Helm] Added ArgoCD `PostSync` annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.
  * Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse.


### Admin changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#admin-changes-1 "Direct link to Admin changes")
  * Added support for Workspaces. See the [Admin concepts guide](https://docs.smith.langchain.com/administration/concepts#workspaces) for more details.
  * Added global setting `orgCreationDisabled` to `values.yaml` to disable creation of new Organizations.
  * Added support for custom TLS certificates for the Azure OpenAI model provider. See the [how-to guide](https://docs.smith.langchain.com/self_hosting/configuration/custom_tls_certificates) for more details.


### Deprecation notices[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#deprecation-notices-1 "Direct link to Deprecation notices")
With the release of v0.6:
  * LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes.


## Week of May 13, 2024 - LangSmith v0.5[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05 "Direct link to Week of May 13, 2024 - LangSmith v0.5")
LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC).
### Breaking changes[](https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes)[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes "Direct link to breaking-changes")
  * We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. **API keys prefixed with`ls__` will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024.**


### New Features since v0.4[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#new-features-since-v04 "Direct link to New Features since v0.4")
  * Role-Based Access Controls. See: <https://blog.langchain.dev/access-control-updates-for-langsmith/>
  * Improved regression testing experience. See: <https://blog.langchain.dev/regression-testing/>
  * Improved production monitoring and automation: See: <https://blog.langchain.dev/langsmith-production-logging-automations/>


### Performance and Reliability Changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#performance-and-reliability-changes-2 "Direct link to Performance and Reliability Changes")
  * Split ingest, session deletion, and automation jobs to execute within separate resource pools.


### Infrastructure changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#infrastructure-changes-2 "Direct link to Infrastructure changes")
  * As of LangSmith v0.4, Clickhouse persistence now uses `50Gi` of storage by default. You can adjust this by changing the `clickhouse.statefulSet.persistence.size` value in your `values.yaml` file. 
    * If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set `clickhouse.statefulSet.persistence.size` to the previous default value of `8Gi`.
    * It is **strongly** recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.
  * New Platform-Backend service used internally. This service also uses it‚Äôs own image. You may need to adjust your helm `values` files accordingly.


### Admin changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#admin-changes-2 "Direct link to Admin changes")
  * Added new Role-Based Access Controls. For more details see the [Admin](https://docs.smith.langchain.com/administration/concepts) and [Set Up Access Control](https://docs.smith.langchain.com/administration/how_to_guides/organization_management/set_up_access_control) sections of the docs.
  * Introduction of PATs and Service Keys. Old API keys have been migrated to service keys.


### Deprecation notices[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#deprecation-notices-2 "Direct link to Deprecation notices")
With the release of v0.5:
  * LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes.


## Week of March 25, 2024 - LangSmith v0.4[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04 "Direct link to Week of March 25, 2024 - LangSmith v0.4")
LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter.
### Breaking changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes-1 "Direct link to Breaking changes")
  * This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. **For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility.** Using a new api key salt will invalidate all existing api keys.
  * This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the `clickhouse.statefulSet.persistence.size` value in your `values.yaml` file. 
    * If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set `clickhouse.statefulSet.persistence.size` to the previous default value of `8Gi`.


### Performance and Reliability Changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#performance-and-reliability-changes-3 "Direct link to Performance and Reliability Changes")
  * Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.


### Infrastructure changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#infrastructure-changes-3 "Direct link to Infrastructure changes")
  * Some our image repositories have been updated. You can see the root repositories in our `values.yaml` file and may need to update mirrors to pick up the new images.
  * Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the `clickhouse.statefulSet.persistence.size` value in your `values.yaml` file. 
    * If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set `clickhouse.statefulSet.persistence.size` to the previous default value of `8Gi`.
  * Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application.


### Admin changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#admin-changes-3 "Direct link to Admin changes")
  * Added an API key salt parameter in `values.yml`. This can be set to a custom value and changing it will invalidate all existing api keys.
  * Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.
  * Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at <https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md>


### Deprecation notices[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#deprecation-notices-3 "Direct link to Deprecation notices")
With the release of 0.4:
  * LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes.


## Week of Februrary 21, 2024 - LangSmith v0.3[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-februrary-21-2024---langsmith-v03 "Direct link to Week of Februrary 21, 2024 - LangSmith v0.3")
LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking.
### Breaking changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes-2 "Direct link to Breaking changes")
  * This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See <https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md> for additional details


### Performance and Reliability Changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#performance-and-reliability-changes-4 "Direct link to Performance and Reliability Changes")
  * Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.


### Admin changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#admin-changes-4 "Direct link to Admin changes")
  * None


### Deprecation notices[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#deprecation-notices-4 "Direct link to Deprecation notices")
With the release of 0.3:
  * LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes.


## Week of January 29, 2024 - LangSmith v0.2[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02 "Direct link to Week of January 29, 2024 - LangSmith v0.2")
LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces.
### Requirements[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#requirements "Direct link to Requirements")
  * This release requires `langsmith-sdk` version ‚â• `0.0.71` (Python) and ‚â• `0.0.56` (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint.


### Breaking changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes-3 "Direct link to Breaking changes")
  * The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. `{"user_id": ..., "user_name":...,}`) and then search using `has(metadata, '{"user_name": ...}')`


### Performance and Reliability Changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#performance-and-reliability-changes-5 "Direct link to Performance and Reliability Changes")
  * Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.
  * Improved performance for updates and deletes on annotation labels.
  * Added pagination of API responses.
  * Fixed an issue impacting natural language searches.


### Infrastructure Changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#infrastructure-changes-4 "Direct link to Infrastructure Changes")
  * Added the `clickhouse` database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith. 
    * Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at <https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md>


### Admin changes[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#admin-changes-5 "Direct link to Admin changes")
  * Increased the maximum number of users per organization from 5 to 100 for new organizations.


### Deprecation notices[‚Äã](https://docs.smith.langchain.com/self_hosting/release_notes#deprecation-notices-5 "Direct link to Deprecation notices")
With the release of 0.2:
  * LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/release_notes%3E).
[PreviousOrganization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)[NextFrequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
  * [Week of October 28, 2024 - LangSmith v0.8](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-october-28-2024---langsmith-v08)
  * [Week of August 26, 2024 - LangSmith v0.7](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-august-26-2024---langsmith-v07)
    * [New Features since v0.6.0](https://docs.smith.langchain.com/self_hosting/release_notes#new-features-since-v060)
    * [Performance and Reliability Changes](https://docs.smith.langchain.com/self_hosting/release_notes#performance-and-reliability-changes)
    * [Infrastructure Changes](https://docs.smith.langchain.com/self_hosting/release_notes#infrastructure-changes)
    * [Admin Changes](https://docs.smith.langchain.com/self_hosting/release_notes#admin-changes)
    * [Deprecation notices](https://docs.smith.langchain.com/self_hosting/release_notes#deprecation-notices)
  * [Week of June 17, 2024 - LangSmith v0.6](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v06)
    * [New Features since v0.5](https://docs.smith.langchain.com/self_hosting/release_notes#new-features-since-v05)
    * [Performance and Reliability Changes](https://docs.smith.langchain.com/self_hosting/release_notes#performance-and-reliability-changes-1)
    * [Infrastructure changes](https://docs.smith.langchain.com/self_hosting/release_notes#infrastructure-changes-1)
    * [Admin changes](https://docs.smith.langchain.com/self_hosting/release_notes#admin-changes-1)
    * [Deprecation notices](https://docs.smith.langchain.com/self_hosting/release_notes#deprecation-notices-1)
  * [Week of May 13, 2024 - LangSmith v0.5](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05)
    * [Breaking changes](https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes)
    * [New Features since v0.4](https://docs.smith.langchain.com/self_hosting/release_notes#new-features-since-v04)
    * [Performance and Reliability Changes](https://docs.smith.langchain.com/self_hosting/release_notes#performance-and-reliability-changes-2)
    * [Infrastructure changes](https://docs.smith.langchain.com/self_hosting/release_notes#infrastructure-changes-2)
    * [Admin changes](https://docs.smith.langchain.com/self_hosting/release_notes#admin-changes-2)
    * [Deprecation notices](https://docs.smith.langchain.com/self_hosting/release_notes#deprecation-notices-2)
  * [Week of March 25, 2024 - LangSmith v0.4](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04)
    * [Breaking changes](https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes-1)
    * [Performance and Reliability Changes](https://docs.smith.langchain.com/self_hosting/release_notes#performance-and-reliability-changes-3)
    * [Infrastructure changes](https://docs.smith.langchain.com/self_hosting/release_notes#infrastructure-changes-3)
    * [Admin changes](https://docs.smith.langchain.com/self_hosting/release_notes#admin-changes-3)
    * [Deprecation notices](https://docs.smith.langchain.com/self_hosting/release_notes#deprecation-notices-3)
  * [Week of Februrary 21, 2024 - LangSmith v0.3](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-februrary-21-2024---langsmith-v03)
    * [Breaking changes](https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes-2)
    * [Performance and Reliability Changes](https://docs.smith.langchain.com/self_hosting/release_notes#performance-and-reliability-changes-4)
    * [Admin changes](https://docs.smith.langchain.com/self_hosting/release_notes#admin-changes-4)
    * [Deprecation notices](https://docs.smith.langchain.com/self_hosting/release_notes#deprecation-notices-4)
  * [Week of January 29, 2024 - LangSmith v0.2](https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02)
    * [Requirements](https://docs.smith.langchain.com/self_hosting/release_notes#requirements)
    * [Breaking changes](https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes-3)
    * [Performance and Reliability Changes](https://docs.smith.langchain.com/self_hosting/release_notes#performance-and-reliability-changes-5)
    * [Infrastructure Changes](https://docs.smith.langchain.com/self_hosting/release_notes#infrastructure-changes-4)
    * [Admin changes](https://docs.smith.langchain.com/self_hosting/release_notes#admin-changes-5)
    * [Deprecation notices](https://docs.smith.langchain.com/self_hosting/release_notes#deprecation-notices-5)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/scripts

[Skip to main content](https://docs.smith.langchain.com/self_hosting/scripts#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/scripts)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
      * [Delete a Workspace](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
      * [Delete an Organization](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
      * [Delete a Single Trace](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
      * [Generate Clickhouse Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
      * [Generate Query Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
      * [Running Postgres Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
      * [Running Clickhouse Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/scripts)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/scripts)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/scripts)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * Scripts


# Scripts for administering LangSmith
This section contains guides for performing common administrative tasks that are not currently available via the LangSmith UI.
**You can find these scripts in the[Helm chart repository](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/scripts).**
  * [Delete an organization](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization): Delete an organization in LangSmith.
  * [Delete a workspace](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace): Delete a workspace in LangSmith.
  * [Delete a trace](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces): Delete a trace in LangSmith.
  * [Generate Clickhouse statistics](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats): Generate Clickhouse statistics.
  * [Generate LangSmith Query statistics](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats): Generate LangSmith Query statistics from Clickhouse.
  * [Running Postgres Support Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries): Running other postgres-related support queries supplied by the LangChain support team.
  * [Running Clickhouse Support Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries): Running other Clickhouse-related support queries supplied by the LangChain support team.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/scripts%3E).
[PreviousArchitectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)[NextDelete a Workspace](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace

[Skip to main content](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
      * [Delete a Workspace](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
      * [Delete an Organization](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
      * [Delete a Single Trace](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
      * [Generate Clickhouse Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
      * [Generate Query Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
      * [Running Postgres Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
      * [Running Clickhouse Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
  * Delete a Workspace


On this page
# Deleting Workspaces
The LangSmith UI does not currently support the deletion of an individual workspace from an organization. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs_history views) and the runs and feedbacks tables and then removing the Workspace from the Postgres tenants table.
This command using the Workspace ID as an argument.
### Prerequisites[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace#prerequisites "Direct link to Prerequisites")
Ensure you have the following tools/items ready.
  1. kubectl
     * <https://kubernetes.io/docs/tasks/tools/>
  2. PostgreSQL client
     * <https://www.postgresql.org/download/>
  3. PostgreSQL database connection:
     * Host
     * Port
     * Username 
       * If using the bundled version, this is `postgres`
     * Password 
       * If using the bundled version, this is `postgres`
     * Database name 
       * If using the bundled version, this is `postgres`
  4. Clickhouse database credentials
     * Host
     * Port
     * Username 
       * If using the bundled version, this is `default`
     * Password 
       * If using the bundled version, this is `password`
     * Database name 
       * If using the bundled version, this is `default`
  5. Connectivity to the PostgreSQL database from the machine you will be running the migration script on.
     * If you are using the bundled version, you may need to port forward the postgresql service to your local machine.
     * Run `kubectl port-forward svc/langsmith-postgres 5432:5432` to port forward the postgresql service to your local machine.
  6. Connectivity to the Clickhouse database from the machine you will be running the migration script on.
     * If you are using the bundled version, you may need to port forward the clickhouse service to your local machine. 
       * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.
     * If you are using Clickhouse Cloud you will want to specify the --ssl flag and use port `8443`
  7. The script to delete a workspace
     * You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/delete_workspace.sh)


### Running the deletion script for a single workspace[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace#running-the-deletion-script-for-a-single-workspace "Direct link to Running the deletion script for a single workspace")
Run the following command to run the workspace removal script:
```
sh delete_workspace.sh <postgres_url> <clickhouse_url> --workspace_id <workspace_id>
```

For example, if you are using the bundled version with port-forwarding, the command would look like:
```
sh delete_workspace.sh "postgres://postgres:postgres@localhost:5432/postgres" "clickhouse://default:password@localhost:8123/default" --workspace_id 4ec70ec7-0808-416a-b836-7100aeec934b
```

If you visit the Langsmith UI, you should now see workspace is deleted.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/scripts/delete_a_workspace%3E).
[PreviousScripts](https://docs.smith.langchain.com/self_hosting/scripts)[NextDelete an Organization](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
  * [Prerequisites](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace#prerequisites)
  * [Running the deletion script for a single workspace](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace#running-the-deletion-script-for-a-single-workspace)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization

[Skip to main content](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
      * [Delete a Workspace](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
      * [Delete an Organization](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
      * [Delete a Single Trace](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
      * [Generate Clickhouse Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
      * [Generate Query Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
      * [Running Postgres Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
      * [Running Clickhouse Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
  * Delete an Organization


On this page
# Deleting Organizations
The LangSmith UI does not currently support the deletion of an individual organization from a self-hosted instance of LangSmith. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs_history views) and the runs and feedbacks tables and then removing the Organization from the Postgres tenants table.
This command using the Organization ID as an argument.
### Prerequisites[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization#prerequisites "Direct link to Prerequisites")
Ensure you have the following tools/items ready.
  1. kubectl
     * <https://kubernetes.io/docs/tasks/tools/>
  2. PostgreSQL client
     * <https://www.postgresql.org/download/>
  3. PostgreSQL database connection:
     * Host
     * Port
     * Username 
       * If using the bundled version, this is `postgres`
     * Password 
       * If using the bundled version, this is `postgres`
     * Database name 
       * If using the bundled version, this is `postgres`
  4. Clickhouse database credentials
     * Host
     * Port
     * Username 
       * If using the bundled version, this is `default`
     * Password 
       * If using the bundled version, this is `password`
     * Database name 
       * If using the bundled version, this is `default`
  5. Connectivity to the PostgreSQL database from the machine you will be running the migration script on.
     * If you are using the bundled version, you may need to port forward the postgresql service to your local machine.
     * Run `kubectl port-forward svc/langsmith-postgres 5432:5432` to port forward the postgresql service to your local machine.
  6. Connectivity to the Clickhouse database from the machine you will be running the migration script on.
     * If you are using the bundled version, you may need to port forward the clickhouse service to your local machine. 
       * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.
     * If you are using Clickhouse Cloud you will want to specify the --ssl flag and use port `8443`
  7. The script to delete an organization
     * You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/delete_organization.sh)


### Running the deletion script for a single organization[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization#running-the-deletion-script-for-a-single-organization "Direct link to Running the deletion script for a single organization")
Run the following command to run the organization removal script:
```
sh delete_organization.sh <postgres_url> <clickhouse_url> --organization_id <organization_id>
```

For example, if you are using the bundled version with port-forwarding, the command would look like:
```
sh delete_organization.sh "postgres://postgres:postgres@localhost:5432/postgres" "clickhouse://default:password@localhost:8123/default" --organization_id 4ec70ec7-0808-416a-b836-7100aeec934b
```

If you visit the Langsmith UI, you should now see organization is no longer present.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/scripts/delete_an_organization%3E).
[PreviousDelete a Workspace](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)[NextDelete a Single Trace](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
  * [Prerequisites](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization#prerequisites)
  * [Running the deletion script for a single organization](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization#running-the-deletion-script-for-a-single-organization)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/scripts/delete_traces

[Skip to main content](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
      * [Delete a Workspace](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
      * [Delete an Organization](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
      * [Delete a Single Trace](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
      * [Generate Clickhouse Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
      * [Generate Query Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
      * [Running Postgres Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
      * [Running Clickhouse Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
  * Delete a Single Trace


On this page
# Deleting Traces
The LangSmith UI does not currently support the deletion of an individual trace. This, however, can be accomplished by directly removing the trace from all materialized views in ClickHouse (except the runs_history views) and the runs and feedback tables themselves.
This command can either be run using a trace ID as an argument or using a file that is a list of trace IDs.
### Prerequisites[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces#prerequisites "Direct link to Prerequisites")
Ensure you have the following tools/items ready.
  1. kubectl
     * <https://kubernetes.io/docs/tasks/tools/>
  2. Clickhouse database credentials
     * Host
     * Port
     * Username 
       * If using the bundled version, this is `default`
     * Password 
       * If using the bundled version, this is `password`
     * Database name 
       * If using the bundled version, this is `default`
  3. Connectivity to the Clickhouse database from the machine you will be running the `delete_trace_by_id` script on.
     * If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
     * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.
  4. The script to delete a trace
     * You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/delete_trace_by_id.sh)


### Running the deletion script for a single trace[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces#running-the-deletion-script-for-a-single-trace "Direct link to Running the deletion script for a single trace")
Run the following command to run the trace deletion script using a single trace ID:
```
sh delete_trace_by_id.sh <clickhouse_url> --trace_id <trace_id>
```

For example, if you are using the bundled version with port-forwarding, the command would look like:
```
sh delete_trace_by_id.sh "clickhouse://default:password@localhost:8123/default" --trace_id 4ec70ec7-0808-416a-b836-7100aeec934b
```

If you visit the Langsmith UI, you should now see specified trace ID is no longer present nor reflected in stats.
### Running the deletion script for a multiple traces from a file with one trace ID per line[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces#running-the-deletion-script-for-a-multiple-traces-from-a-file-with-one-trace-id-per-line "Direct link to Running the deletion script for a multiple traces from a file with one trace ID per line")
Run the following command to run the trace deletion script using a list of trace IDs:
```
sh delete_trace_by_id.sh <clickhouse_url> --file <path/to/foo.txt>
```

For example, if you are using the bundled version with port-forwarding, the command would look like:
```
sh delete_trace_by_id.sh "clickhouse://default:password@localhost:8123/default" --file path/to/traces.txt
```

If you visit the Langsmith UI, you should now see all the specified traces have been removed.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/scripts/delete_traces%3E).
[PreviousDelete an Organization](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)[NextGenerate Clickhouse Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
  * [Prerequisites](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces#prerequisites)
  * [Running the deletion script for a single trace](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces#running-the-deletion-script-for-a-single-trace)
  * [Running the deletion script for a multiple traces from a file with one trace ID per line](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces#running-the-deletion-script-for-a-multiple-traces-from-a-file-with-one-trace-id-per-line)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats

[Skip to main content](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
      * [Delete a Workspace](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
      * [Delete an Organization](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
      * [Delete a Single Trace](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
      * [Generate Clickhouse Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
      * [Generate Query Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
      * [Running Postgres Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
      * [Running Clickhouse Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
  * Generate Clickhouse Stats


On this page
# Generating Clickhouse Stats
As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate Clickhouse statistics that will help us understand memory and CPU consumption and connection concurrency.
This command will generate a CSV that can be shared with the LangChain team.
### Prerequisites[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats#prerequisites "Direct link to Prerequisites")
Ensure you have the following tools/items ready.
  1. kubectl
     * <https://kubernetes.io/docs/tasks/tools/>
  2. Clickhouse database credentials
     * Host
     * Port
     * Username 
       * If using the bundled version, this is `default`
     * Password 
       * If using the bundled version, this is `password`
     * Database name 
       * If using the bundled version, this is `default`
  3. Connectivity to the Clickhouse database from the machine you will be running the `get_clickhouse_stats` script on.
     * If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
     * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.
  4. The script to generate ClickHouse stats
     * You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/get_clickhouse_stats.sh)


### Running the clickhouse stats generation script[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats#running-the-clickhouse-stats-generation-script "Direct link to Running the clickhouse stats generation script")
Run the following command to run the stats generation script:
```
sh get_clickhouse_stats.sh <clickhouse_url> --output path/to/file.csv
```

For example, if you are using the bundled version with port-forwarding, the command would look like:
```
sh get_clickhouse_stats.sh "clickhouse://default:password@localhost:8123/default" --output clickhouse_stats.csv
```

and after running this command you should see a file, clickhouse_stats.csv, has been created with Clickhouse statistics.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/scripts/generate_clickhouse_stats%3E).
[PreviousDelete a Single Trace](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)[NextGenerate Query Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
  * [Prerequisites](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats#prerequisites)
  * [Running the clickhouse stats generation script](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats#running-the-clickhouse-stats-generation-script)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats

[Skip to main content](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
      * [Delete a Workspace](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
      * [Delete an Organization](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
      * [Delete a Single Trace](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
      * [Generate Clickhouse Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
      * [Generate Query Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
      * [Running Postgres Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
      * [Running Clickhouse Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
  * Generate Query Stats


On this page
# Generating Query Stats
As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate LangSmith query statistics that will help us understand the performance of various queries that drive the LangSmith product experience.
This command will generate a CSV that can be shared with the LangChain team.
### Prerequisites[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats#prerequisites "Direct link to Prerequisites")
Ensure you have the following tools/items ready.
  1. kubectl
     * <https://kubernetes.io/docs/tasks/tools/>
  2. Clickhouse database credentials
     * Host
     * Port
     * Username 
       * If using the bundled version, this is `default`
     * Password 
       * If using the bundled version, this is `password`
     * Database name 
       * If using the bundled version, this is `default`
  3. Connectivity to the Clickhouse database from the machine you will be running the `get_query_stats` script on.
     * If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
     * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.
  4. The script to generate query stats
     * You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/get_query_stats.sh)


### Running the query stats generation script[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats#running-the-query-stats-generation-script "Direct link to Running the query stats generation script")
Run the following command to run the stats generation script:
```
sh get_query_stats.sh <clickhouse_url> --output path/to/file.csv
```

For example, if you are using the bundled version with port-forwarding, the command would look like:
```
sh get_query_stats.sh "clickhouse://default:password@localhost:8123/default" --output query_stats.csv
```

and after running this command you should see a file, query_stats.csv, has been created with LangSmith query statistics.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/scripts/generate_query_stats%3E).
[PreviousGenerate Clickhouse Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)[NextRunning Postgres Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
  * [Prerequisites](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats#prerequisites)
  * [Running the query stats generation script](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats#running-the-query-stats-generation-script)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries

[Skip to main content](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
      * [Delete a Workspace](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
      * [Delete an Organization](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
      * [Delete a Single Trace](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
      * [Generate Clickhouse Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
      * [Generate Query Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
      * [Running Postgres Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
      * [Running Clickhouse Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
  * Running Clickhouse Queries


On this page
# Running Support Queries against Clickhouse
This Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining query exception logs from Clickhouse).
This command takes a clickhouse connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the `ch_get_query_exceptions.sql` input file in the `support_queries/clickhouse` directory.
### Prerequisites[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries#prerequisites "Direct link to Prerequisites")
Ensure you have the following tools/items ready.
  1. kubectl
     * <https://kubernetes.io/docs/tasks/tools/>
  2. Clickhouse database credentials
     * Host
     * Port
     * Username 
       * If using the bundled version, this is `default`
     * Password 
       * If using the bundled version, this is `password`
     * Database name 
       * If using the bundled version, this is `default`
  3. Connectivity to the Clickhouse database from the machine you will be running the migration script on.
     * If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
     * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.
  4. The script to run a support query
     * You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/run_support_query_ch.sh)


### Running the query script[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries#running-the-query-script "Direct link to Running the query script")
Run the following command to run the desired query:
```
sh run_support_query_ch.sh <clickhouse_url> --input path/to/query.sql
```

For example, if you are using the bundled version with port-forwarding, the command might look like:
```
sh run_support_query_ch.sh "clickhouse://default:password@localhost:8123/default" --input support_queries/clickhouse/ch_get_query_exceptions.sql
```

which will output query logs for all queries that have thrown exceptions in Clickhouse in the last 7 days. To extract this to a file add the flag `--output path/to/file.csv`
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/scripts/running_ch_support_queries%3E).
[PreviousRunning Postgres Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)[NextInstallation](https://docs.smith.langchain.com/self_hosting/installation)
  * [Prerequisites](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries#prerequisites)
  * [Running the query script](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries#running-the-query-script)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries

[Skip to main content](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
      * [Delete a Workspace](https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace)
      * [Delete an Organization](https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization)
      * [Delete a Single Trace](https://docs.smith.langchain.com/self_hosting/scripts/delete_traces)
      * [Generate Clickhouse Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats)
      * [Generate Query Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)
      * [Running Postgres Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
      * [Running Clickhouse Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
  * Running Postgres Queries


On this page
# Running Support Queries against Postgres
This Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining trace counts for multiple organizations in a single query).
This command takes a postgres connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the `pg_get_trace_counts_daily.sql` input file in the `support_queries/postgres` directory.
### Prerequisites[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries#prerequisites "Direct link to Prerequisites")
Ensure you have the following tools/items ready.
  1. kubectl
     * <https://kubernetes.io/docs/tasks/tools/>
  2. PostgreSQL client
     * <https://www.postgresql.org/download/>
  3. PostgreSQL database connection:
     * Host
     * Port
     * Username 
       * If using the bundled version, this is `postgres`
     * Password 
       * If using the bundled version, this is `postgres`
     * Database name 
       * If using the bundled version, this is `postgres`
  4. Connectivity to the PostgreSQL database from the machine you will be running the migration script on.
     * If you are using the bundled version, you may need to port forward the postgresql service to your local machine.
     * Run `kubectl port-forward svc/langsmith-postgres 5432:5432` to port forward the postgresql service to your local machine.
  5. The script to run a support query
     * You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/run_support_query_pg.sh)


### Running the query script[‚Äã](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries#running-the-query-script "Direct link to Running the query script")
Run the following command to run the desired query:
```
sh run_support_query_pg.sh <postgres_url> --input path/to/query.sql
```

For example, if you are using the bundled version with port-forwarding, the command might look like:
```
sh run_support_query_pg.sh "postgres://postgres:postgres@localhost:5432/postgres" --input support_queries/pg_get_trace_counts_daily.sql
```

which will output the count of daily traces by workspace ID and organization ID. To extract this to a file add the flag `--output path/to/file.csv`
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/scripts/running_pg_support_queries%3E).
[PreviousGenerate Query Stats](https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats)[NextRunning Clickhouse Queries](https://docs.smith.langchain.com/self_hosting/scripts/running_ch_support_queries)
  * [Prerequisites](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries#prerequisites)
  * [Running the query script](https://docs.smith.langchain.com/self_hosting/scripts/running_pg_support_queries#running-the-query-script)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/troubleshooting

[Skip to main content](https://docs.smith.langchain.com/self_hosting/troubleshooting#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/troubleshooting)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/troubleshooting)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/troubleshooting)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/troubleshooting)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * Troubleshooting


On this page
# Troubleshooting
This guide will walk you through common issues you may encounter when running a self-hosted instance of LangSmith.
While running LangSmith, you may encounter unexpected 500 errors, slow performance, or other issues. This guide will help you diagnose and resolve these issues.
The first step in troubleshooting is to check the logs of the various services that make up LangSmith.
If running on Kubernetes, you can check the logs of your deployment by running the following command (replace `langsmith` with the name of your deployment if you deployed with a different name):
```
kubectl logs -l app.kubernetes.io/instance=langsmith --prefix -n <namespace> >> logs.txt
```

If running on Docker, you can check the logs your deployment by running the following command:
```
docker compose logs >> logs.txt
```

Generally, the main services you will want to analyze are:
  * `langsmith-backend`: The main backend service.
  * `langsmith-queue`: The queue service.


## Common issues[‚Äã](https://docs.smith.langchain.com/self_hosting/troubleshooting#common-issues "Direct link to Common issues")
### _DB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT_ENOUGH_SPACE)_[‚Äã](https://docs.smith.langchain.com/self_hosting/troubleshooting#dbexception-cannot-reserve-100-mib-not-enough-space-while-executing-waitforasyncinsert-not_enough_space "Direct link to dbexception-cannot-reserve-100-mib-not-enough-space-while-executing-waitforasyncinsert-not_enough_space")
This error occurs when ClickHouse runs out of disk space. You will need to increase the disk space available to ClickHouse.
#### Kubernetes[‚Äã](https://docs.smith.langchain.com/self_hosting/troubleshooting#kubernetes "Direct link to Kubernetes")
In Kubernetes, you will need to increase the size of the ClickHouse PVC. To achieve this, you can perform the following steps:
  1. Get the storage class of the PVC: `kubectl get pvc data-langsmith-clickhouse-0 -n <namespace> -o jsonpath='{.spec.storageClassName}'`
  2. Ensure the storage class has AllowVolumeExpansion: true: `kubectl get sc <storage-class-name> -o jsonpath='{.allowVolumeExpansion}'`
     * If it is false, some storage classes can be updated to allow volume expansion.
     * To update the storage class, you can run `kubectl patch sc <storage-class-name> -p '{"allowVolumeExpansion": true}'`
     * If this fails, you may need to create a new storage class with the correct settings.
  3. Edit your pvc to have the new size: `kubectl edit pvc data-langsmith-clickhouse-0 -n <namespace>` or `kubectl patch pvc data-langsmith-clickhouse-0 '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}' -n <namespace>`
  4. Update your helm chart `langsmith_config.yaml` to new size(e.g `100 Gi`)
  5. Delete the clickhouse statefulset `kubectl delete statefulset langsmith-clickhouse --cascade=orphan -n <namespace>`
  6. Apply helm chart with updated size (You can follow the upgrade guide [here](https://docs.smith.langchain.com/self_hosting/upgrades))
  7. Your pvc should now have the new size. Verify by running `kubectl get pvc` and `kubectl exec langsmith-clickhouse-0 -- bash -c "df"`


#### Docker[‚Äã](https://docs.smith.langchain.com/self_hosting/troubleshooting#docker "Direct link to Docker")
In Docker, you will need to increase the size of the ClickHouse volume. To achieve this, you can perform the following steps:
  1. Stop your instance of LangSmith. `docker compose down`
  2. If using bind mount, you will need to increase the size of the mount point.
  3. If using a docker `volume`, you will need to allocate more space to the volume/docker.


### _error: Dirty database version 'version'. Fix and force version_[‚Äã](https://docs.smith.langchain.com/self_hosting/troubleshooting#error-dirty-database-version-version-fix-and-force-version "Direct link to error-dirty-database-version-version-fix-and-force-version")
This error occurs when the ClickHouse database is in an inconsistent state with our migrations. You will need to reset to an earlier database version and then rerun your upgrade/migrations.
#### Kubernetes[‚Äã](https://docs.smith.langchain.com/self_hosting/troubleshooting#kubernetes-1 "Direct link to Kubernetes")
  1. Force migration to an earlier version, where version = dirty version - 1.


```
kubectl exec -it deployments/langsmith-backend -- bash -c 'migrate -source "file://clickhouse/migrations" -database "clickhouse://$CLICKHOUSE_HOST:$CLICKHOUSE_NATIVE_PORT?username=$CLICKHOUSE_USER&password=$CLICKHOUSE_PASSWORD&database=$CLICKHOUSE_DB&x-multi-statement=true&x-migrations-table-engine=MergeTree&secure=$CLICKHOUSE_TLS" force <version>'
```

  1. Rerun your upgrade/migrations.


#### Docker[‚Äã](https://docs.smith.langchain.com/self_hosting/troubleshooting#docker-1 "Direct link to Docker")
  1. Force migration to an earlier version, where version = dirty version - 1.


```
docker compose exec langchain-backend migrate -source "file://clickhouse/migrations" -database "clickhouse://$CLICKHOUSE_HOST:$CLICKHOUSE_NATIVE_PORT?username=$CLICKHOUSE_USER&password=$CLICKHOUSE_PASSWORD&database=$CLICKHOUSE_DB&x-multi-statement=true&x-migrations-table-engine=MergeTree&secure=$CLICKHOUSE_TLS" force <version>
```

  1. Rerun your upgrade/migrations.


### _413 - Request Entity Too Large_[‚Äã](https://docs.smith.langchain.com/self_hosting/troubleshooting#413---request-entity-too-large "Direct link to 413---request-entity-too-large")
This error occurs when the request size exceeds the maximum allowed size. You will need to increase the maximum request size in your Nginx configuration.
#### Kubernetes[‚Äã](https://docs.smith.langchain.com/self_hosting/troubleshooting#kubernetes-2 "Direct link to Kubernetes")
  1. Edit your `langsmith_config.yaml` and increase the `frontend.maxBodySize` [value](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/values.yaml#L519). This might look something like this:


```
frontend:maxBodySize:"100M"
```

  1. Apply your changes to the cluster.


### _Details: code: 497, message: default: Not enough privileges. To execute this query, it's necessary to have the grant CREATE ROW POLICY ON default.feedbacks_rmt_[‚Äã](https://docs.smith.langchain.com/self_hosting/troubleshooting#details-code-497-message-default-not-enough-privileges-to-execute-this-query-its-necessary-to-have-the-grant-create-row-policy-on-defaultfeedbacks_rmt "Direct link to details-code-497-message-default-not-enough-privileges-to-execute-this-query-its-necessary-to-have-the-grant-create-row-policy-on-defaultfeedbacks_rmt")
This error occurs when your user does not have the necessary permissions to create row policies in Clickhouse. When deploying the Docker deployment, you need to copy the `users.xml` file from the github repo as well. This adds the `<access_management>` tag to the `users.xml` file, which allows the user to create row policies. Below is the default `users.xml` file that we expect to be used.
```
<clickhouse>  <users>    <default>      <access_management>1</access_management>      <named_collection_control>1</named_collection_control>      <show_named_collections>1</show_named_collections>      <show_named_collections_secrets>1</show_named_collections_secrets>      <profile>default</profile>    </default>  </users>  <profiles>    <default>      <async_insert>1</async_insert>      <async_insert_max_data_size>2000000</async_insert_max_data_size>      <wait_for_async_insert>0</wait_for_async_insert>      <parallel_view_processing>1</parallel_view_processing>      <allow_simdjson>0</allow_simdjson>      <lightweight_deletes_sync>0</lightweight_deletes_sync>    </default>  </profiles></clickhouse>
```

In some environments, your mount point may not be writable by the container. In these cases we suggest building a custom image with the `users.xml` file included.
Example `Dockerfile`:
```
FROM clickhouse/clickhouse-server:24.8COPY ./users.xml /etc/clickhouse-server/users.d/users.xml
```

Then take the following steps:
  1. Build your custom image.


```
docker build -t <image-name> .
```

  1. Update your `docker-compose.yaml` to use the custom image. Make sure to remove the users.xml mount point.


```
langchain-clickhouse:image: <image-name>
```

  1. Restart your instance of LangSmith.


```
docker compose down --volumesdocker compose up
```

#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/troubleshooting%3E).
[PreviousLangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)[NextPricing](https://docs.smith.langchain.com/pricing)
  * [Common issues](https://docs.smith.langchain.com/self_hosting/troubleshooting#common-issues)
    * [ _DB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT_ENOUGH_SPACE)_](https://docs.smith.langchain.com/self_hosting/troubleshooting#dbexception-cannot-reserve-100-mib-not-enough-space-while-executing-waitforasyncinsert-not_enough_space)
    * [_error: Dirty database version 'version'. Fix and force version_](https://docs.smith.langchain.com/self_hosting/troubleshooting#error-dirty-database-version-version-fix-and-force-version)
    * [ _413 - Request Entity Too Large_](https://docs.smith.langchain.com/self_hosting/troubleshooting#413---request-entity-too-large)
    * [ _Details: code: 497, message: default: Not enough privileges. To execute this query, it's necessary to have the grant CREATE ROW POLICY ON default.feedbacks_rmt_](https://docs.smith.langchain.com/self_hosting/troubleshooting#details-code-497-message-default-not-enough-privileges-to-execute-this-query-its-necessary-to-have-the-grant-create-row-policy-on-defaultfeedbacks_rmt)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/upgrades

[Skip to main content](https://docs.smith.langchain.com/self_hosting/upgrades#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/upgrades)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/upgrades)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/upgrades)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/upgrades)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * Upgrades


On this page
# General Upgrade Instructions
For general upgrade instructions, please follow the instructions below. Certain versions may have specific upgrade instructions, which will be detailed in more specific upgrade guides.
## Kubernetes(Helm)[‚Äã](https://docs.smith.langchain.com/self_hosting/upgrades#kuberneteshelm "Direct link to Kubernetes\(Helm\)")
If you don't have the repo added, run the following command to add it:
```
helm repo add langchain https://langchain-ai.github.io/helm/
```

Update your local helm repo
```
helm repo update
```

Update your helm chart config file with any updates that are needed in the new version. These will be detailed in the release notes for the new version.
Run the following command to upgrade the chart(replace version with the version you want to upgrade to):
Namespace
If you are using a namespace other than the default namespace, you will need to specify the namespace in the `helm` and `kubectl` commands by using the `-n <namespace` flag.
```
helm upgrade <release-name> langchain/langsmith --version <version> --values <path-to-values-file>
```

Verify that the upgrade was successful:
```
helm status <release-name>
```

All pods should be in the `Running` state. Verify that clickhouse is running and that both `migrations` jobs have completed.
```
kubectl get podsNAME                   READY  STATUS   RESTARTS  AGElangsmith-backend-95b6d54f5-gz48b    1/1   Running   0     15hlangsmith-pg-migrations-d2z6k      0/1   Completed  0     5h48mlangsmith-ch-migrations-gasvk      0/1   Completed  0     5h48mlangsmith-clickhouse-0          1/1   Running   0     26hlangsmith-frontend-84687d9d45-6cg4r   1/1   Running   0     15hlangsmith-hub-backend-66ffb75fb4-qg6kl  1/1   Running   0     15hlangsmith-playground-85b444d8f7-pl589  1/1   Running   0     15hlangsmith-queue-d58cb64f7-87d68     1/1   Running   0     15h
```

### Validate your deployment:[‚Äã](https://docs.smith.langchain.com/self_hosting/upgrades#validate-your-deployment "Direct link to Validate your deployment:")
  1. Run `kubectl get services`
Output should look something like:


```
NAME             TYPE      CLUSTER-IP    EXTERNAL-IP   PORT(S)           AGEkubernetes          ClusterIP   172.20.0.1    <none>     443/TCP           27dlangsmith-backend      ClusterIP   172.20.22.34   <none>     1984/TCP           21dlangsmith-clickhouse     ClusterIP   172.20.117.62  <none>     8123/TCP,9000/TCP      21dlangsmith-frontend      LoadBalancer  172.20.218.30  <external ip>  80:30093/TCP,443:31130/TCP  21dlangsmith-platform-backend  ClusterIP   172.20.232.183  <none>     1986/TCP           21dlangsmith-playground     ClusterIP   172.20.167.132  <none>     3001/TCP           21dlangsmith-postgres      ClusterIP   172.20.59.63   <none>     5432/TCP           21dlangsmith-redis       ClusterIP   172.20.229.98  <none>     6379/TCP           20d
```

  1. Curl the external ip of the `langsmith-frontend` service:
```
curl <external ip>/api/info{"version":"0.5.7","license_expiration_time":"2033-05-20T20:08:06","batch_ingest_config":{"scale_up_qsize_trigger":1000,"scale_up_nthreads_limit":16,"scale_down_nempty_trigger":4,"size_limit":100,"size_limit_bytes":20971520}}
```



Check that the version matches the version you upgraded to.
  1. Visit the external ip for the `langsmith-frontend` service on your browser
The Langsmith UI should be visible/operational
![.langsmith_ui.png](https://docs.smith.langchain.com/assets/images/langsmith_ui-a308960b13a121598b5c577e7587adfe.png)


## Docker[‚Äã](https://docs.smith.langchain.com/self_hosting/upgrades#docker "Direct link to Docker")
Upgrading the Docker version of LangSmith is a bit more involved than the Helm version and may require a small amount of downtime. Please follow the instructions below to upgrade your Docker version of LangSmith.
  1. Update your `docker-compose.yml` file to the file used in the latest release. You can find this in the [LangSmith SDK GitHub repository](https://github.com/langchain-ai/langsmith-sdk/blob/main/python/langsmith/cli/docker-compose.yaml)
  2. Update your `.env` file with any new environment variables that are required in the new version. These will be detailed in the release notes for the new version.
  3. Run the following command to stop your current LangSmith instance:


```
docker-compose down
```

  1. Run the following command to start your new LangSmith instance in the background:


```
docker-compose up -d
```

If everything ran successfully, you should see all the LangSmith containers running and healthy.
```
CONTAINER ID  IMAGE                 COMMAND         CREATED    STATUS            PORTS                           NAMESe1c8f01a4ffc  langchain/langsmith-frontend:0.5.7   "/entrypoint.sh ngin‚Ä¶"  10 hours ago  Up 40 seconds         0.0.0.0:80->80/tcp, 8080/tcp                cli-langchain-frontend-139e1394846b9  langchain/langsmith-backend:0.5.7   "/bin/sh -c 'exec uv‚Ä¶"  10 hours ago  Up 40 seconds         0.0.0.0:1984->1984/tcp                   cli-langchain-backend-1f8688dd58f2f  langchain/langsmith-go-backend:0.5.7  "./smith-go"       10 hours ago  Up 40 seconds         0.0.0.0:1986->1986/tcp                   cli-langchain-platform-backend-1006f1303b04d  langchain/langsmith-backend:0.5.7   "saq app.workers.que‚Ä¶"  10 hours ago  Up 40 seconds                                      cli-langchain-queue-173a90242ed3a  redis:7                "docker-entrypoint.s‚Ä¶"  10 hours ago  Up About a minute (healthy)  0.0.0.0:63791->6379/tcp                  cli-langchain-redis-1eecf75ca672b  postgres:14.7             "docker-entrypoint.s‚Ä¶"  10 hours ago  Up About a minute (healthy)  0.0.0.0:5433->5432/tcp                   cli-langchain-db-13aa5652a864d  clickhouse/clickhouse-server:23.9   "/entrypoint.sh"     10 hours ago  Up About a minute (healthy)  9009/tcp, 0.0.0.0:8124->8123/tcp, 0.0.0.0:9001->9000/tcp  cli-langchain-clickhouse-184edc329a37f  langchain/langsmith-playground:0.5.7  "docker-entrypoint.s‚Ä¶"  10 hours ago  Up About a minute       0.0.0.0:3001->3001/tcp                   cli-langchain-playground-1
```

### Validate your deployment:[‚Äã](https://docs.smith.langchain.com/self_hosting/upgrades#validate-your-deployment-1 "Direct link to Validate your deployment:")
  1. Curl the exposed port of the `cli-langchain-frontend-1` container:
```
curl localhost:80/info{"version":"0.5.7","license_expiration_time":"2033-05-20T20:08:06","batch_ingest_config":{"scale_up_qsize_trigger":1000,"scale_up_nthreads_limit":16,"scale_down_nempty_trigger":4,"size_limit":100,"size_limit_bytes":20971520}}
```



1 . Visit the exposed port of the `cli-langchain-frontend-1` container on your browser
The Langsmith UI should be visible/operational
![.langsmith_ui.png](https://docs.smith.langchain.com/assets/images/langsmith_ui-a308960b13a121598b5c577e7587adfe.png)
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/upgrades%3E).
[PreviousUsage](https://docs.smith.langchain.com/self_hosting/usage)[NextEgress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
  * [Kubernetes(Helm)](https://docs.smith.langchain.com/self_hosting/upgrades#kuberneteshelm)
    * [Validate your deployment:](https://docs.smith.langchain.com/self_hosting/upgrades#validate-your-deployment)
  * [Docker](https://docs.smith.langchain.com/self_hosting/upgrades#docker)
    * [Validate your deployment:](https://docs.smith.langchain.com/self_hosting/upgrades#validate-your-deployment-1)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/self_hosting/usage

[Skip to main content](https://docs.smith.langchain.com/self_hosting/usage#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/self_hosting/usage)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
    * [Architectural overview](https://docs.smith.langchain.com/self_hosting/architectural_overview)
    * [Scripts](https://docs.smith.langchain.com/self_hosting/scripts)
    * [Installation](https://docs.smith.langchain.com/self_hosting/installation)
    * [Configuration](https://docs.smith.langchain.com/self_hosting/configuration)
    * [Usage](https://docs.smith.langchain.com/self_hosting/usage)
    * [Upgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
    * [Egress for Subscription Metrics and Operational Metadata](https://docs.smith.langchain.com/self_hosting/egress)
    * [Organization Charts](https://docs.smith.langchain.com/self_hosting/organization_charts)
    * [Release notes (self-hosted)](https://docs.smith.langchain.com/self_hosting/release_notes)
    * [Frequently asked questions](https://docs.smith.langchain.com/self_hosting/faq)
    * [LangSmith-managed ClickHouse](https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse)
    * [Troubleshooting](https://docs.smith.langchain.com/self_hosting/troubleshooting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/self_hosting/usage)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/self_hosting/usage)
    * [Evaluation](https://docs.smith.langchain.com/self_hosting/usage)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/self_hosting/usage)


  * [](https://docs.smith.langchain.com/)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * Usage


On this page
# Using your self-hosted instance of LangSmith
This guide will walk you through the process of using your self-hosted instance of LangSmith.
Self-Hosted LangSmith Instance Required
This guide assumes you have already deployed a self-hosted LangSmith instance. If you have not, please refer to the [kubernetes deployment guide](https://docs.smith.langchain.com/self_hosting/installation/kubernetes) or the [docker deployment guide](https://docs.smith.langchain.com/self_hosting/installation/docker).
### Configuring the application you want to use with LangSmith[‚Äã](https://docs.smith.langchain.com/self_hosting/usage#configuring-the-application-you-want-to-use-with-langsmith "Direct link to Configuring the application you want to use with LangSmith")
LangSmith has a single API for interacting with both the hub and the LangSmith backend.
  1. Once you have deployed your instance, you can access the LangSmith UI at `http://<host>`.
  2. The LangSmith API will be available at `http://<host>/api/v1`


To use the API of your instance, you will need to set the following environment variables in your application:
```
LANGSMITH_ENDPOINT=http://<host>/apiLANGSMITH_API_KEY=foo # Set to a legitimate API key if using OAuth
```

You can also configure these variables directly in the LangSmith SDK client:
```
import langsmithlangsmith_client = langsmith.Client(  api_key='<api_key>',  api_url='http://<host>/api/v1',)
```

After setting the above, you should be able to run your code and see the results in your self-hosted instance. We recommend running through the [_quickstart guide_](https://docs.smith.langchain.com/#quick-start) to get a feel for how to use LangSmith.
### API Reference[‚Äã](https://docs.smith.langchain.com/self_hosting/usage#api-reference "Direct link to API Reference")
To access the API reference, navigate to `http://<host>/api/docs` in your browser.
#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/self_hosting/usage%3E).
[PreviousUser management](https://docs.smith.langchain.com/self_hosting/configuration/user_management)[NextUpgrades](https://docs.smith.langchain.com/self_hosting/upgrades)
  * [Configuring the application you want to use with LangSmith](https://docs.smith.langchain.com/self_hosting/usage#configuring-the-application-you-want-to-use-with-langsmith)
  * [API Reference](https://docs.smith.langchain.com/self_hosting/usage#api-reference)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

## https://docs.smith.langchain.com/

[Skip to main content](https://docs.smith.langchain.com/#__docusaurus_skipToContent_fallback)
Join us at [ Interrupt: The Agent AI Conference by LangChain](https://interrupt.langchain.com/) on May 13 & 14 in San Francisco!
[![ü¶úÔ∏èüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/img/langsmith-logo-black.svg)](https://docs.smith.langchain.com/)
[API Reference](https://docs.smith.langchain.com/)
  * [REST](https://api.smith.langchain.com/redoc)
  * [Python](https://docs.smith.langchain.com/reference/python)
  * [JS/TS](https://docs.smith.langchain.com/reference/js)


Search`K`
Region
  * US
  * EU


[Go to App](https://smith.langchain.com/)
  * [Get Started](https://docs.smith.langchain.com/)
  * [Observability](https://docs.smith.langchain.com/observability)
  * [Evaluation](https://docs.smith.langchain.com/evaluation)
  * [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
  * [Deployment (LangGraph Platform)](https://docs.smith.langchain.com/langgraph_cloud)
  * [Administration](https://docs.smith.langchain.com/administration/tutorials)
  * [Self-hosting](https://docs.smith.langchain.com/self_hosting)
  * [Pricing](https://docs.smith.langchain.com/pricing)
  * [Reference](https://docs.smith.langchain.com/reference)
    * [Cloud architecture and scalability](https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability)
    * [Authz and Authn](https://docs.smith.langchain.com/)
      * [Authentication methods](https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods)
    * [data_formats](https://docs.smith.langchain.com/)
    * [Evaluation](https://docs.smith.langchain.com/)
      * [Dataset transformations](https://docs.smith.langchain.com/reference/evaluation/dataset_transformations)
    * [Regions FAQ](https://docs.smith.langchain.com/reference/regions_faq)
    * [sdk_reference](https://docs.smith.langchain.com/)


  * [](https://docs.smith.langchain.com/)
  * Get Started


On this page
# Get started with LangSmith
**LangSmith** is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.
![](https://docs.smith.langchain.com/assets/images/ls-diagram-5be7dd68b135f573a7b0e163692e6800.png)
### [Observability](https://docs.smith.langchain.com/observability)
Analyze traces in LangSmith and configure metrics, dashboards, alerts based on these.
### [Evals](https://docs.smith.langchain.com/evaluation)
Evaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.
### [Prompt Engineering](https://docs.smith.langchain.com/prompt_engineering/quickstarts/quickstart_ui)
Iterate on prompts, with automatic version control and collaboration features.
LangSmith + LangChain OSS
LangSmith is framework-agnostic ‚Äî it can be used with or without LangChain's open source frameworks [`langchain`](https://python.langchain.com) and [`langgraph`](https://langchain-ai.github.io/langgraph/).
If you are using either of these, you can enable LangSmith tracing with a single environment variable. For more see the how-to guide for [setting up LangSmith with LangChain](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain) or [setting up LangSmith with LangGraph](https://docs.smith.langchain.com/observability/how_to_guides/tracing/trace_with_langgraph).
## Observability[‚Äã](https://docs.smith.langchain.com/#observability "Direct link to Observability")
Observability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.
This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.
  * Get started by [adding tracing](https://docs.smith.langchain.com/observability) to your application.
  * [Create dashboards](https://docs.smith.langchain.com/observability/how_to_guides/dashboards) to view key metrics like RPS, error rates and costs.


## Evals[‚Äã](https://docs.smith.langchain.com/#evals "Direct link to Evals")
The quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.
  * Get started by [creating your first evaluation](https://docs.smith.langchain.com/evaluation).
  * Quickly assess the performance of your application using our [off-the-shelf evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators) as a starting point.
  * [Analyze results](https://docs.smith.langchain.com/evaluation/how_to_guides#analyzing-experiment-results) of evaluations in the LangSmith UI and [compare results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results) over time.
  * Easily collect [human feedback](https://docs.smith.langchain.com/evaluation/how_to_guides#annotation-queues-and-human-feedback) on your data to improve your application.


## Prompt Engineering[‚Äã](https://docs.smith.langchain.com/#prompt-engineering "Direct link to Prompt Engineering")
While traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.
  * Get started by [creating your first prompt](https://docs.smith.langchain.com/prompt_engineering/tutorials/optimize_classifier).
  * Iterate on models and prompts using the [Playground](https://docs.smith.langchain.com/prompt_engineering/how_to_guides#playground).
  * [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompts/manage_prompts_programatically) in your application.


#### Was this page helpful?
#### You can leave detailed feedback [on GitHub](https://github.com/langchain-ai/langsmith-docs/issues/new?title=DOC%3A+%3CIssue+related+to+/%3E).
[NextQuick Start](https://docs.smith.langchain.com/observability)
  * [Observability](https://docs.smith.langchain.com/#observability)
  * [Evals](https://docs.smith.langchain.com/#evals)
  * [Prompt Engineering](https://docs.smith.langchain.com/#prompt-engineering)


Community
  * [Discord](https://discord.gg/cU2adEyC7w)
  * [Twitter](https://twitter.com/LangChainAI)


GitHub
  * [Docs Code](https://github.com/langchain-ai/langsmith-docs)
  * [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
  * [Python](https://github.com/langchain-ai/langchain)
  * [JS/TS](https://github.com/langchain-ai/langchainjs)


More
  * [Homepage](https://langchain.com)
  * [Blog](https://blog.langchain.dev)
  * [LangChain Python Docs](https://python.langchain.com/en/latest/)
  * [LangChain JS/TS Docs](https://js.langchain.com/docs/)


Copyright ¬© 2025 LangChain, Inc.


---

